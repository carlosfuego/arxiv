[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.03174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v2",
                "updated": "2024-11-15T07:25:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    25,
                    54,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v1",
                "updated": "2024-11-15T00:37:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "RuQing G. Xu"
                    },
                    {
                        "name": "Devin A. Matthews"
                    }
                ],
                "author_detail": {
                    "name": "Devin A. Matthews"
                },
                "author": "Devin A. Matthews",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09812v1",
                "updated": "2024-11-14T21:01:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T21:01:29Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "title": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments"
                },
                "summary": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v1",
                "updated": "2024-11-14T18:54:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v2",
                "updated": "2024-11-14T17:46:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    46,
                    4,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09546v1",
                "updated": "2024-11-14T16:01:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:01:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)"
                },
                "summary": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB."
                },
                "authors": [
                    {
                        "name": "Dhandeep Challagundla"
                    },
                    {
                        "name": "Ignatius Bezzam"
                    },
                    {
                        "name": "Riadul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Riadul Islam"
                },
                "author": "Riadul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v2",
                "updated": "2024-11-14T15:40:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    40,
                    59,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09473v1",
                "updated": "2024-11-14T14:28:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:28:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing"
                },
                "summary": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples."
                },
                "authors": [
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v1",
                "updated": "2024-11-14T13:22:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09317v1",
                "updated": "2024-11-14T09:50:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:50:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "Pie: Pooling CPU Memory for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: Pooling CPU Memory for LLM Inference"
                },
                "summary": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v1",
                "updated": "2024-11-14T08:25:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Pkd-tree: Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pkd-tree: Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v3",
                "updated": "2024-11-14T01:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    56,
                    11,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v4",
                "updated": "2024-11-13T16:33:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    16,
                    33,
                    33,
                    2,
                    318,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The personalized recommendation system's continuous size growth poses new\nchallenges for model inference. Although weight-sharing algorithms have been\nproposed to reduce embedding table capacity, they increase memory access.\nRecent advancements in processing-in-memory (PIM) successfully enhance the\nrecommendation system's throughput by exploiting memory parallelism, but our\nanalysis shows that those algorithms introduce CPU-PIM communication overhead\ninto prior PIM systems, compromising the PIM throughput. We propose\nProactivePIM, a specialized memory architecture integrated with PIM technology\ntailored to accelerate the weight-sharing algorithms. ProacitvePIM integrates\nan SRAM cache within the PIM with an efficient prefetching scheme to leverage a\nunique locality of the algorithm and eliminate CPU-PIM communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The personalized recommendation system's continuous size growth poses new\nchallenges for model inference. Although weight-sharing algorithms have been\nproposed to reduce embedding table capacity, they increase memory access.\nRecent advancements in processing-in-memory (PIM) successfully enhance the\nrecommendation system's throughput by exploiting memory parallelism, but our\nanalysis shows that those algorithms introduce CPU-PIM communication overhead\ninto prior PIM systems, compromising the PIM throughput. We propose\nProactivePIM, a specialized memory architecture integrated with PIM technology\ntailored to accelerate the weight-sharing algorithms. ProacitvePIM integrates\nan SRAM cache within the PIM with an efficient prefetching scheme to leverage a\nunique locality of the algorithm and eliminate CPU-PIM communication."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08672v1",
                "updated": "2024-11-13T15:07:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T15:07:15Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "title": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks"
                },
                "summary": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Zhibin Gao"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "conference paper with 6 pages and 5 figures. arXiv admin note: text\n  overlap with arXiv:2411.01458",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08312v1",
                "updated": "2024-11-13T03:28:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T03:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "title": "A Novel Extensible Simulation Framework for CXL-Enabled Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Extensible Simulation Framework for CXL-Enabled Systems"
                },
                "summary": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3."
                },
                "authors": [
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Bo Mao"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Mingzhe Zhang"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "arxiv_affiliation": "Peking University",
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08203v1",
                "updated": "2024-11-12T21:50:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T21:50:03Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "title": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing"
                },
                "summary": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads."
                },
                "authors": [
                    {
                        "name": "Jacopo Tagliabue"
                    },
                    {
                        "name": "Ryan Curtin"
                    },
                    {
                        "name": "Ciro Greco"
                    }
                ],
                "author_detail": {
                    "name": "Ciro Greco"
                },
                "author": "Ciro Greco",
                "arxiv_comment": "Pre-print of the paper accepted at DEMAI@IEEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06219v3",
                "updated": "2024-11-12T08:18:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    18,
                    45,
                    1,
                    317,
                    0
                ],
                "published": "2024-05-10T03:06:24Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    3,
                    6,
                    24,
                    4,
                    131,
                    0
                ],
                "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding."
                },
                "authors": [
                    {
                        "name": "Haojie Duanmu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Jiangfei Duan"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v1",
                "updated": "2024-11-12T08:17:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06681v1",
                "updated": "2024-11-11T02:48:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:48:00Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "title": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance."
                },
                "authors": [
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Liang Qian"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Wenjun Zhang"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06680v1",
                "updated": "2024-11-11T02:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:47:05Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "title": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Harald C. Gall"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N19",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06659v1",
                "updated": "2024-11-11T01:53:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T01:53:14Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning"
                },
                "summary": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git ."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Aijia Zhang"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Biqing Qi"
                    }
                ],
                "author_detail": {
                    "name": "Biqing Qi"
                },
                "author": "Biqing Qi",
                "arxiv_comment": "16 pages, 6 figures, 38th Conference on Neural Information Processing\n  Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v2",
                "updated": "2024-11-10T23:04:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    23,
                    4,
                    12,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v4",
                "updated": "2024-11-10T15:58:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    15,
                    58,
                    7,
                    6,
                    315,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04873v2",
                "updated": "2024-11-10T10:08:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    10,
                    8,
                    37,
                    6,
                    315,
                    0
                ],
                "published": "2024-06-07T12:12:25Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    12,
                    12,
                    25,
                    4,
                    159,
                    0
                ],
                "title": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior"
                },
                "summary": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main"
                },
                "authors": [
                    {
                        "name": "Tanvir Mahmud"
                    },
                    {
                        "name": "Mustafa Munir"
                    },
                    {
                        "name": "Radu Marculescu"
                    },
                    {
                        "name": "Diana Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Marculescu"
                },
                "author": "Diana Marculescu",
                "arxiv_comment": "Accepted in WACV 2025. Project page:\n  https://tanvir-utexas.github.io/AdaVE_Demo/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v1",
                "updated": "2024-11-10T08:31:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v1",
                "updated": "2024-11-10T05:12:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v2",
                "updated": "2024-11-08T16:29:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05555v1",
                "updated": "2024-11-08T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality"
                },
                "summary": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively."
                },
                "authors": [
                    {
                        "name": "Ilias Bournias"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    },
                    {
                        "name": "Georgios Zacharopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Zacharopoulos"
                },
                "author": "Georgios Zacharopoulos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v1",
                "updated": "2024-11-08T02:21:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02542v2",
                "updated": "2024-11-07T18:58:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    58,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-04T17:58:03Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    58,
                    3,
                    1,
                    156,
                    0
                ],
                "title": "Loki: Low-rank Keys for Efficient Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loki: Low-rank Keys for Efficient Sparse Attention"
                },
                "summary": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods."
                },
                "authors": [
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "arxiv_comment": "Proceedings of the Thirty-Eighth Annual Conference on Neural\n  Information Processing Systems (Main Conference Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v2",
                "updated": "2024-11-07T17:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    6,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v1",
                "updated": "2024-11-07T14:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16591v2",
                "updated": "2024-11-07T09:33:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    33,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-26T14:50:40Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    14,
                    50,
                    40,
                    6,
                    147,
                    0
                ],
                "title": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification"
                },
                "summary": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter."
                },
                "authors": [
                    {
                        "name": "Qijie Wang"
                    },
                    {
                        "name": "Guandu Liu"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_doi": "10.1145/3664647.3681566",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681566",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Multimedia 2024 Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v2",
                "updated": "2024-11-07T06:40:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    40,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v3",
                "updated": "2024-11-06T09:15:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    15,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jianqiang Ma"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03731v1",
                "updated": "2024-11-06T07:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness"
                },
                "summary": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations)."
                },
                "authors": [
                    {
                        "name": "Abdelmajid Essofi"
                    },
                    {
                        "name": "Ridwan Salahuddeen"
                    },
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Elnura Zhalieva"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02295v1",
                "updated": "2024-11-04T17:21:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:21:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating"
                },
                "summary": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world."
                },
                "authors": [
                    {
                        "name": "Di Ni"
                    },
                    {
                        "name": "Ved Gund"
                    },
                    {
                        "name": "Landon Ivy"
                    },
                    {
                        "name": "Amit Lal"
                    }
                ],
                "author_detail": {
                    "name": "Amit Lal"
                },
                "author": "Amit Lal",
                "arxiv_doi": "10.31438/trf.hh2022.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.31438/trf.hh2022.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and published at Hilton Head Workshop 2022: A Solid-State\n  Sensors, Actuators and Microsystems Workshop",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v2",
                "updated": "2024-11-04T12:14:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    7,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "arxiv_comment": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium, February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v2",
                "updated": "2024-11-04T09:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    40,
                    27,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01754v1",
                "updated": "2024-11-04T02:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun"
                },
                "summary": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "X. -H. Wang"
                    },
                    {
                        "name": "G. Shu"
                    },
                    {
                        "name": "H. Qian"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "Z. Jiang"
                    },
                    {
                        "name": "H. Meng"
                    },
                    {
                        "name": "C. Xing"
                    },
                    {
                        "name": "Q. Zhou"
                    },
                    {
                        "name": "H. Deng"
                    }
                ],
                "author_detail": {
                    "name": "H. Deng"
                },
                "author": "H. Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v2",
                "updated": "2024-11-04T02:08:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu"
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v4",
                "updated": "2024-11-03T09:42:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    42,
                    35,
                    6,
                    308,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01458v1",
                "updated": "2024-11-03T07:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T07:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services"
                },
                "summary": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Khaled Ben Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Ben Letaief"
                },
                "author": "Khaled Ben Letaief",
                "arxiv_comment": "14 pages, 8 figures, 39 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01269v1",
                "updated": "2024-11-02T14:40:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T14:40:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "Disaggregated Database Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Database Management Systems"
                },
                "summary": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Philip A. Bernstein"
                    },
                    {
                        "name": "Dhruba Borthakur"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Jai Menon"
                    },
                    {
                        "name": "Sumit Puri"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Puri"
                },
                "author": "Sumit Puri",
                "arxiv_comment": "This paper appeared in the {\\em Performance Evaluation and\n  Benchmarking} - 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW,\n  Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in\n  Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01246v1",
                "updated": "2024-11-02T13:52:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T13:52:49Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "title": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores"
                },
                "summary": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    },
                    {
                        "name": "Jason Yap"
                    }
                ],
                "author_detail": {
                    "name": "Jason Yap"
                },
                "author": "Jason Yap",
                "arxiv_doi": "10.1145/2663165.2663317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2663165.2663317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version of CAMP appeared in the Proceedings of the\n  ACM/IFIP/USENIX Middleware Conference, Bordeaux, France, December 2014. See\n  https://github.com/scdblab/CAMP for an implementation",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01142v1",
                "updated": "2024-11-02T05:15:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T05:15:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference"
                },
                "summary": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU."
                },
                "authors": [
                    {
                        "name": "Xuanlin Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Pierre-André Noël"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vázquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00131v1",
                "updated": "2024-10-31T18:31:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T18:31:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence"
                },
                "summary": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared."
                },
                "authors": [
                    {
                        "name": "John Whitington"
                    }
                ],
                "author_detail": {
                    "name": "John Whitington"
                },
                "author": "John Whitington",
                "arxiv_doi": "10.1145/2788539.27885",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2788539.27885",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v1",
                "updated": "2024-10-30T02:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "The code is coming soon! For sure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08043v1",
                "updated": "2024-10-30T02:18:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    18,
                    59,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:18:59Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    18,
                    59,
                    2,
                    304,
                    0
                ],
                "title": "Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current\n  Modelling Algorithm Based on Graph Theory for Space Weather Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current\n  Modelling Algorithm Based on Graph Theory for Space Weather Applications"
                },
                "summary": "Geomagnetically Induced Current (GIC) refers to the electromagnetic response\nof the Earth and its conductive modern infrastructures to space weather and\nwould pose a significant threat to high-voltage power grids designed for the\nalternative current operation. To assess the impact of space weather on the\npower grid, one needs to calculate the GIC on a national or continental scale.\nIn this study, we developed a smart and parallelized GIC modelling algorithm,\nGraph GIC. This algorithm deploys a graph representing a power grid in a\nsingle-line diagram, in which substations/transformers act as nodes and\ntransmission lines as edges. With these denotations, a power grid and its\nelectric parameters are mathematically represented with an adjacency matrix and\nan admittance matrix. We used sparse matrix and parallelisation techniques to\nexpedite the intensive computation in cases of large-scale power grids. The\nGraph GIC was validated with a benchmark grid, applied to the GIC calculation\nof the 500 kV power grid of Guangdong, China, and conducted preliminary\nanalysis on the grid's susceptibility to geomagnetic storms. The Graph GIC\nalgorithm has the advantage of an intuitive and highly scalable graph\nrepresentation of a power grid at any scale. It achieves high-accuracy\ncalculation and a speedup of about 18 times after parallelisation. This\nalgorithm could be applied to assess the impact of space weather on a power\ngrid up to continental scales and could be incorporated into global space\nweather modelling frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geomagnetically Induced Current (GIC) refers to the electromagnetic response\nof the Earth and its conductive modern infrastructures to space weather and\nwould pose a significant threat to high-voltage power grids designed for the\nalternative current operation. To assess the impact of space weather on the\npower grid, one needs to calculate the GIC on a national or continental scale.\nIn this study, we developed a smart and parallelized GIC modelling algorithm,\nGraph GIC. This algorithm deploys a graph representing a power grid in a\nsingle-line diagram, in which substations/transformers act as nodes and\ntransmission lines as edges. With these denotations, a power grid and its\nelectric parameters are mathematically represented with an adjacency matrix and\nan admittance matrix. We used sparse matrix and parallelisation techniques to\nexpedite the intensive computation in cases of large-scale power grids. The\nGraph GIC was validated with a benchmark grid, applied to the GIC calculation\nof the 500 kV power grid of Guangdong, China, and conducted preliminary\nanalysis on the grid's susceptibility to geomagnetic storms. The Graph GIC\nalgorithm has the advantage of an intuitive and highly scalable graph\nrepresentation of a power grid at any scale. It achieves high-accuracy\ncalculation and a speedup of about 18 times after parallelisation. This\nalgorithm could be applied to assess the impact of space weather on a power\ngrid up to continental scales and could be incorporated into global space\nweather modelling frameworks."
                },
                "authors": [
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Ding Yuan"
                    },
                    {
                        "name": "Xueshang Feng"
                    },
                    {
                        "name": "Stefaan Poedts"
                    },
                    {
                        "name": "Zhengyang Zou"
                    },
                    {
                        "name": "Song Feng"
                    },
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Tong Yin"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yin"
                },
                "author": "Tong Yin",
                "arxiv_comment": "19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.space-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23317v1",
                "updated": "2024-10-29T20:04:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T20:04:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
                },
                "authors": [
                    {
                        "name": "Dezhan Tu"
                    },
                    {
                        "name": "Danylo Vashchilenko"
                    },
                    {
                        "name": "Yuzhe Lu"
                    },
                    {
                        "name": "Panpan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Xu"
                },
                "author": "Panpan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v3",
                "updated": "2024-10-29T04:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    21,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/DiffewS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v3",
                "updated": "2024-10-29T02:52:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    2,
                    52,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v1",
                "updated": "2024-10-28T19:08:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21266v1",
                "updated": "2024-10-28T17:57:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Online Weighted Paging with Unknown Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Weighted Paging with Unknown Weights"
                },
                "summary": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling."
                },
                "authors": [
                    {
                        "name": "Orin Levy"
                    },
                    {
                        "name": "Noam Touitou"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    }
                ],
                "author_detail": {
                    "name": "Aviv Rosenberg"
                },
                "author": "Aviv Rosenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v2",
                "updated": "2024-10-28T16:42:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    42,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe)."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21073v1",
                "updated": "2024-10-28T14:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices"
                },
                "summary": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board."
                },
                "authors": [
                    {
                        "name": "Hiroki Matsutani"
                    },
                    {
                        "name": "Masaaki Kondo"
                    },
                    {
                        "name": "Kazuki Sunaga"
                    },
                    {
                        "name": "Radu Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Marculescu"
                },
                "author": "Radu Marculescu",
                "arxiv_comment": "ASP-DAC 2025 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v1",
                "updated": "2024-10-28T07:13:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01847v3",
                "updated": "2024-10-27T14:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    14,
                    40,
                    8,
                    6,
                    301,
                    0
                ],
                "published": "2024-04-02T11:12:42Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    11,
                    12,
                    42,
                    1,
                    93,
                    0
                ],
                "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Transformer Pre-training with 2:4 Sparsity"
                },
                "summary": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain."
                },
                "authors": [
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20337v1",
                "updated": "2024-10-27T04:31:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "published": "2024-10-27T04:31:35Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "title": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms"
                },
                "summary": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided."
                },
                "authors": [
                    {
                        "name": "Lorenzo De Stefani"
                    },
                    {
                        "name": "Vedant Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vedant Gupta"
                },
                "author": "Vedant Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04216v3",
                "updated": "2024-10-26T22:19:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    22,
                    19,
                    4,
                    5,
                    300,
                    0
                ],
                "published": "2024-02-06T18:17:02Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    18,
                    17,
                    2,
                    1,
                    37,
                    0
                ],
                "title": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks"
                },
                "summary": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines."
                },
                "authors": [
                    {
                        "name": "Md Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in IEEE Transactions on\n  Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20149v1",
                "updated": "2024-10-26T11:20:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "published": "2024-10-26T11:20:02Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "title": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models"
                },
                "summary": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}."
                },
                "authors": [
                    {
                        "name": "Yabin Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "NIPS 2024 Camera Ready, Codes are available at\n  \\url{https://github.com/YBZh/OpenOOD-VLM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20004v1",
                "updated": "2024-10-25T23:17:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T23:17:56Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "title": "Lightweight, Secure and Stateful Serverless Computing with PSL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight, Secure and Stateful Serverless Computing with PSL"
                },
                "summary": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks."
                },
                "authors": [
                    {
                        "name": "Alexander Thomas"
                    },
                    {
                        "name": "Shubham Mishra"
                    },
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "John Kubiatowicz"
                    }
                ],
                "author_detail": {
                    "name": "John Kubiatowicz"
                },
                "author": "John Kubiatowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05317v2",
                "updated": "2024-10-25T21:09:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    21,
                    9,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-08T01:35:11Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    1,
                    35,
                    11,
                    5,
                    160,
                    0
                ],
                "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoCoCo: Dropping In Convolutions for Long Context Compression"
                },
                "summary": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v2",
                "updated": "2024-10-25T19:45:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    45,
                    33,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19937v1",
                "updated": "2024-10-25T19:18:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T19:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction"
                },
                "summary": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Changjiang Li"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18248v2",
                "updated": "2024-10-25T19:18:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T19:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    53,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Fast Inference for Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Inference for Augmented Large Language Models"
                },
                "summary": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM."
                },
                "authors": [
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Cong Liang"
                    },
                    {
                        "name": "Shiji Xin"
                    },
                    {
                        "name": "Qianru Lao"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v5",
                "updated": "2024-10-25T18:29:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    18,
                    29,
                    43,
                    4,
                    299,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v1",
                "updated": "2024-10-25T07:24:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19123v1",
                "updated": "2024-10-24T19:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T19:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design"
                },
                "summary": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yeonju Ro"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18517v1",
                "updated": "2024-10-24T08:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:06:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing"
                },
                "summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "arxiv_comment": "Under Review by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18441v1",
                "updated": "2024-10-24T05:29:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T05:29:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI"
                },
                "summary": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings."
                },
                "authors": [
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08437v2",
                "updated": "2024-10-23T15:44:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    44,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2023-10-12T16:01:46Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    16,
                    1,
                    46,
                    3,
                    285,
                    0
                ],
                "title": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions"
                },
                "summary": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1145/3700875",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3700875",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024",
                "arxiv_journal_ref": "ACM Computing Surveys 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.10446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10446v1",
                "updated": "2024-11-15T18:59:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    59,
                    51,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T18:59:51Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    59,
                    51,
                    4,
                    320,
                    0
                ],
                "title": "VeriGraph: Scene Graphs for Execution Verifiable Robot Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriGraph: Scene Graphs for Execution Verifiable Robot Planning"
                },
                "summary": "Recent advancements in vision-language models (VLMs) offer potential for\nrobot task planning, but challenges remain due to VLMs' tendency to generate\nincorrect action sequences. To address these limitations, we propose VeriGraph,\na novel framework that integrates VLMs for robotic planning while verifying\naction feasibility. VeriGraph employs scene graphs as an intermediate\nrepresentation, capturing key objects and spatial relationships to improve plan\nverification and refinement. The system generates a scene graph from input\nimages and uses it to iteratively check and correct action sequences generated\nby an LLM-based task planner, ensuring constraints are respected and actions\nare executable. Our approach significantly enhances task completion rates\nacross diverse manipulation scenarios, outperforming baseline methods by 58%\nfor language-based tasks and 30% for image-based tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in vision-language models (VLMs) offer potential for\nrobot task planning, but challenges remain due to VLMs' tendency to generate\nincorrect action sequences. To address these limitations, we propose VeriGraph,\na novel framework that integrates VLMs for robotic planning while verifying\naction feasibility. VeriGraph employs scene graphs as an intermediate\nrepresentation, capturing key objects and spatial relationships to improve plan\nverification and refinement. The system generates a scene graph from input\nimages and uses it to iteratively check and correct action sequences generated\nby an LLM-based task planner, ensuring constraints are respected and actions\nare executable. Our approach significantly enhances task completion rates\nacross diverse manipulation scenarios, outperforming baseline methods by 58%\nfor language-based tasks and 30% for image-based tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Ekpo"
                    },
                    {
                        "name": "Mara Levy"
                    },
                    {
                        "name": "Saksham Suri"
                    },
                    {
                        "name": "Chuong Huynh"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Shrivastava"
                },
                "author": "Abhinav Shrivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10440v1",
                "updated": "2024-11-15T18:58:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    58,
                    31,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T18:58:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    58,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step"
                },
                "summary": "Large language models have demonstrated substantial advancements in reasoning\ncapabilities, particularly through inference-time scaling, as illustrated by\nmodels such as OpenAI's o1. However, current Vision-Language Models (VLMs)\noften struggle to perform systematic and structured reasoning, especially when\nhandling complex visual question-answering tasks. In this work, we introduce\nLLaVA-o1, a novel VLM designed to conduct autonomous multistage reasoning.\nUnlike chain-of-thought prompting, LLaVA-o1 independently engages in sequential\nstages of summarization, visual interpretation, logical reasoning, and\nconclusion generation. This structured approach enables LLaVA-o1 to achieve\nmarked improvements in precision on reasoning-intensive tasks. To accomplish\nthis, we compile the LLaVA-o1-100k dataset, integrating samples from various\nvisual question answering sources and providing structured reasoning\nannotations. Besides, we propose an inference-time stage-level beam search\nmethod, which enables effective inference-time scaling. Remarkably, with only\n100k training samples and a simple yet effective inference time scaling method,\nLLaVA-o1 not only outperforms its base model by 8.9% on a wide range of\nmultimodal reasoning benchmarks, but also surpasses the performance of larger\nand even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and\nLlama-3.2-90B-Vision-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated substantial advancements in reasoning\ncapabilities, particularly through inference-time scaling, as illustrated by\nmodels such as OpenAI's o1. However, current Vision-Language Models (VLMs)\noften struggle to perform systematic and structured reasoning, especially when\nhandling complex visual question-answering tasks. In this work, we introduce\nLLaVA-o1, a novel VLM designed to conduct autonomous multistage reasoning.\nUnlike chain-of-thought prompting, LLaVA-o1 independently engages in sequential\nstages of summarization, visual interpretation, logical reasoning, and\nconclusion generation. This structured approach enables LLaVA-o1 to achieve\nmarked improvements in precision on reasoning-intensive tasks. To accomplish\nthis, we compile the LLaVA-o1-100k dataset, integrating samples from various\nvisual question answering sources and providing structured reasoning\nannotations. Besides, we propose an inference-time stage-level beam search\nmethod, which enables effective inference-time scaling. Remarkably, with only\n100k training samples and a simple yet effective inference time scaling method,\nLLaVA-o1 not only outperforms its base model by 8.9% on a wide range of\nmultimodal reasoning benchmarks, but also surpasses the performance of larger\nand even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and\nLlama-3.2-90B-Vision-Instruct."
                },
                "authors": [
                    {
                        "name": "Guowei Xu"
                    },
                    {
                        "name": "Peng Jin"
                    },
                    {
                        "name": "Li Hao"
                    },
                    {
                        "name": "Yibing Song"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10433v1",
                "updated": "2024-11-15T18:54:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    54,
                    42,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T18:54:42Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    54,
                    42,
                    4,
                    320,
                    0
                ],
                "title": "M-VAR: Decoupled Scale-wise Autoregressive Modeling for High-Quality\n  Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-VAR: Decoupled Scale-wise Autoregressive Modeling for High-Quality\n  Image Generation"
                },
                "summary": "There exists recent work in computer vision, named VAR, that proposes a new\nautoregressive paradigm for image generation. Diverging from the vanilla\nnext-token prediction, VAR structurally reformulates the image generation into\na coarse to fine next-scale prediction. In this paper, we show that this\nscale-wise autoregressive framework can be effectively decoupled into\n\\textit{intra-scale modeling}, which captures local spatial dependencies within\neach scale, and \\textit{inter-scale modeling}, which models cross-scale\nrelationships progressively from coarse-to-fine scales. This decoupling\nstructure allows to rebuild VAR in a more computationally efficient manner.\nSpecifically, for intra-scale modeling -- crucial for generating high-fidelity\nimages -- we retain the original bidirectional self-attention design to ensure\ncomprehensive modeling; for inter-scale modeling, which semantically connects\ndifferent scales but is computationally intensive, we apply linear-complexity\nmechanisms like Mamba to substantially reduce computational overhead. We term\nthis new framework M-VAR. Extensive experiments demonstrate that our method\noutperforms existing models in both image quality and generation speed. For\nexample, our 1.5B model, with fewer parameters and faster inference speed,\noutperforms the largest VAR-d30-2B. Moreover, our largest model M-VAR-d32\nimpressively registers 1.78 FID on ImageNet 256$\\times$256 and outperforms the\nprior-art autoregressive models LlamaGen/VAR by 0.4/0.19 and popular diffusion\nmodels LDM/DiT by 1.82/0.49, respectively. Code is avaiable at\n\\url{https://github.com/OliverRensu/MVAR}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There exists recent work in computer vision, named VAR, that proposes a new\nautoregressive paradigm for image generation. Diverging from the vanilla\nnext-token prediction, VAR structurally reformulates the image generation into\na coarse to fine next-scale prediction. In this paper, we show that this\nscale-wise autoregressive framework can be effectively decoupled into\n\\textit{intra-scale modeling}, which captures local spatial dependencies within\neach scale, and \\textit{inter-scale modeling}, which models cross-scale\nrelationships progressively from coarse-to-fine scales. This decoupling\nstructure allows to rebuild VAR in a more computationally efficient manner.\nSpecifically, for intra-scale modeling -- crucial for generating high-fidelity\nimages -- we retain the original bidirectional self-attention design to ensure\ncomprehensive modeling; for inter-scale modeling, which semantically connects\ndifferent scales but is computationally intensive, we apply linear-complexity\nmechanisms like Mamba to substantially reduce computational overhead. We term\nthis new framework M-VAR. Extensive experiments demonstrate that our method\noutperforms existing models in both image quality and generation speed. For\nexample, our 1.5B model, with fewer parameters and faster inference speed,\noutperforms the largest VAR-d30-2B. Moreover, our largest model M-VAR-d32\nimpressively registers 1.78 FID on ImageNet 256$\\times$256 and outperforms the\nprior-art autoregressive models LlamaGen/VAR by 0.4/0.19 and popular diffusion\nmodels LDM/DiT by 1.82/0.49, respectively. Code is avaiable at\n\\url{https://github.com/OliverRensu/MVAR}."
                },
                "authors": [
                    {
                        "name": "Sucheng Ren"
                    },
                    {
                        "name": "Yaodong Yu"
                    },
                    {
                        "name": "Nataniel Ruiz"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Cihang Xie"
                    }
                ],
                "author_detail": {
                    "name": "Cihang Xie"
                },
                "author": "Cihang Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10431v1",
                "updated": "2024-11-15T18:53:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    53,
                    8,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T18:53:08Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    53,
                    8,
                    4,
                    320,
                    0
                ],
                "title": "Mitigating Parameter Degeneracy using Joint Conditional Diffusion Model\n  for WECC Composite Load Model in Power Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Parameter Degeneracy using Joint Conditional Diffusion Model\n  for WECC Composite Load Model in Power Systems"
                },
                "summary": "Data-driven modeling for dynamic systems has gained widespread attention in\nrecent years. Its inverse formulation, parameter estimation, aims to infer the\ninherent model parameters from observations. However, parameter degeneracy,\nwhere different combinations of parameters yield the same observable output,\nposes a critical barrier to accurately and uniquely identifying model\nparameters. In the context of WECC composite load model (CLM) in power systems,\nutility practitioners have observed that CLM parameters carefully selected for\none fault event may not perform satisfactorily in another fault. Here, we\ninnovate a joint conditional diffusion model-based inverse problem solver\n(JCDI), that incorporates a joint conditioning architecture with simultaneous\ninputs of multi-event observations to improve parameter generalizability.\nSimulation studies on the WECC CLM show that the proposed JCDI effectively\nreduces uncertainties of degenerate parameters, thus the parameter estimation\nerror is decreased by 42.1% compared to a single-event learning scheme. This\nenables the model to achieve high accuracy in predicting power trajectories\nunder different fault events, including electronic load tripping and motor\nstalling, outperforming standard deep reinforcement learning and supervised\nlearning approaches. We anticipate this work will contribute to mitigating\nparameter degeneracy in system dynamics, providing a general parameter\nestimation framework across various scientific domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven modeling for dynamic systems has gained widespread attention in\nrecent years. Its inverse formulation, parameter estimation, aims to infer the\ninherent model parameters from observations. However, parameter degeneracy,\nwhere different combinations of parameters yield the same observable output,\nposes a critical barrier to accurately and uniquely identifying model\nparameters. In the context of WECC composite load model (CLM) in power systems,\nutility practitioners have observed that CLM parameters carefully selected for\none fault event may not perform satisfactorily in another fault. Here, we\ninnovate a joint conditional diffusion model-based inverse problem solver\n(JCDI), that incorporates a joint conditioning architecture with simultaneous\ninputs of multi-event observations to improve parameter generalizability.\nSimulation studies on the WECC CLM show that the proposed JCDI effectively\nreduces uncertainties of degenerate parameters, thus the parameter estimation\nerror is decreased by 42.1% compared to a single-event learning scheme. This\nenables the model to achieve high accuracy in predicting power trajectories\nunder different fault events, including electronic load tripping and motor\nstalling, outperforming standard deep reinforcement learning and supervised\nlearning approaches. We anticipate this work will contribute to mitigating\nparameter degeneracy in system dynamics, providing a general parameter\nestimation framework across various scientific domains."
                },
                "authors": [
                    {
                        "name": "Feiqin Zhu"
                    },
                    {
                        "name": "Dmitrii Torbunov"
                    },
                    {
                        "name": "Yihui Ren"
                    },
                    {
                        "name": "Zhongjing Jiang"
                    },
                    {
                        "name": "Tianqiao Zhao"
                    },
                    {
                        "name": "Amirthagunaraj Yogarathnam"
                    },
                    {
                        "name": "Meng Yue"
                    }
                ],
                "author_detail": {
                    "name": "Meng Yue"
                },
                "author": "Meng Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10427v1",
                "updated": "2024-11-15T18:50:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    50,
                    27,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T18:50:27Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    50,
                    27,
                    4,
                    320,
                    0
                ],
                "title": "SN 2023adsy -- a normal Type Ia Supernova at z=2.9, discovered by JWST",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SN 2023adsy -- a normal Type Ia Supernova at z=2.9, discovered by JWST"
                },
                "summary": "SN 2023adsy, a Type Ia supernova discovered by JWST at z = 2.9, was found to\nbe a peculiar event, being extremely red and faint, but showing very similar\nrest-frame light curve decline rate to the majority of low-redshift SNe Ia. In\nthis paper we show that the red color and faint peak magnitude could be\nexplained by significant reddening/extinction due to dust in the host galaxy.\nIf host galaxy extinction is accounted for, the parameters of the best-fit\nlight curve templates in the SALT3-NIR model are compatible with a slowly\ndeclining, but still normal SN Ia. Comparison of the inferred luminosity\ndistance with the prediction of the LambdaCDM cosmology (assuming H0 = 70\nkm/s/Mpc and OmegaM = 0.315) on the Hubble-diagram suggests no significant\nevolution of the SN Ia peak luminosity at z > 2 redshifts. It is also shown\nthat the discovery of a single SN Ia between 2 < z < 3 within the area of the\nJADES survey during 1 year is consistent with the current estimates for the SN\nIa rates at such redshifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SN 2023adsy, a Type Ia supernova discovered by JWST at z = 2.9, was found to\nbe a peculiar event, being extremely red and faint, but showing very similar\nrest-frame light curve decline rate to the majority of low-redshift SNe Ia. In\nthis paper we show that the red color and faint peak magnitude could be\nexplained by significant reddening/extinction due to dust in the host galaxy.\nIf host galaxy extinction is accounted for, the parameters of the best-fit\nlight curve templates in the SALT3-NIR model are compatible with a slowly\ndeclining, but still normal SN Ia. Comparison of the inferred luminosity\ndistance with the prediction of the LambdaCDM cosmology (assuming H0 = 70\nkm/s/Mpc and OmegaM = 0.315) on the Hubble-diagram suggests no significant\nevolution of the SN Ia peak luminosity at z > 2 redshifts. It is also shown\nthat the discovery of a single SN Ia between 2 < z < 3 within the area of the\nJADES survey during 1 year is consistent with the current estimates for the SN\nIa rates at such redshifts."
                },
                "authors": [
                    {
                        "name": "Jozsef Vinko"
                    },
                    {
                        "name": "Eniko Regos"
                    }
                ],
                "author_detail": {
                    "name": "Eniko Regos"
                },
                "author": "Eniko Regos",
                "arxiv_comment": "Submitted to ApJ. Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06169v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06169v2",
                "updated": "2024-11-15T18:43:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    43,
                    23,
                    4,
                    320,
                    0
                ],
                "published": "2024-10-08T16:13:24Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    16,
                    13,
                    24,
                    1,
                    282,
                    0
                ],
                "title": "Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to\n  See",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to\n  See"
                },
                "summary": "By treating visual tokens from visual encoders as text tokens, Multimodal\nLarge Language Models (MLLMs) have achieved remarkable progress across diverse\nvisual understanding tasks, leveraging the robust architectures of Large\nLanguage Models (LLMs). However, as token counts grow, the quadratic scaling of\ncomputation in LLMs introduces a significant efficiency bottleneck, impeding\nfurther scalability. Although recent approaches have explored pruning visual\ntokens or employing lighter LLM architectures, the computational overhead from\nan increasing number of visual tokens remains a substantial challenge.\n  In this study, we investigate the redundancy in visual computation at both\nthe parameter and computational pattern levels within LLaVA, a representative\nMLLM, and introduce a suite of streamlined strategies to enhance efficiency.\nThese include neighbor-aware visual token attention, pruning of inactive visual\nattention heads, and selective layer dropping for visual computations. By\nimplementing these strategies in LLaVA, we achieve a reduction in computational\ndemands of 88% while maintaining model performance across key benchmarks.\nAdditionally, we validate the existence of visual computational redundancy in\nother MLLMs, such as Qwen2-VL-7B and InternVL-2.0-4B/8B/26B. These results\npresent a novel pathway for MLLMs to handle dense visual tokens with minimal\ncomputational costs. Code and model checkpoints will be released to support\nfurther research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By treating visual tokens from visual encoders as text tokens, Multimodal\nLarge Language Models (MLLMs) have achieved remarkable progress across diverse\nvisual understanding tasks, leveraging the robust architectures of Large\nLanguage Models (LLMs). However, as token counts grow, the quadratic scaling of\ncomputation in LLMs introduces a significant efficiency bottleneck, impeding\nfurther scalability. Although recent approaches have explored pruning visual\ntokens or employing lighter LLM architectures, the computational overhead from\nan increasing number of visual tokens remains a substantial challenge.\n  In this study, we investigate the redundancy in visual computation at both\nthe parameter and computational pattern levels within LLaVA, a representative\nMLLM, and introduce a suite of streamlined strategies to enhance efficiency.\nThese include neighbor-aware visual token attention, pruning of inactive visual\nattention heads, and selective layer dropping for visual computations. By\nimplementing these strategies in LLaVA, we achieve a reduction in computational\ndemands of 88% while maintaining model performance across key benchmarks.\nAdditionally, we validate the existence of visual computational redundancy in\nother MLLMs, such as Qwen2-VL-7B and InternVL-2.0-4B/8B/26B. These results\npresent a novel pathway for MLLMs to handle dense visual tokens with minimal\ncomputational costs. Code and model checkpoints will be released to support\nfurther research."
                },
                "authors": [
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Phu Pham"
                    },
                    {
                        "name": "Wentian Zhao"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Yu-Jhe Li"
                    },
                    {
                        "name": "Jianing Zhou"
                    },
                    {
                        "name": "Daniel Miranda"
                    },
                    {
                        "name": "Ajinkya Kale"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06169v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06169v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10422v1",
                "updated": "2024-11-15T18:42:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    42,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T18:42:48Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    42,
                    48,
                    4,
                    320,
                    0
                ],
                "title": "Evaluating Creativity and Deception in Large Language Models: A\n  Simulation Framework for Multi-Agent Balderdash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Creativity and Deception in Large Language Models: A\n  Simulation Framework for Multi-Agent Balderdash"
                },
                "summary": "Large Language Models (LLMs) have shown impressive capabilities in complex\ntasks and interactive environments, yet their creativity remains underexplored.\nThis paper introduces a simulation framework utilizing the game Balderdash to\nevaluate both the creativity and logical reasoning of LLMs. In Balderdash,\nplayers generate fictitious definitions for obscure terms to deceive others\nwhile identifying correct definitions. Our framework enables multiple LLM\nagents to participate in this game, assessing their ability to produce\nplausible definitions and strategize based on game rules and history. We\nimplemented a centralized game engine featuring various LLMs as participants\nand a judge LLM to evaluate semantic equivalence. Through a series of\nexperiments, we analyzed the performance of different LLMs, examining metrics\nsuch as True Definition Ratio, Deception Ratio, and Correct Guess Ratio. The\nresults provide insights into the creative and deceptive capabilities of LLMs,\nhighlighting their strengths and areas for improvement. Specifically, the study\nreveals that infrequent vocabulary in LLMs' input leads to poor reasoning on\ngame rules and historical context\n(https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive capabilities in complex\ntasks and interactive environments, yet their creativity remains underexplored.\nThis paper introduces a simulation framework utilizing the game Balderdash to\nevaluate both the creativity and logical reasoning of LLMs. In Balderdash,\nplayers generate fictitious definitions for obscure terms to deceive others\nwhile identifying correct definitions. Our framework enables multiple LLM\nagents to participate in this game, assessing their ability to produce\nplausible definitions and strategize based on game rules and history. We\nimplemented a centralized game engine featuring various LLMs as participants\nand a judge LLM to evaluate semantic equivalence. Through a series of\nexperiments, we analyzed the performance of different LLMs, examining metrics\nsuch as True Definition Ratio, Deception Ratio, and Correct Guess Ratio. The\nresults provide insights into the creative and deceptive capabilities of LLMs,\nhighlighting their strengths and areas for improvement. Specifically, the study\nreveals that infrequent vocabulary in LLMs' input leads to poor reasoning on\ngame rules and historical context\n(https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash)."
                },
                "authors": [
                    {
                        "name": "Parsa Hejabi"
                    },
                    {
                        "name": "Elnaz Rahmati"
                    },
                    {
                        "name": "Alireza S. Ziabari"
                    },
                    {
                        "name": "Preni Golazizian"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Morteza Dehghani"
                    }
                ],
                "author_detail": {
                    "name": "Morteza Dehghani"
                },
                "author": "Morteza Dehghani",
                "arxiv_comment": "Accepted at Wordplay: When Language Meets Games @ ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05817v2",
                "updated": "2024-11-15T18:36:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    36,
                    30,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-01T18:21:37Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    18,
                    21,
                    37,
                    4,
                    306,
                    0
                ],
                "title": "Demo: Multi-Modal Seizure Prediction System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: Multi-Modal Seizure Prediction System"
                },
                "summary": "This demo presents SeizNet, an innovative system for predicting epileptic\nseizures benefiting from a multi-modal sensor network and utilizing Deep\nLearning (DL) techniques. Epilepsy affects approximately 65 million people\nworldwide, many of whom experience drug-resistant seizures. SeizNet aims at\nproviding highly accurate alerts, allowing individuals to take preventive\nmeasures without being disturbed by false alarms. SeizNet uses a combination of\ndata collected through either invasive (intracranial electroencephalogram\n(iEEG)) or non-invasive (electroencephalogram (EEG) and electrocardiogram\n(ECG)) sensors, and processed by advanced DL algorithms that are optimized for\nreal-time inference at the edge, ensuring privacy and minimizing data\ntransmission. SeizNet achieves > 97% accuracy in seizure prediction while\nkeeping the size and energy restrictions of an implantable device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This demo presents SeizNet, an innovative system for predicting epileptic\nseizures benefiting from a multi-modal sensor network and utilizing Deep\nLearning (DL) techniques. Epilepsy affects approximately 65 million people\nworldwide, many of whom experience drug-resistant seizures. SeizNet aims at\nproviding highly accurate alerts, allowing individuals to take preventive\nmeasures without being disturbed by false alarms. SeizNet uses a combination of\ndata collected through either invasive (intracranial electroencephalogram\n(iEEG)) or non-invasive (electroencephalogram (EEG) and electrocardiogram\n(ECG)) sensors, and processed by advanced DL algorithms that are optimized for\nreal-time inference at the edge, ensuring privacy and minimizing data\ntransmission. SeizNet achieves > 97% accuracy in seizure prediction while\nkeeping the size and energy restrictions of an implantable device."
                },
                "authors": [
                    {
                        "name": "Ali Saeizadeh"
                    },
                    {
                        "name": "Pietro Brach del Prever"
                    },
                    {
                        "name": "Douglas Schonholtz"
                    },
                    {
                        "name": "Raffaele Guida"
                    },
                    {
                        "name": "Emrecan Demirors"
                    },
                    {
                        "name": "Jorge M. Jimenez"
                    },
                    {
                        "name": "Pedram Johari"
                    },
                    {
                        "name": "Tommaso Melodia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Melodia"
                },
                "author": "Tommaso Melodia",
                "arxiv_comment": "1 page, 1 figure, Proceedings of the IEEE 20th International\n  Conference on Body Sensor Networks (BSN), October 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09584v2",
                "updated": "2024-11-15T18:34:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    34,
                    42,
                    4,
                    320,
                    0
                ],
                "published": "2024-02-14T21:19:33Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    21,
                    19,
                    33,
                    2,
                    45,
                    0
                ],
                "title": "Large Language Model-Based Interpretable Machine Learning Control in\n  Building Energy Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Based Interpretable Machine Learning Control in\n  Building Energy Systems"
                },
                "summary": "The potential of Machine Learning Control (MLC) in HVAC systems is hindered\nby its opaque nature and inference mechanisms, which is challenging for users\nand modelers to fully comprehend, ultimately leading to a lack of trust in\nMLC-based decision-making. To address this challenge, this paper investigates\nand explores Interpretable Machine Learning (IML), a branch of Machine Learning\n(ML) that enhances transparency and understanding of models and their\ninferences, to improve the credibility of MLC and its industrial application in\nHVAC systems. Specifically, we developed an innovative framework that combines\nthe principles of Shapley values and the in-context learning feature of Large\nLanguage Models (LLMs). While the Shapley values are instrumental in dissecting\nthe contributions of various features in ML models, LLM provides an in-depth\nunderstanding of the non-data-driven or rule-based elements in MLC; combining\nthem, LLM further packages these insights into a coherent, human-understandable\nnarrative. The paper presents a case study to demonstrate the feasibility of\nthe developed IML framework for model predictive control-based precooling under\ndemand response events in a virtual testbed. The results indicate that the\ndeveloped framework generates and explains the control signals in accordance\nwith the rule-based rationale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The potential of Machine Learning Control (MLC) in HVAC systems is hindered\nby its opaque nature and inference mechanisms, which is challenging for users\nand modelers to fully comprehend, ultimately leading to a lack of trust in\nMLC-based decision-making. To address this challenge, this paper investigates\nand explores Interpretable Machine Learning (IML), a branch of Machine Learning\n(ML) that enhances transparency and understanding of models and their\ninferences, to improve the credibility of MLC and its industrial application in\nHVAC systems. Specifically, we developed an innovative framework that combines\nthe principles of Shapley values and the in-context learning feature of Large\nLanguage Models (LLMs). While the Shapley values are instrumental in dissecting\nthe contributions of various features in ML models, LLM provides an in-depth\nunderstanding of the non-data-driven or rule-based elements in MLC; combining\nthem, LLM further packages these insights into a coherent, human-understandable\nnarrative. The paper presents a case study to demonstrate the feasibility of\nthe developed IML framework for model predictive control-based precooling under\ndemand response events in a virtual testbed. The results indicate that the\ndeveloped framework generates and explains the control signals in accordance\nwith the rule-based rationale."
                },
                "authors": [
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Zhelun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhelun Chen"
                },
                "author": "Zhelun Chen",
                "arxiv_doi": "10.1016/j.enbuild.2024.114278",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.enbuild.2024.114278",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.09584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Energy and Buildings, 313, 114278 (2024)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10414v1",
                "updated": "2024-11-15T18:34:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    34,
                    7,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T18:34:07Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    34,
                    7,
                    4,
                    320,
                    0
                ],
                "title": "Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding\n  Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding\n  Conversations"
                },
                "summary": "We introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for\nhuman-AI conversations that involves image understanding: it can be used to\nsafeguard content for both multimodal LLM inputs (prompt classification) and\noutputs (response classification). Unlike the previous text-only Llama Guard\nversions (Inan et al., 2023; Llama Team, 2024b,a), it is specifically designed\nto support image reasoning use cases and is optimized to detect harmful\nmultimodal (text and image) prompts and text responses to these prompts. Llama\nGuard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong\nperformance on the internal benchmarks using the MLCommons taxonomy. We also\ntest its robustness against adversarial attacks. We believe that Llama Guard 3\nVision serves as a good starting point to build more capable and robust content\nmoderation tools for human-AI conversation with multimodal capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for\nhuman-AI conversations that involves image understanding: it can be used to\nsafeguard content for both multimodal LLM inputs (prompt classification) and\noutputs (response classification). Unlike the previous text-only Llama Guard\nversions (Inan et al., 2023; Llama Team, 2024b,a), it is specifically designed\nto support image reasoning use cases and is optimized to detect harmful\nmultimodal (text and image) prompts and text responses to these prompts. Llama\nGuard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong\nperformance on the internal benchmarks using the MLCommons taxonomy. We also\ntest its robustness against adversarial attacks. We believe that Llama Guard 3\nVision serves as a good starting point to build more capable and robust content\nmoderation tools for human-AI conversation with multimodal capabilities."
                },
                "authors": [
                    {
                        "name": "Jianfeng Chi"
                    },
                    {
                        "name": "Ujjwal Karn"
                    },
                    {
                        "name": "Hongyuan Zhan"
                    },
                    {
                        "name": "Eric Smith"
                    },
                    {
                        "name": "Javier Rando"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Kate Plawiak"
                    },
                    {
                        "name": "Zacharie Delpierre Coudert"
                    },
                    {
                        "name": "Kartikeya Upasani"
                    },
                    {
                        "name": "Mahesh Pasupuleti"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Pasupuleti"
                },
                "author": "Mahesh Pasupuleti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19572v2",
                "updated": "2024-11-15T18:31:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    31,
                    46,
                    4,
                    320,
                    0
                ],
                "published": "2024-03-28T16:56:39Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    16,
                    56,
                    39,
                    3,
                    88,
                    0
                ],
                "title": "Swarm Characteristics Classification Using Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Swarm Characteristics Classification Using Neural Networks"
                },
                "summary": "Understanding the characteristics of swarming autonomous agents is critical\nfor defense and security applications. This article presents a study on using\nsupervised neural network time series classification (NN TSC) to predict key\nattributes and tactics of swarming autonomous agents for military contexts.\nSpecifically, NN TSC is applied to infer two binary attributes - communication\nand proportional navigation - which combine to define four mutually exclusive\nswarm tactics. We identify a gap in literature on using NNs for swarm\nclassification and demonstrate the effectiveness of NN TSC in rapidly deducing\nintelligence about attacking swarms to inform counter-maneuvers. Through\nsimulated swarm-vs-swarm engagements, we evaluate NN TSC performance in terms\nof observation window requirements, noise robustness, and scalability to swarm\nsize. Key findings show NNs can predict swarm behaviors with 97% accuracy using\nshort observation windows of 20 time steps, while also demonstrating graceful\ndegradation down to 80% accuracy under 50% noise, as well as excellent\nscalability to swarm sizes from 10 to 100 agents. These capabilities are\npromising for real-time decision-making support in defense scenarios by rapidly\ninferring insights about swarm behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the characteristics of swarming autonomous agents is critical\nfor defense and security applications. This article presents a study on using\nsupervised neural network time series classification (NN TSC) to predict key\nattributes and tactics of swarming autonomous agents for military contexts.\nSpecifically, NN TSC is applied to infer two binary attributes - communication\nand proportional navigation - which combine to define four mutually exclusive\nswarm tactics. We identify a gap in literature on using NNs for swarm\nclassification and demonstrate the effectiveness of NN TSC in rapidly deducing\nintelligence about attacking swarms to inform counter-maneuvers. Through\nsimulated swarm-vs-swarm engagements, we evaluate NN TSC performance in terms\nof observation window requirements, noise robustness, and scalability to swarm\nsize. Key findings show NNs can predict swarm behaviors with 97% accuracy using\nshort observation windows of 20 time steps, while also demonstrating graceful\ndegradation down to 80% accuracy under 50% noise, as well as excellent\nscalability to swarm sizes from 10 to 100 agents. These capabilities are\npromising for real-time decision-making support in defense scenarios by rapidly\ninferring insights about swarm behavior."
                },
                "authors": [
                    {
                        "name": "Donald W. Peltier III"
                    },
                    {
                        "name": "Isaac Kaminer"
                    },
                    {
                        "name": "Abram Clark"
                    },
                    {
                        "name": "Marko Orescanin"
                    }
                ],
                "author_detail": {
                    "name": "Marko Orescanin"
                },
                "author": "Marko Orescanin",
                "arxiv_doi": "10.1109/TAES.2024.3447615",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TAES.2024.3447615",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.19572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Added funding acknowledgment and author bios",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.00141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.00141v2",
                "updated": "2024-11-15T18:19:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    19,
                    41,
                    4,
                    320,
                    0
                ],
                "published": "2023-08-31T21:26:36Z",
                "published_parsed": [
                    2023,
                    8,
                    31,
                    21,
                    26,
                    36,
                    3,
                    243,
                    0
                ],
                "title": "Causal Inference under Network Interference Using a Mixture of\n  Randomized Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Inference under Network Interference Using a Mixture of\n  Randomized Experiments"
                },
                "summary": "In randomized experiments, the classic Stable Unit Treatment Value Assumption\n(SUTVA) posits that the outcome for one experimental unit is unaffected by the\ntreatment assignments of other units. However, this assumption is frequently\nviolated in settings such as online marketplaces and social networks, where\ninterference between units is common. We address the estimation of the total\ntreatment effect in a network interference model by employing a mixed\nrandomization design that combines two widely used experimental methods:\nBernoulli randomization, where treatment is assigned independently to each\nunit, and cluster-based randomization, where treatment is assigned at the\naggregate level. The mixed randomization design simultaneously incorporates\nboth methods, thereby mitigating the bias present in cluster-based designs. We\npropose an unbiased estimator for the total treatment effect under this mixed\ndesign and show that its variance is bounded by $O(d^2 n^{-1} p^{-1}\n(1-p)^{-1})$, where $d$ is the maximum degree of the network, $n$ is the\nnetwork size, and $p$ is the treatment probability. Additionally, we establish\na lower bound of $\\Omega(d^{1.5} n^{-1} p^{-1} (1-p)^{-1})$ for the variance of\nany mixed design. Moreover, when the interference weights on the network's\nedges are unknown, we propose a weight-invariant design that achieves a\nvariance bound of $O(d^3 n^{-1} p^{-1} (1-p)^{-1})$, which is aligned with the\nestimator introduced by Cortez-Rodriguez et al. (2023) under similar\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In randomized experiments, the classic Stable Unit Treatment Value Assumption\n(SUTVA) posits that the outcome for one experimental unit is unaffected by the\ntreatment assignments of other units. However, this assumption is frequently\nviolated in settings such as online marketplaces and social networks, where\ninterference between units is common. We address the estimation of the total\ntreatment effect in a network interference model by employing a mixed\nrandomization design that combines two widely used experimental methods:\nBernoulli randomization, where treatment is assigned independently to each\nunit, and cluster-based randomization, where treatment is assigned at the\naggregate level. The mixed randomization design simultaneously incorporates\nboth methods, thereby mitigating the bias present in cluster-based designs. We\npropose an unbiased estimator for the total treatment effect under this mixed\ndesign and show that its variance is bounded by $O(d^2 n^{-1} p^{-1}\n(1-p)^{-1})$, where $d$ is the maximum degree of the network, $n$ is the\nnetwork size, and $p$ is the treatment probability. Additionally, we establish\na lower bound of $\\Omega(d^{1.5} n^{-1} p^{-1} (1-p)^{-1})$ for the variance of\nany mixed design. Moreover, when the interference weights on the network's\nedges are unknown, we propose a weight-invariant design that achieves a\nvariance bound of $O(d^3 n^{-1} p^{-1} (1-p)^{-1})$, which is aligned with the\nestimator introduced by Cortez-Rodriguez et al. (2023) under similar\nconditions."
                },
                "authors": [
                    {
                        "name": "Yiming Jiang"
                    },
                    {
                        "name": "He Wang"
                    }
                ],
                "author_detail": {
                    "name": "He Wang"
                },
                "author": "He Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.00141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.00141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10393v1",
                "updated": "2024-11-15T18:01:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    1,
                    40,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T18:01:40Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    1,
                    40,
                    4,
                    320,
                    0
                ],
                "title": "Guaranteed Bounds on Posterior Distributions of Discrete Probabilistic\n  Programs with Loops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guaranteed Bounds on Posterior Distributions of Discrete Probabilistic\n  Programs with Loops"
                },
                "summary": "We study the problem of bounding the posterior distribution of discrete\nprobabilistic programs with unbounded support, loops, and conditioning. Loops\npose the main difficulty in this setting: even if exact Bayesian inference is\npossible, the state of the art requires user-provided loop invariant templates.\nBy contrast, we aim to find guaranteed bounds, which sandwich the true\ndistribution. They are fully automated, applicable to more programs and provide\nmore provable guarantees than approximate sampling-based inference. Since lower\nbounds can be obtained by unrolling loops, the main challenge is upper bounds,\nand we attack it in two ways. The first is called residual mass semantics,\nwhich is a flat bound based on the residual probability mass of a loop. The\napproach is simple, efficient, and has provable guarantees.\n  The main novelty of our work is the second approach, called geometric bound\nsemantics. It operates on a novel family of distributions, called eventually\ngeometric distributions (EGDs), and can bound the distribution of loops with a\nnew form of loop invariants called contraction invariants. The invariant\nsynthesis problem reduces to a system of polynomial inequality constraints,\nwhich is a decidable problem with automated solvers. If a solution exists, it\nyields an exponentially decreasing bound on the whole distribution, and can\ntherefore bound moments and tail asymptotics as well, not just probabilities as\nin the first approach.\n  Both semantics enjoy desirable theoretical properties. In particular, we\nprove soundness and convergence, i.e. the bounds converge to the exact\nposterior as loops are unrolled further. On the practical side, we describe\nDiabolo, a fully-automated implementation of both semantics, and evaluate them\non a variety of benchmarks from the literature, demonstrating their general\napplicability and the utility of the resulting bounds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of bounding the posterior distribution of discrete\nprobabilistic programs with unbounded support, loops, and conditioning. Loops\npose the main difficulty in this setting: even if exact Bayesian inference is\npossible, the state of the art requires user-provided loop invariant templates.\nBy contrast, we aim to find guaranteed bounds, which sandwich the true\ndistribution. They are fully automated, applicable to more programs and provide\nmore provable guarantees than approximate sampling-based inference. Since lower\nbounds can be obtained by unrolling loops, the main challenge is upper bounds,\nand we attack it in two ways. The first is called residual mass semantics,\nwhich is a flat bound based on the residual probability mass of a loop. The\napproach is simple, efficient, and has provable guarantees.\n  The main novelty of our work is the second approach, called geometric bound\nsemantics. It operates on a novel family of distributions, called eventually\ngeometric distributions (EGDs), and can bound the distribution of loops with a\nnew form of loop invariants called contraction invariants. The invariant\nsynthesis problem reduces to a system of polynomial inequality constraints,\nwhich is a decidable problem with automated solvers. If a solution exists, it\nyields an exponentially decreasing bound on the whole distribution, and can\ntherefore bound moments and tail asymptotics as well, not just probabilities as\nin the first approach.\n  Both semantics enjoy desirable theoretical properties. In particular, we\nprove soundness and convergence, i.e. the bounds converge to the exact\nposterior as loops are unrolled further. On the practical side, we describe\nDiabolo, a fully-automated implementation of both semantics, and evaluate them\non a variety of benchmarks from the literature, demonstrating their general\napplicability and the utility of the resulting bounds."
                },
                "authors": [
                    {
                        "name": "Fabian Zaiser"
                    },
                    {
                        "name": "Andrzej S. Murawski"
                    },
                    {
                        "name": "C. -H. Luke Ong"
                    }
                ],
                "author_detail": {
                    "name": "C. -H. Luke Ong"
                },
                "author": "C. -H. Luke Ong",
                "arxiv_comment": "Full version of the POPL 2025 article, including proofs and other\n  supplementary material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.3.1; F.3.2; D.2.4; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.14204v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.14204v3",
                "updated": "2024-11-15T17:41:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    17,
                    41,
                    13,
                    4,
                    320,
                    0
                ],
                "published": "2023-11-23T21:05:40Z",
                "published_parsed": [
                    2023,
                    11,
                    23,
                    21,
                    5,
                    40,
                    3,
                    327,
                    0
                ],
                "title": "Reproducible Aggregation of Sample-Split Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducible Aggregation of Sample-Split Statistics"
                },
                "summary": "Statistical inference is often simplified by sample-splitting. This\nsimplification comes at the cost of the introduction of randomness not native\nto the data. We propose a simple procedure for sequentially aggregating\nstatistics constructed with multiple splits of the same sample. The user\nspecifies a bound and a nominal error rate. If the procedure is implemented\ntwice on the same data, the nominal error rate approximates the chance that the\nresults differ by more than the bound. We illustrate the application of the\nprocedure to several widely applied econometric methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference is often simplified by sample-splitting. This\nsimplification comes at the cost of the introduction of randomness not native\nto the data. We propose a simple procedure for sequentially aggregating\nstatistics constructed with multiple splits of the same sample. The user\nspecifies a bound and a nominal error rate. If the procedure is implemented\ntwice on the same data, the nominal error rate approximates the chance that the\nresults differ by more than the bound. We illustrate the application of the\nprocedure to several widely applied econometric methods."
                },
                "authors": [
                    {
                        "name": "David M. Ritzwoller"
                    },
                    {
                        "name": "Joseph P. Romano"
                    }
                ],
                "author_detail": {
                    "name": "Joseph P. Romano"
                },
                "author": "Joseph P. Romano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.14204v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.14204v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10371v1",
                "updated": "2024-11-15T17:19:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    17,
                    19,
                    42,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T17:19:42Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    17,
                    19,
                    42,
                    4,
                    320,
                    0
                ],
                "title": "A Survey of Event Causality Identification: Principles, Taxonomy,\n  Challenges, and Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Event Causality Identification: Principles, Taxonomy,\n  Challenges, and Assessment"
                },
                "summary": "Event Causality Identification (ECI) has become a crucial task in Natural\nLanguage Processing (NLP), aimed at automatically extracting causalities from\ntextual data. In this survey, we systematically address the foundational\nprinciples, technical frameworks, and challenges of ECI, offering a\ncomprehensive taxonomy to categorize and clarify current research\nmethodologies, as well as a quantitative assessment of existing models. We\nfirst establish a conceptual framework for ECI, outlining key definitions,\nproblem formulations, and evaluation standards. Our taxonomy classifies ECI\nmethods according to the two primary tasks of sentence-level (SECI) and\ndocument-level (DECI) event causality identification. For SECI, we examine\nfeature pattern-based matching, deep semantic encoding, causal knowledge\npre-training and prompt-based fine-tuning, and external knowledge enhancement\nmethods. For DECI, we highlight approaches focused on event graph reasoning and\nprompt-based techniques to address the complexity of cross-sentence causal\ninference. Additionally, we analyze the strengths, limitations, and open\nchallenges of each approach. We further conduct an extensive quantitative\nevaluation of various ECI methods on two benchmark datasets. Finally, we\nexplore future research directions, highlighting promising pathways to overcome\ncurrent limitations and broaden ECI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event Causality Identification (ECI) has become a crucial task in Natural\nLanguage Processing (NLP), aimed at automatically extracting causalities from\ntextual data. In this survey, we systematically address the foundational\nprinciples, technical frameworks, and challenges of ECI, offering a\ncomprehensive taxonomy to categorize and clarify current research\nmethodologies, as well as a quantitative assessment of existing models. We\nfirst establish a conceptual framework for ECI, outlining key definitions,\nproblem formulations, and evaluation standards. Our taxonomy classifies ECI\nmethods according to the two primary tasks of sentence-level (SECI) and\ndocument-level (DECI) event causality identification. For SECI, we examine\nfeature pattern-based matching, deep semantic encoding, causal knowledge\npre-training and prompt-based fine-tuning, and external knowledge enhancement\nmethods. For DECI, we highlight approaches focused on event graph reasoning and\nprompt-based techniques to address the complexity of cross-sentence causal\ninference. Additionally, we analyze the strengths, limitations, and open\nchallenges of each approach. We further conduct an extensive quantitative\nevaluation of various ECI methods on two benchmark datasets. Finally, we\nexplore future research directions, highlighting promising pathways to overcome\ncurrent limitations and broaden ECI applications."
                },
                "authors": [
                    {
                        "name": "Zefan Zeng"
                    },
                    {
                        "name": "Qing Cheng"
                    },
                    {
                        "name": "Xingchen Hu"
                    },
                    {
                        "name": "Yuehang Si"
                    },
                    {
                        "name": "Zhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Liu"
                },
                "author": "Zhong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10362v1",
                "updated": "2024-11-15T17:11:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    17,
                    11,
                    13,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T17:11:13Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    17,
                    11,
                    13,
                    4,
                    320,
                    0
                ],
                "title": "Interactive Cycle Model -- The Linkage Combination among Automatic\n  Speech Recognition, Large Language Models and Smart Glasses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Cycle Model -- The Linkage Combination among Automatic\n  Speech Recognition, Large Language Models and Smart Glasses"
                },
                "summary": "This research proposes the interaction loop model \"ASR-LLM-Smart Glasses\",\nwhich model combines automatic speech recognition, large language model and\nsmart glasses to facilitate seamless human-computer interaction. And the\nmethodology of this research involves decomposing the interaction process into\ndifferent stages and elements. Speech is captured and processed by ASR, then\nanalyzed and interpreted by LLM. The results are then transmitted to smart\nglasses for display. The feedback loop is complete when the user interacts with\nthe displayed data. Mathematical formulas are used to quantify the performance\nof the model that revolves around core evaluation points: accuracy, coherence,\nand latency during ASR speech-to-text conversion. The research results are\nprovided theoretically to test and evaluate the feasibility and performance of\nthe model. Although such human-computer interaction products have not yet\nappeared in the industry, the performance indicators of this model in enhancing\nuser experience in fields that rely on human-computer interaction have also\nverified its utility as a technology to promote human-computer interaction. In\naddition, this research pioneered the idea of integrating cutting-edge\ntechnologies such as generative pre-trained Transformer models into unique\ninteraction models, LLM provides raw value through powerful evaluation\ntechniques and innovative use, which provides a new perspective to evaluate and\nenhanced human-computer interaction.\n  Keywords: Automatic speech recognition, Large Language Model, Smart glasses,\nInteraction mechanism",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research proposes the interaction loop model \"ASR-LLM-Smart Glasses\",\nwhich model combines automatic speech recognition, large language model and\nsmart glasses to facilitate seamless human-computer interaction. And the\nmethodology of this research involves decomposing the interaction process into\ndifferent stages and elements. Speech is captured and processed by ASR, then\nanalyzed and interpreted by LLM. The results are then transmitted to smart\nglasses for display. The feedback loop is complete when the user interacts with\nthe displayed data. Mathematical formulas are used to quantify the performance\nof the model that revolves around core evaluation points: accuracy, coherence,\nand latency during ASR speech-to-text conversion. The research results are\nprovided theoretically to test and evaluate the feasibility and performance of\nthe model. Although such human-computer interaction products have not yet\nappeared in the industry, the performance indicators of this model in enhancing\nuser experience in fields that rely on human-computer interaction have also\nverified its utility as a technology to promote human-computer interaction. In\naddition, this research pioneered the idea of integrating cutting-edge\ntechnologies such as generative pre-trained Transformer models into unique\ninteraction models, LLM provides raw value through powerful evaluation\ntechniques and innovative use, which provides a new perspective to evaluate and\nenhanced human-computer interaction.\n  Keywords: Automatic speech recognition, Large Language Model, Smart glasses,\nInteraction mechanism"
                },
                "authors": [
                    {
                        "name": "Libo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Libo Wang"
                },
                "author": "Libo Wang",
                "arxiv_comment": "OpenReview submitted. 11 pages of text and 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10351v1",
                "updated": "2024-11-15T16:55:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    55,
                    57,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:55:57Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    55,
                    57,
                    4,
                    320,
                    0
                ],
                "title": "Bias Unveiled: Investigating Social Bias in LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Unveiled: Investigating Social Bias in LLM-Generated Code"
                },
                "summary": "Large language models (LLMs) have significantly advanced the field of\nautomated code generation. However, a notable research gap exists in the\nevaluation of social biases that may be present in the code produced by LLMs.\nTo solve this issue, we propose a novel fairness framework, i.e., Solar, to\nassess and mitigate the social biases of LLM-generated code. Specifically,\nSolar can automatically generate test cases for quantitatively uncovering\nsocial biases of the auto-generated code by LLMs. To quantify the severity of\nsocial biases in generated code, we develop a dataset that covers a diverse set\nof social problems. We applied Solar and the crafted dataset to four\nstate-of-the-art LLMs for code generation. Our evaluation reveals severe bias\nin the LLM-generated code from all the subject LLMs. Furthermore, we explore\nseveral strategies for bias mitigation, including Chain-of-Thought (CoT)\nprompting, combining positive role-playing with CoT prompting and iterative\nprompting. Our experiments show that iterative prompting can effectively reduce\nsocial bias in LLM-generated code by up to 90%. Solar is highly extensible to\nevaluate new social problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced the field of\nautomated code generation. However, a notable research gap exists in the\nevaluation of social biases that may be present in the code produced by LLMs.\nTo solve this issue, we propose a novel fairness framework, i.e., Solar, to\nassess and mitigate the social biases of LLM-generated code. Specifically,\nSolar can automatically generate test cases for quantitatively uncovering\nsocial biases of the auto-generated code by LLMs. To quantify the severity of\nsocial biases in generated code, we develop a dataset that covers a diverse set\nof social problems. We applied Solar and the crafted dataset to four\nstate-of-the-art LLMs for code generation. Our evaluation reveals severe bias\nin the LLM-generated code from all the subject LLMs. Furthermore, we explore\nseveral strategies for bias mitigation, including Chain-of-Thought (CoT)\nprompting, combining positive role-playing with CoT prompting and iterative\nprompting. Our experiments show that iterative prompting can effectively reduce\nsocial bias in LLM-generated code by up to 90%. Solar is highly extensible to\nevaluate new social problems."
                },
                "authors": [
                    {
                        "name": "Lin Ling"
                    },
                    {
                        "name": "Fazle Rabbi"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Jinqiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiu Yang"
                },
                "author": "Jinqiu Yang",
                "arxiv_comment": "9pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04412v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04412v3",
                "updated": "2024-11-15T16:53:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    53,
                    18,
                    4,
                    320,
                    0
                ],
                "published": "2024-05-07T15:39:45Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    15,
                    39,
                    45,
                    1,
                    128,
                    0
                ],
                "title": "The Silicon Ceiling: Auditing GPT's Race and Gender Biases in Hiring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Silicon Ceiling: Auditing GPT's Race and Gender Biases in Hiring"
                },
                "summary": "Large language models (LLMs) are increasingly being introduced in workplace\nsettings, with the goals of improving efficiency and fairness. However,\nconcerns have arisen regarding these models' potential to reflect or exacerbate\nsocial biases and stereotypes. This study explores the potential impact of LLMs\non hiring practices. To do so, we conduct an AI audit of race and gender biases\nin one commonly-used LLM, OpenAI's GPT-3.5, taking inspiration from the history\nof traditional offline resume audits. We conduct two studies using names with\nvaried race and gender connotations: resume assessment (Study 1) and resume\ngeneration (Study 2). In Study 1, we ask GPT to score resumes with 32 different\nnames (4 names for each combination of the 2 gender and 4 racial groups) and\ntwo anonymous options across 10 occupations and 3 evaluation tasks (overall\nrating, willingness to interview, and hireability). We find that the model\nreflects some biases based on stereotypes. In Study 2, we prompt GPT to create\nresumes (10 for each name) for fictitious job candidates. When generating\nresumes, GPT reveals underlying biases; women's resumes had occupations with\nless experience, while Asian and Hispanic resumes had immigrant markers, such\nas non-native English and non-U.S. education and work experiences. Our findings\ncontribute to a growing body of literature on LLM biases, particularly in\nworkplace contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being introduced in workplace\nsettings, with the goals of improving efficiency and fairness. However,\nconcerns have arisen regarding these models' potential to reflect or exacerbate\nsocial biases and stereotypes. This study explores the potential impact of LLMs\non hiring practices. To do so, we conduct an AI audit of race and gender biases\nin one commonly-used LLM, OpenAI's GPT-3.5, taking inspiration from the history\nof traditional offline resume audits. We conduct two studies using names with\nvaried race and gender connotations: resume assessment (Study 1) and resume\ngeneration (Study 2). In Study 1, we ask GPT to score resumes with 32 different\nnames (4 names for each combination of the 2 gender and 4 racial groups) and\ntwo anonymous options across 10 occupations and 3 evaluation tasks (overall\nrating, willingness to interview, and hireability). We find that the model\nreflects some biases based on stereotypes. In Study 2, we prompt GPT to create\nresumes (10 for each name) for fictitious job candidates. When generating\nresumes, GPT reveals underlying biases; women's resumes had occupations with\nless experience, while Asian and Hispanic resumes had immigrant markers, such\nas non-native English and non-U.S. education and work experiences. Our findings\ncontribute to a growing body of literature on LLM biases, particularly in\nworkplace contexts."
                },
                "authors": [
                    {
                        "name": "Lena Armstrong"
                    },
                    {
                        "name": "Abbey Liu"
                    },
                    {
                        "name": "Stephen MacNeil"
                    },
                    {
                        "name": "Danaë Metaxa"
                    }
                ],
                "author_detail": {
                    "name": "Danaë Metaxa"
                },
                "author": "Danaë Metaxa",
                "arxiv_doi": "10.1145/3689904.3694699",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689904.3694699",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.04412v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04412v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10340v1",
                "updated": "2024-11-15T16:40:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    40,
                    43,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:40:43Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    40,
                    43,
                    4,
                    320,
                    0
                ],
                "title": "Domain Adaptation-based Edge Computing for Cross-Conditions Fault\n  Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Adaptation-based Edge Computing for Cross-Conditions Fault\n  Diagnosis"
                },
                "summary": "Fault diagnosis technology supports the healthy operation of mechanical\nequipment. However, the variations conditions during the operation of\nmechanical equipment lead to significant disparities in data distribution,\nposing challenges to fault diagnosis. Furthermore, when deploying applications,\ntraditional methods often encounter issues such as latency and data security.\nTherefore, conducting fault diagnosis and deploying application methods under\ncross-operating conditions holds significant value. This paper proposes a\ndomain adaptation-based lightweight fault diagnosis framework for edge\ncomputing scenarios. Incorporating the local maximum mean discrepancy into\nknowledge transfer aligns the feature distributions of different domains in a\nhigh-dimensional feature space, to discover a common feature space across\ndomains. The acquired fault diagnosis expertise from the cloud-model is\ntransferred to the lightweight edge-model using adaptation knowledge transfer\nmethods. While ensuring real-time diagnostic capabilities, accurate fault\ndiagnosis is achieved across working conditions. We conducted validation\nexperiments on the NVIDIA Jetson Xavier NX kit. In terms of diagnostic\nperformance, the proposed method significantly improved diagnostic accuracy,\nwith average increases of 34.44% and 17.33% compared to the comparison method,\nrespectively. Regarding lightweight effectiveness, proposed method achieved an\naverage inference speed increase of 80.47%. Additionally, compared to the\ncloud-model, the parameter count of the edge-model decreased by 96.37%, while\nthe Flops decreased by 83.08%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault diagnosis technology supports the healthy operation of mechanical\nequipment. However, the variations conditions during the operation of\nmechanical equipment lead to significant disparities in data distribution,\nposing challenges to fault diagnosis. Furthermore, when deploying applications,\ntraditional methods often encounter issues such as latency and data security.\nTherefore, conducting fault diagnosis and deploying application methods under\ncross-operating conditions holds significant value. This paper proposes a\ndomain adaptation-based lightweight fault diagnosis framework for edge\ncomputing scenarios. Incorporating the local maximum mean discrepancy into\nknowledge transfer aligns the feature distributions of different domains in a\nhigh-dimensional feature space, to discover a common feature space across\ndomains. The acquired fault diagnosis expertise from the cloud-model is\ntransferred to the lightweight edge-model using adaptation knowledge transfer\nmethods. While ensuring real-time diagnostic capabilities, accurate fault\ndiagnosis is achieved across working conditions. We conducted validation\nexperiments on the NVIDIA Jetson Xavier NX kit. In terms of diagnostic\nperformance, the proposed method significantly improved diagnostic accuracy,\nwith average increases of 34.44% and 17.33% compared to the comparison method,\nrespectively. Regarding lightweight effectiveness, proposed method achieved an\naverage inference speed increase of 80.47%. Additionally, compared to the\ncloud-model, the parameter count of the edge-model decreased by 96.37%, while\nthe Flops decreased by 83.08%."
                },
                "authors": [
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Chu Wang"
                    },
                    {
                        "name": "Jinhong Wu"
                    },
                    {
                        "name": "Ziyang Yu"
                    },
                    {
                        "name": "Qi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhou"
                },
                "author": "Qi Zhou",
                "arxiv_comment": "28 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10337v1",
                "updated": "2024-11-15T16:36:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    36,
                    21,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:36:21Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    36,
                    21,
                    4,
                    320,
                    0
                ],
                "title": "On the Cost of Model-Serving Frameworks: An Experimental Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Cost of Model-Serving Frameworks: An Experimental Evaluation"
                },
                "summary": "In machine learning (ML), the inference phase is the process of applying\npre-trained models to new, unseen data with the objective of making\npredictions. During the inference phase, end-users interact with ML services to\ngain insights, recommendations, or actions based on the input data. For this\nreason, serving strategies are nowadays crucial for deploying and managing\nmodels in production environments effectively. These strategies ensure that\nmodels are available, scalable, reliable, and performant for real-world\napplications, such as time series forecasting, image classification, natural\nlanguage processing, and so on. In this paper, we evaluate the performances of\nfive widely-used model serving frameworks (TensorFlow Serving, TorchServe,\nMLServer, MLflow, and BentoML) under four different scenarios (malware\ndetection, cryptocoin prices forecasting, image classification, and sentiment\nanalysis). We demonstrate that TensorFlow Serving is able to outperform all the\nother frameworks in serving deep learning (DL) models. Moreover, we show that\nDL-specific frameworks (TensorFlow Serving and TorchServe) display\nsignificantly lower latencies than the three general-purpose ML frameworks\n(BentoML, MLFlow, and MLServer).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In machine learning (ML), the inference phase is the process of applying\npre-trained models to new, unseen data with the objective of making\npredictions. During the inference phase, end-users interact with ML services to\ngain insights, recommendations, or actions based on the input data. For this\nreason, serving strategies are nowadays crucial for deploying and managing\nmodels in production environments effectively. These strategies ensure that\nmodels are available, scalable, reliable, and performant for real-world\napplications, such as time series forecasting, image classification, natural\nlanguage processing, and so on. In this paper, we evaluate the performances of\nfive widely-used model serving frameworks (TensorFlow Serving, TorchServe,\nMLServer, MLflow, and BentoML) under four different scenarios (malware\ndetection, cryptocoin prices forecasting, image classification, and sentiment\nanalysis). We demonstrate that TensorFlow Serving is able to outperform all the\nother frameworks in serving deep learning (DL) models. Moreover, we show that\nDL-specific frameworks (TensorFlow Serving and TorchServe) display\nsignificantly lower latencies than the three general-purpose ML frameworks\n(BentoML, MLFlow, and MLServer)."
                },
                "authors": [
                    {
                        "name": "Pasquale De Rosa"
                    },
                    {
                        "name": "Yérom-David Bromberg"
                    },
                    {
                        "name": "Pascal Felber"
                    },
                    {
                        "name": "Djob Mvondo"
                    },
                    {
                        "name": "Valerio Schiavoni"
                    }
                ],
                "author_detail": {
                    "name": "Valerio Schiavoni"
                },
                "author": "Valerio Schiavoni",
                "arxiv_doi": "10.1109/IC2E61754.2024.00032",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IC2E61754.2024.00032",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.10337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2024 IEEE International Conference on Cloud Engineering (IC2E)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10332v1",
                "updated": "2024-11-15T16:32:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    32,
                    34,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:32:34Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    32,
                    34,
                    4,
                    320,
                    0
                ],
                "title": "Number it: Temporal Grounding Videos like Flipping Manga",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Number it: Temporal Grounding Videos like Flipping Manga"
                },
                "summary": "Video Large Language Models (Vid-LLMs) have made remarkable advancements in\ncomprehending video content for QA dialogue. However, they struggle to extend\nthis visual understanding to tasks requiring precise temporal localization,\nknown as Video Temporal Grounding (VTG). To address this gap, we introduce\nNumber-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual\ncomprehension with temporal grounding by adding unique numerical identifiers to\neach video frame. Treating a video as a sequence of numbered frame images,\nNumPro transforms VTG into an intuitive process: flipping through manga panels\nin sequence. This allows Vid-LLMs to \"read\" event timelines, accurately linking\nvisual content with corresponding temporal information. Our experiments\ndemonstrate that NumPro significantly boosts VTG performance of top-tier\nVid-LLMs without additional computational cost. Furthermore, fine-tuning on a\nNumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing\nprevious top-performing methods by up to 6.9\\% in mIoU for moment retrieval and\n8.5\\% in mAP for highlight detection. The code will be available at\nhttps://github.com/yongliang-wu/NumPro.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (Vid-LLMs) have made remarkable advancements in\ncomprehending video content for QA dialogue. However, they struggle to extend\nthis visual understanding to tasks requiring precise temporal localization,\nknown as Video Temporal Grounding (VTG). To address this gap, we introduce\nNumber-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual\ncomprehension with temporal grounding by adding unique numerical identifiers to\neach video frame. Treating a video as a sequence of numbered frame images,\nNumPro transforms VTG into an intuitive process: flipping through manga panels\nin sequence. This allows Vid-LLMs to \"read\" event timelines, accurately linking\nvisual content with corresponding temporal information. Our experiments\ndemonstrate that NumPro significantly boosts VTG performance of top-tier\nVid-LLMs without additional computational cost. Furthermore, fine-tuning on a\nNumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing\nprevious top-performing methods by up to 6.9\\% in mIoU for moment retrieval and\n8.5\\% in mAP for highlight detection. The code will be available at\nhttps://github.com/yongliang-wu/NumPro."
                },
                "authors": [
                    {
                        "name": "Yongliang Wu"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Yuyang Sun"
                    },
                    {
                        "name": "Yizhou Zhou"
                    },
                    {
                        "name": "Wenbo Zhu"
                    },
                    {
                        "name": "Fengyun Rao"
                    },
                    {
                        "name": "Bernt Schiele"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05818v2",
                "updated": "2024-11-15T16:23:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    23,
                    17,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-02T12:02:09Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    12,
                    2,
                    9,
                    5,
                    307,
                    0
                ],
                "title": "Open LLMs are Necessary for Current Private Adaptations and Outperform\n  their Closed Alternatives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open LLMs are Necessary for Current Private Adaptations and Outperform\n  their Closed Alternatives"
                },
                "summary": "While open Large Language Models (LLMs) have made significant progress, they\nstill fall short of matching the performance of their closed, proprietary\ncounterparts, making the latter attractive even for the use on highly private\ndata. Recently, various new methods have been proposed to adapt closed LLMs to\nprivate data without leaking private information to third parties and/or the\nLLM provider. In this work, we analyze the privacy protection and performance\nof the four most recent methods for private adaptation of closed LLMs. By\nexamining their threat models and thoroughly comparing their performance under\ndifferent privacy levels according to differential privacy (DP), various LLM\narchitectures, and multiple datasets for classification and generation tasks,\nwe find that: (1) all the methods leak query data, i.e., the (potentially\nsensitive) user data that is queried at inference time, to the LLM provider,\n(2) three out of four methods also leak large fractions of private training\ndata to the LLM provider while the method that protects private data requires a\nlocal open LLM, (3) all the methods exhibit lower performance compared to three\nprivate gradient-based adaptation methods for local open LLMs, and (4) the\nprivate adaptation methods for closed LLMs incur higher monetary training and\nquery costs than running the alternative methods on local open LLMs. This\nyields the conclusion that, to achieve truly privacy-preserving LLM adaptations\nthat yield high performance and more privacy at lower costs, taking into\naccount current methods and models, one should use open LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While open Large Language Models (LLMs) have made significant progress, they\nstill fall short of matching the performance of their closed, proprietary\ncounterparts, making the latter attractive even for the use on highly private\ndata. Recently, various new methods have been proposed to adapt closed LLMs to\nprivate data without leaking private information to third parties and/or the\nLLM provider. In this work, we analyze the privacy protection and performance\nof the four most recent methods for private adaptation of closed LLMs. By\nexamining their threat models and thoroughly comparing their performance under\ndifferent privacy levels according to differential privacy (DP), various LLM\narchitectures, and multiple datasets for classification and generation tasks,\nwe find that: (1) all the methods leak query data, i.e., the (potentially\nsensitive) user data that is queried at inference time, to the LLM provider,\n(2) three out of four methods also leak large fractions of private training\ndata to the LLM provider while the method that protects private data requires a\nlocal open LLM, (3) all the methods exhibit lower performance compared to three\nprivate gradient-based adaptation methods for local open LLMs, and (4) the\nprivate adaptation methods for closed LLMs incur higher monetary training and\nquery costs than running the alternative methods on local open LLMs. This\nyields the conclusion that, to achieve truly privacy-preserving LLM adaptations\nthat yield high performance and more privacy at lower costs, taking into\naccount current methods and models, one should use open LLMs."
                },
                "authors": [
                    {
                        "name": "Vincent Hanke"
                    },
                    {
                        "name": "Tom Blanchard"
                    },
                    {
                        "name": "Franziska Boenisch"
                    },
                    {
                        "name": "Iyiola Emmanuel Olatunji"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Adam Dziedzic"
                    }
                ],
                "author_detail": {
                    "name": "Adam Dziedzic"
                },
                "author": "Adam Dziedzic",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10316v1",
                "updated": "2024-11-15T16:14:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    14,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:14:48Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    14,
                    48,
                    4,
                    320,
                    0
                ],
                "title": "M3TR: Generalist HD Map Construction with Variable Map Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M3TR: Generalist HD Map Construction with Variable Map Priors"
                },
                "summary": "Autonomous vehicles require road information for their operation, usually in\nform of HD maps. Since offline maps eventually become outdated or may only be\npartially available, online HD map construction methods have been proposed to\ninfer map information from live sensor data. A key issue remains how to exploit\nsuch partial or outdated map information as a prior. We introduce M3TR\n(Multi-Masking Map Transformer), a generalist approach for HD map construction\nboth with and without map priors. We address shortcomings in ground truth\ngeneration for Argoverse 2 and nuScenes and propose the first realistic\nscenarios with semantically diverse map priors. Examining various query\ndesigns, we use an improved method for integrating prior map elements into a HD\nmap construction model, increasing performance by +4.3 mAP. Finally, we show\nthat training across all prior scenarios yields a single Generalist model,\nwhose performance is on par with previous Expert models that can handle only\none specific type of map prior. M3TR thus is the first model capable of\nleveraging variable map priors, making it suitable for real-world deployment.\nCode is available at https://github.com/immel-f/m3tr",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous vehicles require road information for their operation, usually in\nform of HD maps. Since offline maps eventually become outdated or may only be\npartially available, online HD map construction methods have been proposed to\ninfer map information from live sensor data. A key issue remains how to exploit\nsuch partial or outdated map information as a prior. We introduce M3TR\n(Multi-Masking Map Transformer), a generalist approach for HD map construction\nboth with and without map priors. We address shortcomings in ground truth\ngeneration for Argoverse 2 and nuScenes and propose the first realistic\nscenarios with semantically diverse map priors. Examining various query\ndesigns, we use an improved method for integrating prior map elements into a HD\nmap construction model, increasing performance by +4.3 mAP. Finally, we show\nthat training across all prior scenarios yields a single Generalist model,\nwhose performance is on par with previous Expert models that can handle only\none specific type of map prior. M3TR thus is the first model capable of\nleveraging variable map priors, making it suitable for real-world deployment.\nCode is available at https://github.com/immel-f/m3tr"
                },
                "authors": [
                    {
                        "name": "Fabian Immel"
                    },
                    {
                        "name": "Richard Fehler"
                    },
                    {
                        "name": "Frank Bieder"
                    },
                    {
                        "name": "Jan-Hendrik Pauls"
                    },
                    {
                        "name": "Christoph Stiller"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Stiller"
                },
                "author": "Christoph Stiller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10311v1",
                "updated": "2024-11-15T16:08:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    8,
                    19,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:08:19Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    8,
                    19,
                    4,
                    320,
                    0
                ],
                "title": "Planar Novikov-Shubin invariant for adjacency matrices of structured\n  directed dense random graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planar Novikov-Shubin invariant for adjacency matrices of structured\n  directed dense random graphs"
                },
                "summary": "The Novikov-Shubin invariant associated to a graph provides information about\nthe accumulation of eigenvalues of the corresponding adjacency matrix close to\nthe origin. For a directed graph these eigenvalues lie in the complex plane and\nhaving a finite value for the planar Novikov-Shubin invariant indicates a\npolynomial behaviour of the eigenvalue density as a function of the distance to\nzero. We provide a complete description of these invariants for dense random\ndigraphs with constant batch sizes, i.e. for the directed stochastic block\nmodel. The invariants depend only on which batches in the graph are connected\nby non-zero edge densities. We present an explicit finite step algorithm for\ntheir computation. For the proof we identify the asymptotic spectral density\nwith the distribution of a $\\mathbb{C}^K$-valued circular element in\noperator-valued free probability theory. We determine the spectral density in\nthe bulk regime by solving the associated Dyson equation and infer the singular\nbehaviour of this density close to the origin by determining the exponents\nassociated to the power law with which the resolvent entries of the adjacency\nmatrix that corresponds to the individual batches diverge to infinity or\nconverge to zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Novikov-Shubin invariant associated to a graph provides information about\nthe accumulation of eigenvalues of the corresponding adjacency matrix close to\nthe origin. For a directed graph these eigenvalues lie in the complex plane and\nhaving a finite value for the planar Novikov-Shubin invariant indicates a\npolynomial behaviour of the eigenvalue density as a function of the distance to\nzero. We provide a complete description of these invariants for dense random\ndigraphs with constant batch sizes, i.e. for the directed stochastic block\nmodel. The invariants depend only on which batches in the graph are connected\nby non-zero edge densities. We present an explicit finite step algorithm for\ntheir computation. For the proof we identify the asymptotic spectral density\nwith the distribution of a $\\mathbb{C}^K$-valued circular element in\noperator-valued free probability theory. We determine the spectral density in\nthe bulk regime by solving the associated Dyson equation and infer the singular\nbehaviour of this density close to the origin by determining the exponents\nassociated to the power law with which the resolvent entries of the adjacency\nmatrix that corresponds to the individual batches diverge to infinity or\nconverge to zero."
                },
                "authors": [
                    {
                        "name": "Torben Krüger"
                    },
                    {
                        "name": "David Renfrew"
                    }
                ],
                "author_detail": {
                    "name": "David Renfrew"
                },
                "author": "David Renfrew",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60B20, 15B52, 46Txx",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10294v1",
                "updated": "2024-11-15T15:52:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    52,
                    15,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T15:52:15Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    52,
                    15,
                    4,
                    320,
                    0
                ],
                "title": "Static network structure cannot stabilize cooperation among Large\n  Language Model agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static network structure cannot stabilize cooperation among Large\n  Language Model agents"
                },
                "summary": "Large language models (LLMs) are increasingly used to model human social\nbehavior, with recent research exploring their ability to simulate social\ndynamics. Here, we test whether LLMs mirror human behavior in social dilemmas,\nwhere individual and collective interests conflict. Humans generally cooperate\nmore than expected in laboratory settings, showing less cooperation in\nwell-mixed populations but more in fixed networks. In contrast, LLMs tend to\nexhibit greater cooperation in well-mixed settings. This raises a key question:\nAre LLMs about to emulate human behavior in cooperative dilemmas on networks?\nIn this study, we examine networked interactions where agents repeatedly engage\nin the Prisoner's Dilemma within both well-mixed and structured network\nconfigurations, aiming to identify parallels in cooperative behavior between\nLLMs and humans. Our findings indicate critical distinctions: while humans tend\nto cooperate more within structured networks, LLMs display increased\ncooperation mainly in well-mixed environments, with limited adjustment to\nnetworked contexts. Notably, LLM cooperation also varies across model types,\nillustrating the complexities of replicating human-like social adaptability in\nartificial agents. These results highlight a crucial gap: LLMs struggle to\nemulate the nuanced, adaptive social strategies humans deploy in fixed\nnetworks. Unlike human participants, LLMs do not alter their cooperative\nbehavior in response to network structures or evolving social contexts, missing\nthe reciprocity norms that humans adaptively employ. This limitation points to\na fundamental need in future LLM design -- to integrate a deeper comprehension\nof social norms, enabling more authentic modeling of human-like cooperation and\nadaptability in networked environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used to model human social\nbehavior, with recent research exploring their ability to simulate social\ndynamics. Here, we test whether LLMs mirror human behavior in social dilemmas,\nwhere individual and collective interests conflict. Humans generally cooperate\nmore than expected in laboratory settings, showing less cooperation in\nwell-mixed populations but more in fixed networks. In contrast, LLMs tend to\nexhibit greater cooperation in well-mixed settings. This raises a key question:\nAre LLMs about to emulate human behavior in cooperative dilemmas on networks?\nIn this study, we examine networked interactions where agents repeatedly engage\nin the Prisoner's Dilemma within both well-mixed and structured network\nconfigurations, aiming to identify parallels in cooperative behavior between\nLLMs and humans. Our findings indicate critical distinctions: while humans tend\nto cooperate more within structured networks, LLMs display increased\ncooperation mainly in well-mixed environments, with limited adjustment to\nnetworked contexts. Notably, LLM cooperation also varies across model types,\nillustrating the complexities of replicating human-like social adaptability in\nartificial agents. These results highlight a crucial gap: LLMs struggle to\nemulate the nuanced, adaptive social strategies humans deploy in fixed\nnetworks. Unlike human participants, LLMs do not alter their cooperative\nbehavior in response to network structures or evolving social contexts, missing\nthe reciprocity norms that humans adaptively employ. This limitation points to\na fundamental need in future LLM design -- to integrate a deeper comprehension\nof social norms, enabling more authentic modeling of human-like cooperation and\nadaptability in networked environments."
                },
                "authors": [
                    {
                        "name": "Jin Han"
                    },
                    {
                        "name": "Balaraju Battu"
                    },
                    {
                        "name": "Ivan Romić"
                    },
                    {
                        "name": "Talal Rahwan"
                    },
                    {
                        "name": "Petter Holme"
                    }
                ],
                "author_detail": {
                    "name": "Petter Holme"
                },
                "author": "Petter Holme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10281v1",
                "updated": "2024-11-15T15:36:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    36,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T15:36:48Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    36,
                    48,
                    4,
                    320,
                    0
                ],
                "title": "Multidimensional Byte Pair Encoding: Shortened Sequences for Improved\n  Visual Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multidimensional Byte Pair Encoding: Shortened Sequences for Improved\n  Visual Data Generation"
                },
                "summary": "In language processing, transformers benefit greatly from text being\ncondensed. This is achieved through a larger vocabulary that captures word\nfragments instead of plain characters. This is often done with Byte Pair\nEncoding. In the context of images, tokenisation of visual data is usually\nlimited to regular grids obtained from quantisation methods, without global\ncontent awareness. Our work improves tokenisation of visual data by bringing\nByte Pair Encoding from 1D to multiple dimensions, as a complementary add-on to\nexisting compression. We achieve this through counting constellations of token\npairs and replacing the most frequent token pair with a newly introduced token.\nThe multidimensionality only increases the computation time by a factor of 2\nfor images, making it applicable even to large datasets like ImageNet within\nminutes on consumer hardware. This is a lossless preprocessing step. Our\nevaluation shows improved training and inference performance of transformers on\nvisual data achieved by compressing frequent constellations of tokens: The\nresulting sequences are shorter, with more uniformly distributed information\ncontent, e.g. condensing empty regions in an image into single tokens. As our\nexperiments show, these condensed sequences are easier to process. We\nadditionally introduce a strategy to amplify this compression further by\nclustering the vocabulary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In language processing, transformers benefit greatly from text being\ncondensed. This is achieved through a larger vocabulary that captures word\nfragments instead of plain characters. This is often done with Byte Pair\nEncoding. In the context of images, tokenisation of visual data is usually\nlimited to regular grids obtained from quantisation methods, without global\ncontent awareness. Our work improves tokenisation of visual data by bringing\nByte Pair Encoding from 1D to multiple dimensions, as a complementary add-on to\nexisting compression. We achieve this through counting constellations of token\npairs and replacing the most frequent token pair with a newly introduced token.\nThe multidimensionality only increases the computation time by a factor of 2\nfor images, making it applicable even to large datasets like ImageNet within\nminutes on consumer hardware. This is a lossless preprocessing step. Our\nevaluation shows improved training and inference performance of transformers on\nvisual data achieved by compressing frequent constellations of tokens: The\nresulting sequences are shorter, with more uniformly distributed information\ncontent, e.g. condensing empty regions in an image into single tokens. As our\nexperiments show, these condensed sequences are easier to process. We\nadditionally introduce a strategy to amplify this compression further by\nclustering the vocabulary."
                },
                "authors": [
                    {
                        "name": "Tim Elsner"
                    },
                    {
                        "name": "Paula Usinger"
                    },
                    {
                        "name": "Julius Nehring-Wirxel"
                    },
                    {
                        "name": "Gregor Kobsik"
                    },
                    {
                        "name": "Victor Czech"
                    },
                    {
                        "name": "Yanjiang He"
                    },
                    {
                        "name": "Isaak Lim"
                    },
                    {
                        "name": "Leif Kobbelt"
                    }
                ],
                "author_detail": {
                    "name": "Leif Kobbelt"
                },
                "author": "Leif Kobbelt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10272v1",
                "updated": "2024-11-15T15:28:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    28,
                    42,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T15:28:42Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    28,
                    42,
                    4,
                    320,
                    0
                ],
                "title": "Scaling Law for Post-training after Model Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Law for Post-training after Model Pruning"
                },
                "summary": "Large language models (LLMs) based on the Transformer architecture are widely\nemployed across various domains and tasks. However, their increasing size\nimposes significant hardware demands, limiting practical deployment. To\nmitigate this, model pruning techniques have been developed to create more\nefficient models while maintaining high performance. Despite this,\npost-training after pruning is crucial for performance recovery and can be\nresource-intensive. This paper investigates the post-training requirements of\npruned LLMs and introduces a scaling law to determine the optimal amount of\npost-training data. Post-training experiments with the Llama-3 and Qwen-2.5\nseries models, pruned using depth pruning, width pruning, and 2:4\nsemi-structured pruning, show that higher pruning ratios necessitate more\npost-training data for performance recovery, whereas larger LLMs require less.\nThe proposed scaling law predicts a model's loss based on its parameter counts\nbefore and after pruning, as well as the post-training token counts.\nFurthermore, we find that the scaling law established from smaller LLMs can be\nreliably extrapolated to larger LLMs. This work provides valuable insights into\nthe post-training of pruned LLMs and offers a practical scaling law for\noptimizing post-training data usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on the Transformer architecture are widely\nemployed across various domains and tasks. However, their increasing size\nimposes significant hardware demands, limiting practical deployment. To\nmitigate this, model pruning techniques have been developed to create more\nefficient models while maintaining high performance. Despite this,\npost-training after pruning is crucial for performance recovery and can be\nresource-intensive. This paper investigates the post-training requirements of\npruned LLMs and introduces a scaling law to determine the optimal amount of\npost-training data. Post-training experiments with the Llama-3 and Qwen-2.5\nseries models, pruned using depth pruning, width pruning, and 2:4\nsemi-structured pruning, show that higher pruning ratios necessitate more\npost-training data for performance recovery, whereas larger LLMs require less.\nThe proposed scaling law predicts a model's loss based on its parameter counts\nbefore and after pruning, as well as the post-training token counts.\nFurthermore, we find that the scaling law established from smaller LLMs can be\nreliably extrapolated to larger LLMs. This work provides valuable insights into\nthe post-training of pruned LLMs and offers a practical scaling law for\noptimizing post-training data usage."
                },
                "authors": [
                    {
                        "name": "Xiaodong Chen"
                    },
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Cuiping Li"
                    },
                    {
                        "name": "Hong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hong Chen"
                },
                "author": "Hong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10261v1",
                "updated": "2024-11-15T15:08:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    8,
                    4,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T15:08:04Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    8,
                    4,
                    4,
                    320,
                    0
                ],
                "title": "Partial Scene Text Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partial Scene Text Retrieval"
                },
                "summary": "The task of partial scene text retrieval involves localizing and searching\nfor text instances that are the same or similar to a given query text from an\nimage gallery. However, existing methods can only handle text-line instances,\nleaving the problem of searching for partial patches within these text-line\ninstances unsolved due to a lack of patch annotations in the training data. To\naddress this issue, we propose a network that can simultaneously retrieve both\ntext-line instances and their partial patches. Our method embeds the two types\nof data (query text and scene text instances) into a shared feature space and\nmeasures their cross-modal similarities. To handle partial patches, our\nproposed approach adopts a Multiple Instance Learning (MIL) approach to learn\ntheir similarities with query text, without requiring extra annotations.\nHowever, constructing bags, which is a standard step of conventional MIL\napproaches, can introduce numerous noisy samples for training, and lower\ninference speed. To address this issue, we propose a Ranking MIL (RankMIL)\napproach to adaptively filter those noisy samples. Additionally, we present a\nDynamic Partial Match Algorithm (DPMA) that can directly search for the target\npartial patch from a text-line instance during the inference stage, without\nrequiring bags. This greatly improves the search efficiency and the performance\nof retrieving partial patches. The source code and dataset are available at\nhttps://github.com/lanfeng4659/PSTR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of partial scene text retrieval involves localizing and searching\nfor text instances that are the same or similar to a given query text from an\nimage gallery. However, existing methods can only handle text-line instances,\nleaving the problem of searching for partial patches within these text-line\ninstances unsolved due to a lack of patch annotations in the training data. To\naddress this issue, we propose a network that can simultaneously retrieve both\ntext-line instances and their partial patches. Our method embeds the two types\nof data (query text and scene text instances) into a shared feature space and\nmeasures their cross-modal similarities. To handle partial patches, our\nproposed approach adopts a Multiple Instance Learning (MIL) approach to learn\ntheir similarities with query text, without requiring extra annotations.\nHowever, constructing bags, which is a standard step of conventional MIL\napproaches, can introduce numerous noisy samples for training, and lower\ninference speed. To address this issue, we propose a Ranking MIL (RankMIL)\napproach to adaptively filter those noisy samples. Additionally, we present a\nDynamic Partial Match Algorithm (DPMA) that can directly search for the target\npartial patch from a text-line instance during the inference stage, without\nrequiring bags. This greatly improves the search efficiency and the performance\nof retrieving partial patches. The source code and dataset are available at\nhttps://github.com/lanfeng4659/PSTR."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Minghui Liao"
                    },
                    {
                        "name": "Zhouyi Xie"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "Accepted on TPAMI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17710v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17710v3",
                "updated": "2024-11-15T14:57:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    14,
                    57,
                    28,
                    4,
                    320,
                    0
                ],
                "published": "2024-03-26T13:58:00Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    13,
                    58,
                    0,
                    1,
                    86,
                    0
                ],
                "title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge"
                },
                "summary": "LLM-as-a-Judge uses a large language model (LLM) to select the best response\nfrom a set of candidates for a given question. LLM-as-a-Judge has many\napplications such as LLM-powered search, reinforcement learning with AI\nfeedback (RLAIF), and tool selection. In this work, we propose JudgeDeceiver,\nan optimization-based prompt injection attack to LLM-as-a-Judge. JudgeDeceiver\ninjects a carefully crafted sequence into an attacker-controlled candidate\nresponse such that LLM-as-a-Judge selects the candidate response for an\nattacker-chosen question no matter what other candidate responses are.\nSpecifically, we formulate finding such sequence as an optimization problem and\npropose a gradient based method to approximately solve it. Our extensive\nevaluation shows that JudgeDeceive is highly effective, and is much more\neffective than existing prompt injection attacks that manually craft the\ninjected sequences and jailbreak attacks when extended to our problem. We also\nshow the effectiveness of JudgeDeceiver in three case studies, i.e.,\nLLM-powered search, RLAIF, and tool selection. Moreover, we consider defenses\nincluding known-answer detection, perplexity detection, and perplexity windowed\ndetection. Our results show these defenses are insufficient, highlighting the\nurgent need for developing new defense strategies. Our implementation is\navailable at this repository: https://github.com/ShiJiawenwen/JudgeDeceiver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge uses a large language model (LLM) to select the best response\nfrom a set of candidates for a given question. LLM-as-a-Judge has many\napplications such as LLM-powered search, reinforcement learning with AI\nfeedback (RLAIF), and tool selection. In this work, we propose JudgeDeceiver,\nan optimization-based prompt injection attack to LLM-as-a-Judge. JudgeDeceiver\ninjects a carefully crafted sequence into an attacker-controlled candidate\nresponse such that LLM-as-a-Judge selects the candidate response for an\nattacker-chosen question no matter what other candidate responses are.\nSpecifically, we formulate finding such sequence as an optimization problem and\npropose a gradient based method to approximately solve it. Our extensive\nevaluation shows that JudgeDeceive is highly effective, and is much more\neffective than existing prompt injection attacks that manually craft the\ninjected sequences and jailbreak attacks when extended to our problem. We also\nshow the effectiveness of JudgeDeceiver in three case studies, i.e.,\nLLM-powered search, RLAIF, and tool selection. Moreover, we consider defenses\nincluding known-answer detection, perplexity detection, and perplexity windowed\ndetection. Our results show these defenses are insufficient, highlighting the\nurgent need for developing new defense strategies. Our implementation is\navailable at this repository: https://github.com/ShiJiawenwen/JudgeDeceiver."
                },
                "authors": [
                    {
                        "name": "Jiawen Shi"
                    },
                    {
                        "name": "Zenghui Yuan"
                    },
                    {
                        "name": "Yinuo Liu"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Neil Zhenqiang Gong"
                },
                "author": "Neil Zhenqiang Gong",
                "arxiv_comment": "To appear in the Proceedings of The ACM Conference on Computer and\n  Communications Security (CCS), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17710v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17710v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10223v1",
                "updated": "2024-11-15T14:36:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    14,
                    36,
                    46,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T14:36:46Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    14,
                    36,
                    46,
                    4,
                    320,
                    0
                ],
                "title": "Ultra High Energy Cosmic Rays versus Models of High Energy Hadronic\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra High Energy Cosmic Rays versus Models of High Energy Hadronic\n  Interactions"
                },
                "summary": "We evaluate the consistency of hadronic interaction models in the CORSIKA\nsimulation package with publicly available fluorescence telescope data from the\nPierre Auger Observatory. By comparing the first few central moments of the\nextended air shower depth maximum distributions, as extracted from measured\nevents, to those predicted by the best-fit inferred compositions, we derive a\nstatistical measure of the consistency of a given hadronic model with data. To\nmitigate possible systematic biases, we include all primaries up to iron,\ncompensate for the differences between the measured and simulated energy\nspectra of cosmic rays and account for other known systematic effects.\nAdditionally, we study the effects of including higher central moments in the\nfit and project our results to larger statistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We evaluate the consistency of hadronic interaction models in the CORSIKA\nsimulation package with publicly available fluorescence telescope data from the\nPierre Auger Observatory. By comparing the first few central moments of the\nextended air shower depth maximum distributions, as extracted from measured\nevents, to those predicted by the best-fit inferred compositions, we derive a\nstatistical measure of the consistency of a given hadronic model with data. To\nmitigate possible systematic biases, we include all primaries up to iron,\ncompensate for the differences between the measured and simulated energy\nspectra of cosmic rays and account for other known systematic effects.\nAdditionally, we study the effects of including higher central moments in the\nfit and project our results to larger statistics."
                },
                "authors": [
                    {
                        "name": "Blaž Bortolato"
                    },
                    {
                        "name": "Jernej F. Kamenik"
                    },
                    {
                        "name": "Michele Tammaro"
                    }
                ],
                "author_detail": {
                    "name": "Michele Tammaro"
                },
                "author": "Michele Tammaro",
                "arxiv_comment": "9 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10218v1",
                "updated": "2024-11-15T14:24:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    14,
                    24,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T14:24:38Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    14,
                    24,
                    38,
                    4,
                    320,
                    0
                ],
                "title": "Bayesian Adaptive Tucker Decompositions for Tensor Factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Adaptive Tucker Decompositions for Tensor Factorization"
                },
                "summary": "Tucker tensor decomposition offers a more effective representation for\nmultiway data compared to the widely used PARAFAC model. However, its\nflexibility brings the challenge of selecting the appropriate latent\nmulti-rank. To overcome the issue of pre-selecting the latent multi-rank, we\nintroduce a Bayesian adaptive Tucker decomposition model that infers the\nmulti-rank automatically via an infinite increasing shrinkage prior. The model\nintroduces local sparsity in the core tensor, inducing rich and at the same\ntime parsimonious dependency structures. Posterior inference proceeds via an\nefficient adaptive Gibbs sampler, supporting both continuous and binary data\nand allowing for straightforward missing data imputation when dealing with\nincomplete multiway data. We discuss fundamental properties of the proposed\nmodeling framework, providing theoretical justification. Simulation studies and\napplications to chemometrics and complex ecological data offer compelling\nevidence of its advantages over existing tensor factorization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tucker tensor decomposition offers a more effective representation for\nmultiway data compared to the widely used PARAFAC model. However, its\nflexibility brings the challenge of selecting the appropriate latent\nmulti-rank. To overcome the issue of pre-selecting the latent multi-rank, we\nintroduce a Bayesian adaptive Tucker decomposition model that infers the\nmulti-rank automatically via an infinite increasing shrinkage prior. The model\nintroduces local sparsity in the core tensor, inducing rich and at the same\ntime parsimonious dependency structures. Posterior inference proceeds via an\nefficient adaptive Gibbs sampler, supporting both continuous and binary data\nand allowing for straightforward missing data imputation when dealing with\nincomplete multiway data. We discuss fundamental properties of the proposed\nmodeling framework, providing theoretical justification. Simulation studies and\napplications to chemometrics and complex ecological data offer compelling\nevidence of its advantages over existing tensor factorization methods."
                },
                "authors": [
                    {
                        "name": "Federica Stolf"
                    },
                    {
                        "name": "Antonio Canale"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Canale"
                },
                "author": "Antonio Canale",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10213v1",
                "updated": "2024-11-15T14:19:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    14,
                    19,
                    15,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T14:19:15Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    14,
                    19,
                    15,
                    4,
                    320,
                    0
                ],
                "title": "An Empirical Study on LLM-based Agents for Automated Bug Fixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on LLM-based Agents for Automated Bug Fixing"
                },
                "summary": "Large language models (LLMs) and LLM-based Agents have been applied to fix\nbugs automatically, demonstrating the capability in addressing software defects\nby engaging in development environment interaction, iterative validation and\ncode modification. However, systematic analysis of these agent and non-agent\nsystems remain limited, particularly regarding performance variations among\ntop-performing ones. In this paper, we examine seven proprietary and\nopen-source systems on the SWE-bench Lite benchmark for automated bug fixing.\nWe first assess each system's overall performance, noting instances solvable by\nall or none of these sytems, and explore why some instances are uniquely solved\nby specific system types. We also compare fault localization accuracy at file\nand line levels and evaluate bug reproduction capabilities, identifying\ninstances solvable only through dynamic reproduction. Through analysis, we\nconcluded that further optimization is needed in both the LLM itself and the\ndesign of Agentic flow to improve the effectiveness of the Agent in bug fixing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and LLM-based Agents have been applied to fix\nbugs automatically, demonstrating the capability in addressing software defects\nby engaging in development environment interaction, iterative validation and\ncode modification. However, systematic analysis of these agent and non-agent\nsystems remain limited, particularly regarding performance variations among\ntop-performing ones. In this paper, we examine seven proprietary and\nopen-source systems on the SWE-bench Lite benchmark for automated bug fixing.\nWe first assess each system's overall performance, noting instances solvable by\nall or none of these sytems, and explore why some instances are uniquely solved\nby specific system types. We also compare fault localization accuracy at file\nand line levels and evaluate bug reproduction capabilities, identifying\ninstances solvable only through dynamic reproduction. Through analysis, we\nconcluded that further optimization is needed in both the LLM itself and the\ndesign of Agentic flow to improve the effectiveness of the Agent in bug fixing."
                },
                "authors": [
                    {
                        "name": "Xiangxin Meng"
                    },
                    {
                        "name": "Zexiong Ma"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Chao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Chao Peng"
                },
                "author": "Chao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10204v1",
                "updated": "2024-11-15T14:10:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    14,
                    10,
                    52,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T14:10:52Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    14,
                    10,
                    52,
                    4,
                    320,
                    0
                ],
                "title": "Fused Gromov-Wasserstein Variance Decomposition with Linear Optimal\n  Transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fused Gromov-Wasserstein Variance Decomposition with Linear Optimal\n  Transport"
                },
                "summary": "Wasserstein distances form a family of metrics on spaces of probability\nmeasures that have recently seen many applications. However, statistical\nanalysis in these spaces is complex due to the nonlinearity of Wasserstein\nspaces. One potential solution to this problem is Linear Optimal Transport\n(LOT). This method allows one to find a Euclidean embedding, called LOT\nembedding, of measures in some Wasserstein spaces, but some information is lost\nin this embedding. So, to understand whether statistical analysis relying on\nLOT embeddings can make valid inferences about original data, it is helpful to\nquantify how well these embeddings describe that data. To answer this question,\nwe present a decomposition of the Fr\\'echet variance of a set of measures in\nthe 2-Wasserstein space, which allows one to compute the percentage of variance\nexplained by LOT embeddings of those measures. We then extend this\ndecomposition to the Fused Gromov-Wasserstein setting. We also present several\nexperiments that explore the relationship between the dimension of the LOT\nembedding, the percentage of variance explained by the embedding, and the\nclassification accuracy of machine learning classifiers built on the embedded\ndata. We use the MNIST handwritten digits dataset, IMDB-50000 dataset, and\nDiffusion Tensor MRI images for these experiments. Our results illustrate the\neffectiveness of low dimensional LOT embeddings in terms of the percentage of\nvariance explained and the classification accuracy of models built on the\nembedded data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wasserstein distances form a family of metrics on spaces of probability\nmeasures that have recently seen many applications. However, statistical\nanalysis in these spaces is complex due to the nonlinearity of Wasserstein\nspaces. One potential solution to this problem is Linear Optimal Transport\n(LOT). This method allows one to find a Euclidean embedding, called LOT\nembedding, of measures in some Wasserstein spaces, but some information is lost\nin this embedding. So, to understand whether statistical analysis relying on\nLOT embeddings can make valid inferences about original data, it is helpful to\nquantify how well these embeddings describe that data. To answer this question,\nwe present a decomposition of the Fr\\'echet variance of a set of measures in\nthe 2-Wasserstein space, which allows one to compute the percentage of variance\nexplained by LOT embeddings of those measures. We then extend this\ndecomposition to the Fused Gromov-Wasserstein setting. We also present several\nexperiments that explore the relationship between the dimension of the LOT\nembedding, the percentage of variance explained by the embedding, and the\nclassification accuracy of machine learning classifiers built on the embedded\ndata. We use the MNIST handwritten digits dataset, IMDB-50000 dataset, and\nDiffusion Tensor MRI images for these experiments. Our results illustrate the\neffectiveness of low dimensional LOT embeddings in terms of the percentage of\nvariance explained and the classification accuracy of models built on the\nembedded data."
                },
                "authors": [
                    {
                        "name": "Michael Wilson"
                    },
                    {
                        "name": "Tom Needham"
                    },
                    {
                        "name": "Anuj Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Srivastava"
                },
                "author": "Anuj Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08119v2",
                "updated": "2024-11-15T13:57:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    13,
                    57,
                    6,
                    4,
                    320,
                    0
                ],
                "published": "2024-10-10T17:02:48Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    2,
                    48,
                    3,
                    284,
                    0
                ],
                "title": "Q-VLM: Post-training Quantization for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-VLM: Post-training Quantization for Large Vision-Language Models"
                },
                "summary": "In this paper, we propose a post-training quantization framework of large\nvision-language models (LVLMs) for efficient multi-modal inference.\nConventional quantization methods sequentially search the layer-wise rounding\nfunctions by minimizing activation discretization errors, which fails to\nacquire optimal quantization strategy without considering cross-layer\ndependency. On the contrary, we mine the cross-layer dependency that\nsignificantly influences discretization errors of the entire vision-language\nmodel, and embed this dependency into optimal quantization strategy searching\nwith low search cost. Specifically, we observe the strong correlation between\nthe activation entropy and the cross-layer dependency concerning output\ndiscretization errors. Therefore, we employ the entropy as the proxy to\npartition blocks optimally, which aims to achieve satisfying trade-offs between\ndiscretization errors and the search cost. Moreover, we optimize the visual\nencoder to disentangle the cross-layer dependency for fine-grained\ndecomposition of search space, so that the search cost is further reduced\nwithout harming the quantization accuracy. Experimental results demonstrate\nthat our method compresses the memory by 2.78x and increase generate speed by\n1.44x about 13B LLaVA model without performance degradation on diverse\nmulti-modal reasoning tasks. Code is available at\nhttps://github.com/ChangyuanWang17/QVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a post-training quantization framework of large\nvision-language models (LVLMs) for efficient multi-modal inference.\nConventional quantization methods sequentially search the layer-wise rounding\nfunctions by minimizing activation discretization errors, which fails to\nacquire optimal quantization strategy without considering cross-layer\ndependency. On the contrary, we mine the cross-layer dependency that\nsignificantly influences discretization errors of the entire vision-language\nmodel, and embed this dependency into optimal quantization strategy searching\nwith low search cost. Specifically, we observe the strong correlation between\nthe activation entropy and the cross-layer dependency concerning output\ndiscretization errors. Therefore, we employ the entropy as the proxy to\npartition blocks optimally, which aims to achieve satisfying trade-offs between\ndiscretization errors and the search cost. Moreover, we optimize the visual\nencoder to disentangle the cross-layer dependency for fine-grained\ndecomposition of search space, so that the search cost is further reduced\nwithout harming the quantization accuracy. Experimental results demonstrate\nthat our method compresses the memory by 2.78x and increase generate speed by\n1.44x about 13B LLaVA model without performance degradation on diverse\nmulti-modal reasoning tasks. Code is available at\nhttps://github.com/ChangyuanWang17/QVLM."
                },
                "authors": [
                    {
                        "name": "Changyuan Wang"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Xiuwei Xu"
                    },
                    {
                        "name": "Yansong Tang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10184v1",
                "updated": "2024-11-15T13:33:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    13,
                    33,
                    10,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T13:33:10Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    13,
                    33,
                    10,
                    4,
                    320,
                    0
                ],
                "title": "Agentic LLMs in the Supply Chain: Towards Autonomous Multi-Agent\n  Consensus-Seeking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic LLMs in the Supply Chain: Towards Autonomous Multi-Agent\n  Consensus-Seeking"
                },
                "summary": "This paper explores how Large Language Models (LLMs) can automate\nconsensus-seeking in supply chain management (SCM), where frequent decisions on\nproblems such as inventory levels and delivery times require coordination among\ncompanies. Traditional SCM relies on human consensus in decision-making to\navoid emergent problems like the bullwhip effect. Some routine consensus\nprocesses, especially those that are time-intensive and costly, can be\nautomated. Existing solutions for automated coordination have faced challenges\ndue to high entry barriers locking out SMEs, limited capabilities, and limited\nadaptability in complex scenarios. However, recent advances in Generative AI,\nparticularly LLMs, show promise in overcoming these barriers. LLMs, trained on\nvast datasets can negotiate, reason, and plan, facilitating near-human-level\nconsensus at scale with minimal entry barriers. In this work, we identify key\nlimitations in existing approaches and propose autonomous LLM agents to address\nthese gaps. We introduce a series of novel, supply chain-specific\nconsensus-seeking frameworks tailored for LLM agents and validate the\neffectiveness of our approach through a case study in inventory management. To\naccelerate progress within the SCM community, we open-source our code,\nproviding a foundation for further advancements in LLM-powered autonomous\nsupply chain solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how Large Language Models (LLMs) can automate\nconsensus-seeking in supply chain management (SCM), where frequent decisions on\nproblems such as inventory levels and delivery times require coordination among\ncompanies. Traditional SCM relies on human consensus in decision-making to\navoid emergent problems like the bullwhip effect. Some routine consensus\nprocesses, especially those that are time-intensive and costly, can be\nautomated. Existing solutions for automated coordination have faced challenges\ndue to high entry barriers locking out SMEs, limited capabilities, and limited\nadaptability in complex scenarios. However, recent advances in Generative AI,\nparticularly LLMs, show promise in overcoming these barriers. LLMs, trained on\nvast datasets can negotiate, reason, and plan, facilitating near-human-level\nconsensus at scale with minimal entry barriers. In this work, we identify key\nlimitations in existing approaches and propose autonomous LLM agents to address\nthese gaps. We introduce a series of novel, supply chain-specific\nconsensus-seeking frameworks tailored for LLM agents and validate the\neffectiveness of our approach through a case study in inventory management. To\naccelerate progress within the SCM community, we open-source our code,\nproviding a foundation for further advancements in LLM-powered autonomous\nsupply chain solutions."
                },
                "authors": [
                    {
                        "name": "Valeria Jannelli"
                    },
                    {
                        "name": "Stefan Schoepf"
                    },
                    {
                        "name": "Matthias Bickel"
                    },
                    {
                        "name": "Torbjørn Netland"
                    },
                    {
                        "name": "Alexandra Brintrup"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Brintrup"
                },
                "author": "Alexandra Brintrup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10168v1",
                "updated": "2024-11-15T13:16:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    13,
                    16,
                    11,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T13:16:11Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    13,
                    16,
                    11,
                    4,
                    320,
                    0
                ],
                "title": "Evaluating the role of `Constitutions' for learning from AI feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the role of `Constitutions' for learning from AI feedback"
                },
                "summary": "The growing capabilities of large language models (LLMs) have led to their\nuse as substitutes for human feedback for training and assessing other LLMs.\nThese methods often rely on `constitutions', written guidelines which a critic\nmodel uses to provide feedback and improve generations. We investigate how the\nchoice of constitution affects feedback quality by using four different\nconstitutions to improve patient-centered communication in medical interviews.\nIn pairwise comparisons conducted by 215 human raters, we found that detailed\nconstitutions led to better results regarding emotive qualities. However, none\nof the constitutions outperformed the baseline in learning more\npractically-oriented skills related to information gathering and provision. Our\nfindings indicate that while detailed constitutions should be prioritised,\nthere are possible limitations to the effectiveness of AI feedback as a reward\nsignal in certain areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing capabilities of large language models (LLMs) have led to their\nuse as substitutes for human feedback for training and assessing other LLMs.\nThese methods often rely on `constitutions', written guidelines which a critic\nmodel uses to provide feedback and improve generations. We investigate how the\nchoice of constitution affects feedback quality by using four different\nconstitutions to improve patient-centered communication in medical interviews.\nIn pairwise comparisons conducted by 215 human raters, we found that detailed\nconstitutions led to better results regarding emotive qualities. However, none\nof the constitutions outperformed the baseline in learning more\npractically-oriented skills related to information gathering and provision. Our\nfindings indicate that while detailed constitutions should be prioritised,\nthere are possible limitations to the effectiveness of AI feedback as a reward\nsignal in certain areas."
                },
                "authors": [
                    {
                        "name": "Saskia Redgate"
                    },
                    {
                        "name": "Andrew M. Bean"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "arxiv_comment": "4 pages, 2 figures. In NeurIPS 2024 Workshop on Language Gamification",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10163v1",
                "updated": "2024-11-15T13:12:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    13,
                    12,
                    29,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T13:12:29Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    13,
                    12,
                    29,
                    4,
                    320,
                    0
                ],
                "title": "Compound-QA: A Benchmark for Evaluating LLMs on Compound Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compound-QA: A Benchmark for Evaluating LLMs on Compound Questions"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance across\nvarious tasks, prompting researchers to develop diverse evaluation benchmarks.\nHowever, existing benchmarks typically measure the ability of LLMs to respond\nto individual questions, neglecting the complex interactions in real-world\napplications. In this paper, we introduce Compound Question Synthesis (CQ-Syn)\nto create the Compound-QA benchmark, focusing on compound questions with\nmultiple sub-questions. This benchmark is derived from existing QA datasets,\nannotated with proprietary LLMs and verified by humans for accuracy. It\nencompasses five categories: Factual-Statement, Cause-and-Effect,\nHypothetical-Analysis, Comparison-and-Selection, and Evaluation-and-Suggestion.\nIt evaluates the LLM capability in terms of three dimensions including\nunderstanding, reasoning, and knowledge. Our assessment of eight open-source\nLLMs using Compound-QA reveals distinct patterns in their responses to compound\nquestions, which are significantly poorer than those to non-compound questions.\nAdditionally, we investigate various methods to enhance LLMs performance on\ncompound questions. The results indicate that these approaches significantly\nimprove the models' comprehension and reasoning abilities on compound\nquestions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance across\nvarious tasks, prompting researchers to develop diverse evaluation benchmarks.\nHowever, existing benchmarks typically measure the ability of LLMs to respond\nto individual questions, neglecting the complex interactions in real-world\napplications. In this paper, we introduce Compound Question Synthesis (CQ-Syn)\nto create the Compound-QA benchmark, focusing on compound questions with\nmultiple sub-questions. This benchmark is derived from existing QA datasets,\nannotated with proprietary LLMs and verified by humans for accuracy. It\nencompasses five categories: Factual-Statement, Cause-and-Effect,\nHypothetical-Analysis, Comparison-and-Selection, and Evaluation-and-Suggestion.\nIt evaluates the LLM capability in terms of three dimensions including\nunderstanding, reasoning, and knowledge. Our assessment of eight open-source\nLLMs using Compound-QA reveals distinct patterns in their responses to compound\nquestions, which are significantly poorer than those to non-compound questions.\nAdditionally, we investigate various methods to enhance LLMs performance on\ncompound questions. The results indicate that these approaches significantly\nimprove the models' comprehension and reasoning abilities on compound\nquestions."
                },
                "authors": [
                    {
                        "name": "Yutao Hou"
                    },
                    {
                        "name": "Yajing Luo"
                    },
                    {
                        "name": "Zhiwen Ruan"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Weifeng Ge"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Guanhua Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guanhua Chen"
                },
                "author": "Guanhua Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10153v1",
                "updated": "2024-11-15T12:52:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    52,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T12:52:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    52,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "BONE: a unifying framework for Bayesian online learning in\n  non-stationary environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BONE: a unifying framework for Bayesian online learning in\n  non-stationary environments"
                },
                "summary": "We propose a unifying framework for methods that perform Bayesian online\nlearning in non-stationary environments. We call the framework BONE, which\nstands for (B)ayesian (O)nline learning in (N)on-stationary (E)nvironments.\nBONE provides a common structure to tackle a variety of problems, including\nonline continual learning, prequential forecasting, and contextual bandits. The\nframework requires specifying three modelling choices: (i) a model for\nmeasurements (e.g., a neural network), (ii) an auxiliary process to model\nnon-stationarity (e.g., the time since the last changepoint), and (iii) a\nconditional prior over model parameters (e.g., a multivariate Gaussian). The\nframework also requires two algorithmic choices, which we use to carry out\napproximate inference under this framework: (i) an algorithm to estimate\nbeliefs (posterior distribution) about the model parameters given the auxiliary\nvariable, and (ii) an algorithm to estimate beliefs about the auxiliary\nvariable. We show how this modularity allows us to write many different\nexisting methods as instances of BONE; we also use this framework to propose a\nnew method. We then experimentally compare existing methods with our proposed\nnew method on several datasets; we provide insights into the situations that\nmake one method more suitable than another for a given task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a unifying framework for methods that perform Bayesian online\nlearning in non-stationary environments. We call the framework BONE, which\nstands for (B)ayesian (O)nline learning in (N)on-stationary (E)nvironments.\nBONE provides a common structure to tackle a variety of problems, including\nonline continual learning, prequential forecasting, and contextual bandits. The\nframework requires specifying three modelling choices: (i) a model for\nmeasurements (e.g., a neural network), (ii) an auxiliary process to model\nnon-stationarity (e.g., the time since the last changepoint), and (iii) a\nconditional prior over model parameters (e.g., a multivariate Gaussian). The\nframework also requires two algorithmic choices, which we use to carry out\napproximate inference under this framework: (i) an algorithm to estimate\nbeliefs (posterior distribution) about the model parameters given the auxiliary\nvariable, and (ii) an algorithm to estimate beliefs about the auxiliary\nvariable. We show how this modularity allows us to write many different\nexisting methods as instances of BONE; we also use this framework to propose a\nnew method. We then experimentally compare existing methods with our proposed\nnew method on several datasets; we provide insights into the situations that\nmake one method more suitable than another for a given task."
                },
                "authors": [
                    {
                        "name": "Gerardo Duran-Martin"
                    },
                    {
                        "name": "Leandro Sánchez-Betancourt"
                    },
                    {
                        "name": "Alexander Y. Shestopaloff"
                    },
                    {
                        "name": "Kevin Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Murphy"
                },
                "author": "Kevin Murphy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14979v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14979v5",
                "updated": "2024-11-15T12:46:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    46,
                    30,
                    4,
                    320,
                    0
                ],
                "published": "2024-10-19T05:01:56Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    5,
                    1,
                    56,
                    5,
                    293,
                    0
                ],
                "title": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration From Cognitive Psychology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration From Cognitive Psychology"
                },
                "summary": "The cognitive mechanism by which Large Language Models (LLMs) solve\nmathematical problems remains a widely debated and unresolved issue. Currently,\nthere is little interpretable experimental evidence that connects LLMs'\nproblem-solving with human cognitive psychology.To determine if LLMs possess\nhuman-like mathematical reasoning, we modified the problems used in the human\nCognitive Reflection Test (CRT). Our results show that, even with the use of\nChains of Thought (CoT) prompts, mainstream LLMs, including the latest o1 model\n(noted for its reasoning capabilities), have a high error rate when solving\nthese modified CRT problems. Specifically, the average accuracy rate dropped by\nup to 50% compared to the original questions.Further analysis of LLMs'\nincorrect answers suggests that they primarily rely on pattern matching from\ntheir training data, which aligns more with human intuition (System 1 thinking)\nrather than with human-like reasoning (System 2 thinking). This finding\nchallenges the belief that LLMs have genuine mathematical reasoning abilities\ncomparable to humans. As a result, this work may adjust overly optimistic views\non LLMs' progress towards artificial general intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cognitive mechanism by which Large Language Models (LLMs) solve\nmathematical problems remains a widely debated and unresolved issue. Currently,\nthere is little interpretable experimental evidence that connects LLMs'\nproblem-solving with human cognitive psychology.To determine if LLMs possess\nhuman-like mathematical reasoning, we modified the problems used in the human\nCognitive Reflection Test (CRT). Our results show that, even with the use of\nChains of Thought (CoT) prompts, mainstream LLMs, including the latest o1 model\n(noted for its reasoning capabilities), have a high error rate when solving\nthese modified CRT problems. Specifically, the average accuracy rate dropped by\nup to 50% compared to the original questions.Further analysis of LLMs'\nincorrect answers suggests that they primarily rely on pattern matching from\ntheir training data, which aligns more with human intuition (System 1 thinking)\nrather than with human-like reasoning (System 2 thinking). This finding\nchallenges the belief that LLMs have genuine mathematical reasoning abilities\ncomparable to humans. As a result, this work may adjust overly optimistic views\non LLMs' progress towards artificial general intelligence."
                },
                "authors": [
                    {
                        "name": "Wei Xie"
                    },
                    {
                        "name": "Shuoyoucheng Ma"
                    },
                    {
                        "name": "Zhenhua Wang"
                    },
                    {
                        "name": "Enze Wang"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Xiaobing Sun"
                    },
                    {
                        "name": "Baosheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Baosheng Wang"
                },
                "author": "Baosheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14979v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14979v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10145v1",
                "updated": "2024-11-15T12:39:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    39,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T12:39:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    39,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "An Effective Framework to Help Large Language Models Handle\n  Numeric-involved Long-context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Effective Framework to Help Large Language Models Handle\n  Numeric-involved Long-context Tasks"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nhandling long texts and have almost perfect performance in traditional\nretrieval tasks. However, their performance significantly degrades when it\ncomes to numerical calculations in the long-context. Numeric-involved\nlong-context tasks typically cannot be addressed by current LLMs in normal\nsettings due to their inherent limitations in simultaneously handling complex\nand massive information. Some CoT like prompting methods can improve accuracy\nbut demands massive output tokens, which is costly and slow. To address this\nissue, we propose a workflow, which decompose a numeric-involved long-context\ntask into 4 low-level subtasks: judging, extracting and processing with code\nand conclusion. The former 2 subtasks is relatively simple, which allows us to\nuse smaller models for efficiently processing long context. When numerical\ncalculations are required, we use code generated by LLMs to avoid the\ndisadvantage of LLM not being good at calculations. The results in 2\nnumeric-involved long-context benchmarks demonstrate our workflow can not only\nimprove accuracy, but also significantly reduce the cost of API calls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nhandling long texts and have almost perfect performance in traditional\nretrieval tasks. However, their performance significantly degrades when it\ncomes to numerical calculations in the long-context. Numeric-involved\nlong-context tasks typically cannot be addressed by current LLMs in normal\nsettings due to their inherent limitations in simultaneously handling complex\nand massive information. Some CoT like prompting methods can improve accuracy\nbut demands massive output tokens, which is costly and slow. To address this\nissue, we propose a workflow, which decompose a numeric-involved long-context\ntask into 4 low-level subtasks: judging, extracting and processing with code\nand conclusion. The former 2 subtasks is relatively simple, which allows us to\nuse smaller models for efficiently processing long context. When numerical\ncalculations are required, we use code generated by LLMs to avoid the\ndisadvantage of LLM not being good at calculations. The results in 2\nnumeric-involved long-context benchmarks demonstrate our workflow can not only\nimprove accuracy, but also significantly reduce the cost of API calls."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yijiong Yu"
                },
                "author": "Yijiong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10143v1",
                "updated": "2024-11-15T12:33:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    33,
                    58,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T12:33:58Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    33,
                    58,
                    4,
                    320,
                    0
                ],
                "title": "Cascaded Prediction and Asynchronous Execution of Iterative Algorithms\n  on Heterogeneous Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascaded Prediction and Asynchronous Execution of Iterative Algorithms\n  on Heterogeneous Platforms"
                },
                "summary": "Owing to the diverse scales and varying distributions of sparse matrices\narising from practical problems, a multitude of choices are present in the\ndesign and implementation of sparse matrix-vector multiplication (SpMV).\nResearchers have proposed many machine learning-based optimization methods for\nSpMV. However, these efforts only support one area of sparse matrix format\nselection, SpMV algorithm selection, or parameter configuration, and rarely\nconsider a large amount of time overhead associated with feature extraction,\nmodel inference, and compression format conversion. This paper introduces a\nmachine learning-based cascaded prediction method for SpMV computations that\nspans various computing stages and hierarchies. Besides, an asynchronous and\nconcurrent computing model has been designed and implemented for runtime model\nprediction and iterative algorithm solving on heterogeneous computing\nplatforms. It not only offers comprehensive support for the iterative\nalgorithm-solving process leveraging machine learning technology, but also\neffectively mitigates the preprocessing overheads. Experimental results\ndemonstrate that the cascaded prediction introduced in this paper accelerates\nSpMV by 1.33x on average, and the iterative algorithm, enhanced by cascaded\nprediction and asynchronous execution, optimizes by 2.55x on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to the diverse scales and varying distributions of sparse matrices\narising from practical problems, a multitude of choices are present in the\ndesign and implementation of sparse matrix-vector multiplication (SpMV).\nResearchers have proposed many machine learning-based optimization methods for\nSpMV. However, these efforts only support one area of sparse matrix format\nselection, SpMV algorithm selection, or parameter configuration, and rarely\nconsider a large amount of time overhead associated with feature extraction,\nmodel inference, and compression format conversion. This paper introduces a\nmachine learning-based cascaded prediction method for SpMV computations that\nspans various computing stages and hierarchies. Besides, an asynchronous and\nconcurrent computing model has been designed and implemented for runtime model\nprediction and iterative algorithm solving on heterogeneous computing\nplatforms. It not only offers comprehensive support for the iterative\nalgorithm-solving process leveraging machine learning technology, but also\neffectively mitigates the preprocessing overheads. Experimental results\ndemonstrate that the cascaded prediction introduced in this paper accelerates\nSpMV by 1.33x on average, and the iterative algorithm, enhanced by cascaded\nprediction and asynchronous execution, optimizes by 2.55x on average."
                },
                "authors": [
                    {
                        "name": "Jianhua Gao"
                    },
                    {
                        "name": "Bingjie Liu"
                    },
                    {
                        "name": "Yizhuo Wang"
                    },
                    {
                        "name": "Weixing Ji"
                    },
                    {
                        "name": "Hua Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Huang"
                },
                "author": "Hua Huang",
                "arxiv_comment": "12 pages, 9 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68-02, 68W10, 65F50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "A.1; D.1.3; G.1.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10137v1",
                "updated": "2024-11-15T12:23:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    23,
                    12,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T12:23:12Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    23,
                    12,
                    4,
                    320,
                    0
                ],
                "title": "Legal Evalutions and Challenges of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal Evalutions and Challenges of Large Language Models"
                },
                "summary": "In this paper, we review legal testing methods based on Large Language Models\n(LLMs), using the OPENAI o1 model as a case study to evaluate the performance\nof large models in applying legal provisions. We compare current\nstate-of-the-art LLMs, including open-source, closed-source, and legal-specific\nmodels trained specifically for the legal domain. Systematic tests are\nconducted on English and Chinese legal cases, and the results are analyzed in\ndepth. Through systematic testing of legal cases from common law systems and\nChina, this paper explores the strengths and weaknesses of LLMs in\nunderstanding and applying legal texts, reasoning through legal issues, and\npredicting judgments. The experimental results highlight both the potential and\nlimitations of LLMs in legal applications, particularly in terms of challenges\nrelated to the interpretation of legal language and the accuracy of legal\nreasoning. Finally, the paper provides a comprehensive analysis of the\nadvantages and disadvantages of various types of models, offering valuable\ninsights and references for the future application of AI in the legal field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we review legal testing methods based on Large Language Models\n(LLMs), using the OPENAI o1 model as a case study to evaluate the performance\nof large models in applying legal provisions. We compare current\nstate-of-the-art LLMs, including open-source, closed-source, and legal-specific\nmodels trained specifically for the legal domain. Systematic tests are\nconducted on English and Chinese legal cases, and the results are analyzed in\ndepth. Through systematic testing of legal cases from common law systems and\nChina, this paper explores the strengths and weaknesses of LLMs in\nunderstanding and applying legal texts, reasoning through legal issues, and\npredicting judgments. The experimental results highlight both the potential and\nlimitations of LLMs in legal applications, particularly in terms of challenges\nrelated to the interpretation of legal language and the accuracy of legal\nreasoning. Finally, the paper provides a comprehensive analysis of the\nadvantages and disadvantages of various types of models, offering valuable\ninsights and references for the future application of AI in the legal field."
                },
                "authors": [
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Huan Zhao"
                    },
                    {
                        "name": "Zhenyuan Yang"
                    },
                    {
                        "name": "Peng Shu"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Haobo Sun"
                    },
                    {
                        "name": "Ruixi Liang"
                    },
                    {
                        "name": "Shixin Li"
                    },
                    {
                        "name": "Pengcheng Shi"
                    },
                    {
                        "name": "Longjun Ma"
                    },
                    {
                        "name": "Zongjia Liu"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Tianyang Zhong"
                    },
                    {
                        "name": "Yutong Zhang"
                    },
                    {
                        "name": "Chong Ma"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Tuo Zhang"
                    },
                    {
                        "name": "Tianli Ding"
                    },
                    {
                        "name": "Yudan Ren"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Xi Jiang"
                    },
                    {
                        "name": "Shu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shu Zhang"
                },
                "author": "Shu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10129v1",
                "updated": "2024-11-15T12:01:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    1,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T12:01:38Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    1,
                    38,
                    4,
                    320,
                    0
                ],
                "title": "Prompting and Fine-tuning Large Language Models for Automated Code\n  Review Comment Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting and Fine-tuning Large Language Models for Automated Code\n  Review Comment Generation"
                },
                "summary": "Generating accurate code review comments remains a significant challenge due\nto the inherently diverse and non-unique nature of the task output. Large\nlanguage models pretrained on both programming and natural language data tend\nto perform well in code-oriented tasks. However, large-scale pretraining is not\nalways feasible due to its environmental impact and project-specific\ngeneralizability issues. In this work, first we fine-tune open-source Large\nlanguage models (LLM) in parameter-efficient, quantized low-rank (QLoRA)\nfashion on consumer-grade hardware to improve review comment generation. Recent\nstudies demonstrate the efficacy of augmenting semantic metadata information\ninto prompts to boost performance in other code-related tasks. To explore this\nin code review activities, we also prompt proprietary, closed-source LLMs\naugmenting the input code patch with function call graphs and code summaries.\nBoth of our strategies improve the review comment generation performance, with\nfunction call graph augmented few-shot prompting on the GPT-3.5 model\nsurpassing the pretrained baseline by around 90% BLEU-4 score on the\nCodeReviewer dataset. Moreover, few-shot prompted Gemini-1.0 Pro, QLoRA\nfine-tuned Code Llama and Llama 3.1 models achieve competitive results (ranging\nfrom 25% to 83% performance improvement) on this task. An additional human\nevaluation study further validates our experimental findings, reflecting\nreal-world developers' perceptions of LLM-generated code review comments based\non relevant qualitative metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating accurate code review comments remains a significant challenge due\nto the inherently diverse and non-unique nature of the task output. Large\nlanguage models pretrained on both programming and natural language data tend\nto perform well in code-oriented tasks. However, large-scale pretraining is not\nalways feasible due to its environmental impact and project-specific\ngeneralizability issues. In this work, first we fine-tune open-source Large\nlanguage models (LLM) in parameter-efficient, quantized low-rank (QLoRA)\nfashion on consumer-grade hardware to improve review comment generation. Recent\nstudies demonstrate the efficacy of augmenting semantic metadata information\ninto prompts to boost performance in other code-related tasks. To explore this\nin code review activities, we also prompt proprietary, closed-source LLMs\naugmenting the input code patch with function call graphs and code summaries.\nBoth of our strategies improve the review comment generation performance, with\nfunction call graph augmented few-shot prompting on the GPT-3.5 model\nsurpassing the pretrained baseline by around 90% BLEU-4 score on the\nCodeReviewer dataset. Moreover, few-shot prompted Gemini-1.0 Pro, QLoRA\nfine-tuned Code Llama and Llama 3.1 models achieve competitive results (ranging\nfrom 25% to 83% performance improvement) on this task. An additional human\nevaluation study further validates our experimental findings, reflecting\nreal-world developers' perceptions of LLM-generated code review comments based\non relevant qualitative metrics."
                },
                "authors": [
                    {
                        "name": "Md. Asif Haider"
                    },
                    {
                        "name": "Ayesha Binte Mostofa"
                    },
                    {
                        "name": "Sk. Sabit Bin Mosaddek"
                    },
                    {
                        "name": "Anindya Iqbal"
                    },
                    {
                        "name": "Toufique Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Toufique Ahmed"
                },
                "author": "Toufique Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09510v2",
                "updated": "2024-11-15T10:47:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    10,
                    47,
                    37,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-14T15:19:01Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    19,
                    1,
                    3,
                    319,
                    0
                ],
                "title": "Communication Compression for Tensor Parallel LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication Compression for Tensor Parallel LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have pushed the frontier of artificial\nintelligence but are comprised of hundreds of billions of parameters and\noperations. For faster inference latency, LLMs are deployed on multiple\nhardware accelerators through various Model Parallelism strategies. Our paper\nlooks into the details on one such strategy - Tensor Parallel - and proposes to\nreduce latency by compressing inter-accelerator communication. We leverage fine\ngrained quantization techniques to compress selected activations by 3.5 - 4.5x.\nOur proposed method leads up to 2x reduction of time-to-first-token (TTFT) with\nnegligible model performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have pushed the frontier of artificial\nintelligence but are comprised of hundreds of billions of parameters and\noperations. For faster inference latency, LLMs are deployed on multiple\nhardware accelerators through various Model Parallelism strategies. Our paper\nlooks into the details on one such strategy - Tensor Parallel - and proposes to\nreduce latency by compressing inter-accelerator communication. We leverage fine\ngrained quantization techniques to compress selected activations by 3.5 - 4.5x.\nOur proposed method leads up to 2x reduction of time-to-first-token (TTFT) with\nnegligible model performance degradation."
                },
                "authors": [
                    {
                        "name": "Jan Hansen-Palmus"
                    },
                    {
                        "name": "Michael Truong Le"
                    },
                    {
                        "name": "Oliver Hausdörfer"
                    },
                    {
                        "name": "Alok Verma"
                    }
                ],
                "author_detail": {
                    "name": "Alok Verma"
                },
                "author": "Alok Verma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10083v1",
                "updated": "2024-11-15T10:01:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    10,
                    1,
                    52,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T10:01:52Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    10,
                    1,
                    52,
                    4,
                    320,
                    0
                ],
                "title": "Xmodel-1.5: An 1B-scale Multilingual LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Xmodel-1.5: An 1B-scale Multilingual LLM"
                },
                "summary": "We introduce Xmodel-1.5, a novel 1-billion-parameter multilingual large model\npretrained on approximately 2 trillion tokens. The model demonstrates strong\nperformance across several languages, with particularly notable results in\nThai, Arabic, and French, alongside its effectiveness in Chinese and English.\nIn addition, we contribute to the research community by releasing a Thai\nevaluation dataset, which includes hundreds of questions annotated by students\nfrom Chulalongkorn University's School of Integrated Innovation. While the\nresults are promising, we acknowledge that there is still room for improvement.\nWe hope this work advances ongoing efforts in multilingual AI research and\npromotes better cross-linguistic understanding in various natural language\nprocessing tasks. Our models and code are publicly available on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Xmodel-1.5, a novel 1-billion-parameter multilingual large model\npretrained on approximately 2 trillion tokens. The model demonstrates strong\nperformance across several languages, with particularly notable results in\nThai, Arabic, and French, alongside its effectiveness in Chinese and English.\nIn addition, we contribute to the research community by releasing a Thai\nevaluation dataset, which includes hundreds of questions annotated by students\nfrom Chulalongkorn University's School of Integrated Innovation. While the\nresults are promising, we acknowledge that there is still room for improvement.\nWe hope this work advances ongoing efforts in multilingual AI research and\npromotes better cross-linguistic understanding in various natural language\nprocessing tasks. Our models and code are publicly available on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM."
                },
                "authors": [
                    {
                        "name": "Wang Qun"
                    },
                    {
                        "name": "Liu Yang"
                    },
                    {
                        "name": "Lin Qingquan"
                    },
                    {
                        "name": "Jiang Ling"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Ling"
                },
                "author": "Jiang Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10080v1",
                "updated": "2024-11-15T09:50:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    50,
                    27,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T09:50:27Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    50,
                    27,
                    4,
                    320,
                    0
                ],
                "title": "Understanding The Effect Of Temperature On Alignment With Human Opinions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding The Effect Of Temperature On Alignment With Human Opinions"
                },
                "summary": "With the increasing capabilities of LLMs, recent studies focus on\nunderstanding whose opinions are represented by them and how to effectively\nextract aligned opinion distributions. We conducted an empirical analysis of\nthree straightforward methods for obtaining distributions and evaluated the\nresults across a variety of metrics. Our findings suggest that sampling and\nlog-probability approaches with simple parameter adjustments can return better\naligned outputs in subjective tasks compared to direct prompting. Yet, assuming\nmodels reflect human opinions may be limiting, highlighting the need for\nfurther research on how human subjectivity affects model uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing capabilities of LLMs, recent studies focus on\nunderstanding whose opinions are represented by them and how to effectively\nextract aligned opinion distributions. We conducted an empirical analysis of\nthree straightforward methods for obtaining distributions and evaluated the\nresults across a variety of metrics. Our findings suggest that sampling and\nlog-probability approaches with simple parameter adjustments can return better\naligned outputs in subjective tasks compared to direct prompting. Yet, assuming\nmodels reflect human opinions may be limiting, highlighting the need for\nfurther research on how human subjectivity affects model uncertainty."
                },
                "authors": [
                    {
                        "name": "Maja Pavlovic"
                    },
                    {
                        "name": "Massimo Poesio"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Poesio"
                },
                "author": "Massimo Poesio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10069v1",
                "updated": "2024-11-15T09:33:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    33,
                    47,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T09:33:47Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    33,
                    47,
                    4,
                    320,
                    0
                ],
                "title": "Layer Importance and Hallucination Analysis in Large Language Models via\n  Enhanced Activation Variance-Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer Importance and Hallucination Analysis in Large Language Models via\n  Enhanced Activation Variance-Sparsity"
                },
                "summary": "Evaluating the importance of different layers in large language models (LLMs)\nis crucial for optimizing model performance and interpretability. This paper\nfirst explores layer importance using the Activation Variance-Sparsity Score\n(AVSS), which combines normalized activation variance and sparsity to quantify\neach layer's contribution to overall model performance. By ranking layers based\non AVSS and pruning the least impactful 25\\%, our experiments on tasks such as\nquestion answering, language modeling, and sentiment classification show that\nover 90\\% of the original performance is retained, highlighting potential\nredundancies in LLM architectures. Building on AVSS, we propose an enhanced\nversion tailored to assess hallucination propensity across layers (EAVSS). This\nimproved approach introduces Hallucination-Specific Activation Variance (HSAV)\nand Hallucination-Specific Sparsity (HSS) metrics, allowing precise\nidentification of hallucination-prone layers. By incorporating contrastive\nlearning on these layers, we effectively mitigate hallucination generation,\ncontributing to more robust and efficient LLMs(The maximum performance\nimprovement is 12\\%). Our results on the NQ, SciQ, TriviaQA, TruthfulQA, and\nWikiQA datasets demonstrate the efficacy of this method, offering a\ncomprehensive framework for both layer importance evaluation and hallucination\nmitigation in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the importance of different layers in large language models (LLMs)\nis crucial for optimizing model performance and interpretability. This paper\nfirst explores layer importance using the Activation Variance-Sparsity Score\n(AVSS), which combines normalized activation variance and sparsity to quantify\neach layer's contribution to overall model performance. By ranking layers based\non AVSS and pruning the least impactful 25\\%, our experiments on tasks such as\nquestion answering, language modeling, and sentiment classification show that\nover 90\\% of the original performance is retained, highlighting potential\nredundancies in LLM architectures. Building on AVSS, we propose an enhanced\nversion tailored to assess hallucination propensity across layers (EAVSS). This\nimproved approach introduces Hallucination-Specific Activation Variance (HSAV)\nand Hallucination-Specific Sparsity (HSS) metrics, allowing precise\nidentification of hallucination-prone layers. By incorporating contrastive\nlearning on these layers, we effectively mitigate hallucination generation,\ncontributing to more robust and efficient LLMs(The maximum performance\nimprovement is 12\\%). Our results on the NQ, SciQ, TriviaQA, TruthfulQA, and\nWikiQA datasets demonstrate the efficacy of this method, offering a\ncomprehensive framework for both layer importance evaluation and hallucination\nmitigation in LLMs."
                },
                "authors": [
                    {
                        "name": "Zichen Song"
                    },
                    {
                        "name": "Sitan Huang"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Zhongfeng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Kang"
                },
                "author": "Zhongfeng Kang",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06740v2",
                "updated": "2024-11-15T09:31:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    31,
                    52,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-11T06:25:13Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    6,
                    25,
                    13,
                    0,
                    316,
                    0
                ],
                "title": "Dockformer: A transformer-based molecular docking paradigm for\n  large-scale virtual screening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dockformer: A transformer-based molecular docking paradigm for\n  large-scale virtual screening"
                },
                "summary": "Molecular docking enables virtual screening of compound libraries to identify\npotential ligands that target proteins of interest, a crucial step in drug\ndevelopment; however, as the size of the compound library increases, the\ncomputational complexity of traditional docking models increases. Deep learning\nalgorithms can provide data-driven research and development models to increase\nthe speed of the docking process. Unfortunately, few models can achieve\nsuperior screening performance compared to that of traditional models.\nTherefore, a novel deep learning-based docking approach named Dockformer is\nintroduced in this study. Dockformer leverages multimodal information to\ncapture the geometric topology and structural knowledge of molecules and can\ndirectly generate binding conformations with the corresponding confidence\nmeasures in an end-to-end manner. The experimental results show that Dockformer\nachieves success rates of 90.53\\% and 82.71\\% on the PDBbind core set and\nPoseBusters benchmarks, respectively, and more than a 100-fold increase in the\ninference process speed, outperforming almost all state-of-the-art docking\nmethods. In addition, the ability of Dockformer to identify the main protease\ninhibitors of coronaviruses is demonstrated in a real-world virtual screening\nscenario. Considering its high docking accuracy and screening efficiency,\nDockformer can be regarded as a powerful and robust tool in the field of drug\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular docking enables virtual screening of compound libraries to identify\npotential ligands that target proteins of interest, a crucial step in drug\ndevelopment; however, as the size of the compound library increases, the\ncomputational complexity of traditional docking models increases. Deep learning\nalgorithms can provide data-driven research and development models to increase\nthe speed of the docking process. Unfortunately, few models can achieve\nsuperior screening performance compared to that of traditional models.\nTherefore, a novel deep learning-based docking approach named Dockformer is\nintroduced in this study. Dockformer leverages multimodal information to\ncapture the geometric topology and structural knowledge of molecules and can\ndirectly generate binding conformations with the corresponding confidence\nmeasures in an end-to-end manner. The experimental results show that Dockformer\nachieves success rates of 90.53\\% and 82.71\\% on the PDBbind core set and\nPoseBusters benchmarks, respectively, and more than a 100-fold increase in the\ninference process speed, outperforming almost all state-of-the-art docking\nmethods. In addition, the ability of Dockformer to identify the main protease\ninhibitors of coronaviruses is demonstrated in a real-world virtual screening\nscenario. Considering its high docking accuracy and screening efficiency,\nDockformer can be regarded as a powerful and robust tool in the field of drug\ndesign."
                },
                "authors": [
                    {
                        "name": "Zhangfan Yang"
                    },
                    {
                        "name": "Junkai Ji"
                    },
                    {
                        "name": "Shan He"
                    },
                    {
                        "name": "Jianqiang Li"
                    },
                    {
                        "name": "Ruibin Bai"
                    },
                    {
                        "name": "Zexuan Zhu"
                    },
                    {
                        "name": "Yew Soon Ong"
                    }
                ],
                "author_detail": {
                    "name": "Yew Soon Ong"
                },
                "author": "Yew Soon Ong",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10066v1",
                "updated": "2024-11-15T09:31:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    31,
                    23,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T09:31:23Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    31,
                    23,
                    4,
                    320,
                    0
                ],
                "title": "Explanation of the exceptionally strong timing noise of PSR J0337+1715\n  by a circum-ternary planet and consequences for gravity tests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanation of the exceptionally strong timing noise of PSR J0337+1715\n  by a circum-ternary planet and consequences for gravity tests"
                },
                "summary": "Context: Timing of pulsar PSR J0337+1715 provides a unique opportunity to\ntest the strong equivalence principle (SEP) with a strongly self-gravitating\nobject. This is due to its unique situation in a triple stellar system with two\nwhite dwarfs.\n  Aims: Our previous study suggested the presence of a strong low-frequency\nsignal in the timing residuals. We set out to model it on a longer dataset in\norder to determine its nature and improve accuracy.\n  Methods: Three models are considered: chromatic or achromatic red-noise, and\na small planet in a hierarchical orbit with the triple stellar system. These\nmodels are implemented in our numerical timing model. We perform Bayesian\ninference of posterior distributions. Best fits are compared using\ninformation-theoretic criteria.\n  Results: Chromatic red noise from dispersion-measure variations is ruled out.\nAchromatic red noise or a planet in keplerian orbit provide the best fits. If\nit is red noise then it appears exceptionally strong. Assuming the presence of\na planet, we obtain a marginal detection of mutual interactions which allows us\nto constrain its mass to $\\sim 0.5 M_{\\rm Moon}$ as well as its inclination.\nThe latter is intriguingly coincident with a Kozai resonance. We show that a\nlonger observation span will ultimately lead to a clear signature of the planet\nmodel due to its mutual interactions with the triple system. We produce new\nlimits on SEP violation: $|\\Delta| < 1.5\\cdot 10^{-6}$ or $|\\Delta| < 2.3\\cdot\n10^{-6}$ at 95\\% confidence level under the planet or red-noise hypothesis,\nrespectively. This model dependence emphasises the need for additional data and\nmodel selection. As a by-product, we estimate a rather low supernova kick\nvelocity of $\\sim 110-125 \\rm km/s$, strengthening the idea that it is a\nnecessary condition for the formation of pulsar triple systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Timing of pulsar PSR J0337+1715 provides a unique opportunity to\ntest the strong equivalence principle (SEP) with a strongly self-gravitating\nobject. This is due to its unique situation in a triple stellar system with two\nwhite dwarfs.\n  Aims: Our previous study suggested the presence of a strong low-frequency\nsignal in the timing residuals. We set out to model it on a longer dataset in\norder to determine its nature and improve accuracy.\n  Methods: Three models are considered: chromatic or achromatic red-noise, and\na small planet in a hierarchical orbit with the triple stellar system. These\nmodels are implemented in our numerical timing model. We perform Bayesian\ninference of posterior distributions. Best fits are compared using\ninformation-theoretic criteria.\n  Results: Chromatic red noise from dispersion-measure variations is ruled out.\nAchromatic red noise or a planet in keplerian orbit provide the best fits. If\nit is red noise then it appears exceptionally strong. Assuming the presence of\na planet, we obtain a marginal detection of mutual interactions which allows us\nto constrain its mass to $\\sim 0.5 M_{\\rm Moon}$ as well as its inclination.\nThe latter is intriguingly coincident with a Kozai resonance. We show that a\nlonger observation span will ultimately lead to a clear signature of the planet\nmodel due to its mutual interactions with the triple system. We produce new\nlimits on SEP violation: $|\\Delta| < 1.5\\cdot 10^{-6}$ or $|\\Delta| < 2.3\\cdot\n10^{-6}$ at 95\\% confidence level under the planet or red-noise hypothesis,\nrespectively. This model dependence emphasises the need for additional data and\nmodel selection. As a by-product, we estimate a rather low supernova kick\nvelocity of $\\sim 110-125 \\rm km/s$, strengthening the idea that it is a\nnecessary condition for the formation of pulsar triple systems."
                },
                "authors": [
                    {
                        "name": "Guillaume Voisin"
                    },
                    {
                        "name": "Ismaël Cognard"
                    },
                    {
                        "name": "Melaine Saillenfest"
                    },
                    {
                        "name": "Thomas Tauris"
                    },
                    {
                        "name": "Norbert Wex"
                    },
                    {
                        "name": "Lucas Guillemot"
                    },
                    {
                        "name": "Gilles Theureau"
                    },
                    {
                        "name": "P. C. C. Freire"
                    },
                    {
                        "name": "Michael Kramer"
                    }
                ],
                "author_detail": {
                    "name": "Michael Kramer"
                },
                "arxiv_affiliation": "MPIFR",
                "author": "Michael Kramer",
                "arxiv_comment": "Astronomy and Astrophysics - A\\&A, In press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10061v1",
                "updated": "2024-11-15T09:23:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    23,
                    18,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T09:23:18Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    23,
                    18,
                    4,
                    320,
                    0
                ],
                "title": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation"
                },
                "summary": "Recent work on human animation usually involves audio, pose, or movement maps\nconditions, thereby achieves vivid animation quality. However, these methods\noften face practical challenges due to extra control conditions, cumbersome\ncondition injection modules, or limitation to head region driving. Hence, we\nask if it is possible to achieve striking half-body human animation while\nsimplifying unnecessary conditions. To this end, we propose a half-body human\nanimation method, dubbed EchoMimicV2, that leverages a novel Audio-Pose Dynamic\nHarmonization strategy, including Pose Sampling and Audio Diffusion, to enhance\nhalf-body details, facial and gestural expressiveness, and meanwhile reduce\nconditions redundancy. To compensate for the scarcity of half-body data, we\nutilize Head Partial Attention to seamlessly accommodate headshot data into our\ntraining framework, which can be omitted during inference, providing a free\nlunch for animation. Furthermore, we design the Phase-specific Denoising Loss\nto guide motion, detail, and low-level quality for animation in specific\nphases, respectively. Besides, we also present a novel benchmark for evaluating\nthe effectiveness of half-body human animation. Extensive experiments and\nanalyses demonstrate that EchoMimicV2 surpasses existing methods in both\nquantitative and qualitative evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work on human animation usually involves audio, pose, or movement maps\nconditions, thereby achieves vivid animation quality. However, these methods\noften face practical challenges due to extra control conditions, cumbersome\ncondition injection modules, or limitation to head region driving. Hence, we\nask if it is possible to achieve striking half-body human animation while\nsimplifying unnecessary conditions. To this end, we propose a half-body human\nanimation method, dubbed EchoMimicV2, that leverages a novel Audio-Pose Dynamic\nHarmonization strategy, including Pose Sampling and Audio Diffusion, to enhance\nhalf-body details, facial and gestural expressiveness, and meanwhile reduce\nconditions redundancy. To compensate for the scarcity of half-body data, we\nutilize Head Partial Attention to seamlessly accommodate headshot data into our\ntraining framework, which can be omitted during inference, providing a free\nlunch for animation. Furthermore, we design the Phase-specific Denoising Loss\nto guide motion, detail, and low-level quality for animation in specific\nphases, respectively. Besides, we also present a novel benchmark for evaluating\nthe effectiveness of half-body human animation. Extensive experiments and\nanalyses demonstrate that EchoMimicV2 surpasses existing methods in both\nquantitative and qualitative evaluations."
                },
                "authors": [
                    {
                        "name": "Rang Meng"
                    },
                    {
                        "name": "Xingyu Zhang"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Chenguang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Ma"
                },
                "author": "Chenguang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10055v1",
                "updated": "2024-11-15T09:17:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    17,
                    40,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T09:17:40Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    17,
                    40,
                    4,
                    320,
                    0
                ],
                "title": "Towards unearthing neglected climate innovations from scientific\n  literature using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards unearthing neglected climate innovations from scientific\n  literature using Large Language Models"
                },
                "summary": "Climate change poses an urgent global threat, needing the rapid\nidentification and deployment of innovative solutions. We hypothesise that many\nof these solutions already exist within scientific literature but remain\nunderutilised. To address this gap, this study employs a curated dataset\nsourced from OpenAlex, a comprehensive repository of scientific papers.\nUtilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate\ntitle-abstract pairs from scientific papers on seven dimensions, covering\nclimate change mitigation potential, stage of technological development, and\nreadiness for deployment. The outputs of the language models are then compared\nwith human evaluations to assess their effectiveness in identifying promising\nyet overlooked climate innovations. Our findings suggest that these LLM-based\nmodels can effectively augment human expertise, uncovering climate solutions\nthat are potentially impactful but with far greater speed, throughput and\nconsistency. Here, we focused on UK-based solutions, but the workflow is\nregion-agnostic. This work contributes to the discovery of neglected\ninnovations in scientific literature and demonstrates the potential of AI in\nenhancing climate action strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climate change poses an urgent global threat, needing the rapid\nidentification and deployment of innovative solutions. We hypothesise that many\nof these solutions already exist within scientific literature but remain\nunderutilised. To address this gap, this study employs a curated dataset\nsourced from OpenAlex, a comprehensive repository of scientific papers.\nUtilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate\ntitle-abstract pairs from scientific papers on seven dimensions, covering\nclimate change mitigation potential, stage of technological development, and\nreadiness for deployment. The outputs of the language models are then compared\nwith human evaluations to assess their effectiveness in identifying promising\nyet overlooked climate innovations. Our findings suggest that these LLM-based\nmodels can effectively augment human expertise, uncovering climate solutions\nthat are potentially impactful but with far greater speed, throughput and\nconsistency. Here, we focused on UK-based solutions, but the workflow is\nregion-agnostic. This work contributes to the discovery of neglected\ninnovations in scientific literature and demonstrates the potential of AI in\nenhancing climate action strategies."
                },
                "authors": [
                    {
                        "name": "César Quilodrán-Casas"
                    },
                    {
                        "name": "Christopher Waite"
                    },
                    {
                        "name": "Nicole Alhadeff"
                    },
                    {
                        "name": "Diyona Dsouza"
                    },
                    {
                        "name": "Cathal Hughes"
                    },
                    {
                        "name": "Larissa Kunstel-Tabet"
                    },
                    {
                        "name": "Alyssa Gilbert"
                    }
                ],
                "author_detail": {
                    "name": "Alyssa Gilbert"
                },
                "author": "Alyssa Gilbert",
                "arxiv_comment": "10 pages. Accepted in the LatinX in AI workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.01938v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.01938v4",
                "updated": "2024-11-15T09:01:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    1,
                    57,
                    4,
                    320,
                    0
                ],
                "published": "2022-11-03T16:25:57Z",
                "published_parsed": [
                    2022,
                    11,
                    3,
                    16,
                    25,
                    57,
                    3,
                    307,
                    0
                ],
                "title": "A novel family of beta mixture models for the differential analysis of\n  DNA methylation data: an application to prostate cancer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A novel family of beta mixture models for the differential analysis of\n  DNA methylation data: an application to prostate cancer"
                },
                "summary": "Identifying differentially methylated cytosine-guanine dinucleotide (CpG)\nsites between benign and tumour samples can assist in understanding disease.\nHowever, differential analysis of bounded DNA methylation data often requires\ndata transformation, reducing biological interpretability. To address this, a\nfamily of beta mixture models (BMMs) is proposed that (i) objectively infers\nmethylation state thresholds and (ii) identifies differentially methylated CpG\nsites (DMCs) given untransformed, beta-valued methylation data. The BMMs\nachieve this through model-based clustering of CpG sites and by employing\nparameter constraints, facilitating application to different study settings.\nInference proceeds via an expectation-maximisation algorithm, with an\napproximate maximization step providing tractability and computational\nfeasibility.\n  Performance of the BMMs is assessed through thorough simulation studies, and\nthe BMMs are used for differential analyses of DNA methylation data from a\nprostate cancer study. Intuitive and biologically interpretable methylation\nstate thresholds are inferred and DMCs are identified, including those related\nto genes such as GSTP1, RASSF1 and RARB, known for their role in prostate\ncancer development. Gene ontology analysis of the DMCs revealed significant\nenrichment in cancer-related pathways, demonstrating the utility of BMMs to\nreveal biologically relevant insights. An R package betaclust facilitates\nwidespread use of BMMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying differentially methylated cytosine-guanine dinucleotide (CpG)\nsites between benign and tumour samples can assist in understanding disease.\nHowever, differential analysis of bounded DNA methylation data often requires\ndata transformation, reducing biological interpretability. To address this, a\nfamily of beta mixture models (BMMs) is proposed that (i) objectively infers\nmethylation state thresholds and (ii) identifies differentially methylated CpG\nsites (DMCs) given untransformed, beta-valued methylation data. The BMMs\nachieve this through model-based clustering of CpG sites and by employing\nparameter constraints, facilitating application to different study settings.\nInference proceeds via an expectation-maximisation algorithm, with an\napproximate maximization step providing tractability and computational\nfeasibility.\n  Performance of the BMMs is assessed through thorough simulation studies, and\nthe BMMs are used for differential analyses of DNA methylation data from a\nprostate cancer study. Intuitive and biologically interpretable methylation\nstate thresholds are inferred and DMCs are identified, including those related\nto genes such as GSTP1, RASSF1 and RARB, known for their role in prostate\ncancer development. Gene ontology analysis of the DMCs revealed significant\nenrichment in cancer-related pathways, demonstrating the utility of BMMs to\nreveal biologically relevant insights. An R package betaclust facilitates\nwidespread use of BMMs."
                },
                "authors": [
                    {
                        "name": "Koyel Majumdar"
                    },
                    {
                        "name": "Romina Silva"
                    },
                    {
                        "name": "Antoinette Sabrina Perry"
                    },
                    {
                        "name": "Ronald William Watson"
                    },
                    {
                        "name": "Andrea Rau"
                    },
                    {
                        "name": "Florence Jaffrezic"
                    },
                    {
                        "name": "Thomas Brendan Murphy"
                    },
                    {
                        "name": "Isobel Claire Gormley"
                    }
                ],
                "author_detail": {
                    "name": "Isobel Claire Gormley"
                },
                "author": "Isobel Claire Gormley",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.01938v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.01938v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06740v3",
                "updated": "2024-11-15T08:36:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    8,
                    36,
                    51,
                    4,
                    320,
                    0
                ],
                "published": "2024-08-13T09:00:35Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    0,
                    35,
                    1,
                    226,
                    0
                ],
                "title": "DiffLoRA: Generating Personalized Low-Rank Adaptation Weights with\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffLoRA: Generating Personalized Low-Rank Adaptation Weights with\n  Diffusion"
                },
                "summary": "Personalized text-to-image generation has gained significant attention for\nits capability to generate high-fidelity portraits of specific identities\nconditioned on user-defined prompts. Existing methods typically involve\ntest-time fine-tuning or incorporating an additional pre-trained branch.\nHowever, these approaches struggle to simultaneously address efficiency,\nidentity fidelity, and the preservation of the model's original generative\ncapabilities. In this paper, we propose DiffLoRA, an efficient method that\nleverages the diffusion model as a hypernetwork to predict personalized\nLow-Rank Adaptation (LoRA) weights based on the reference images. By\nincorporating these LoRA weights into the off-the-shelf text-to-image model,\nDiffLoRA enables zero-shot personalization during inference, eliminating the\nneed for post-processing optimization. Moreover, we introduce a novel\nidentity-oriented LoRA weights construction pipeline to facilitate the training\nprocess of DiffLoRA. The dataset generated through this pipeline enables\nDiffLoRA to produce consistently high-quality LoRA weights. Notably, the\ndistinctive properties of the diffusion model enhance the generation of\nsuperior weights by employing probabilistic modeling to capture intricate\nstructural patterns and thoroughly explore the weight space. Comprehensive\nexperimental results demonstrate that DiffLoRA outperforms existing\npersonalization approaches across multiple benchmarks, achieving both time\nefficiency and maintaining identity fidelity throughout the personalization\nprocess.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized text-to-image generation has gained significant attention for\nits capability to generate high-fidelity portraits of specific identities\nconditioned on user-defined prompts. Existing methods typically involve\ntest-time fine-tuning or incorporating an additional pre-trained branch.\nHowever, these approaches struggle to simultaneously address efficiency,\nidentity fidelity, and the preservation of the model's original generative\ncapabilities. In this paper, we propose DiffLoRA, an efficient method that\nleverages the diffusion model as a hypernetwork to predict personalized\nLow-Rank Adaptation (LoRA) weights based on the reference images. By\nincorporating these LoRA weights into the off-the-shelf text-to-image model,\nDiffLoRA enables zero-shot personalization during inference, eliminating the\nneed for post-processing optimization. Moreover, we introduce a novel\nidentity-oriented LoRA weights construction pipeline to facilitate the training\nprocess of DiffLoRA. The dataset generated through this pipeline enables\nDiffLoRA to produce consistently high-quality LoRA weights. Notably, the\ndistinctive properties of the diffusion model enhance the generation of\nsuperior weights by employing probabilistic modeling to capture intricate\nstructural patterns and thoroughly explore the weight space. Comprehensive\nexperimental results demonstrate that DiffLoRA outperforms existing\npersonalization approaches across multiple benchmarks, achieving both time\nefficiency and maintaining identity fidelity throughout the personalization\nprocess."
                },
                "authors": [
                    {
                        "name": "Yujia Wu"
                    },
                    {
                        "name": "Yiming Shi"
                    },
                    {
                        "name": "Jiwei Wei"
                    },
                    {
                        "name": "Chengwei Sun"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Heng Tao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Heng Tao Shen"
                },
                "author": "Heng Tao Shen",
                "arxiv_comment": "9 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10034v1",
                "updated": "2024-11-15T08:32:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    8,
                    32,
                    3,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T08:32:03Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    8,
                    32,
                    3,
                    4,
                    320,
                    0
                ],
                "title": "EveGuard: Defeating Vibration-based Side-Channel Eavesdropping with\n  Audio Adversarial Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EveGuard: Defeating Vibration-based Side-Channel Eavesdropping with\n  Audio Adversarial Perturbations"
                },
                "summary": "Vibrometry-based side channels pose a significant privacy risk, exploiting\nsensors like mmWave radars, light sensors, and accelerometers to detect\nvibrations from sound sources or proximate objects, enabling speech\neavesdropping. Despite various proposed defenses, these involve costly hardware\nsolutions with inherent physical limitations. This paper presents EveGuard, a\nsoftware-driven defense framework that creates adversarial audio, protecting\nvoice privacy from side channels without compromising human perception. We\nleverage the distinct sensing capabilities of side channels and traditional\nmicrophones where side channels capture vibrations and microphones record\nchanges in air pressure, resulting in different frequency responses. EveGuard\nfirst proposes a perturbation generator model (PGM) that effectively suppresses\nsensor-based eavesdropping while maintaining high audio quality. Second, to\nenable end-to-end training of PGM, we introduce a new domain translation task\ncalled Eve-GAN for inferring an eavesdropped signal from a given audio. We\nfurther apply few-shot learning to mitigate the data collection overhead for\nEve-GAN training. Our extensive experiments show that EveGuard achieves a\nprotection rate of more than 97 percent from audio classifiers and\nsignificantly hinders eavesdropped audio reconstruction. We further validate\nthe performance of EveGuard across three adaptive attack mechanisms. We have\nconducted a user study to verify the perceptual quality of our perturbed audio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vibrometry-based side channels pose a significant privacy risk, exploiting\nsensors like mmWave radars, light sensors, and accelerometers to detect\nvibrations from sound sources or proximate objects, enabling speech\neavesdropping. Despite various proposed defenses, these involve costly hardware\nsolutions with inherent physical limitations. This paper presents EveGuard, a\nsoftware-driven defense framework that creates adversarial audio, protecting\nvoice privacy from side channels without compromising human perception. We\nleverage the distinct sensing capabilities of side channels and traditional\nmicrophones where side channels capture vibrations and microphones record\nchanges in air pressure, resulting in different frequency responses. EveGuard\nfirst proposes a perturbation generator model (PGM) that effectively suppresses\nsensor-based eavesdropping while maintaining high audio quality. Second, to\nenable end-to-end training of PGM, we introduce a new domain translation task\ncalled Eve-GAN for inferring an eavesdropped signal from a given audio. We\nfurther apply few-shot learning to mitigate the data collection overhead for\nEve-GAN training. Our extensive experiments show that EveGuard achieves a\nprotection rate of more than 97 percent from audio classifiers and\nsignificantly hinders eavesdropped audio reconstruction. We further validate\nthe performance of EveGuard across three adaptive attack mechanisms. We have\nconducted a user study to verify the perceptual quality of our perturbed audio."
                },
                "authors": [
                    {
                        "name": "Jung-Woo Chang"
                    },
                    {
                        "name": "Ke Sun"
                    },
                    {
                        "name": "David Xia"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10032v1",
                "updated": "2024-11-15T08:20:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    8,
                    20,
                    26,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T08:20:26Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    8,
                    20,
                    26,
                    4,
                    320,
                    0
                ],
                "title": "VMID: A Multimodal Fusion LLM Framework for Detecting and Identifying\n  Misinformation of Short Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VMID: A Multimodal Fusion LLM Framework for Detecting and Identifying\n  Misinformation of Short Videos"
                },
                "summary": "Short video platforms have become important channels for news dissemination,\noffering a highly engaging and immediate way for users to access current events\nand share information. However, these platforms have also emerged as\nsignificant conduits for the rapid spread of misinformation, as fake news and\nrumors can leverage the visual appeal and wide reach of short videos to\ncirculate extensively among audiences. Existing fake news detection methods\nmainly rely on single-modal information, such as text or images, or apply only\nbasic fusion techniques, limiting their ability to handle the complex,\nmulti-layered information inherent in short videos. To address these\nlimitations, this paper presents a novel fake news detection method based on\nmultimodal information, designed to identify misinformation through a\nmulti-level analysis of video content. This approach effectively utilizes\ndifferent modal representations to generate a unified textual description,\nwhich is then fed into a large language model for comprehensive evaluation. The\nproposed framework successfully integrates multimodal features within videos,\nsignificantly enhancing the accuracy and reliability of fake news detection.\nExperimental results demonstrate that the proposed approach outperforms\nexisting models in terms of accuracy, robustness, and utilization of multimodal\ninformation, achieving an accuracy of 90.93%, which is significantly higher\nthan the best baseline model (SV-FEND) at 81.05%. Furthermore, case studies\nprovide additional evidence of the effectiveness of the approach in accurately\ndistinguishing between fake news, debunking content, and real incidents,\nhighlighting its reliability and robustness in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Short video platforms have become important channels for news dissemination,\noffering a highly engaging and immediate way for users to access current events\nand share information. However, these platforms have also emerged as\nsignificant conduits for the rapid spread of misinformation, as fake news and\nrumors can leverage the visual appeal and wide reach of short videos to\ncirculate extensively among audiences. Existing fake news detection methods\nmainly rely on single-modal information, such as text or images, or apply only\nbasic fusion techniques, limiting their ability to handle the complex,\nmulti-layered information inherent in short videos. To address these\nlimitations, this paper presents a novel fake news detection method based on\nmultimodal information, designed to identify misinformation through a\nmulti-level analysis of video content. This approach effectively utilizes\ndifferent modal representations to generate a unified textual description,\nwhich is then fed into a large language model for comprehensive evaluation. The\nproposed framework successfully integrates multimodal features within videos,\nsignificantly enhancing the accuracy and reliability of fake news detection.\nExperimental results demonstrate that the proposed approach outperforms\nexisting models in terms of accuracy, robustness, and utilization of multimodal\ninformation, achieving an accuracy of 90.93%, which is significantly higher\nthan the best baseline model (SV-FEND) at 81.05%. Furthermore, case studies\nprovide additional evidence of the effectiveness of the approach in accurately\ndistinguishing between fake news, debunking content, and real incidents,\nhighlighting its reliability and robustness in real-world applications."
                },
                "authors": [
                    {
                        "name": "Weihao Zhong"
                    },
                    {
                        "name": "Yinhao Xiao"
                    },
                    {
                        "name": "Minghui Xu"
                    },
                    {
                        "name": "Xiuzhen Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xiuzhen Cheng"
                },
                "author": "Xiuzhen Cheng",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2211.10973 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09312v2",
                "updated": "2024-11-15T08:17:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    8,
                    17,
                    22,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-14T09:38:58Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    38,
                    58,
                    3,
                    319,
                    0
                ],
                "title": "Approximate Probabilistic Inference for Time-Series Data A Robust Latent\n  Gaussian Model With Temporal Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Probabilistic Inference for Time-Series Data A Robust Latent\n  Gaussian Model With Temporal Awareness"
                },
                "summary": "The development of robust generative models for highly varied non-stationary\ntime series data is a complex yet important problem. Traditional models for\ntime series data prediction, such as Long Short-Term Memory (LSTM), are\ninefficient and generalize poorly as they cannot capture complex temporal\nrelationships. In this paper, we present a probabilistic generative model that\ncan be trained to capture temporal information, and that is robust to data\nerrors. We call it Time Deep Latent Gaussian Model (tDLGM). Its novel\narchitecture is inspired by Deep Latent Gaussian Model (DLGM). Our model is\ntrained to minimize a loss function based on the negative log loss. One\ncontributing factor to Time Deep Latent Gaussian Model (tDLGM) robustness is\nour regularizer, which accounts for data trends. Experiments conducted show\nthat tDLGM is able to reconstruct and generate complex time series data, and\nthat it is robust against to noise and faulty data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of robust generative models for highly varied non-stationary\ntime series data is a complex yet important problem. Traditional models for\ntime series data prediction, such as Long Short-Term Memory (LSTM), are\ninefficient and generalize poorly as they cannot capture complex temporal\nrelationships. In this paper, we present a probabilistic generative model that\ncan be trained to capture temporal information, and that is robust to data\nerrors. We call it Time Deep Latent Gaussian Model (tDLGM). Its novel\narchitecture is inspired by Deep Latent Gaussian Model (DLGM). Our model is\ntrained to minimize a loss function based on the negative log loss. One\ncontributing factor to Time Deep Latent Gaussian Model (tDLGM) robustness is\nour regularizer, which accounts for data trends. Experiments conducted show\nthat tDLGM is able to reconstruct and generate complex time series data, and\nthat it is robust against to noise and faulty data."
                },
                "authors": [
                    {
                        "name": "Anton Johansson"
                    },
                    {
                        "name": "Arunselvan Ramaswamy"
                    }
                ],
                "author_detail": {
                    "name": "Arunselvan Ramaswamy"
                },
                "author": "Arunselvan Ramaswamy",
                "arxiv_comment": "New revision added a space between \"for\" and \"Time-Series\" in the\n  title",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10027v1",
                "updated": "2024-11-15T08:13:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    8,
                    13,
                    51,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T08:13:51Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    8,
                    13,
                    51,
                    4,
                    320,
                    0
                ],
                "title": "XLSR-Mamba: A Dual-Column Bidirectional State Space Model for Spoofing\n  Attack Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XLSR-Mamba: A Dual-Column Bidirectional State Space Model for Spoofing\n  Attack Detection"
                },
                "summary": "Transformers and their variants have achieved great success in speech\nprocessing. However, their multi-head self-attention mechanism is\ncomputationally expensive. Therefore, one novel selective state space model,\nMamba, has been proposed as an alternative. Building on its success in\nautomatic speech recognition, we apply Mamba for spoofing attack detection.\nMamba is well-suited for this task as it can capture the artifacts in spoofed\nspeech signals by handling long-length sequences. However, Mamba's performance\nmay suffer when it is trained with limited labeled data. To mitigate this, we\npropose combining a new structure of Mamba based on a dual-column architecture\nwith self-supervised learning, using the pre-trained wav2vec 2.0 model. The\nexperiments show that our proposed approach achieves competitive results and\nfaster inference on the ASVspoof 2021 LA and DF datasets, and on the more\nchallenging In-the-Wild dataset, it emerges as the strongest candidate for\nspoofing attack detection. The code will be publicly released in due course.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers and their variants have achieved great success in speech\nprocessing. However, their multi-head self-attention mechanism is\ncomputationally expensive. Therefore, one novel selective state space model,\nMamba, has been proposed as an alternative. Building on its success in\nautomatic speech recognition, we apply Mamba for spoofing attack detection.\nMamba is well-suited for this task as it can capture the artifacts in spoofed\nspeech signals by handling long-length sequences. However, Mamba's performance\nmay suffer when it is trained with limited labeled data. To mitigate this, we\npropose combining a new structure of Mamba based on a dual-column architecture\nwith self-supervised learning, using the pre-trained wav2vec 2.0 model. The\nexperiments show that our proposed approach achieves competitive results and\nfaster inference on the ASVspoof 2021 LA and DF datasets, and on the more\nchallenging In-the-Wild dataset, it emerges as the strongest candidate for\nspoofing attack detection. The code will be publicly released in due course."
                },
                "authors": [
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Rohan Kumar Das"
                    }
                ],
                "author_detail": {
                    "name": "Rohan Kumar Das"
                },
                "author": "Rohan Kumar Das",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10020v1",
                "updated": "2024-11-15T07:54:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    54,
                    19,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T07:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    54,
                    19,
                    4,
                    320,
                    0
                ],
                "title": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?"
                },
                "summary": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BiomedBERT in terms of performance, generalizability, computational\nresources, and throughput to BiomedBERT. Results: LLaMA models outperformed\nBiomedBERT across datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BiomedBERT by 7%\n(F1) on NER and 4% on RE. However, LLaMA models required more computing\nresources and ran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE\npackage featuring both models, available at https://kiwi.clinicalnlp.org/.\nConclusion: This study is among the first to develop and evaluate a\ncomprehensive clinical IE system using open-source LLMs. Results indicate that\nLLaMA models outperform BiomedBERT for clinical NER and RE but with higher\ncomputational costs and lower throughputs. These findings highlight that\nchoosing between LLMs and traditional deep learning methods for clinical IE\napplications should remain task-specific, taking into account both performance\nmetrics and practical considerations such as available computing resources and\nthe intended use case scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BiomedBERT in terms of performance, generalizability, computational\nresources, and throughput to BiomedBERT. Results: LLaMA models outperformed\nBiomedBERT across datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BiomedBERT by 7%\n(F1) on NER and 4% on RE. However, LLaMA models required more computing\nresources and ran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE\npackage featuring both models, available at https://kiwi.clinicalnlp.org/.\nConclusion: This study is among the first to develop and evaluate a\ncomprehensive clinical IE system using open-source LLMs. Results indicate that\nLLaMA models outperform BiomedBERT for clinical NER and RE but with higher\ncomputational costs and lower throughputs. These findings highlight that\nchoosing between LLMs and traditional deep learning methods for clinical IE\napplications should remain task-specific, taking into account both performance\nmetrics and practical considerations such as available computing resources and\nthe intended use case scenarios."
                },
                "authors": [
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Xu Zuo"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Vipina K. Keloth"
                    },
                    {
                        "name": "Vincent J. Zhang"
                    },
                    {
                        "name": "Ruey-Ling Weng"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Xiaoqian Jiang"
                    },
                    {
                        "name": "Kirk E. Roberts"
                    },
                    {
                        "name": "Hua Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xu"
                },
                "author": "Hua Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19282v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19282v2",
                "updated": "2024-11-15T07:44:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    44,
                    9,
                    4,
                    320,
                    0
                ],
                "published": "2024-09-28T08:23:23Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    8,
                    23,
                    23,
                    5,
                    272,
                    0
                ],
                "title": "Reliable Interval Estimation for the Fidelity of Entangled States in\n  Scenarios with General Noise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable Interval Estimation for the Fidelity of Entangled States in\n  Scenarios with General Noise"
                },
                "summary": "Fidelity estimation for entangled states constitutes an essential building\nblock for quality control and error detection in quantum networks. Nonetheless,\nquantum networks often encounter heterogeneous and correlated noise, leading to\nexcessive uncertainty in the estimated fidelity. In this paper, the uncertainty\nassociated with the estimated fidelity under conditions of general noise is\nconstrained by jointly employing random sampling, a thought experiment, and\nBayesian inference, resulting in a credible interval for fidelity that is valid\nin the presence of general noise. The proposed credible interval incorporates\nall even moments of the posterior distribution to enhance estimation accuracy.\nFactors influencing the estimation accuracy are identified and analyzed.\nSpecifically, the issue of excessive measurements is addressed, emphasizing the\nnecessity of properly determining the measurement ratio for fidelity estimation\nunder general noise conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fidelity estimation for entangled states constitutes an essential building\nblock for quality control and error detection in quantum networks. Nonetheless,\nquantum networks often encounter heterogeneous and correlated noise, leading to\nexcessive uncertainty in the estimated fidelity. In this paper, the uncertainty\nassociated with the estimated fidelity under conditions of general noise is\nconstrained by jointly employing random sampling, a thought experiment, and\nBayesian inference, resulting in a credible interval for fidelity that is valid\nin the presence of general noise. The proposed credible interval incorporates\nall even moments of the posterior distribution to enhance estimation accuracy.\nFactors influencing the estimation accuracy are identified and analyzed.\nSpecifically, the issue of excessive measurements is addressed, emphasizing the\nnecessity of properly determining the measurement ratio for fidelity estimation\nunder general noise conditions."
                },
                "authors": [
                    {
                        "name": "Liangzhong Ruan"
                    },
                    {
                        "name": "Bas Dirkse"
                    }
                ],
                "author_detail": {
                    "name": "Bas Dirkse"
                },
                "author": "Bas Dirkse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19282v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19282v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10009v1",
                "updated": "2024-11-15T07:42:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    42,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T07:42:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    42,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "Semiparametric inference for impulse response functions using\n  double/debiased machine learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semiparametric inference for impulse response functions using\n  double/debiased machine learning"
                },
                "summary": "We introduce a double/debiased machine learning (DML) estimator for the\nimpulse response function (IRF) in settings where a time series of interest is\nsubjected to multiple discrete treatments, assigned over time, which can have a\ncausal effect on future outcomes. The proposed estimator can rely on fully\nnonparametric relations between treatment and outcome variables, opening up the\npossibility to use flexible machine learning approaches to estimate IRFs. To\nthis end, we extend the theory of DML from an i.i.d. to a time series setting\nand show that the proposed DML estimator for the IRF is consistent and\nasymptotically normally distributed at the parametric rate, allowing for\nsemiparametric inference for dynamic effects in a time series setting. The\nproperties of the estimator are validated numerically in finite samples by\napplying it to learn the IRF in the presence of serial dependence in both the\nconfounder and observation innovation processes. We also illustrate the\nmethodology empirically by applying it to the estimation of the effects of\nmacroeconomic shocks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a double/debiased machine learning (DML) estimator for the\nimpulse response function (IRF) in settings where a time series of interest is\nsubjected to multiple discrete treatments, assigned over time, which can have a\ncausal effect on future outcomes. The proposed estimator can rely on fully\nnonparametric relations between treatment and outcome variables, opening up the\npossibility to use flexible machine learning approaches to estimate IRFs. To\nthis end, we extend the theory of DML from an i.i.d. to a time series setting\nand show that the proposed DML estimator for the IRF is consistent and\nasymptotically normally distributed at the parametric rate, allowing for\nsemiparametric inference for dynamic effects in a time series setting. The\nproperties of the estimator are validated numerically in finite samples by\napplying it to learn the IRF in the presence of serial dependence in both the\nconfounder and observation innovation processes. We also illustrate the\nmethodology empirically by applying it to the estimation of the effects of\nmacroeconomic shocks."
                },
                "authors": [
                    {
                        "name": "Daniele Ballinari"
                    },
                    {
                        "name": "Alexander Wehrli"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Wehrli"
                },
                "author": "Alexander Wehrli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10008v1",
                "updated": "2024-11-15T07:42:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    42,
                    1,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T07:42:01Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    42,
                    1,
                    4,
                    320,
                    0
                ],
                "title": "Graph-based Complexity for Causal Effect by Empirical Plug-in",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Complexity for Causal Effect by Empirical Plug-in"
                },
                "summary": "This paper focuses on the computational complexity of computing empirical\nplug-in estimates for causal effect queries. Given a causal graph and\nobservational data, any identifiable causal query can be estimated from an\nexpression over the observed variables, called the estimand. The estimand can\nthen be evaluated by plugging in probabilities computed empirically from data.\nIn contrast to conventional wisdom, which assumes that high dimensional\nprobabilistic functions will lead to exponential evaluation time of the\nestimand. We show that computation can be done efficiently, potentially in time\nlinear in the data size, depending on the estimand's hypergraph.\n  In particular, we show that both the treewidth and hypertree width of the\nestimand's structure bound the evaluation complexity of the plug-in estimands,\nanalogous to their role in the complexity of probabilistic inference in\ngraphical models. Often, the hypertree width provides a more effective bound,\nsince the empirical distributions are sparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper focuses on the computational complexity of computing empirical\nplug-in estimates for causal effect queries. Given a causal graph and\nobservational data, any identifiable causal query can be estimated from an\nexpression over the observed variables, called the estimand. The estimand can\nthen be evaluated by plugging in probabilities computed empirically from data.\nIn contrast to conventional wisdom, which assumes that high dimensional\nprobabilistic functions will lead to exponential evaluation time of the\nestimand. We show that computation can be done efficiently, potentially in time\nlinear in the data size, depending on the estimand's hypergraph.\n  In particular, we show that both the treewidth and hypertree width of the\nestimand's structure bound the evaluation complexity of the plug-in estimands,\nanalogous to their role in the complexity of probabilistic inference in\ngraphical models. Often, the hypertree width provides a more effective bound,\nsince the empirical distributions are sparse."
                },
                "authors": [
                    {
                        "name": "Rina Dechter"
                    },
                    {
                        "name": "Annie Raichev"
                    },
                    {
                        "name": "Alexander Ihler"
                    },
                    {
                        "name": "Jin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Jin Tian"
                },
                "author": "Jin Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10006v1",
                "updated": "2024-11-15T07:35:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    35,
                    47,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T07:35:47Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    35,
                    47,
                    4,
                    320,
                    0
                ],
                "title": "Orca: Enhancing Role-Playing Abilities of Large Language Models by\n  Integrating Personality Traits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orca: Enhancing Role-Playing Abilities of Large Language Models by\n  Integrating Personality Traits"
                },
                "summary": "Large language models has catalyzed the development of personalized dialogue\nsystems, numerous role-playing conversational agents have emerged. While\nprevious research predominantly focused on enhancing the model's capability to\nfollow instructions by designing character profiles, neglecting the\npsychological factors that drive human conversations. In this paper, we propose\nOrca, a framework for data processing and training LLMs of custom characters by\nintegrating personality traits. Orca comprises four stages: (1) Personality\ntraits inferring, leverage LLMs to infer user's BigFive personality trait\nreports and scores. (2) Data Augment, simulate user's profile, background\nstory, and psychological activities. (3) Dataset construction,\npersonality-conditioned instruction prompting (PCIP) to stimulate LLMs. (4)\nModeling and Training, personality-conditioned instruction tuning (PTIT and\nPSIT), using the generated data to enhance existing open-source LLMs. We\nintroduce OrcaBench, the first benchmark for evaluating the quality of content\ngenerated by LLMs on social platforms across multiple scales. Our experiments\ndemonstrate that our proposed model achieves superior performance on this\nbenchmark, demonstrating its excellence and effectiveness in perceiving\npersonality traits that significantly improve role-playing abilities. Our Code\nis available at https://github.com/Aipura/Orca.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models has catalyzed the development of personalized dialogue\nsystems, numerous role-playing conversational agents have emerged. While\nprevious research predominantly focused on enhancing the model's capability to\nfollow instructions by designing character profiles, neglecting the\npsychological factors that drive human conversations. In this paper, we propose\nOrca, a framework for data processing and training LLMs of custom characters by\nintegrating personality traits. Orca comprises four stages: (1) Personality\ntraits inferring, leverage LLMs to infer user's BigFive personality trait\nreports and scores. (2) Data Augment, simulate user's profile, background\nstory, and psychological activities. (3) Dataset construction,\npersonality-conditioned instruction prompting (PCIP) to stimulate LLMs. (4)\nModeling and Training, personality-conditioned instruction tuning (PTIT and\nPSIT), using the generated data to enhance existing open-source LLMs. We\nintroduce OrcaBench, the first benchmark for evaluating the quality of content\ngenerated by LLMs on social platforms across multiple scales. Our experiments\ndemonstrate that our proposed model achieves superior performance on this\nbenchmark, demonstrating its excellence and effectiveness in perceiving\npersonality traits that significantly improve role-playing abilities. Our Code\nis available at https://github.com/Aipura/Orca."
                },
                "authors": [
                    {
                        "name": "Yuxuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Huang"
                },
                "author": "Yuxuan Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04422v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04422v5",
                "updated": "2024-11-15T07:07:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    7,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-10-06T09:29:19Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    9,
                    29,
                    19,
                    6,
                    280,
                    0
                ],
                "title": "Hyper-multi-step: The Truth Behind Difficult Long-context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyper-multi-step: The Truth Behind Difficult Long-context Tasks"
                },
                "summary": "Long-context language models (LCLM), characterized by their extensive context\nwindow, is becoming increasingly popular. Meanwhile, many long-context\nbenchmarks present challenging tasks that even the most advanced LCLMs struggle\nto complete. However, the underlying sources of various challenging\nlong-context tasks have seldom been studied. To bridge this gap, we conduct\nexperiments to indicate their difficulty stems primarily from two basic issues:\n\"multi-matching retrieval,\" which requires the simultaneous retrieval of\nmultiple items, and \"logic-based retrieval,\" which necessitates logical\njudgment within retrieval criteria. These two problems, while seemingly\nstraightforward, actually exceed the capabilities of LCLMs because they are\nproven to be hyper-multi-step (demanding numerous steps to solve) in nature.\nThis finding could explain why LLMs struggle with more advanced long-context\ntasks, providing a more accurate perspective for rethinking solutions for them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models (LCLM), characterized by their extensive context\nwindow, is becoming increasingly popular. Meanwhile, many long-context\nbenchmarks present challenging tasks that even the most advanced LCLMs struggle\nto complete. However, the underlying sources of various challenging\nlong-context tasks have seldom been studied. To bridge this gap, we conduct\nexperiments to indicate their difficulty stems primarily from two basic issues:\n\"multi-matching retrieval,\" which requires the simultaneous retrieval of\nmultiple items, and \"logic-based retrieval,\" which necessitates logical\njudgment within retrieval criteria. These two problems, while seemingly\nstraightforward, actually exceed the capabilities of LCLMs because they are\nproven to be hyper-multi-step (demanding numerous steps to solve) in nature.\nThis finding could explain why LLMs struggle with more advanced long-context\ntasks, providing a more accurate perspective for rethinking solutions for them."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Ma Xiufa"
                    },
                    {
                        "name": "Fang Jianwei"
                    },
                    {
                        "name": "Zhi Xu"
                    },
                    {
                        "name": "Su Guangyao"
                    },
                    {
                        "name": "Wang Jiancheng"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Zhixiao Qi"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Weifeng Liu"
                    },
                    {
                        "name": "Ran Chen"
                    },
                    {
                        "name": "Ji Pei"
                    }
                ],
                "author_detail": {
                    "name": "Ji Pei"
                },
                "author": "Ji Pei",
                "arxiv_comment": "Our code is publicly available at\n  https://github.com/yuyijiong/hard_retrieval_for_llm and the datasets is at\n  https://huggingface.co/datasets/yuyijiong/difficult_retrieval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04422v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04422v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.00381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.00381v2",
                "updated": "2024-11-15T06:51:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    51,
                    26,
                    4,
                    320,
                    0
                ],
                "published": "2022-03-01T12:01:53Z",
                "published_parsed": [
                    2022,
                    3,
                    1,
                    12,
                    1,
                    53,
                    1,
                    60,
                    0
                ],
                "title": "A fully precessing higher-mode surrogate model of effective-one-body\n  waveforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fully precessing higher-mode surrogate model of effective-one-body\n  waveforms"
                },
                "summary": "We present a surrogate model of \\texttt{SEOBNRv4PHM}, a fully precessing\ntime-domain effective-one-body waveform model including subdominant modes. We\nfollow an approach similar to that used to build recent numerical relativity\nsurrogate models. Our surrogate is 5000M in duration, covers mass-ratios up to\n1:20 and dimensionless spin magnitudes up to 0.8. Validating the surrogate\nagainst an independent test set we find that the bulk of the surrogate errors\nis less than $\\sim 1\\%$ in mismatch, which is similar to the modelling error of\n\\texttt{SEOBNRv4PHM} itself. At high total mass a few percent of configurations\ncan exceed this threshold if they are highly precessing and they exceed a\nmass-ratio of 1:4. This surrogate is nearly two orders of magnitude faster than\nthe underlying time-domain \\texttt{SEOBNRv4PHM} model and can be evaluated in\n$\\sim 50$ ms. Bayesian inference analyses with \\texttt{SEOBNRv4PHM} are\ntypically very computationally demanding and can take from weeks to months to\ncomplete. The two order of magnitude speedup attained by our surrogate model\nenables practical parameter estimation analyses with this waveform family. This\nis \\emph{crucial} because Bayesian inference allows us to recover the masses\nand spins of binary black hole mergers given a model of the emitted\ngravitational waveform along with a description of the noise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a surrogate model of \\texttt{SEOBNRv4PHM}, a fully precessing\ntime-domain effective-one-body waveform model including subdominant modes. We\nfollow an approach similar to that used to build recent numerical relativity\nsurrogate models. Our surrogate is 5000M in duration, covers mass-ratios up to\n1:20 and dimensionless spin magnitudes up to 0.8. Validating the surrogate\nagainst an independent test set we find that the bulk of the surrogate errors\nis less than $\\sim 1\\%$ in mismatch, which is similar to the modelling error of\n\\texttt{SEOBNRv4PHM} itself. At high total mass a few percent of configurations\ncan exceed this threshold if they are highly precessing and they exceed a\nmass-ratio of 1:4. This surrogate is nearly two orders of magnitude faster than\nthe underlying time-domain \\texttt{SEOBNRv4PHM} model and can be evaluated in\n$\\sim 50$ ms. Bayesian inference analyses with \\texttt{SEOBNRv4PHM} are\ntypically very computationally demanding and can take from weeks to months to\ncomplete. The two order of magnitude speedup attained by our surrogate model\nenables practical parameter estimation analyses with this waveform family. This\nis \\emph{crucial} because Bayesian inference allows us to recover the masses\nand spins of binary black hole mergers given a model of the emitted\ngravitational waveform along with a description of the noise."
                },
                "authors": [
                    {
                        "name": "Bhooshan Gadre"
                    },
                    {
                        "name": "Michael Pürrer"
                    },
                    {
                        "name": "Scott E. Field"
                    },
                    {
                        "name": "Serguei Ossokine"
                    },
                    {
                        "name": "Vijay Varma"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Varma"
                },
                "author": "Vijay Varma",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.00381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.00381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11282v3",
                "updated": "2024-11-15T06:48:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    48,
                    58,
                    4,
                    320,
                    0
                ],
                "published": "2023-12-18T15:23:06Z",
                "published_parsed": [
                    2023,
                    12,
                    18,
                    15,
                    23,
                    6,
                    0,
                    352,
                    0
                ],
                "title": "Evaluating and Enhancing Large Language Models for Conversational\n  Reasoning on Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Enhancing Large Language Models for Conversational\n  Reasoning on Knowledge Graphs"
                },
                "summary": "The development of large language models (LLMs) has been catalyzed by\nadvancements in pre-training techniques. These models have demonstrated robust\nreasoning capabilities through manually designed prompts. In this work, we\nevaluate the conversational reasoning capabilities of the current\nstate-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the\nperformance of LLMs is constrained due to a lack of KG environment awareness\nand the difficulties in developing effective optimization mechanisms for\nintermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG\nreasoning agent designed to deliver precise and adaptable predictions on KG\npaths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate\nstate information within each reasoning step. We reframe the challenge of\nmulti-hop reasoning on the KG as a sequential decision-making task. Utilizing\nthe Proximal Policy Optimization (PPO) online policy gradient reinforcement\nlearning algorithm, our model is optimized to learn from rich reward signals.\nAdditionally, we conduct an evaluation of our model and GPT-4 on the OpenDialKG\ndataset. The experimental results reveal that LLaMA-2-7B-ARK outperforms the\ncurrent state-of-the-art model by 5.28 percentage points, with a performance\nrate of 36.39% on the target@1 evaluation metric. Meanwhile, GPT-4 scored\n14.91%, further demonstrating the effectiveness of our method. Our code is\navailable on GitHub (https://github.com/Aipura/LLM-ARK) for further access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has been catalyzed by\nadvancements in pre-training techniques. These models have demonstrated robust\nreasoning capabilities through manually designed prompts. In this work, we\nevaluate the conversational reasoning capabilities of the current\nstate-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the\nperformance of LLMs is constrained due to a lack of KG environment awareness\nand the difficulties in developing effective optimization mechanisms for\nintermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG\nreasoning agent designed to deliver precise and adaptable predictions on KG\npaths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate\nstate information within each reasoning step. We reframe the challenge of\nmulti-hop reasoning on the KG as a sequential decision-making task. Utilizing\nthe Proximal Policy Optimization (PPO) online policy gradient reinforcement\nlearning algorithm, our model is optimized to learn from rich reward signals.\nAdditionally, we conduct an evaluation of our model and GPT-4 on the OpenDialKG\ndataset. The experimental results reveal that LLaMA-2-7B-ARK outperforms the\ncurrent state-of-the-art model by 5.28 percentage points, with a performance\nrate of 36.39% on the target@1 evaluation metric. Meanwhile, GPT-4 scored\n14.91%, further demonstrating the effectiveness of our method. Our code is\navailable on GitHub (https://github.com/Aipura/LLM-ARK) for further access."
                },
                "authors": [
                    {
                        "name": "Yuxuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Huang"
                },
                "author": "Yuxuan Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09978v1",
                "updated": "2024-11-15T06:21:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    21,
                    13,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T06:21:13Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    21,
                    13,
                    4,
                    320,
                    0
                ],
                "title": "HistoLens: An LLM-Powered Framework for Multi-Layered Analysis of\n  Historical Texts -- A Case Application of Yantie Lun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HistoLens: An LLM-Powered Framework for Multi-Layered Analysis of\n  Historical Texts -- A Case Application of Yantie Lun"
                },
                "summary": "This paper proposes HistoLens, a multi-layered analysis framework for\nhistorical texts based on Large Language Models (LLMs). Using the important\nWestern Han dynasty text \"Yantie Lun\" as a case study, we demonstrate the\nframework's potential applications in historical research and education.\nHistoLens integrates NLP technology (especially LLMs), including named entity\nrecognition, knowledge graph construction, and geographic information\nvisualization. The paper showcases how HistoLens explores Western Han culture\nin \"Yantie Lun\" through multi-dimensional, visual, and quantitative methods,\nfocusing particularly on the influence of Confucian and Legalist thoughts on\npolitical, economic, military, and ethnic. We also demonstrate how HistoLens\nconstructs a machine teaching scenario using LLMs for explainable analysis,\nbased on a dataset of Confucian and Legalist ideas extracted with LLM\nassistance. This approach offers novel and diverse perspectives for studying\nhistorical texts like \"Yantie Lun\" and provides new auxiliary tools for history\neducation. The framework aims to equip historians and learners with\nLLM-assisted tools to facilitate in-depth, multi-layered analysis of historical\ntexts and foster innovation in historical education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes HistoLens, a multi-layered analysis framework for\nhistorical texts based on Large Language Models (LLMs). Using the important\nWestern Han dynasty text \"Yantie Lun\" as a case study, we demonstrate the\nframework's potential applications in historical research and education.\nHistoLens integrates NLP technology (especially LLMs), including named entity\nrecognition, knowledge graph construction, and geographic information\nvisualization. The paper showcases how HistoLens explores Western Han culture\nin \"Yantie Lun\" through multi-dimensional, visual, and quantitative methods,\nfocusing particularly on the influence of Confucian and Legalist thoughts on\npolitical, economic, military, and ethnic. We also demonstrate how HistoLens\nconstructs a machine teaching scenario using LLMs for explainable analysis,\nbased on a dataset of Confucian and Legalist ideas extracted with LLM\nassistance. This approach offers novel and diverse perspectives for studying\nhistorical texts like \"Yantie Lun\" and provides new auxiliary tools for history\neducation. The framework aims to equip historians and learners with\nLLM-assisted tools to facilitate in-depth, multi-layered analysis of historical\ntexts and foster innovation in historical education."
                },
                "authors": [
                    {
                        "name": "Yifan Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Zeng"
                },
                "author": "Yifan Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09974v1",
                "updated": "2024-11-15T06:08:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    8,
                    57,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T06:08:57Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    8,
                    57,
                    4,
                    320,
                    0
                ],
                "title": "Experiences from Using LLMs for Repository Mining Studies in Empirical\n  Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experiences from Using LLMs for Repository Mining Studies in Empirical\n  Software Engineering"
                },
                "summary": "Context: The emergence of Large Language Models (LLMs) has significantly\ntransformed Software Engineering (SE) by providing innovative methods for\nanalyzing software repositories. Objectives: Our objective is to establish a\npractical framework for future SE researchers needing to enhance the data\ncollection and dataset while conducting software repository mining studies\nusing LLMs. Method: This experience report shares insights from two previous\nrepository mining studies, focusing on the methodologies used for creating,\nrefining, and validating prompts that enhance the output of LLMs, particularly\nin the context of data collection in empirical studies. Results: Our research\npackages a framework, coined Prompt Refinement and Insights for Mining\nEmpirical Software repositories (PRIMES), consisting of a checklist that can\nimprove LLM usage performance, enhance output quality, and minimize errors\nthrough iterative processes and comparisons among different LLMs. We also\nemphasize the significance of reproducibility by implementing mechanisms for\ntracking model results. Conclusion: Our findings indicate that standardizing\nprompt engineering and using PRIMES can enhance the reliability and\nreproducibility of studies utilizing LLMs. Ultimately, this work calls for\nfurther research to address challenges like hallucinations, model biases, and\ncost-effectiveness in integrating LLMs into workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: The emergence of Large Language Models (LLMs) has significantly\ntransformed Software Engineering (SE) by providing innovative methods for\nanalyzing software repositories. Objectives: Our objective is to establish a\npractical framework for future SE researchers needing to enhance the data\ncollection and dataset while conducting software repository mining studies\nusing LLMs. Method: This experience report shares insights from two previous\nrepository mining studies, focusing on the methodologies used for creating,\nrefining, and validating prompts that enhance the output of LLMs, particularly\nin the context of data collection in empirical studies. Results: Our research\npackages a framework, coined Prompt Refinement and Insights for Mining\nEmpirical Software repositories (PRIMES), consisting of a checklist that can\nimprove LLM usage performance, enhance output quality, and minimize errors\nthrough iterative processes and comparisons among different LLMs. We also\nemphasize the significance of reproducibility by implementing mechanisms for\ntracking model results. Conclusion: Our findings indicate that standardizing\nprompt engineering and using PRIMES can enhance the reliability and\nreproducibility of studies utilizing LLMs. Ultimately, this work calls for\nfurther research to address challenges like hallucinations, model biases, and\ncost-effectiveness in integrating LLMs into workflows."
                },
                "authors": [
                    {
                        "name": "Vincenzo de Martino"
                    },
                    {
                        "name": "Joel Castaño"
                    },
                    {
                        "name": "Fabio Palomba"
                    },
                    {
                        "name": "Xavier Franch"
                    },
                    {
                        "name": "Silverio Martínez-Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Silverio Martínez-Fernández"
                },
                "author": "Silverio Martínez-Fernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09972v1",
                "updated": "2024-11-15T06:05:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    5,
                    45,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T06:05:45Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    5,
                    45,
                    4,
                    320,
                    0
                ],
                "title": "Large Language Models as User-Agents for Evaluating\n  Task-Oriented-Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as User-Agents for Evaluating\n  Task-Oriented-Dialogue Systems"
                },
                "summary": "Traditionally, offline datasets have been used to evaluate task-oriented\ndialogue (TOD) models. These datasets lack context awareness, making them\nsuboptimal benchmarks for conversational systems. In contrast, user-agents,\nwhich are context-aware, can simulate the variability and unpredictability of\nhuman conversations, making them better alternatives as evaluators. Prior\nresearch has utilized large language models (LLMs) to develop user-agents. Our\nwork builds upon this by using LLMs to create user-agents for the evaluation of\nTOD systems. This involves prompting an LLM, using in-context examples as\nguidance, and tracking the user-goal state. Our evaluation of diversity and\ntask completion metrics for the user-agents shows improved performance with the\nuse of better prompts. Additionally, we propose methodologies for the automatic\nevaluation of TOD models within this dynamic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditionally, offline datasets have been used to evaluate task-oriented\ndialogue (TOD) models. These datasets lack context awareness, making them\nsuboptimal benchmarks for conversational systems. In contrast, user-agents,\nwhich are context-aware, can simulate the variability and unpredictability of\nhuman conversations, making them better alternatives as evaluators. Prior\nresearch has utilized large language models (LLMs) to develop user-agents. Our\nwork builds upon this by using LLMs to create user-agents for the evaluation of\nTOD systems. This involves prompting an LLM, using in-context examples as\nguidance, and tracking the user-goal state. Our evaluation of diversity and\ntask completion metrics for the user-agents shows improved performance with the\nuse of better prompts. Additionally, we propose methodologies for the automatic\nevaluation of TOD models within this dynamic framework."
                },
                "authors": [
                    {
                        "name": "Taaha Kazi"
                    },
                    {
                        "name": "Ruiliang Lyu"
                    },
                    {
                        "name": "Sizhe Zhou"
                    },
                    {
                        "name": "Dilek Hakkani-Tur"
                    },
                    {
                        "name": "Gokhan Tur"
                    }
                ],
                "author_detail": {
                    "name": "Gokhan Tur"
                },
                "author": "Gokhan Tur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09969v1",
                "updated": "2024-11-15T05:55:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    5,
                    55,
                    23,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T05:55:23Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    5,
                    55,
                    23,
                    4,
                    320,
                    0
                ],
                "title": "Steering AI-Driven Personalization of Scientific Text for General\n  Audiences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering AI-Driven Personalization of Scientific Text for General\n  Audiences"
                },
                "summary": "Digital media platforms (e.g., social media, science blogs) offer\nopportunities to communicate scientific content to general audiences at scale.\nHowever, these audiences vary in their scientific expertise, literacy levels,\nand personal backgrounds, making effective science communication challenging.\nTo address this challenge, we designed TranSlider, an AI-powered tool that\ngenerates personalized translations of scientific text based on individual user\nprofiles (e.g., hobbies, location, and education). Our tool features an\ninteractive slider that allows users to steer the degree of personalization\nfrom 0 (weakly relatable) to 100 (strongly relatable), leveraging LLMs to\ngenerate the translations with given degrees. Through an exploratory study with\n15 participants, we investigated both the utility of these AI-personalized\ntranslations and how interactive reading features influenced users'\nunderstanding and reading experiences. We found that participants who preferred\nhigher degrees of personalization appreciated the relatable and contextual\ntranslations, while those who preferred lower degrees valued concise\ntranslations with subtle contextualization. Furthermore, participants reported\nthe compounding effect of multiple translations on their understanding of\nscientific content. Given these findings, we discuss several implications of\nAI-personalized translation tools in facilitating communication in\ncollaborative contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital media platforms (e.g., social media, science blogs) offer\nopportunities to communicate scientific content to general audiences at scale.\nHowever, these audiences vary in their scientific expertise, literacy levels,\nand personal backgrounds, making effective science communication challenging.\nTo address this challenge, we designed TranSlider, an AI-powered tool that\ngenerates personalized translations of scientific text based on individual user\nprofiles (e.g., hobbies, location, and education). Our tool features an\ninteractive slider that allows users to steer the degree of personalization\nfrom 0 (weakly relatable) to 100 (strongly relatable), leveraging LLMs to\ngenerate the translations with given degrees. Through an exploratory study with\n15 participants, we investigated both the utility of these AI-personalized\ntranslations and how interactive reading features influenced users'\nunderstanding and reading experiences. We found that participants who preferred\nhigher degrees of personalization appreciated the relatable and contextual\ntranslations, while those who preferred lower degrees valued concise\ntranslations with subtle contextualization. Furthermore, participants reported\nthe compounding effect of multiple translations on their understanding of\nscientific content. Given these findings, we discuss several implications of\nAI-personalized translation tools in facilitating communication in\ncollaborative contexts."
                },
                "authors": [
                    {
                        "name": "Taewook Kim"
                    },
                    {
                        "name": "Dhruv Agarwal"
                    },
                    {
                        "name": "Jordan Ackerman"
                    },
                    {
                        "name": "Manaswi Saha"
                    }
                ],
                "author_detail": {
                    "name": "Manaswi Saha"
                },
                "author": "Manaswi Saha",
                "arxiv_comment": "23 pages, 5 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09955v1",
                "updated": "2024-11-15T05:18:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    5,
                    18,
                    15,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T05:18:15Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    5,
                    18,
                    15,
                    4,
                    320,
                    0
                ],
                "title": "Instruction-Guided Editing Controls for Images and Multimedia: A Survey\n  in LLM era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-Guided Editing Controls for Images and Multimedia: A Survey\n  in LLM era"
                },
                "summary": "The rapid advancement of large language models (LLMs) and multimodal learning\nhas transformed digital content creation and manipulation. Traditional visual\nediting tools require significant expertise, limiting accessibility. Recent\nstrides in instruction-based editing have enabled intuitive interaction with\nvisual content, using natural language as a bridge between user intent and\ncomplex editing operations. This survey provides an overview of these\ntechniques, focusing on how LLMs and multimodal models empower users to achieve\nprecise visual modifications without deep technical knowledge. By synthesizing\nover 100 publications, we explore methods from generative adversarial networks\nto diffusion models, examining multimodal integration for fine-grained content\ncontrol. We discuss practical applications across domains such as fashion, 3D\nscene manipulation, and video synthesis, highlighting increased accessibility\nand alignment with human intuition. Our survey compares existing literature,\nemphasizing LLM-empowered editing, and identifies key challenges to stimulate\nfurther research. We aim to democratize powerful visual editing across various\nindustries, from entertainment to education. Interested readers are encouraged\nto access our repository at\nhttps://github.com/tamlhp/awesome-instruction-editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) and multimodal learning\nhas transformed digital content creation and manipulation. Traditional visual\nediting tools require significant expertise, limiting accessibility. Recent\nstrides in instruction-based editing have enabled intuitive interaction with\nvisual content, using natural language as a bridge between user intent and\ncomplex editing operations. This survey provides an overview of these\ntechniques, focusing on how LLMs and multimodal models empower users to achieve\nprecise visual modifications without deep technical knowledge. By synthesizing\nover 100 publications, we explore methods from generative adversarial networks\nto diffusion models, examining multimodal integration for fine-grained content\ncontrol. We discuss practical applications across domains such as fashion, 3D\nscene manipulation, and video synthesis, highlighting increased accessibility\nand alignment with human intuition. Our survey compares existing literature,\nemphasizing LLM-empowered editing, and identifies key challenges to stimulate\nfurther research. We aim to democratize powerful visual editing across various\nindustries, from entertainment to education. Interested readers are encouraged\nto access our repository at\nhttps://github.com/tamlhp/awesome-instruction-editing."
                },
                "authors": [
                    {
                        "name": "Thanh Tam Nguyen"
                    },
                    {
                        "name": "Zhao Ren"
                    },
                    {
                        "name": "Trinh Pham"
                    },
                    {
                        "name": "Phi Le Nguyen"
                    },
                    {
                        "name": "Hongzhi Yin"
                    },
                    {
                        "name": "Quoc Viet Hung Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Quoc Viet Hung Nguyen"
                },
                "author": "Quoc Viet Hung Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09944v1",
                "updated": "2024-11-15T04:44:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    44,
                    34,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T04:44:34Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    44,
                    34,
                    4,
                    320,
                    0
                ],
                "title": "SlimLM: An Efficient Small Language Model for On-Device Document\n  Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimLM: An Efficient Small Language Model for On-Device Document\n  Assistance"
                },
                "summary": "While small language models (SLMs) show promises for mobile deployment, their\nreal-world performance and applications on smartphones remains underexplored.\nWe present SlimLM, a series of SLMs optimized for document assistance tasks on\nmobile devices. Through extensive experiments on a Samsung Galaxy S24, we\nidentify the optimal trade-offs between model size (ranging from 125M to 7B\nparameters), context length, and inference time for efficient on-device\nprocessing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on\nDocAssist, our constructed dataset for summarization, question answering and\nsuggestion tasks. Our smallest model demonstrates efficient performance on S24,\nwhile larger variants offer enhanced capabilities within mobile constraints. We\nevaluate SlimLM against existing SLMs, showing comparable or superior\nperformance and offering a benchmark for future research in on-device language\nmodels. We also provide an Android application, offering practical insights\ninto SLM deployment. Our findings provide valuable insights and illuminate the\ncapabilities of running advanced language models on high-end smartphones,\npotentially reducing server costs and enhancing privacy through on-device\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While small language models (SLMs) show promises for mobile deployment, their\nreal-world performance and applications on smartphones remains underexplored.\nWe present SlimLM, a series of SLMs optimized for document assistance tasks on\nmobile devices. Through extensive experiments on a Samsung Galaxy S24, we\nidentify the optimal trade-offs between model size (ranging from 125M to 7B\nparameters), context length, and inference time for efficient on-device\nprocessing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on\nDocAssist, our constructed dataset for summarization, question answering and\nsuggestion tasks. Our smallest model demonstrates efficient performance on S24,\nwhile larger variants offer enhanced capabilities within mobile constraints. We\nevaluate SlimLM against existing SLMs, showing comparable or superior\nperformance and offering a benchmark for future research in on-device language\nmodels. We also provide an Android application, offering practical insights\ninto SLM deployment. Our findings provide valuable insights and illuminate the\ncapabilities of running advanced language models on high-end smartphones,\npotentially reducing server costs and enhancing privacy through on-device\nprocessing."
                },
                "authors": [
                    {
                        "name": "Thang M. Pham"
                    },
                    {
                        "name": "Phat T. Nguyen"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Trung Bui"
                    }
                ],
                "author_detail": {
                    "name": "Trung Bui"
                },
                "author": "Trung Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09943v1",
                "updated": "2024-11-15T04:43:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    43,
                    44,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T04:43:44Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    43,
                    44,
                    4,
                    320,
                    0
                ],
                "title": "Zero-shot Voice Conversion with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Voice Conversion with Diffusion Transformers"
                },
                "summary": "Zero-shot voice conversion aims to transform a source speech utterance to\nmatch the timbre of a reference speech from an unseen speaker. Traditional\napproaches struggle with timbre leakage, insufficient timbre representation,\nand mismatches between training and inference tasks. We propose Seed-VC, a\nnovel framework that addresses these issues by introducing an external timbre\nshifter during training to perturb the source speech timbre, mitigating leakage\nand aligning training with inference. Additionally, we employ a diffusion\ntransformer that leverages the entire reference speech context, capturing\nfine-grained timbre features through in-context learning. Experiments\ndemonstrate that Seed-VC outperforms strong baselines like OpenVoice and\nCosyVoice, achieving higher speaker similarity and lower word error rates in\nzero-shot voice conversion tasks. We further extend our approach to zero-shot\nsinging voice conversion by incorporating fundamental frequency (F0)\nconditioning, resulting in comparative performance to current state-of-the-art\nmethods. Our findings highlight the effectiveness of Seed-VC in overcoming core\nchallenges, paving the way for more accurate and versatile voice conversion\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot voice conversion aims to transform a source speech utterance to\nmatch the timbre of a reference speech from an unseen speaker. Traditional\napproaches struggle with timbre leakage, insufficient timbre representation,\nand mismatches between training and inference tasks. We propose Seed-VC, a\nnovel framework that addresses these issues by introducing an external timbre\nshifter during training to perturb the source speech timbre, mitigating leakage\nand aligning training with inference. Additionally, we employ a diffusion\ntransformer that leverages the entire reference speech context, capturing\nfine-grained timbre features through in-context learning. Experiments\ndemonstrate that Seed-VC outperforms strong baselines like OpenVoice and\nCosyVoice, achieving higher speaker similarity and lower word error rates in\nzero-shot voice conversion tasks. We further extend our approach to zero-shot\nsinging voice conversion by incorporating fundamental frequency (F0)\nconditioning, resulting in comparative performance to current state-of-the-art\nmethods. Our findings highlight the effectiveness of Seed-VC in overcoming core\nchallenges, paving the way for more accurate and versatile voice conversion\nsystems."
                },
                "authors": [
                    {
                        "name": "Songting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Songting Liu"
                },
                "author": "Songting Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02170v2",
                "updated": "2024-11-15T04:30:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    30,
                    4,
                    4,
                    320,
                    0
                ],
                "published": "2023-10-03T16:05:48Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    16,
                    5,
                    48,
                    1,
                    276,
                    0
                ],
                "title": "A Dynamic LLM-Powered Agent Network for Task-Oriented Agent\n  Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dynamic LLM-Powered Agent Network for Task-Oriented Agent\n  Collaboration"
                },
                "summary": "Recent studies show that collaborating multiple large language model (LLM)\npowered agents is a promising way for task solving. However, current approaches\nare constrained by using a fixed number of agents and static communication\nstructures. In this work, we propose automatically selecting a team of agents\nfrom candidates to collaborate in a dynamic communication structure toward\ndifferent tasks and domains. Specifically, we build a framework named Dynamic\nLLM-Powered Agent Network ($\\textbf{DyLAN}$) for LLM-powered agent\ncollaboration, operating a two-stage paradigm: (1) Team Optimization and (2)\nTask Solving. During the first stage, we utilize an $\\textit{agent selection}$\nalgorithm, based on an unsupervised metric called $\\textit{Agent Importance\nScore}$, enabling the selection of best agents according to their contributions\nin a preliminary trial, oriented to the given task. Then, in the second stage,\nthe selected agents collaborate dynamically according to the query.\nEmpirically, we demonstrate that DyLAN outperforms strong baselines in code\ngeneration, decision-making, general reasoning, and arithmetic reasoning tasks\nwith moderate computational cost. On specific subjects in MMLU, selecting a\nteam of agents in the team optimization stage improves accuracy by up to 25.0%\nin DyLAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that collaborating multiple large language model (LLM)\npowered agents is a promising way for task solving. However, current approaches\nare constrained by using a fixed number of agents and static communication\nstructures. In this work, we propose automatically selecting a team of agents\nfrom candidates to collaborate in a dynamic communication structure toward\ndifferent tasks and domains. Specifically, we build a framework named Dynamic\nLLM-Powered Agent Network ($\\textbf{DyLAN}$) for LLM-powered agent\ncollaboration, operating a two-stage paradigm: (1) Team Optimization and (2)\nTask Solving. During the first stage, we utilize an $\\textit{agent selection}$\nalgorithm, based on an unsupervised metric called $\\textit{Agent Importance\nScore}$, enabling the selection of best agents according to their contributions\nin a preliminary trial, oriented to the given task. Then, in the second stage,\nthe selected agents collaborate dynamically according to the query.\nEmpirically, we demonstrate that DyLAN outperforms strong baselines in code\ngeneration, decision-making, general reasoning, and arithmetic reasoning tasks\nwith moderate computational cost. On specific subjects in MMLU, selecting a\nteam of agents in the team optimization stage improves accuracy by up to 25.0%\nin DyLAN."
                },
                "authors": [
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Yanzhe Zhang"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "Published in COLM2024. Code Repo: https://github.com/SALT-NLP/DyLAN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09937v1",
                "updated": "2024-11-15T04:22:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    22,
                    21,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T04:22:21Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    22,
                    21,
                    4,
                    320,
                    0
                ],
                "title": "Refined and Segmented Price Sentiment Indices from Survey Comments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refined and Segmented Price Sentiment Indices from Survey Comments"
                },
                "summary": "We aim to enhance a price sentiment index and to more precisely understand\nprice trends from the perspective of not only consumers but also businesses. We\nextract comments related to prices from the Economy Watchers Survey conducted\nby the Cabinet Office of Japan and classify price trends using a large language\nmodel (LLM). We classify whether the survey sample reflects the perspective of\nconsumers or businesses, and whether the comments pertain to goods or services\nby utilizing information on the fields of comments and the industries of\nrespondents included in the Economy Watchers Survey. From these classified\nprice-related comments, we construct price sentiment indices not only for a\ngeneral purpose but also for more specific objectives by combining perspectives\non consumers and prices, as well as goods and services. It becomes possible to\nachieve a more accurate classification of price directions by employing a LLM\nfor classification. Furthermore, integrating the outputs of multiple LLMs\nsuggests the potential for the better performance of the classification. The\nuse of more accurately classified comments allows for the construction of an\nindex with a higher correlation to existing indices than previous studies. We\ndemonstrate that the correlation of the price index for consumers, which has a\nlarger sample size, is further enhanced by selecting comments for aggregation\nbased on the industry of the survey respondents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We aim to enhance a price sentiment index and to more precisely understand\nprice trends from the perspective of not only consumers but also businesses. We\nextract comments related to prices from the Economy Watchers Survey conducted\nby the Cabinet Office of Japan and classify price trends using a large language\nmodel (LLM). We classify whether the survey sample reflects the perspective of\nconsumers or businesses, and whether the comments pertain to goods or services\nby utilizing information on the fields of comments and the industries of\nrespondents included in the Economy Watchers Survey. From these classified\nprice-related comments, we construct price sentiment indices not only for a\ngeneral purpose but also for more specific objectives by combining perspectives\non consumers and prices, as well as goods and services. It becomes possible to\nachieve a more accurate classification of price directions by employing a LLM\nfor classification. Furthermore, integrating the outputs of multiple LLMs\nsuggests the potential for the better performance of the classification. The\nuse of more accurately classified comments allows for the construction of an\nindex with a higher correlation to existing indices than previous studies. We\ndemonstrate that the correlation of the price index for consumers, which has a\nlarger sample size, is further enhanced by selecting comments for aggregation\nbased on the industry of the survey respondents."
                },
                "authors": [
                    {
                        "name": "Masahiro Suzuki"
                    },
                    {
                        "name": "Hiroki Sakaji"
                    }
                ],
                "author_detail": {
                    "name": "Hiroki Sakaji"
                },
                "author": "Hiroki Sakaji",
                "arxiv_comment": "Accepted to IEEE BigData 2024. 9 pages, 11 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02827v3",
                "updated": "2024-11-15T04:17:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    17,
                    35,
                    4,
                    320,
                    0
                ],
                "published": "2024-04-03T15:59:42Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    15,
                    59,
                    42,
                    2,
                    94,
                    0
                ],
                "title": "BAdam: A Memory Efficient Full Parameter Optimization Method for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BAdam: A Memory Efficient Full Parameter Optimization Method for Large\n  Language Models"
                },
                "summary": "This work presents BAdam, an optimization method that leverages the block\ncoordinate descent (BCD) framework with Adam's update rule. BAdam offers a\nmemory efficient approach to the full parameter finetuning of large language\nmodels. We conduct a theoretical convergence analysis for BAdam in the\ndeterministic case. Experimentally, we apply BAdam to finetune the Llama 3-8B\nand Llama 3-70B models using a single RTX3090-24GB GPU and 4 A100-80GB GPUs,\nrespectively. The results confirm BAdam's efficiency in terms of memory usage,\nrunning time, and optimization capability. Furthermore, the downstream\nperformance evaluation based on MT-bench and math benchmarks shows that BAdam\noutperforms existing memory efficient baselines such as LoRA. It also\ndemonstrates that BAdam can achieve comparable or even superior performance\ncompared to Adam. Finally, the ablation study using SGD's update rule\nillustrates the suitability of BCD for finetuning LLMs. Our code can be easily\nintegrated into any PyTorch-based codebase and is available at\nhttps://github.com/Ledzy/BAdam.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents BAdam, an optimization method that leverages the block\ncoordinate descent (BCD) framework with Adam's update rule. BAdam offers a\nmemory efficient approach to the full parameter finetuning of large language\nmodels. We conduct a theoretical convergence analysis for BAdam in the\ndeterministic case. Experimentally, we apply BAdam to finetune the Llama 3-8B\nand Llama 3-70B models using a single RTX3090-24GB GPU and 4 A100-80GB GPUs,\nrespectively. The results confirm BAdam's efficiency in terms of memory usage,\nrunning time, and optimization capability. Furthermore, the downstream\nperformance evaluation based on MT-bench and math benchmarks shows that BAdam\noutperforms existing memory efficient baselines such as LoRA. It also\ndemonstrates that BAdam can achieve comparable or even superior performance\ncompared to Adam. Finally, the ablation study using SGD's update rule\nillustrates the suitability of BCD for finetuning LLMs. Our code can be easily\nintegrated into any PyTorch-based codebase and is available at\nhttps://github.com/Ledzy/BAdam."
                },
                "authors": [
                    {
                        "name": "Qijun Luo"
                    },
                    {
                        "name": "Hengxu Yu"
                    },
                    {
                        "name": "Xiao Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Li"
                },
                "author": "Xiao Li",
                "arxiv_comment": "Accepted for Publication in Conference on Neural Information\n  Processing Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09933v1",
                "updated": "2024-11-15T04:16:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    16,
                    50,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T04:16:50Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    16,
                    50,
                    4,
                    320,
                    0
                ],
                "title": "JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by\n  Evolutionary Optimization of Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by\n  Evolutionary Optimization of Model Merging"
                },
                "summary": "With the rapid advancement of large language models (LLMs), foundational\nmodels (FMs) have seen significant advancements. Healthcare is one of the most\ncrucial application areas for these FMs, given the significant time and effort\nrequired for physicians to analyze large volumes of patient data. Recent\nefforts have focused on adapting multimodal FMs to the medical domain through\ntechniques like instruction-tuning, leading to the development of medical\nfoundation models (MFMs). However, these approaches typically require large\namounts of training data to effectively adapt models to the medical field.\nMoreover, most existing models are trained on English datasets, limiting their\npracticality in non-English-speaking regions where healthcare professionals and\npatients are not always fluent in English. The need for translation introduces\nadditional costs and inefficiencies. To address these challenges, we propose a\n\\textbf{J}apanese \\textbf{Radi}ology report generation model enhanced by\n\\textbf{Evo}lutionary optimization of model merging (JRadiEvo). This is the\nfirst attempt to extend a non-medical vision-language foundation model to the\nmedical domain through evolutionary optimization of model merging. We\nsuccessfully created a model that generates accurate Japanese reports from\nX-ray images using only 50 translated samples from publicly available data.\nThis model, developed with highly efficient use of limited data, outperformed\nleading models from recent research trained on much larger datasets.\nAdditionally, with only 8 billion parameters, this relatively compact\nfoundation model can be deployed locally within hospitals, making it a\npractical solution for environments where APIs and other external services\ncannot be used due to strict privacy and security requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large language models (LLMs), foundational\nmodels (FMs) have seen significant advancements. Healthcare is one of the most\ncrucial application areas for these FMs, given the significant time and effort\nrequired for physicians to analyze large volumes of patient data. Recent\nefforts have focused on adapting multimodal FMs to the medical domain through\ntechniques like instruction-tuning, leading to the development of medical\nfoundation models (MFMs). However, these approaches typically require large\namounts of training data to effectively adapt models to the medical field.\nMoreover, most existing models are trained on English datasets, limiting their\npracticality in non-English-speaking regions where healthcare professionals and\npatients are not always fluent in English. The need for translation introduces\nadditional costs and inefficiencies. To address these challenges, we propose a\n\\textbf{J}apanese \\textbf{Radi}ology report generation model enhanced by\n\\textbf{Evo}lutionary optimization of model merging (JRadiEvo). This is the\nfirst attempt to extend a non-medical vision-language foundation model to the\nmedical domain through evolutionary optimization of model merging. We\nsuccessfully created a model that generates accurate Japanese reports from\nX-ray images using only 50 translated samples from publicly available data.\nThis model, developed with highly efficient use of limited data, outperformed\nleading models from recent research trained on much larger datasets.\nAdditionally, with only 8 billion parameters, this relatively compact\nfoundation model can be deployed locally within hospitals, making it a\npractical solution for environments where APIs and other external services\ncannot be used due to strict privacy and security requirements."
                },
                "authors": [
                    {
                        "name": "Kaito Baba"
                    },
                    {
                        "name": "Ryota Yagi"
                    },
                    {
                        "name": "Junichiro Takahashi"
                    },
                    {
                        "name": "Risa Kishikawa"
                    },
                    {
                        "name": "Satoshi Kodera"
                    }
                ],
                "author_detail": {
                    "name": "Satoshi Kodera"
                },
                "author": "Satoshi Kodera",
                "arxiv_comment": "Accepted by NeurIPS'24 Workshop on AIM-FM: Advancements In Medical\n  Foundation Models: Explainability, Robustness, Security, and Beyond",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09928v1",
                "updated": "2024-11-15T04:00:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    0,
                    54,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T04:00:54Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    0,
                    54,
                    4,
                    320,
                    0
                ],
                "title": "Is Precise Recovery Necessary? A Task-Oriented Imputation Approach for\n  Time Series Forecasting on Variable Subset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Precise Recovery Necessary? A Task-Oriented Imputation Approach for\n  Time Series Forecasting on Variable Subset"
                },
                "summary": "Variable Subset Forecasting (VSF) refers to a unique scenario in multivariate\ntime series forecasting, where available variables in the inference phase are\nonly a subset of the variables in the training phase. VSF presents significant\nchallenges as the entire time series may be missing, and neither inter- nor\nintra-variable correlations persist. Such conditions impede the effectiveness\nof traditional imputation methods, primarily focusing on filling in individual\nmissing data points. Inspired by the principle of feature engineering that not\nall variables contribute positively to forecasting, we propose Task-Oriented\nImputation for VSF (TOI-VSF), a novel framework shifts the focus from accurate\ndata recovery to directly support the downstream forecasting task. TOI-VSF\nincorporates a self-supervised imputation module, agnostic to the forecasting\nmodel, designed to fill in missing variables while preserving the vital\ncharacteristics and temporal patterns of time series data. Additionally, we\nimplement a joint learning strategy for imputation and forecasting, ensuring\nthat the imputation process is directly aligned with and beneficial to the\nforecasting objective. Extensive experiments across four datasets demonstrate\nthe superiority of TOI-VSF, outperforming baseline methods by $15\\%$ on\naverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable Subset Forecasting (VSF) refers to a unique scenario in multivariate\ntime series forecasting, where available variables in the inference phase are\nonly a subset of the variables in the training phase. VSF presents significant\nchallenges as the entire time series may be missing, and neither inter- nor\nintra-variable correlations persist. Such conditions impede the effectiveness\nof traditional imputation methods, primarily focusing on filling in individual\nmissing data points. Inspired by the principle of feature engineering that not\nall variables contribute positively to forecasting, we propose Task-Oriented\nImputation for VSF (TOI-VSF), a novel framework shifts the focus from accurate\ndata recovery to directly support the downstream forecasting task. TOI-VSF\nincorporates a self-supervised imputation module, agnostic to the forecasting\nmodel, designed to fill in missing variables while preserving the vital\ncharacteristics and temporal patterns of time series data. Additionally, we\nimplement a joint learning strategy for imputation and forecasting, ensuring\nthat the imputation process is directly aligned with and beneficial to the\nforecasting objective. Extensive experiments across four datasets demonstrate\nthe superiority of TOI-VSF, outperforming baseline methods by $15\\%$ on\naverage."
                },
                "authors": [
                    {
                        "name": "Qi Hao"
                    },
                    {
                        "name": "Runchang Liang"
                    },
                    {
                        "name": "Yue Gao"
                    },
                    {
                        "name": "Hao Dong"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Lu Jiang"
                    },
                    {
                        "name": "Pengyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Pengyang Wang"
                },
                "author": "Pengyang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10942v3",
                "updated": "2024-11-15T03:48:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    48,
                    7,
                    4,
                    320,
                    0
                ],
                "published": "2024-06-16T13:44:41Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    13,
                    44,
                    41,
                    6,
                    168,
                    0
                ],
                "title": "Effective Generative AI: The Human-Algorithm Centaur",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective Generative AI: The Human-Algorithm Centaur"
                },
                "summary": "Advanced analytics science methods have enabled combining the power of\nartificial and human intelligence, creating \\textit{centaurs} that allow\nsuperior decision-making. Centaurs are hybrid human-algorithm models that\ncombine both formal analytics and human intuition in a symbiotic manner within\ntheir learning and reasoning process. We argue that the future of AI\ndevelopment and use in many domains needs to focus more on centaurs as opposed\nto other AI approaches. This paradigm shift towards centaur-based AI methods\nraises some fundamental questions: How are centaurs different from other\nhuman-in-the-loop methods? What are the most effective methods for creating\ncentaurs? When should centaurs be used, and when should the lead be given to\npure AI models? Doesn't the incorporation of human intuition -- which at times\ncan be misleading -- in centaurs' decision-making process degrade its\nperformance compared to pure AI methods? This work aims to address these\nfundamental questions, focusing on recent advancements in generative AI, and\nespecially in Large Language Models (LLMs), as a main case study to illustrate\ncentaurs' critical essentiality to future AI endeavors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced analytics science methods have enabled combining the power of\nartificial and human intelligence, creating \\textit{centaurs} that allow\nsuperior decision-making. Centaurs are hybrid human-algorithm models that\ncombine both formal analytics and human intuition in a symbiotic manner within\ntheir learning and reasoning process. We argue that the future of AI\ndevelopment and use in many domains needs to focus more on centaurs as opposed\nto other AI approaches. This paradigm shift towards centaur-based AI methods\nraises some fundamental questions: How are centaurs different from other\nhuman-in-the-loop methods? What are the most effective methods for creating\ncentaurs? When should centaurs be used, and when should the lead be given to\npure AI models? Doesn't the incorporation of human intuition -- which at times\ncan be misleading -- in centaurs' decision-making process degrade its\nperformance compared to pure AI methods? This work aims to address these\nfundamental questions, focusing on recent advancements in generative AI, and\nespecially in Large Language Models (LLMs), as a main case study to illustrate\ncentaurs' critical essentiality to future AI endeavors."
                },
                "authors": [
                    {
                        "name": "Soroush Saghafian"
                    },
                    {
                        "name": "Lihi Idan"
                    }
                ],
                "author_detail": {
                    "name": "Lihi Idan"
                },
                "author": "Lihi Idan",
                "arxiv_doi": "10.1162/99608f92.19d78478",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1162/99608f92.19d78478",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To Appear in SI: Future Shock, Harvard Data Science Review\n  (https://hdsr.mitpress.mit.edu/specialissue5)",
                "arxiv_journal_ref": "Harvard Data Science Review (2024)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09921v1",
                "updated": "2024-11-15T03:45:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    45,
                    9,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T03:45:09Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    45,
                    9,
                    4,
                    320,
                    0
                ],
                "title": "Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at\n  Pixel Level",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at\n  Pixel Level"
                },
                "summary": "In this paper, we introduce Motion-Grounded Video Reasoning, a new motion\nunderstanding task that requires generating visual answers (video segmentation\nmasks) according to the input question, and hence needs implicit spatiotemporal\nreasoning and grounding. This task extends existing spatiotemporal grounding\nwork focusing on explicit action/motion grounding, to a more general format by\nenabling implicit reasoning via questions. To facilitate the development of the\nnew task, we collect a large-scale dataset called GROUNDMORE, which comprises\n1,715 video clips, 249K object masks that are deliberately designed with 4\nquestion types (Causal, Sequential, Counterfactual, and Descriptive) for\nbenchmarking deep and comprehensive motion reasoning abilities. GROUNDMORE\nuniquely requires models to generate visual answers, providing a more concrete\nand visually interpretable response than plain texts. It evaluates models on\nboth spatiotemporal grounding and reasoning, fostering to address complex\nchallenges in motion-related video reasoning, temporal perception, and\npixel-level understanding. Furthermore, we introduce a novel baseline model\nnamed Motion-Grounded Video Reasoning Assistant (MORA). MORA incorporates the\nmultimodal reasoning ability from the Multimodal LLM, the pixel-level\nperception capability from the grounding model (SAM), and the temporal\nperception ability from a lightweight localization head. MORA achieves\nrespectable performance on GROUNDMORE outperforming the best existing visual\ngrounding baseline model by an average of 21.5% relatively. We hope this novel\nand challenging task will pave the way for future advancements in robust and\ngeneral motion understanding via video reasoning segmentation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Motion-Grounded Video Reasoning, a new motion\nunderstanding task that requires generating visual answers (video segmentation\nmasks) according to the input question, and hence needs implicit spatiotemporal\nreasoning and grounding. This task extends existing spatiotemporal grounding\nwork focusing on explicit action/motion grounding, to a more general format by\nenabling implicit reasoning via questions. To facilitate the development of the\nnew task, we collect a large-scale dataset called GROUNDMORE, which comprises\n1,715 video clips, 249K object masks that are deliberately designed with 4\nquestion types (Causal, Sequential, Counterfactual, and Descriptive) for\nbenchmarking deep and comprehensive motion reasoning abilities. GROUNDMORE\nuniquely requires models to generate visual answers, providing a more concrete\nand visually interpretable response than plain texts. It evaluates models on\nboth spatiotemporal grounding and reasoning, fostering to address complex\nchallenges in motion-related video reasoning, temporal perception, and\npixel-level understanding. Furthermore, we introduce a novel baseline model\nnamed Motion-Grounded Video Reasoning Assistant (MORA). MORA incorporates the\nmultimodal reasoning ability from the Multimodal LLM, the pixel-level\nperception capability from the grounding model (SAM), and the temporal\nperception ability from a lightweight localization head. MORA achieves\nrespectable performance on GROUNDMORE outperforming the best existing visual\ngrounding baseline model by an average of 21.5% relatively. We hope this novel\nand challenging task will pave the way for future advancements in robust and\ngeneral motion understanding via video reasoning segmentation"
                },
                "authors": [
                    {
                        "name": "Andong Deng"
                    },
                    {
                        "name": "Tongjia Chen"
                    },
                    {
                        "name": "Shoubin Yu"
                    },
                    {
                        "name": "Taojiannan Yang"
                    },
                    {
                        "name": "Lincoln Spencer"
                    },
                    {
                        "name": "Yapeng Tian"
                    },
                    {
                        "name": "Ajmal Saeed Mian"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Chen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chen Chen"
                },
                "author": "Chen Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09916v1",
                "updated": "2024-11-15T03:29:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    29,
                    41,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T03:29:41Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    29,
                    41,
                    4,
                    320,
                    0
                ],
                "title": "LLMs are Imperfect, Then What? An Empirical Study on LLM Failures in\n  Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Imperfect, Then What? An Empirical Study on LLM Failures in\n  Software Engineering"
                },
                "summary": "Software engineers are integrating AI assistants into their workflows to\nenhance productivity and reduce cognitive strain. However, experiences vary\nsignificantly, with some engineers finding large language models (LLMs), like\nChatGPT, beneficial, while others consider them counterproductive. Researchers\nalso found that ChatGPT's answers included incorrect information. Given the\nfact that LLMs are still imperfect, it is important to understand how to best\nincorporate LLMs into the workflow for software engineering (SE) task\ncompletion. Therefore, we conducted an observational study with 22 participants\nusing ChatGPT as a coding assistant in a non-trivial SE task to understand the\npractices, challenges, and opportunities for using LLMs for SE tasks. We\nidentified the cases where ChatGPT failed, their root causes, and the\ncorresponding mitigation solutions used by users. These findings contribute to\nthe overall understanding and strategies for human-AI interaction on SE tasks.\nOur study also highlights future research and tooling support directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software engineers are integrating AI assistants into their workflows to\nenhance productivity and reduce cognitive strain. However, experiences vary\nsignificantly, with some engineers finding large language models (LLMs), like\nChatGPT, beneficial, while others consider them counterproductive. Researchers\nalso found that ChatGPT's answers included incorrect information. Given the\nfact that LLMs are still imperfect, it is important to understand how to best\nincorporate LLMs into the workflow for software engineering (SE) task\ncompletion. Therefore, we conducted an observational study with 22 participants\nusing ChatGPT as a coding assistant in a non-trivial SE task to understand the\npractices, challenges, and opportunities for using LLMs for SE tasks. We\nidentified the cases where ChatGPT failed, their root causes, and the\ncorresponding mitigation solutions used by users. These findings contribute to\nthe overall understanding and strategies for human-AI interaction on SE tasks.\nOur study also highlights future research and tooling support directions."
                },
                "authors": [
                    {
                        "name": "Jiessie Tie"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Tianshi Li"
                    },
                    {
                        "name": "Syed Ishtiaque Ahmed"
                    },
                    {
                        "name": "Dakuo Wang"
                    },
                    {
                        "name": "Shurui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shurui Zhou"
                },
                "author": "Shurui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17743v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17743v3",
                "updated": "2024-11-15T03:25:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    25,
                    40,
                    4,
                    320,
                    0
                ],
                "published": "2024-05-28T01:55:35Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    1,
                    55,
                    35,
                    1,
                    149,
                    0
                ],
                "title": "ORLM: A Customizable Framework in Training Large Models for Automated\n  Optimization Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORLM: A Customizable Framework in Training Large Models for Automated\n  Optimization Modeling"
                },
                "summary": "Optimization modeling and solving play a critical role in the application of\nOperations Research (OR) tools to address real-world problems, yet they pose\nchallenges and require extensive expertise from OR experts. With the advent of\nlarge language models (LLMs), new opportunities have emerged to streamline and\nautomate these tasks. However, current research predominantly relies on\nclosed-source LLMs such as GPT-4, along with extensive prompt engineering\ntechniques. This reliance stems from the scarcity of high-quality training\ndatasets for optimization modeling, resulting in elevated costs, prolonged\nprocessing times, and privacy concerns. To address these challenges, our work\nis the first to propose a viable path for training open-source LLMs that are\ncapable of optimization modeling as well as developing and executing solver\ncodes, eventually leading to a superior ability for automating optimization\nmodeling and solving. Particularly, we introduce a semi-automated data\nsynthesis framework designed for optimization modeling issues, named\nOR-Instruct. This framework merges the training data requirements of large\nmodels with the unique characteristics of optimization modeling problems, and\nallows for customizable enhancements tailored to specific scenarios or modeling\ntypes. To evaluate the performance of our proposed framework, we present the\nIndustryOR benchmark, the inaugural industrial standard for evaluating LLMs in\nsolving practical OR problems. Utilizing data synthesized through OR-Instruct,\nwe train various open-source LLMs with a capacity of 7 billion parameters\n(dubbed ORLMs). The resulting model demonstrates significantly enhanced\noptimization modeling capabilities, achieving state-of-the-art performance\nacross the NL4OPT, MAMO, and IndustryOR benchmarks. Our code and data are\navailable at \\url{https://github.com/Cardinal-Operations/ORLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization modeling and solving play a critical role in the application of\nOperations Research (OR) tools to address real-world problems, yet they pose\nchallenges and require extensive expertise from OR experts. With the advent of\nlarge language models (LLMs), new opportunities have emerged to streamline and\nautomate these tasks. However, current research predominantly relies on\nclosed-source LLMs such as GPT-4, along with extensive prompt engineering\ntechniques. This reliance stems from the scarcity of high-quality training\ndatasets for optimization modeling, resulting in elevated costs, prolonged\nprocessing times, and privacy concerns. To address these challenges, our work\nis the first to propose a viable path for training open-source LLMs that are\ncapable of optimization modeling as well as developing and executing solver\ncodes, eventually leading to a superior ability for automating optimization\nmodeling and solving. Particularly, we introduce a semi-automated data\nsynthesis framework designed for optimization modeling issues, named\nOR-Instruct. This framework merges the training data requirements of large\nmodels with the unique characteristics of optimization modeling problems, and\nallows for customizable enhancements tailored to specific scenarios or modeling\ntypes. To evaluate the performance of our proposed framework, we present the\nIndustryOR benchmark, the inaugural industrial standard for evaluating LLMs in\nsolving practical OR problems. Utilizing data synthesized through OR-Instruct,\nwe train various open-source LLMs with a capacity of 7 billion parameters\n(dubbed ORLMs). The resulting model demonstrates significantly enhanced\noptimization modeling capabilities, achieving state-of-the-art performance\nacross the NL4OPT, MAMO, and IndustryOR benchmarks. Our code and data are\navailable at \\url{https://github.com/Cardinal-Operations/ORLM}."
                },
                "authors": [
                    {
                        "name": "Chenyu Huang"
                    },
                    {
                        "name": "Zhengyang Tang"
                    },
                    {
                        "name": "Dongdong Ge"
                    },
                    {
                        "name": "Shixi Hu"
                    },
                    {
                        "name": "Ruoqing Jiang"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Zizhuo Wang"
                    },
                    {
                        "name": "Xin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zheng"
                },
                "author": "Xin Zheng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17743v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17743v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07571v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07571v2",
                "updated": "2024-11-15T03:20:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    20,
                    57,
                    4,
                    320,
                    0
                ],
                "published": "2024-10-10T03:12:03Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    12,
                    3,
                    3,
                    284,
                    0
                ],
                "title": "How Does Vision-Language Adaptation Impact the Safety of Vision Language\n  Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Does Vision-Language Adaptation Impact the Safety of Vision Language\n  Models?"
                },
                "summary": "Vision-Language adaptation (VL adaptation) transforms Large Language Models\n(LLMs) into Large Vision-Language Models (LVLMs) for multimodal tasks, but this\nprocess often compromises the inherent safety capabilities embedded in the\noriginal LLMs. Despite potential harmfulness due to weakened safety measures,\nin-depth analysis on the effects of VL adaptation on safety remains\nunder-explored. This study examines how VL adaptation influences safety and\nevaluates the impact of safety fine-tuning methods. Our analysis reveals that\nsafety degradation occurs during VL adaptation, even when the training data is\nsafe. While safety tuning techniques like supervised fine-tuning with safety\ndatasets or reinforcement learning from human feedback mitigate some risks,\nthey still lead to safety degradation and a reduction in helpfulness due to\nover-rejection issues. Further analysis of internal model weights suggests that\nVL adaptation may impact certain safety-related layers, potentially lowering\noverall safety levels. Additionally, our findings demonstrate that the\nobjectives of VL adaptation and safety tuning are divergent, which often\nresults in their simultaneous application being suboptimal. To address this, we\nsuggest the weight merging approach as an optimal solution effectively reducing\nsafety degradation while maintaining helpfulness. These insights help guide the\ndevelopment of more reliable and secure LVLMs for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language adaptation (VL adaptation) transforms Large Language Models\n(LLMs) into Large Vision-Language Models (LVLMs) for multimodal tasks, but this\nprocess often compromises the inherent safety capabilities embedded in the\noriginal LLMs. Despite potential harmfulness due to weakened safety measures,\nin-depth analysis on the effects of VL adaptation on safety remains\nunder-explored. This study examines how VL adaptation influences safety and\nevaluates the impact of safety fine-tuning methods. Our analysis reveals that\nsafety degradation occurs during VL adaptation, even when the training data is\nsafe. While safety tuning techniques like supervised fine-tuning with safety\ndatasets or reinforcement learning from human feedback mitigate some risks,\nthey still lead to safety degradation and a reduction in helpfulness due to\nover-rejection issues. Further analysis of internal model weights suggests that\nVL adaptation may impact certain safety-related layers, potentially lowering\noverall safety levels. Additionally, our findings demonstrate that the\nobjectives of VL adaptation and safety tuning are divergent, which often\nresults in their simultaneous application being suboptimal. To address this, we\nsuggest the weight merging approach as an optimal solution effectively reducing\nsafety degradation while maintaining helpfulness. These insights help guide the\ndevelopment of more reliable and secure LVLMs for real-world applications."
                },
                "authors": [
                    {
                        "name": "Seongyun Lee"
                    },
                    {
                        "name": "Geewook Kim"
                    },
                    {
                        "name": "Jiyeon Kim"
                    },
                    {
                        "name": "Hyunji Lee"
                    },
                    {
                        "name": "Hoyeon Chang"
                    },
                    {
                        "name": "Sue Hyun Park"
                    },
                    {
                        "name": "Minjoon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Minjoon Seo"
                },
                "author": "Minjoon Seo",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07571v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07571v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11797v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11797v2",
                "updated": "2024-11-15T03:14:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    14,
                    45,
                    4,
                    320,
                    0
                ],
                "published": "2024-10-15T17:20:50Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    20,
                    50,
                    1,
                    289,
                    0
                ],
                "title": "Heterogeneous Treatment Effects under Network Interference: A\n  Nonparametric Approach Based on Node Connectivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Treatment Effects under Network Interference: A\n  Nonparametric Approach Based on Node Connectivity"
                },
                "summary": "In network settings, interference between units makes causal inference more\nchallenging as outcomes may depend on the treatments received by others in the\nnetwork. Typical estimands in network settings focus on treatment effects\naggregated across individuals in the population. We propose a framework for\nestimating node-wise counterfactual means, allowing for more granular insights\ninto the impact of network structure on treatment effect heterogeneity. We\ndevelop a doubly robust and non-parametric estimation procedure, KECENI (Kernel\nEstimation of Causal Effect under Network Interference), which offers\nconsistency and asymptotic normality under network dependence. The utility of\nthis method is demonstrated through an application to microfinance data,\nrevealing the impact of network characteristics on treatment effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In network settings, interference between units makes causal inference more\nchallenging as outcomes may depend on the treatments received by others in the\nnetwork. Typical estimands in network settings focus on treatment effects\naggregated across individuals in the population. We propose a framework for\nestimating node-wise counterfactual means, allowing for more granular insights\ninto the impact of network structure on treatment effect heterogeneity. We\ndevelop a doubly robust and non-parametric estimation procedure, KECENI (Kernel\nEstimation of Causal Effect under Network Interference), which offers\nconsistency and asymptotic normality under network dependence. The utility of\nthis method is demonstrated through an application to microfinance data,\nrevealing the impact of network characteristics on treatment effects."
                },
                "authors": [
                    {
                        "name": "Heejong Bong"
                    },
                    {
                        "name": "Colin B. Fogarty"
                    },
                    {
                        "name": "Elizaveta Levina"
                    },
                    {
                        "name": "Ji Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhu"
                },
                "author": "Ji Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11797v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11797v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09911v1",
                "updated": "2024-11-15T03:14:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    14,
                    11,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T03:14:11Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    14,
                    11,
                    4,
                    320,
                    0
                ],
                "title": "DiffFNO: Diffusion Fourier Neural Operator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffFNO: Diffusion Fourier Neural Operator"
                },
                "summary": "We introduce DiffFNO, a novel diffusion framework for arbitrary-scale\nsuper-resolution strengthened by a Weighted Fourier Neural Operator (WFNO).\nMode Re-balancing in WFNO effectively captures critical frequency components,\nsignificantly improving the reconstruction of high-frequency image details that\nare crucial for super-resolution tasks. Gated Fusion Mechanism (GFM) adaptively\ncomplements WFNO's spectral features with spatial features from an\nAttention-based Neural Operator (AttnNO). This enhances the network's\ncapability to capture both global structures and local details. Adaptive\nTime-Step (ATS) ODE solver, a deterministic sampling strategy, accelerates\ninference without sacrificing output quality by dynamically adjusting\nintegration step sizes ATS. Extensive experiments demonstrate that DiffFNO\nachieves state-of-the-art (SOTA) results, outperforming existing methods across\nvarious scaling factors by a margin of 2 to 4 dB in PSNR, including those\nbeyond the training distribution. It also achieves this at lower inference\ntime. Our approach sets a new standard in super-resolution, delivering both\nsuperior accuracy and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DiffFNO, a novel diffusion framework for arbitrary-scale\nsuper-resolution strengthened by a Weighted Fourier Neural Operator (WFNO).\nMode Re-balancing in WFNO effectively captures critical frequency components,\nsignificantly improving the reconstruction of high-frequency image details that\nare crucial for super-resolution tasks. Gated Fusion Mechanism (GFM) adaptively\ncomplements WFNO's spectral features with spatial features from an\nAttention-based Neural Operator (AttnNO). This enhances the network's\ncapability to capture both global structures and local details. Adaptive\nTime-Step (ATS) ODE solver, a deterministic sampling strategy, accelerates\ninference without sacrificing output quality by dynamically adjusting\nintegration step sizes ATS. Extensive experiments demonstrate that DiffFNO\nachieves state-of-the-art (SOTA) results, outperforming existing methods across\nvarious scaling factors by a margin of 2 to 4 dB in PSNR, including those\nbeyond the training distribution. It also achieves this at lower inference\ntime. Our approach sets a new standard in super-resolution, delivering both\nsuperior accuracy and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Hao Tang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Tang"
                },
                "author": "Hao Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05478v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05478v4",
                "updated": "2024-11-15T03:13:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    13,
                    0,
                    4,
                    320,
                    0
                ],
                "published": "2024-07-07T19:33:30Z",
                "published_parsed": [
                    2024,
                    7,
                    7,
                    19,
                    33,
                    30,
                    6,
                    189,
                    0
                ],
                "title": "Sequential Gaussian Variational Inference for Nonlinear State Estimation\n  and Its Application in Robot Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Gaussian Variational Inference for Nonlinear State Estimation\n  and Its Application in Robot Navigation"
                },
                "summary": "Probabilistic state estimation is essential for robots navigating uncertain\nenvironments. Accurately and efficiently managing uncertainty in estimated\nstates is key to robust robotic operation. However, nonlinearities in robotic\nplatforms pose significant challenges that require advanced estimation\ntechniques. Gaussian variational inference (GVI) offers an optimization\nperspective on the estimation problem, providing analytically tractable\nsolutions and efficiencies derived from the geometry of Gaussian space. We\npropose a Sequential Gaussian Variational Inference (S-GVI) method to address\nnonlinearity and provide efficient sequential inference processes. Our approach\nintegrates sequential Bayesian principles into the GVI framework, which are\naddressed using statistical approximations and gradient updates on the\ninformation geometry. Validations through simulations and real-world\nexperiments demonstrate significant improvements in state estimation over the\nMaximum A Posteriori (MAP) estimation method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic state estimation is essential for robots navigating uncertain\nenvironments. Accurately and efficiently managing uncertainty in estimated\nstates is key to robust robotic operation. However, nonlinearities in robotic\nplatforms pose significant challenges that require advanced estimation\ntechniques. Gaussian variational inference (GVI) offers an optimization\nperspective on the estimation problem, providing analytically tractable\nsolutions and efficiencies derived from the geometry of Gaussian space. We\npropose a Sequential Gaussian Variational Inference (S-GVI) method to address\nnonlinearity and provide efficient sequential inference processes. Our approach\nintegrates sequential Bayesian principles into the GVI framework, which are\naddressed using statistical approximations and gradient updates on the\ninformation geometry. Validations through simulations and real-world\nexperiments demonstrate significant improvements in state estimation over the\nMaximum A Posteriori (MAP) estimation method."
                },
                "authors": [
                    {
                        "name": "Min-Won Seo"
                    },
                    {
                        "name": "Solmaz S. Kia"
                    }
                ],
                "author_detail": {
                    "name": "Solmaz S. Kia"
                },
                "author": "Solmaz S. Kia",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05478v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05478v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09909v1",
                "updated": "2024-11-15T03:11:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    11,
                    19,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T03:11:19Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    11,
                    19,
                    4,
                    320,
                    0
                ],
                "title": "AMXFP4: Taming Activation Outliers with Asymmetric Microscaling\n  Floating-Point for 4-bit LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMXFP4: Taming Activation Outliers with Asymmetric Microscaling\n  Floating-Point for 4-bit LLM Inference"
                },
                "summary": "Scaling Large Language Models (LLMs) with extended context lengths has\nincreased the need for efficient low-bit quantization to manage their\nsubstantial computational demands. However, reducing precision to 4 bits\nfrequently degrades performance due to activation outliers. To address this, we\npropose Asymmetric Microscaling 4-bit Floating-Point (AMXFP4) for efficient LLM\ninference. This novel data format leverages asymmetric shared scales to\nmitigate outliers while naturally capturing the asymmetry introduced by\ngroup-wise quantization. Unlike conventional 4-bit quantization methods that\nrely on data rotation and costly calibration, AMXFP4 uses asymmetric shared\nscales for direct 4-bit casting, achieving near-ideal quantization accuracy\nacross various LLM tasks, including multi-turn conversations, long-context\nreasoning, and visual question answering. Our AMXFP4 format significantly\noutperforms MXFP4 and other leading quantization techniques, enabling robust,\ncalibration-free 4-bit inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Large Language Models (LLMs) with extended context lengths has\nincreased the need for efficient low-bit quantization to manage their\nsubstantial computational demands. However, reducing precision to 4 bits\nfrequently degrades performance due to activation outliers. To address this, we\npropose Asymmetric Microscaling 4-bit Floating-Point (AMXFP4) for efficient LLM\ninference. This novel data format leverages asymmetric shared scales to\nmitigate outliers while naturally capturing the asymmetry introduced by\ngroup-wise quantization. Unlike conventional 4-bit quantization methods that\nrely on data rotation and costly calibration, AMXFP4 uses asymmetric shared\nscales for direct 4-bit casting, achieving near-ideal quantization accuracy\nacross various LLM tasks, including multi-turn conversations, long-context\nreasoning, and visual question answering. Our AMXFP4 format significantly\noutperforms MXFP4 and other leading quantization techniques, enabling robust,\ncalibration-free 4-bit inference."
                },
                "authors": [
                    {
                        "name": "Janghwan Lee"
                    },
                    {
                        "name": "Jiwoong Park"
                    },
                    {
                        "name": "Jinseok Kim"
                    },
                    {
                        "name": "Yongjik Kim"
                    },
                    {
                        "name": "Jungju Oh"
                    },
                    {
                        "name": "Jinwook Oh"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09410v2",
                "updated": "2024-11-15T02:53:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    2,
                    53,
                    36,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-14T13:00:23Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    0,
                    23,
                    3,
                    319,
                    0
                ],
                "title": "LLM-assisted Explicit and Implicit Multi-interest Learning Framework for\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-assisted Explicit and Implicit Multi-interest Learning Framework for\n  Sequential Recommendation"
                },
                "summary": "Multi-interest modeling in current recommender systems (RS) is mainly based\non user behavioral data, capturing user interest preferences from multiple\ndimensions. However, since behavioral data is implicit and often highly sparse,\nit is challenging to understand users' complex and diverse interests. Recent\nstudies have shown that the rich semantic information in the text can\neffectively supplement the deficiencies of behavioral data. Despite this, it is\nstill difficult for small models to directly extract semantic features\nassociated with users' deep interests. That is, how to effectively align\nsemantics with behavioral information to form a more comprehensive and accurate\nunderstanding of user interests has become a critical research problem. To\naddress this, we propose an LLM-assisted explicit and implicit multi-interest\nlearning framework (named EIMF) to model user interests on two levels: behavior\nand semantics. The framework consists of two parts: Implicit Behavioral\nInterest Module (IBIM) and Explicit Semantic Interest Module (ESIM). The\ntraditional multi-interest RS model in IBIM can learn users' implicit\nbehavioral interests from interactions with items. In ESIM, we first adopt a\nclustering algorithm to select typical samples and design a prompting strategy\non LLM to obtain explicit semantic interests. Furthermore, in the training\nphase, the semantic interests of typical samples can enhance the representation\nlearning of behavioral interests based on the multi-task learning on semantic\nprediction and modality alignment. Therefore, in the inference stage, accurate\nrecommendations can be achieved with only the user's behavioral data. Extensive\nexperiments on real-world datasets demonstrate the effectiveness of the\nproposed EIMF framework, which effectively and efficiently combines small\nmodels with LLM to improve the accuracy of multi-interest modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-interest modeling in current recommender systems (RS) is mainly based\non user behavioral data, capturing user interest preferences from multiple\ndimensions. However, since behavioral data is implicit and often highly sparse,\nit is challenging to understand users' complex and diverse interests. Recent\nstudies have shown that the rich semantic information in the text can\neffectively supplement the deficiencies of behavioral data. Despite this, it is\nstill difficult for small models to directly extract semantic features\nassociated with users' deep interests. That is, how to effectively align\nsemantics with behavioral information to form a more comprehensive and accurate\nunderstanding of user interests has become a critical research problem. To\naddress this, we propose an LLM-assisted explicit and implicit multi-interest\nlearning framework (named EIMF) to model user interests on two levels: behavior\nand semantics. The framework consists of two parts: Implicit Behavioral\nInterest Module (IBIM) and Explicit Semantic Interest Module (ESIM). The\ntraditional multi-interest RS model in IBIM can learn users' implicit\nbehavioral interests from interactions with items. In ESIM, we first adopt a\nclustering algorithm to select typical samples and design a prompting strategy\non LLM to obtain explicit semantic interests. Furthermore, in the training\nphase, the semantic interests of typical samples can enhance the representation\nlearning of behavioral interests based on the multi-task learning on semantic\nprediction and modality alignment. Therefore, in the inference stage, accurate\nrecommendations can be achieved with only the user's behavioral data. Extensive\nexperiments on real-world datasets demonstrate the effectiveness of the\nproposed EIMF framework, which effectively and efficiently combines small\nmodels with LLM to improve the accuracy of multi-interest modeling."
                },
                "authors": [
                    {
                        "name": "Shutong Qiao"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09902v1",
                "updated": "2024-11-15T02:50:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    2,
                    50,
                    17,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T02:50:17Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    2,
                    50,
                    17,
                    4,
                    320,
                    0
                ],
                "title": "Investigation of the non-thermal X-ray emission from the supernova\n  remnant CTB 37B hosting the magnetar CXOU J171405.7$-$381031",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigation of the non-thermal X-ray emission from the supernova\n  remnant CTB 37B hosting the magnetar CXOU J171405.7$-$381031"
                },
                "summary": "We present a detailed X-ray investigation of a region (S1) exhibiting\nnon-thermal X-ray emission within the supernova remnant (SNR) CTB 37B hosting\nthe magnetar CXOU J171405.7$-$381031. Previous analyses modeled this emission\nwith a power law (PL), inferring various values for the photon index ($\\Gamma$)\nand absorbing column density ($N_{\\rm H}$). Based on these, S1 was suggested to\nbe the SNR shell, a background pulsar wind nebula (PWN), or an interaction\nregion between the SNR and a molecular cloud. Our analysis of a larger dataset\nfavors a steepening (broken or curved PL) spectrum over a straight PL, with the\nbest-fit broken power-law (BPL) parameters of $\\Gamma=1.23\\pm0.23$ and\n$2.24\\pm0.16$ below and above a break at $5.57\\pm0.52$ keV, respectively.\nHowever, a simple PL or srcut model cannot be definitively ruled out. For the\nBPL model, the inferred $N_{\\rm H}=(4.08\\pm0.72)\\times 10^{22}\\rm \\ cm^{-2}$\ntowards S1 is consistent with that of the SNR, suggesting a physical\nassociation. The BPL-inferred spectral break $\\Delta \\Gamma \\approx 1$ and hard\n$\\Gamma$ can be naturally explained by a non-thermal bremsstrahlung (NTB)\nmodel. We present an evolutionary NTB model that reproduces the observed\nspectrum, which indicates the presence of sub-relativistic electrons within S1.\nHowever, alternate explanations for S1, an unrelated PWN or the SNR shock with\nunusually efficient acceleration, cannot be ruled out. We discuss these\nexplanations and their implications for gamma-ray emission from CTB 37B, and\ndescribe future observations that could settle the origin of S1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a detailed X-ray investigation of a region (S1) exhibiting\nnon-thermal X-ray emission within the supernova remnant (SNR) CTB 37B hosting\nthe magnetar CXOU J171405.7$-$381031. Previous analyses modeled this emission\nwith a power law (PL), inferring various values for the photon index ($\\Gamma$)\nand absorbing column density ($N_{\\rm H}$). Based on these, S1 was suggested to\nbe the SNR shell, a background pulsar wind nebula (PWN), or an interaction\nregion between the SNR and a molecular cloud. Our analysis of a larger dataset\nfavors a steepening (broken or curved PL) spectrum over a straight PL, with the\nbest-fit broken power-law (BPL) parameters of $\\Gamma=1.23\\pm0.23$ and\n$2.24\\pm0.16$ below and above a break at $5.57\\pm0.52$ keV, respectively.\nHowever, a simple PL or srcut model cannot be definitively ruled out. For the\nBPL model, the inferred $N_{\\rm H}=(4.08\\pm0.72)\\times 10^{22}\\rm \\ cm^{-2}$\ntowards S1 is consistent with that of the SNR, suggesting a physical\nassociation. The BPL-inferred spectral break $\\Delta \\Gamma \\approx 1$ and hard\n$\\Gamma$ can be naturally explained by a non-thermal bremsstrahlung (NTB)\nmodel. We present an evolutionary NTB model that reproduces the observed\nspectrum, which indicates the presence of sub-relativistic electrons within S1.\nHowever, alternate explanations for S1, an unrelated PWN or the SNR shock with\nunusually efficient acceleration, cannot be ruled out. We discuss these\nexplanations and their implications for gamma-ray emission from CTB 37B, and\ndescribe future observations that could settle the origin of S1."
                },
                "authors": [
                    {
                        "name": "Chanho Kim"
                    },
                    {
                        "name": "Jaegeun Park"
                    },
                    {
                        "name": "Hongjun An"
                    },
                    {
                        "name": "Kaya Mori"
                    },
                    {
                        "name": "Stephen P. Reynolds"
                    },
                    {
                        "name": "Samar Safi-Harb"
                    },
                    {
                        "name": "Shuo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shuo Zhang"
                },
                "arxiv_affiliation": "Michigan State University",
                "author": "Shuo Zhang",
                "arxiv_comment": "11 pages. 4 figures, Accepted for publication in Apj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09318v2",
                "updated": "2024-11-15T02:42:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    2,
                    42,
                    59,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-14T10:00:33Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    0,
                    33,
                    3,
                    319,
                    0
                ],
                "title": "DriveThru: a Document Extraction Platform and Benchmark Datasets for\n  Indonesian Local Language Archives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriveThru: a Document Extraction Platform and Benchmark Datasets for\n  Indonesian Local Language Archives"
                },
                "summary": "Indonesia is one of the most diverse countries linguistically. However,\ndespite this linguistic diversity, Indonesian languages remain underrepresented\nin Natural Language Processing (NLP) research and technologies. In the past two\nyears, several efforts have been conducted to construct NLP resources for\nIndonesian languages. However, most of these efforts have been focused on\ncreating manual resources thus difficult to scale to more languages. Although\nmany Indonesian languages do not have a web presence, locally there are\nresources that document these languages well in printed forms such as books,\nmagazines, and newspapers. Digitizing these existing resources will enable\nscaling of Indonesian language resource construction to many more languages. In\nthis paper, we propose an alternative method of creating datasets by digitizing\ndocuments, which have not previously been used to build digital language\nresources in Indonesia. DriveThru is a platform for extracting document content\nutilizing Optical Character Recognition (OCR) techniques in its system to\nprovide language resource building with less manual effort and cost. This paper\nalso studies the utility of current state-of-the-art LLM for post-OCR\ncorrection to show the capability of increasing the character accuracy rate\n(CAR) and word accuracy rate (WAR) compared to off-the-shelf OCR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indonesia is one of the most diverse countries linguistically. However,\ndespite this linguistic diversity, Indonesian languages remain underrepresented\nin Natural Language Processing (NLP) research and technologies. In the past two\nyears, several efforts have been conducted to construct NLP resources for\nIndonesian languages. However, most of these efforts have been focused on\ncreating manual resources thus difficult to scale to more languages. Although\nmany Indonesian languages do not have a web presence, locally there are\nresources that document these languages well in printed forms such as books,\nmagazines, and newspapers. Digitizing these existing resources will enable\nscaling of Indonesian language resource construction to many more languages. In\nthis paper, we propose an alternative method of creating datasets by digitizing\ndocuments, which have not previously been used to build digital language\nresources in Indonesia. DriveThru is a platform for extracting document content\nutilizing Optical Character Recognition (OCR) techniques in its system to\nprovide language resource building with less manual effort and cost. This paper\nalso studies the utility of current state-of-the-art LLM for post-OCR\ncorrection to show the capability of increasing the character accuracy rate\n(CAR) and word accuracy rate (WAR) compared to off-the-shelf OCR."
                },
                "authors": [
                    {
                        "name": "Mohammad Rifqi Farhansyah"
                    },
                    {
                        "name": "Muhammad Zuhdi Fikri Johari"
                    },
                    {
                        "name": "Afinzaki Amiral"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    },
                    {
                        "name": "Kumara Ari Yuana"
                    },
                    {
                        "name": "Derry Tanti Wijaya"
                    }
                ],
                "author_detail": {
                    "name": "Derry Tanti Wijaya"
                },
                "author": "Derry Tanti Wijaya",
                "arxiv_comment": "12 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13977v2",
                "updated": "2024-11-15T02:32:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    2,
                    32,
                    57,
                    4,
                    320,
                    0
                ],
                "published": "2024-06-20T03:54:41Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    3,
                    54,
                    41,
                    3,
                    172,
                    0
                ],
                "title": "Similarity-aware Syncretic Latent Diffusion Model for Medical Image\n  Translation with Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity-aware Syncretic Latent Diffusion Model for Medical Image\n  Translation with Representation Learning"
                },
                "summary": "Non-contrast CT (NCCT) imaging may reduce image contrast and anatomical\nvisibility, potentially increasing diagnostic uncertainty. In contrast,\ncontrast-enhanced CT (CECT) facilitates the observation of regions of interest\n(ROI). Leading generative models, especially the conditional diffusion model,\ndemonstrate remarkable capabilities in medical image modality transformation.\nTypical conditional diffusion models commonly generate images with guidance of\nsegmentation labels for medical modal transformation. Limited access to\nauthentic guidance and its low cardinality can pose challenges to the practical\nclinical application of conditional diffusion models. To achieve an equilibrium\nof generative quality and clinical practices, we propose a novel Syncretic\ngenerative model based on the latent diffusion model for medical image\ntranslation (S$^2$LDM), which can realize high-fidelity reconstruction without\ndemand of additional condition during inference. S$^2$LDM enhances the\nsimilarity in distinct modal images via syncretic encoding and diffusing,\npromoting amalgamated information in the latent space and generating medical\nimages with more details in contrast-enhanced regions. However, syncretic\nlatent spaces in the frequency domain tend to favor lower frequencies, commonly\nlocate in identical anatomic structures. Thus, S$^2$LDM applies adaptive\nsimilarity loss and dynamic similarity to guide the generation and supplements\nthe shortfall in high-frequency details throughout the training process.\nQuantitative experiments confirm the effectiveness of our approach in medical\nimage translation. Our code will release lately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-contrast CT (NCCT) imaging may reduce image contrast and anatomical\nvisibility, potentially increasing diagnostic uncertainty. In contrast,\ncontrast-enhanced CT (CECT) facilitates the observation of regions of interest\n(ROI). Leading generative models, especially the conditional diffusion model,\ndemonstrate remarkable capabilities in medical image modality transformation.\nTypical conditional diffusion models commonly generate images with guidance of\nsegmentation labels for medical modal transformation. Limited access to\nauthentic guidance and its low cardinality can pose challenges to the practical\nclinical application of conditional diffusion models. To achieve an equilibrium\nof generative quality and clinical practices, we propose a novel Syncretic\ngenerative model based on the latent diffusion model for medical image\ntranslation (S$^2$LDM), which can realize high-fidelity reconstruction without\ndemand of additional condition during inference. S$^2$LDM enhances the\nsimilarity in distinct modal images via syncretic encoding and diffusing,\npromoting amalgamated information in the latent space and generating medical\nimages with more details in contrast-enhanced regions. However, syncretic\nlatent spaces in the frequency domain tend to favor lower frequencies, commonly\nlocate in identical anatomic structures. Thus, S$^2$LDM applies adaptive\nsimilarity loss and dynamic similarity to guide the generation and supplements\nthe shortfall in high-frequency details throughout the training process.\nQuantitative experiments confirm the effectiveness of our approach in medical\nimage translation. Our code will release lately."
                },
                "authors": [
                    {
                        "name": "Tingyi Lin"
                    },
                    {
                        "name": "Pengju Lyu"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Yuqing Wang"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Jianjun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jianjun Zhu"
                },
                "author": "Jianjun Zhu",
                "arxiv_comment": "We decide to modify the majority of the content",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09887v1",
                "updated": "2024-11-15T02:18:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    2,
                    18,
                    35,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T02:18:35Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    2,
                    18,
                    35,
                    4,
                    320,
                    0
                ],
                "title": "Planning by Simulation: Motion Planning with Learning-based Parallel\n  Scenario Prediction for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning by Simulation: Motion Planning with Learning-based Parallel\n  Scenario Prediction for Autonomous Driving"
                },
                "summary": "Planning safe trajectories for autonomous vehicles is essential for\noperational safety but remains extremely challenging due to the complex\ninteractions among traffic participants. Recent autonomous driving frameworks\nhave focused on improving prediction accuracy to explicitly model these\ninteractions. However, some methods overlook the significant influence of the\nego vehicle's planning on the possible trajectories of other agents, which can\nalter prediction accuracy and lead to unsafe planning decisions. In this paper,\nwe propose a novel motion Planning approach by Simulation with learning-based\nparallel scenario prediction (PS). PS deduces predictions iteratively based on\nMonte Carlo Tree Search (MCTS), jointly inferring scenarios that cooperate with\nthe ego vehicle's planning set. Our method simulates possible scenes and\ncalculates their costs after the ego vehicle executes potential actions. To\nbalance and prune unreasonable actions and scenarios, we adopt MCTS as the\nfoundation to explore possible future interactions encoded within the\nprediction network. Moreover, the query-centric trajectory prediction\nstreamlines our scene generation, enabling a sophisticated framework that\ncaptures the mutual influence between other agents' predictions and the ego\nvehicle's planning. We evaluate our framework on the Argoverse 2 dataset, and\nthe results demonstrate that our approach effectively achieves parallel ego\nvehicle planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning safe trajectories for autonomous vehicles is essential for\noperational safety but remains extremely challenging due to the complex\ninteractions among traffic participants. Recent autonomous driving frameworks\nhave focused on improving prediction accuracy to explicitly model these\ninteractions. However, some methods overlook the significant influence of the\nego vehicle's planning on the possible trajectories of other agents, which can\nalter prediction accuracy and lead to unsafe planning decisions. In this paper,\nwe propose a novel motion Planning approach by Simulation with learning-based\nparallel scenario prediction (PS). PS deduces predictions iteratively based on\nMonte Carlo Tree Search (MCTS), jointly inferring scenarios that cooperate with\nthe ego vehicle's planning set. Our method simulates possible scenes and\ncalculates their costs after the ego vehicle executes potential actions. To\nbalance and prune unreasonable actions and scenarios, we adopt MCTS as the\nfoundation to explore possible future interactions encoded within the\nprediction network. Moreover, the query-centric trajectory prediction\nstreamlines our scene generation, enabling a sophisticated framework that\ncaptures the mutual influence between other agents' predictions and the ego\nvehicle's planning. We evaluate our framework on the Argoverse 2 dataset, and\nthe results demonstrate that our approach effectively achieves parallel ego\nvehicle planning."
                },
                "authors": [
                    {
                        "name": "Tian Niu"
                    },
                    {
                        "name": "Kaizhao Zhang"
                    },
                    {
                        "name": "Zhongxue Gan"
                    },
                    {
                        "name": "Wenchao Ding"
                    }
                ],
                "author_detail": {
                    "name": "Wenchao Ding"
                },
                "author": "Wenchao Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18027v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18027v2",
                "updated": "2024-11-15T02:07:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    2,
                    7,
                    34,
                    4,
                    320,
                    0
                ],
                "published": "2024-06-26T02:49:28Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    2,
                    49,
                    28,
                    2,
                    178,
                    0
                ],
                "title": "Automated Clinical Data Extraction with Knowledge Conditioned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Clinical Data Extraction with Knowledge Conditioned LLMs"
                },
                "summary": "The extraction of lung lesion information from clinical and medical imaging\nreports is crucial for research on and clinical care of lung-related diseases.\nLarge language models (LLMs) can be effective at interpreting unstructured text\nin reports, but they often hallucinate due to a lack of domain-specific\nknowledge, leading to reduced accuracy and posing challenges for use in\nclinical settings. To address this, we propose a novel framework that aligns\ngenerated internal knowledge with external knowledge through in-context\nlearning (ICL). Our framework employs a retriever to identify relevant units of\ninternal or external knowledge and a grader to evaluate the truthfulness and\nhelpfulness of the retrieved internal-knowledge rules, to align and update the\nknowledge bases. Experiments with expert-curated test datasets demonstrate that\nthis ICL approach can increase the F1 score for key fields (lesion size, margin\nand solidity) by an average of 12.9% over existing ICL methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extraction of lung lesion information from clinical and medical imaging\nreports is crucial for research on and clinical care of lung-related diseases.\nLarge language models (LLMs) can be effective at interpreting unstructured text\nin reports, but they often hallucinate due to a lack of domain-specific\nknowledge, leading to reduced accuracy and posing challenges for use in\nclinical settings. To address this, we propose a novel framework that aligns\ngenerated internal knowledge with external knowledge through in-context\nlearning (ICL). Our framework employs a retriever to identify relevant units of\ninternal or external knowledge and a grader to evaluate the truthfulness and\nhelpfulness of the retrieved internal-knowledge rules, to align and update the\nknowledge bases. Experiments with expert-curated test datasets demonstrate that\nthis ICL approach can increase the F1 score for key fields (lesion size, margin\nand solidity) by an average of 12.9% over existing ICL methods."
                },
                "authors": [
                    {
                        "name": "Diya Li"
                    },
                    {
                        "name": "Asim Kadav"
                    },
                    {
                        "name": "Aijing Gao"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Richard Bourgon"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bourgon"
                },
                "author": "Richard Bourgon",
                "arxiv_comment": "COLING25 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18027v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18027v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09880v1",
                "updated": "2024-11-15T02:01:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    2,
                    1,
                    32,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T02:01:32Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    2,
                    1,
                    32,
                    4,
                    320,
                    0
                ],
                "title": "Maximum entropy inference of reaction-diffusion models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum entropy inference of reaction-diffusion models"
                },
                "summary": "Reaction-diffusion equations are commonly used to model a diverse array of\ncomplex systems, including biological, chemical, and physical processes.\nTypically, these models are phenomenological, requiring the fitting of\nparameters to experimental data. In the present work, we introduce a novel\nformalism to construct reaction-diffusion models that is grounded in the\nprinciple of maximum entropy. This new formalism aims to incorporate various\ntypes of experimental data, including ensemble currents, distributions at\ndifferent points in time, or moments of such. To this end, we expand the\nframework of Schr\\\"odinger bridges and Maximum Caliber problems to nonlinear\ninteracting systems. We illustrate the usefulness of the proposed approach by\nmodeling the evolution of (i) a morphogen across the fin of a zebrafish and\n(ii) the population of two varieties of toads in Poland, so as to match the\nexperimental data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reaction-diffusion equations are commonly used to model a diverse array of\ncomplex systems, including biological, chemical, and physical processes.\nTypically, these models are phenomenological, requiring the fitting of\nparameters to experimental data. In the present work, we introduce a novel\nformalism to construct reaction-diffusion models that is grounded in the\nprinciple of maximum entropy. This new formalism aims to incorporate various\ntypes of experimental data, including ensemble currents, distributions at\ndifferent points in time, or moments of such. To this end, we expand the\nframework of Schr\\\"odinger bridges and Maximum Caliber problems to nonlinear\ninteracting systems. We illustrate the usefulness of the proposed approach by\nmodeling the evolution of (i) a morphogen across the fin of a zebrafish and\n(ii) the population of two varieties of toads in Poland, so as to match the\nexperimental data."
                },
                "authors": [
                    {
                        "name": "Olga Movilla Miangolarra"
                    },
                    {
                        "name": "Asmaa Eldesoukey"
                    },
                    {
                        "name": "Ander Movilla Miangolarra"
                    },
                    {
                        "name": "Tryphon T. Georgiou"
                    }
                ],
                "author_detail": {
                    "name": "Tryphon T. Georgiou"
                },
                "author": "Tryphon T. Georgiou",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09874v1",
                "updated": "2024-11-15T01:49:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    1,
                    49,
                    17,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T01:49:17Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    1,
                    49,
                    17,
                    4,
                    320,
                    0
                ],
                "title": "A Hybrid Artificial Intelligence System for Automated EEG Background\n  Analysis and Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid Artificial Intelligence System for Automated EEG Background\n  Analysis and Report Generation"
                },
                "summary": "Electroencephalography (EEG) plays a crucial role in the diagnosis of various\nneurological disorders. However, small hospitals and clinics often lack\nadvanced EEG signal analysis systems and are prone to misinterpretation in\nmanual EEG reading. This study proposes an innovative hybrid artificial\nintelligence (AI) system for automatic interpretation of EEG background\nactivity and report generation. The system combines deep learning models for\nposterior dominant rhythm (PDR) prediction, unsupervised artifact removal, and\nexpert-designed algorithms for abnormality detection. For PDR prediction, 1530\nlabeled EEGs were used, and the best ensemble model achieved a mean absolute\nerror (MAE) of 0.237, a root mean square error (RMSE) of 0.359, an accuracy of\n91.8% within a 0.6Hz error, and an accuracy of 99% within a 1.2Hz error. The AI\nsystem significantly outperformed neurologists in detecting generalized\nbackground slowing (p = 0.02; F1: AI 0.93, neurologists 0.82) and demonstrated\nimproved focal abnormality detection, although not statistically significant (p\n= 0.79; F1: AI 0.71, neurologists 0.55). Validation on both an internal dataset\nand the Temple University Abnormal EEG Corpus showed consistent performance\n(F1: 0.884 and 0.835, respectively; p = 0.66), demonstrating generalizability.\nThe use of large language models (LLMs) for report generation demonstrated 100%\naccuracy, verified by three other independent LLMs. This hybrid AI system\nprovides an easily scalable and accurate solution for EEG interpretation in\nresource-limited settings, assisting neurologists in improving diagnostic\naccuracy and reducing misdiagnosis rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electroencephalography (EEG) plays a crucial role in the diagnosis of various\nneurological disorders. However, small hospitals and clinics often lack\nadvanced EEG signal analysis systems and are prone to misinterpretation in\nmanual EEG reading. This study proposes an innovative hybrid artificial\nintelligence (AI) system for automatic interpretation of EEG background\nactivity and report generation. The system combines deep learning models for\nposterior dominant rhythm (PDR) prediction, unsupervised artifact removal, and\nexpert-designed algorithms for abnormality detection. For PDR prediction, 1530\nlabeled EEGs were used, and the best ensemble model achieved a mean absolute\nerror (MAE) of 0.237, a root mean square error (RMSE) of 0.359, an accuracy of\n91.8% within a 0.6Hz error, and an accuracy of 99% within a 1.2Hz error. The AI\nsystem significantly outperformed neurologists in detecting generalized\nbackground slowing (p = 0.02; F1: AI 0.93, neurologists 0.82) and demonstrated\nimproved focal abnormality detection, although not statistically significant (p\n= 0.79; F1: AI 0.71, neurologists 0.55). Validation on both an internal dataset\nand the Temple University Abnormal EEG Corpus showed consistent performance\n(F1: 0.884 and 0.835, respectively; p = 0.66), demonstrating generalizability.\nThe use of large language models (LLMs) for report generation demonstrated 100%\naccuracy, verified by three other independent LLMs. This hybrid AI system\nprovides an easily scalable and accurate solution for EEG interpretation in\nresource-limited settings, assisting neurologists in improving diagnostic\naccuracy and reducing misdiagnosis rates."
                },
                "authors": [
                    {
                        "name": "Chin-Sung Tung"
                    },
                    {
                        "name": "Sheng-Fu Liang"
                    },
                    {
                        "name": "Shu-Feng Chang"
                    },
                    {
                        "name": "Chung-Ping Young"
                    }
                ],
                "author_detail": {
                    "name": "Chung-Ping Young"
                },
                "author": "Chung-Ping Young",
                "arxiv_doi": "10.1109/JBHI.2024.3496996",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JBHI.2024.3496996",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.09874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Example code available at https://github.com/tcs211/AI_EEEG_REPORT",
                "arxiv_journal_ref": "IEEE Journal of Biomedical and Health Informatics (2024)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09873v1",
                "updated": "2024-11-15T01:48:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    1,
                    48,
                    8,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T01:48:08Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    1,
                    48,
                    8,
                    4,
                    320,
                    0
                ],
                "title": "LLM-Powered AI Tutors with Personas for d/Deaf and Hard-of-Hearing\n  Online Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered AI Tutors with Personas for d/Deaf and Hard-of-Hearing\n  Online Learners"
                },
                "summary": "Intelligent tutoring systems (ITS) using artificial intelligence (AI)\ntechnology have shown promise in supporting learners with diverse abilities;\nhowever, they often fail to meet the specific communication needs and cultural\nnuances needed by d/Deaf and Hard-of-Hearing (DHH) learners. As large language\nmodels (LLMs) provide new opportunities to incorporate personas to AI-based\ntutors and support dynamic interactive dialogue, this paper explores how DHH\nlearners perceive LLM-powered ITS with different personas and identified design\nsuggestions for improving the interaction. We developed an interface that\nallows DHH learners to interact with ChatGPT and three LLM-powered AI tutors\nwith different experiences in DHH education while the learners watch an\neducational video. A user study with 16 DHH participants showed that they\nperceived conversations with the AI tutors who had DHH education experiences to\nbe more human-like and trustworthy due to the tutors' cultural knowledge of DHH\ncommunities. Participants also suggested providing more transparency regarding\nthe tutors' background information to clarify each AI tutor's position within\nthe DHH community. We discuss design implications for more inclusive LLM-based\nsystems, such as supports for the multimodality of sign language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent tutoring systems (ITS) using artificial intelligence (AI)\ntechnology have shown promise in supporting learners with diverse abilities;\nhowever, they often fail to meet the specific communication needs and cultural\nnuances needed by d/Deaf and Hard-of-Hearing (DHH) learners. As large language\nmodels (LLMs) provide new opportunities to incorporate personas to AI-based\ntutors and support dynamic interactive dialogue, this paper explores how DHH\nlearners perceive LLM-powered ITS with different personas and identified design\nsuggestions for improving the interaction. We developed an interface that\nallows DHH learners to interact with ChatGPT and three LLM-powered AI tutors\nwith different experiences in DHH education while the learners watch an\neducational video. A user study with 16 DHH participants showed that they\nperceived conversations with the AI tutors who had DHH education experiences to\nbe more human-like and trustworthy due to the tutors' cultural knowledge of DHH\ncommunities. Participants also suggested providing more transparency regarding\nthe tutors' background information to clarify each AI tutor's position within\nthe DHH community. We discuss design implications for more inclusive LLM-based\nsystems, such as supports for the multimodality of sign language."
                },
                "authors": [
                    {
                        "name": "Haocong Cheng"
                    },
                    {
                        "name": "Si Chen"
                    },
                    {
                        "name": "Christopher Perdriau"
                    },
                    {
                        "name": "Yun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yun Huang"
                },
                "author": "Yun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08719v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08719v2",
                "updated": "2024-11-15T01:32:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    1,
                    32,
                    51,
                    4,
                    320,
                    0
                ],
                "published": "2024-07-11T17:53:23Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    17,
                    53,
                    23,
                    3,
                    193,
                    0
                ],
                "title": "The Impact of Astrophysical Priors on Parameter Inference for GW230529",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Astrophysical Priors on Parameter Inference for GW230529"
                },
                "summary": "We investigate the effects of prior selection on the inferred mass and spin\nparameters of the neutron star-black hole merger GW230529\\_181500.\nSpecifically, we explore models motivated by astrophysical considerations,\nincluding massive binary and pulsar evolution. We examine mass and spin\ndistributions of neutron stars constrained by radio pulsar observations,\nalongside black hole spin observations from previous gravitational wave\ndetections. We show that the inferred mass distribution highly depends upon the\nspin prior. Specifically, under the most restrictive, binary stellar evolution\nmodels, we obtain narrower distributions of masses with a black hole mass of\n$4.3^{+0.1}_{-0.1}\\,M_{\\odot}$and neutron star mass of\n$1.3^{+0.03}_{-0.03}\\,M_{\\odot}$ where, somewhat surprisingly, it is the prior\non component spins which has the greatest impact on the inferred mass\ndistributions. Re-weighting using neutron star mass and spin priors from\nobservations of radio pulsars, with black hole spins from observations of\ngravitational waves, yields the black hole and the neutron star masses to be\n$3.8^{+0.5}_{-0.6} \\,M_\\odot$ and $1.4^{+0.2}_{-0.1} \\,M_\\odot$ respectively.\nThe sequence of compact object formation -- whether the neutron star or the\nblack hole formed first -- cannot be determined at the observed signal-to-noise\nratio. However, there is no evidence that the black hole was tidally spun up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the effects of prior selection on the inferred mass and spin\nparameters of the neutron star-black hole merger GW230529\\_181500.\nSpecifically, we explore models motivated by astrophysical considerations,\nincluding massive binary and pulsar evolution. We examine mass and spin\ndistributions of neutron stars constrained by radio pulsar observations,\nalongside black hole spin observations from previous gravitational wave\ndetections. We show that the inferred mass distribution highly depends upon the\nspin prior. Specifically, under the most restrictive, binary stellar evolution\nmodels, we obtain narrower distributions of masses with a black hole mass of\n$4.3^{+0.1}_{-0.1}\\,M_{\\odot}$and neutron star mass of\n$1.3^{+0.03}_{-0.03}\\,M_{\\odot}$ where, somewhat surprisingly, it is the prior\non component spins which has the greatest impact on the inferred mass\ndistributions. Re-weighting using neutron star mass and spin priors from\nobservations of radio pulsars, with black hole spins from observations of\ngravitational waves, yields the black hole and the neutron star masses to be\n$3.8^{+0.5}_{-0.6} \\,M_\\odot$ and $1.4^{+0.2}_{-0.1} \\,M_\\odot$ respectively.\nThe sequence of compact object formation -- whether the neutron star or the\nblack hole formed first -- cannot be determined at the observed signal-to-noise\nratio. However, there is no evidence that the black hole was tidally spun up."
                },
                "authors": [
                    {
                        "name": "Debatri Chattopadhyay"
                    },
                    {
                        "name": "Sama Al-Shammari"
                    },
                    {
                        "name": "Fabio Antonini"
                    },
                    {
                        "name": "Stephen Fairhurst"
                    },
                    {
                        "name": "Benjamin Miles"
                    },
                    {
                        "name": "Vivien Raymond"
                    }
                ],
                "author_detail": {
                    "name": "Vivien Raymond"
                },
                "author": "Vivien Raymond",
                "arxiv_doi": "10.1093/mnrasl/slae099",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnrasl/slae099",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.08719v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08719v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in MNRAS Letters",
                "arxiv_journal_ref": "Monthly Notices of the Royal Astronomical Society: Letters (2024),\n  536, L19",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09298v2",
                "updated": "2024-11-15T01:09:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    1,
                    9,
                    27,
                    4,
                    320,
                    0
                ],
                "published": "2024-10-11T23:07:19Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    23,
                    7,
                    19,
                    4,
                    285,
                    0
                ],
                "title": "DeepOSets: Non-Autoregressive In-Context Learning of Supervised Learning\n  Operators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepOSets: Non-Autoregressive In-Context Learning of Supervised Learning\n  Operators"
                },
                "summary": "We introduce DeepSets Operator Networks (DeepOSets), an efficient,\nnon-autoregressive neural network architecture for in-context operator\nlearning. In-context learning allows a trained machine learning model to learn\nfrom a user prompt without further training. DeepOSets adds in-context learning\ncapabilities to Deep Operator Networks (DeepONets) by combining it with the\nDeepSets architecture. As the first non-autoregressive model for in-context\noperator learning, DeepOSets allow the user prompt to be processed in parallel,\nleading to significant computational savings. Here, we present the application\nof DeepOSets in the problem of learning supervised learning algorithms, which\nare operators mapping a finite-dimensional space of labeled data into an\ninfinite-dimensional hypothesis space of prediction functions. In an empirical\ncomparison with a popular autoregressive (transformer-based) model for\nin-context learning of linear regression in one and five dimensions, DeepOSets\nreduced the number of model weights by several orders of magnitude and required\na fraction of training and inference time. Furthermore, DeepOSets proved to be\nless sensitive to noise, significantly outperforming the transformer model in\nnoisy settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DeepSets Operator Networks (DeepOSets), an efficient,\nnon-autoregressive neural network architecture for in-context operator\nlearning. In-context learning allows a trained machine learning model to learn\nfrom a user prompt without further training. DeepOSets adds in-context learning\ncapabilities to Deep Operator Networks (DeepONets) by combining it with the\nDeepSets architecture. As the first non-autoregressive model for in-context\noperator learning, DeepOSets allow the user prompt to be processed in parallel,\nleading to significant computational savings. Here, we present the application\nof DeepOSets in the problem of learning supervised learning algorithms, which\nare operators mapping a finite-dimensional space of labeled data into an\ninfinite-dimensional hypothesis space of prediction functions. In an empirical\ncomparison with a popular autoregressive (transformer-based) model for\nin-context learning of linear regression in one and five dimensions, DeepOSets\nreduced the number of model weights by several orders of magnitude and required\na fraction of training and inference time. Furthermore, DeepOSets proved to be\nless sensitive to noise, significantly outperforming the transformer model in\nnoisy settings."
                },
                "authors": [
                    {
                        "name": "Shao-Ting Chiu"
                    },
                    {
                        "name": "Junyuan Hong"
                    },
                    {
                        "name": "Ulisses Braga-Neto"
                    }
                ],
                "author_detail": {
                    "name": "Ulisses Braga-Neto"
                },
                "author": "Ulisses Braga-Neto",
                "arxiv_comment": "Janossy pooling results were added; Figures 1 and 2 were updated;\n  minor edits were made throughout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09864v1",
                "updated": "2024-11-15T01:01:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    1,
                    1,
                    44,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T01:01:44Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    1,
                    1,
                    44,
                    4,
                    320,
                    0
                ],
                "title": "Uncertainty Propagation within Chained Models for Machine Learning\n  Reconstruction of Neutrino-LAr Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Propagation within Chained Models for Machine Learning\n  Reconstruction of Neutrino-LAr Interactions"
                },
                "summary": "Sequential or chained models are increasingly prevalent in machine learning\nfor scientific applications, due to their flexibility and ease of development.\nChained models are particularly useful when a task is separable into distinct\nsteps with a hierarchy of meaningful intermediate representations. In\nreliability-critical tasks, it is important to quantify the confidence of model\ninferences. However, chained models pose an additional challenge for\nuncertainty quantification, especially when input uncertainties need to be\npropagated. In such cases, a fully uncertainty-aware chain of models is\nrequired, where each step accepts a probability distribution over the input\nspace, and produces a probability distribution over the output space. In this\nwork, we present a case study for adapting a single model within an existing\nchain, designed for reconstruction within neutrino-Argon interactions,\ndeveloped for neutrino oscillation experiments such as MicroBooNE, ICARUS, and\nthe future DUNE experiment. We test the performance of an input\nuncertainty-enabled model against an uncertainty-blinded model using a method\nfor generating synthetic noise. By comparing these two, we assess the increase\nin inference quality achieved by exposing models to upstream uncertainty\nestimates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential or chained models are increasingly prevalent in machine learning\nfor scientific applications, due to their flexibility and ease of development.\nChained models are particularly useful when a task is separable into distinct\nsteps with a hierarchy of meaningful intermediate representations. In\nreliability-critical tasks, it is important to quantify the confidence of model\ninferences. However, chained models pose an additional challenge for\nuncertainty quantification, especially when input uncertainties need to be\npropagated. In such cases, a fully uncertainty-aware chain of models is\nrequired, where each step accepts a probability distribution over the input\nspace, and produces a probability distribution over the output space. In this\nwork, we present a case study for adapting a single model within an existing\nchain, designed for reconstruction within neutrino-Argon interactions,\ndeveloped for neutrino oscillation experiments such as MicroBooNE, ICARUS, and\nthe future DUNE experiment. We test the performance of an input\nuncertainty-enabled model against an uncertainty-blinded model using a method\nfor generating synthetic noise. By comparing these two, we assess the increase\nin inference quality achieved by exposing models to upstream uncertainty\nestimates."
                },
                "authors": [
                    {
                        "name": "D. Douglas"
                    },
                    {
                        "name": "A. Mishra"
                    },
                    {
                        "name": "D. Ratner"
                    },
                    {
                        "name": "F. Petersen"
                    }
                ],
                "author_detail": {
                    "name": "F. Petersen"
                },
                "author": "F. Petersen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18856v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18856v2",
                "updated": "2024-11-15T00:15:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    15,
                    18,
                    4,
                    320,
                    0
                ],
                "published": "2024-10-24T15:41:56Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    41,
                    56,
                    3,
                    298,
                    0
                ],
                "title": "Demystifying Large Language Models for Medicine: A Primer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Large Language Models for Medicine: A Primer"
                },
                "summary": "Large language models (LLMs) represent a transformative class of AI tools\ncapable of revolutionizing various aspects of healthcare by generating\nhuman-like responses across diverse contexts and adapting to novel tasks\nfollowing human instructions. Their potential application spans a broad range\nof medical tasks, such as clinical documentation, matching patients to clinical\ntrials, and answering medical questions. In this primer paper, we propose an\nactionable guideline to help healthcare professionals more efficiently utilize\nLLMs in their work, along with a set of best practices. This approach consists\nof several main phases, including formulating the task, choosing LLMs, prompt\nengineering, fine-tuning, and deployment. We start with the discussion of\ncritical considerations in identifying healthcare tasks that align with the\ncore capabilities of LLMs and selecting models based on the selected task and\ndata, performance requirements, and model interface. We then review the\nstrategies, such as prompt engineering and fine-tuning, to adapt standard LLMs\nto specialized medical tasks. Deployment considerations, including regulatory\ncompliance, ethical guidelines, and continuous monitoring for fairness and\nbias, are also discussed. By providing a structured step-by-step methodology,\nthis tutorial aims to equip healthcare professionals with the tools necessary\nto effectively integrate LLMs into clinical practice, ensuring that these\npowerful technologies are applied in a safe, reliable, and impactful manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a transformative class of AI tools\ncapable of revolutionizing various aspects of healthcare by generating\nhuman-like responses across diverse contexts and adapting to novel tasks\nfollowing human instructions. Their potential application spans a broad range\nof medical tasks, such as clinical documentation, matching patients to clinical\ntrials, and answering medical questions. In this primer paper, we propose an\nactionable guideline to help healthcare professionals more efficiently utilize\nLLMs in their work, along with a set of best practices. This approach consists\nof several main phases, including formulating the task, choosing LLMs, prompt\nengineering, fine-tuning, and deployment. We start with the discussion of\ncritical considerations in identifying healthcare tasks that align with the\ncore capabilities of LLMs and selecting models based on the selected task and\ndata, performance requirements, and model interface. We then review the\nstrategies, such as prompt engineering and fine-tuning, to adapt standard LLMs\nto specialized medical tasks. Deployment considerations, including regulatory\ncompliance, ethical guidelines, and continuous monitoring for fairness and\nbias, are also discussed. By providing a structured step-by-step methodology,\nthis tutorial aims to equip healthcare professionals with the tools necessary\nto effectively integrate LLMs into clinical practice, ensuring that these\npowerful technologies are applied in a safe, reliable, and impactful manner."
                },
                "authors": [
                    {
                        "name": "Qiao Jin"
                    },
                    {
                        "name": "Nicholas Wan"
                    },
                    {
                        "name": "Robert Leaman"
                    },
                    {
                        "name": "Shubo Tian"
                    },
                    {
                        "name": "Zhizheng Wang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Guangzhi Xiong"
                    },
                    {
                        "name": "Po-Ting Lai"
                    },
                    {
                        "name": "Qingqing Zhu"
                    },
                    {
                        "name": "Benjamin Hou"
                    },
                    {
                        "name": "Maame Sarfo-Gyamfi"
                    },
                    {
                        "name": "Gongbo Zhang"
                    },
                    {
                        "name": "Aidan Gilson"
                    },
                    {
                        "name": "Balu Bhasuran"
                    },
                    {
                        "name": "Zhe He"
                    },
                    {
                        "name": "Aidong Zhang"
                    },
                    {
                        "name": "Jimeng Sun"
                    },
                    {
                        "name": "Chunhua Weng"
                    },
                    {
                        "name": "Ronald M. Summers"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Zhiyong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Lu"
                },
                "author": "Zhiyong Lu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18856v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18856v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10930v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10930v3",
                "updated": "2024-11-15T00:09:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    9,
                    44,
                    4,
                    320,
                    0
                ],
                "published": "2024-01-31T17:52:52Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    17,
                    52,
                    52,
                    2,
                    31,
                    0
                ],
                "title": "ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters"
                },
                "summary": "The self-attention mechanism distinguishes transformer-based large language\nmodels (LLMs) apart from convolutional and recurrent neural networks. Despite\nthe performance improvement, achieving real-time LLM inference on silicon\nremains challenging due to the extensive use of Softmax in self-attention. In\naddition to the non-linearity, the low arithmetic intensity significantly\nlimits processing parallelism, especially when working with longer contexts. To\naddress this challenge, we propose Constant Softmax (ConSmax), a\nsoftware-hardware co-design that serves as an efficient alternative to Softmax.\nConSmax utilizes differentiable normalization parameters to eliminate the need\nfor maximum searching and denominator summation in Softmax. This approach\nenables extensive parallelization while still executing the essential functions\nof Softmax. Moreover, a scalable ConSmax hardware design with a bitwidth-split\nlook-up table (LUT) can achieve lossless non-linear operations and support\nmixed-precision computing. Experimental results show that ConSmax achieves a\nminuscule power consumption of 0.2mW and an area of 0.0008mm^2 at 1250MHz\nworking frequency in 16nm FinFET technology. For open-source contribution, we\nfurther implement our design with the OpenROAD toolchain under SkyWater's 130nm\nCMOS technology. The corresponding power is 2.69mW and the area is 0.007mm^2.\nConSmax achieves 3.35x power savings and 2.75x area savings in 16nm technology,\nand 3.15x power savings and 4.14x area savings with the open-source EDA\ntoolchain. In the meantime, it also maintains comparable accuracy on the GPT-2\nmodel and the WikiText103 dataset. The project is available at\nhttps://github.com/ReaLLMASIC/ConSmax",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The self-attention mechanism distinguishes transformer-based large language\nmodels (LLMs) apart from convolutional and recurrent neural networks. Despite\nthe performance improvement, achieving real-time LLM inference on silicon\nremains challenging due to the extensive use of Softmax in self-attention. In\naddition to the non-linearity, the low arithmetic intensity significantly\nlimits processing parallelism, especially when working with longer contexts. To\naddress this challenge, we propose Constant Softmax (ConSmax), a\nsoftware-hardware co-design that serves as an efficient alternative to Softmax.\nConSmax utilizes differentiable normalization parameters to eliminate the need\nfor maximum searching and denominator summation in Softmax. This approach\nenables extensive parallelization while still executing the essential functions\nof Softmax. Moreover, a scalable ConSmax hardware design with a bitwidth-split\nlook-up table (LUT) can achieve lossless non-linear operations and support\nmixed-precision computing. Experimental results show that ConSmax achieves a\nminuscule power consumption of 0.2mW and an area of 0.0008mm^2 at 1250MHz\nworking frequency in 16nm FinFET technology. For open-source contribution, we\nfurther implement our design with the OpenROAD toolchain under SkyWater's 130nm\nCMOS technology. The corresponding power is 2.69mW and the area is 0.007mm^2.\nConSmax achieves 3.35x power savings and 2.75x area savings in 16nm technology,\nand 3.15x power savings and 4.14x area savings with the open-source EDA\ntoolchain. In the meantime, it also maintains comparable accuracy on the GPT-2\nmodel and the WikiText103 dataset. The project is available at\nhttps://github.com/ReaLLMASIC/ConSmax"
                },
                "authors": [
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Guanchen Tao"
                    },
                    {
                        "name": "Yifei Zou"
                    },
                    {
                        "name": "Derek Chow"
                    },
                    {
                        "name": "Zichen Fan"
                    },
                    {
                        "name": "Kauna Lei"
                    },
                    {
                        "name": "Bangfei Pan"
                    },
                    {
                        "name": "Dennis Sylvester"
                    },
                    {
                        "name": "Gregory Kielian"
                    },
                    {
                        "name": "Mehdi Saligane"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Saligane"
                },
                "author": "Mehdi Saligane",
                "arxiv_journal_ref": "International Conference on Computer-Aided Design 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10930v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10930v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.10446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10446v1",
                "updated": "2024-11-15T18:59:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    59,
                    51,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T18:59:51Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    59,
                    51,
                    4,
                    320,
                    0
                ],
                "title": "VeriGraph: Scene Graphs for Execution Verifiable Robot Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriGraph: Scene Graphs for Execution Verifiable Robot Planning"
                },
                "summary": "Recent advancements in vision-language models (VLMs) offer potential for\nrobot task planning, but challenges remain due to VLMs' tendency to generate\nincorrect action sequences. To address these limitations, we propose VeriGraph,\na novel framework that integrates VLMs for robotic planning while verifying\naction feasibility. VeriGraph employs scene graphs as an intermediate\nrepresentation, capturing key objects and spatial relationships to improve plan\nverification and refinement. The system generates a scene graph from input\nimages and uses it to iteratively check and correct action sequences generated\nby an LLM-based task planner, ensuring constraints are respected and actions\nare executable. Our approach significantly enhances task completion rates\nacross diverse manipulation scenarios, outperforming baseline methods by 58%\nfor language-based tasks and 30% for image-based tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in vision-language models (VLMs) offer potential for\nrobot task planning, but challenges remain due to VLMs' tendency to generate\nincorrect action sequences. To address these limitations, we propose VeriGraph,\na novel framework that integrates VLMs for robotic planning while verifying\naction feasibility. VeriGraph employs scene graphs as an intermediate\nrepresentation, capturing key objects and spatial relationships to improve plan\nverification and refinement. The system generates a scene graph from input\nimages and uses it to iteratively check and correct action sequences generated\nby an LLM-based task planner, ensuring constraints are respected and actions\nare executable. Our approach significantly enhances task completion rates\nacross diverse manipulation scenarios, outperforming baseline methods by 58%\nfor language-based tasks and 30% for image-based tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Ekpo"
                    },
                    {
                        "name": "Mara Levy"
                    },
                    {
                        "name": "Saksham Suri"
                    },
                    {
                        "name": "Chuong Huynh"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Shrivastava"
                },
                "author": "Abhinav Shrivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06169v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06169v2",
                "updated": "2024-11-15T18:43:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    43,
                    23,
                    4,
                    320,
                    0
                ],
                "published": "2024-10-08T16:13:24Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    16,
                    13,
                    24,
                    1,
                    282,
                    0
                ],
                "title": "Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to\n  See",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to\n  See"
                },
                "summary": "By treating visual tokens from visual encoders as text tokens, Multimodal\nLarge Language Models (MLLMs) have achieved remarkable progress across diverse\nvisual understanding tasks, leveraging the robust architectures of Large\nLanguage Models (LLMs). However, as token counts grow, the quadratic scaling of\ncomputation in LLMs introduces a significant efficiency bottleneck, impeding\nfurther scalability. Although recent approaches have explored pruning visual\ntokens or employing lighter LLM architectures, the computational overhead from\nan increasing number of visual tokens remains a substantial challenge.\n  In this study, we investigate the redundancy in visual computation at both\nthe parameter and computational pattern levels within LLaVA, a representative\nMLLM, and introduce a suite of streamlined strategies to enhance efficiency.\nThese include neighbor-aware visual token attention, pruning of inactive visual\nattention heads, and selective layer dropping for visual computations. By\nimplementing these strategies in LLaVA, we achieve a reduction in computational\ndemands of 88% while maintaining model performance across key benchmarks.\nAdditionally, we validate the existence of visual computational redundancy in\nother MLLMs, such as Qwen2-VL-7B and InternVL-2.0-4B/8B/26B. These results\npresent a novel pathway for MLLMs to handle dense visual tokens with minimal\ncomputational costs. Code and model checkpoints will be released to support\nfurther research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By treating visual tokens from visual encoders as text tokens, Multimodal\nLarge Language Models (MLLMs) have achieved remarkable progress across diverse\nvisual understanding tasks, leveraging the robust architectures of Large\nLanguage Models (LLMs). However, as token counts grow, the quadratic scaling of\ncomputation in LLMs introduces a significant efficiency bottleneck, impeding\nfurther scalability. Although recent approaches have explored pruning visual\ntokens or employing lighter LLM architectures, the computational overhead from\nan increasing number of visual tokens remains a substantial challenge.\n  In this study, we investigate the redundancy in visual computation at both\nthe parameter and computational pattern levels within LLaVA, a representative\nMLLM, and introduce a suite of streamlined strategies to enhance efficiency.\nThese include neighbor-aware visual token attention, pruning of inactive visual\nattention heads, and selective layer dropping for visual computations. By\nimplementing these strategies in LLaVA, we achieve a reduction in computational\ndemands of 88% while maintaining model performance across key benchmarks.\nAdditionally, we validate the existence of visual computational redundancy in\nother MLLMs, such as Qwen2-VL-7B and InternVL-2.0-4B/8B/26B. These results\npresent a novel pathway for MLLMs to handle dense visual tokens with minimal\ncomputational costs. Code and model checkpoints will be released to support\nfurther research."
                },
                "authors": [
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Phu Pham"
                    },
                    {
                        "name": "Wentian Zhao"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Yu-Jhe Li"
                    },
                    {
                        "name": "Jianing Zhou"
                    },
                    {
                        "name": "Daniel Miranda"
                    },
                    {
                        "name": "Ajinkya Kale"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06169v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06169v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10422v1",
                "updated": "2024-11-15T18:42:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    42,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T18:42:48Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    42,
                    48,
                    4,
                    320,
                    0
                ],
                "title": "Evaluating Creativity and Deception in Large Language Models: A\n  Simulation Framework for Multi-Agent Balderdash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Creativity and Deception in Large Language Models: A\n  Simulation Framework for Multi-Agent Balderdash"
                },
                "summary": "Large Language Models (LLMs) have shown impressive capabilities in complex\ntasks and interactive environments, yet their creativity remains underexplored.\nThis paper introduces a simulation framework utilizing the game Balderdash to\nevaluate both the creativity and logical reasoning of LLMs. In Balderdash,\nplayers generate fictitious definitions for obscure terms to deceive others\nwhile identifying correct definitions. Our framework enables multiple LLM\nagents to participate in this game, assessing their ability to produce\nplausible definitions and strategize based on game rules and history. We\nimplemented a centralized game engine featuring various LLMs as participants\nand a judge LLM to evaluate semantic equivalence. Through a series of\nexperiments, we analyzed the performance of different LLMs, examining metrics\nsuch as True Definition Ratio, Deception Ratio, and Correct Guess Ratio. The\nresults provide insights into the creative and deceptive capabilities of LLMs,\nhighlighting their strengths and areas for improvement. Specifically, the study\nreveals that infrequent vocabulary in LLMs' input leads to poor reasoning on\ngame rules and historical context\n(https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive capabilities in complex\ntasks and interactive environments, yet their creativity remains underexplored.\nThis paper introduces a simulation framework utilizing the game Balderdash to\nevaluate both the creativity and logical reasoning of LLMs. In Balderdash,\nplayers generate fictitious definitions for obscure terms to deceive others\nwhile identifying correct definitions. Our framework enables multiple LLM\nagents to participate in this game, assessing their ability to produce\nplausible definitions and strategize based on game rules and history. We\nimplemented a centralized game engine featuring various LLMs as participants\nand a judge LLM to evaluate semantic equivalence. Through a series of\nexperiments, we analyzed the performance of different LLMs, examining metrics\nsuch as True Definition Ratio, Deception Ratio, and Correct Guess Ratio. The\nresults provide insights into the creative and deceptive capabilities of LLMs,\nhighlighting their strengths and areas for improvement. Specifically, the study\nreveals that infrequent vocabulary in LLMs' input leads to poor reasoning on\ngame rules and historical context\n(https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash)."
                },
                "authors": [
                    {
                        "name": "Parsa Hejabi"
                    },
                    {
                        "name": "Elnaz Rahmati"
                    },
                    {
                        "name": "Alireza S. Ziabari"
                    },
                    {
                        "name": "Preni Golazizian"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Morteza Dehghani"
                    }
                ],
                "author_detail": {
                    "name": "Morteza Dehghani"
                },
                "author": "Morteza Dehghani",
                "arxiv_comment": "Accepted at Wordplay: When Language Meets Games @ ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09584v2",
                "updated": "2024-11-15T18:34:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    34,
                    42,
                    4,
                    320,
                    0
                ],
                "published": "2024-02-14T21:19:33Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    21,
                    19,
                    33,
                    2,
                    45,
                    0
                ],
                "title": "Large Language Model-Based Interpretable Machine Learning Control in\n  Building Energy Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Based Interpretable Machine Learning Control in\n  Building Energy Systems"
                },
                "summary": "The potential of Machine Learning Control (MLC) in HVAC systems is hindered\nby its opaque nature and inference mechanisms, which is challenging for users\nand modelers to fully comprehend, ultimately leading to a lack of trust in\nMLC-based decision-making. To address this challenge, this paper investigates\nand explores Interpretable Machine Learning (IML), a branch of Machine Learning\n(ML) that enhances transparency and understanding of models and their\ninferences, to improve the credibility of MLC and its industrial application in\nHVAC systems. Specifically, we developed an innovative framework that combines\nthe principles of Shapley values and the in-context learning feature of Large\nLanguage Models (LLMs). While the Shapley values are instrumental in dissecting\nthe contributions of various features in ML models, LLM provides an in-depth\nunderstanding of the non-data-driven or rule-based elements in MLC; combining\nthem, LLM further packages these insights into a coherent, human-understandable\nnarrative. The paper presents a case study to demonstrate the feasibility of\nthe developed IML framework for model predictive control-based precooling under\ndemand response events in a virtual testbed. The results indicate that the\ndeveloped framework generates and explains the control signals in accordance\nwith the rule-based rationale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The potential of Machine Learning Control (MLC) in HVAC systems is hindered\nby its opaque nature and inference mechanisms, which is challenging for users\nand modelers to fully comprehend, ultimately leading to a lack of trust in\nMLC-based decision-making. To address this challenge, this paper investigates\nand explores Interpretable Machine Learning (IML), a branch of Machine Learning\n(ML) that enhances transparency and understanding of models and their\ninferences, to improve the credibility of MLC and its industrial application in\nHVAC systems. Specifically, we developed an innovative framework that combines\nthe principles of Shapley values and the in-context learning feature of Large\nLanguage Models (LLMs). While the Shapley values are instrumental in dissecting\nthe contributions of various features in ML models, LLM provides an in-depth\nunderstanding of the non-data-driven or rule-based elements in MLC; combining\nthem, LLM further packages these insights into a coherent, human-understandable\nnarrative. The paper presents a case study to demonstrate the feasibility of\nthe developed IML framework for model predictive control-based precooling under\ndemand response events in a virtual testbed. The results indicate that the\ndeveloped framework generates and explains the control signals in accordance\nwith the rule-based rationale."
                },
                "authors": [
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Zhelun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhelun Chen"
                },
                "author": "Zhelun Chen",
                "arxiv_doi": "10.1016/j.enbuild.2024.114278",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.enbuild.2024.114278",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.09584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Energy and Buildings, 313, 114278 (2024)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10414v1",
                "updated": "2024-11-15T18:34:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    34,
                    7,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T18:34:07Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    34,
                    7,
                    4,
                    320,
                    0
                ],
                "title": "Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding\n  Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding\n  Conversations"
                },
                "summary": "We introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for\nhuman-AI conversations that involves image understanding: it can be used to\nsafeguard content for both multimodal LLM inputs (prompt classification) and\noutputs (response classification). Unlike the previous text-only Llama Guard\nversions (Inan et al., 2023; Llama Team, 2024b,a), it is specifically designed\nto support image reasoning use cases and is optimized to detect harmful\nmultimodal (text and image) prompts and text responses to these prompts. Llama\nGuard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong\nperformance on the internal benchmarks using the MLCommons taxonomy. We also\ntest its robustness against adversarial attacks. We believe that Llama Guard 3\nVision serves as a good starting point to build more capable and robust content\nmoderation tools for human-AI conversation with multimodal capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for\nhuman-AI conversations that involves image understanding: it can be used to\nsafeguard content for both multimodal LLM inputs (prompt classification) and\noutputs (response classification). Unlike the previous text-only Llama Guard\nversions (Inan et al., 2023; Llama Team, 2024b,a), it is specifically designed\nto support image reasoning use cases and is optimized to detect harmful\nmultimodal (text and image) prompts and text responses to these prompts. Llama\nGuard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong\nperformance on the internal benchmarks using the MLCommons taxonomy. We also\ntest its robustness against adversarial attacks. We believe that Llama Guard 3\nVision serves as a good starting point to build more capable and robust content\nmoderation tools for human-AI conversation with multimodal capabilities."
                },
                "authors": [
                    {
                        "name": "Jianfeng Chi"
                    },
                    {
                        "name": "Ujjwal Karn"
                    },
                    {
                        "name": "Hongyuan Zhan"
                    },
                    {
                        "name": "Eric Smith"
                    },
                    {
                        "name": "Javier Rando"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Kate Plawiak"
                    },
                    {
                        "name": "Zacharie Delpierre Coudert"
                    },
                    {
                        "name": "Kartikeya Upasani"
                    },
                    {
                        "name": "Mahesh Pasupuleti"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Pasupuleti"
                },
                "author": "Mahesh Pasupuleti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23472v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23472v2",
                "updated": "2024-11-15T17:18:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    17,
                    18,
                    57,
                    4,
                    320,
                    0
                ],
                "published": "2024-10-30T21:32:56Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    32,
                    56,
                    2,
                    304,
                    0
                ],
                "title": "Risk Sources and Risk Management Measures in Support of Standards for\n  General-Purpose AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk Sources and Risk Management Measures in Support of Standards for\n  General-Purpose AI Systems"
                },
                "summary": "There is an urgent need to identify both short and long-term risks from newly\nemerging types of Artificial Intelligence (AI), as well as available risk\nmanagement measures. In response, and to support global efforts in regulating\nAI and writing safety standards, we compile an extensive catalog of risk\nsources and risk management measures for general-purpose AI (GPAI) systems,\ncomplete with descriptions and supporting examples where relevant. This work\ninvolves identifying technical, operational, and societal risks across model\ndevelopment, training, and deployment stages, as well as surveying established\nand experimental methods for managing these risks. To the best of our\nknowledge, this paper is the first of its kind to provide extensive\ndocumentation of both GPAI risk sources and risk management measures that are\ndescriptive, self-contained and neutral with respect to any existing regulatory\nframework. This work intends to help AI providers, standards experts,\nresearchers, policymakers, and regulators in identifying and mitigating\nsystemic risks from GPAI systems. For this reason, the catalog is released\nunder a public domain license for ease of direct use by stakeholders in AI\ngovernance and standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an urgent need to identify both short and long-term risks from newly\nemerging types of Artificial Intelligence (AI), as well as available risk\nmanagement measures. In response, and to support global efforts in regulating\nAI and writing safety standards, we compile an extensive catalog of risk\nsources and risk management measures for general-purpose AI (GPAI) systems,\ncomplete with descriptions and supporting examples where relevant. This work\ninvolves identifying technical, operational, and societal risks across model\ndevelopment, training, and deployment stages, as well as surveying established\nand experimental methods for managing these risks. To the best of our\nknowledge, this paper is the first of its kind to provide extensive\ndocumentation of both GPAI risk sources and risk management measures that are\ndescriptive, self-contained and neutral with respect to any existing regulatory\nframework. This work intends to help AI providers, standards experts,\nresearchers, policymakers, and regulators in identifying and mitigating\nsystemic risks from GPAI systems. For this reason, the catalog is released\nunder a public domain license for ease of direct use by stakeholders in AI\ngovernance and standards."
                },
                "authors": [
                    {
                        "name": "Rokas Gipiškis"
                    },
                    {
                        "name": "Ayrton San Joaquin"
                    },
                    {
                        "name": "Ze Shen Chin"
                    },
                    {
                        "name": "Adrian Regenfuß"
                    },
                    {
                        "name": "Ariel Gil"
                    },
                    {
                        "name": "Koen Holtman"
                    }
                ],
                "author_detail": {
                    "name": "Koen Holtman"
                },
                "author": "Koen Holtman",
                "arxiv_comment": "92 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23472v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23472v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10362v1",
                "updated": "2024-11-15T17:11:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    17,
                    11,
                    13,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T17:11:13Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    17,
                    11,
                    13,
                    4,
                    320,
                    0
                ],
                "title": "Interactive Cycle Model -- The Linkage Combination among Automatic\n  Speech Recognition, Large Language Models and Smart Glasses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Cycle Model -- The Linkage Combination among Automatic\n  Speech Recognition, Large Language Models and Smart Glasses"
                },
                "summary": "This research proposes the interaction loop model \"ASR-LLM-Smart Glasses\",\nwhich model combines automatic speech recognition, large language model and\nsmart glasses to facilitate seamless human-computer interaction. And the\nmethodology of this research involves decomposing the interaction process into\ndifferent stages and elements. Speech is captured and processed by ASR, then\nanalyzed and interpreted by LLM. The results are then transmitted to smart\nglasses for display. The feedback loop is complete when the user interacts with\nthe displayed data. Mathematical formulas are used to quantify the performance\nof the model that revolves around core evaluation points: accuracy, coherence,\nand latency during ASR speech-to-text conversion. The research results are\nprovided theoretically to test and evaluate the feasibility and performance of\nthe model. Although such human-computer interaction products have not yet\nappeared in the industry, the performance indicators of this model in enhancing\nuser experience in fields that rely on human-computer interaction have also\nverified its utility as a technology to promote human-computer interaction. In\naddition, this research pioneered the idea of integrating cutting-edge\ntechnologies such as generative pre-trained Transformer models into unique\ninteraction models, LLM provides raw value through powerful evaluation\ntechniques and innovative use, which provides a new perspective to evaluate and\nenhanced human-computer interaction.\n  Keywords: Automatic speech recognition, Large Language Model, Smart glasses,\nInteraction mechanism",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research proposes the interaction loop model \"ASR-LLM-Smart Glasses\",\nwhich model combines automatic speech recognition, large language model and\nsmart glasses to facilitate seamless human-computer interaction. And the\nmethodology of this research involves decomposing the interaction process into\ndifferent stages and elements. Speech is captured and processed by ASR, then\nanalyzed and interpreted by LLM. The results are then transmitted to smart\nglasses for display. The feedback loop is complete when the user interacts with\nthe displayed data. Mathematical formulas are used to quantify the performance\nof the model that revolves around core evaluation points: accuracy, coherence,\nand latency during ASR speech-to-text conversion. The research results are\nprovided theoretically to test and evaluate the feasibility and performance of\nthe model. Although such human-computer interaction products have not yet\nappeared in the industry, the performance indicators of this model in enhancing\nuser experience in fields that rely on human-computer interaction have also\nverified its utility as a technology to promote human-computer interaction. In\naddition, this research pioneered the idea of integrating cutting-edge\ntechnologies such as generative pre-trained Transformer models into unique\ninteraction models, LLM provides raw value through powerful evaluation\ntechniques and innovative use, which provides a new perspective to evaluate and\nenhanced human-computer interaction.\n  Keywords: Automatic speech recognition, Large Language Model, Smart glasses,\nInteraction mechanism"
                },
                "authors": [
                    {
                        "name": "Libo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Libo Wang"
                },
                "author": "Libo Wang",
                "arxiv_comment": "OpenReview submitted. 11 pages of text and 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10351v1",
                "updated": "2024-11-15T16:55:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    55,
                    57,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:55:57Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    55,
                    57,
                    4,
                    320,
                    0
                ],
                "title": "Bias Unveiled: Investigating Social Bias in LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Unveiled: Investigating Social Bias in LLM-Generated Code"
                },
                "summary": "Large language models (LLMs) have significantly advanced the field of\nautomated code generation. However, a notable research gap exists in the\nevaluation of social biases that may be present in the code produced by LLMs.\nTo solve this issue, we propose a novel fairness framework, i.e., Solar, to\nassess and mitigate the social biases of LLM-generated code. Specifically,\nSolar can automatically generate test cases for quantitatively uncovering\nsocial biases of the auto-generated code by LLMs. To quantify the severity of\nsocial biases in generated code, we develop a dataset that covers a diverse set\nof social problems. We applied Solar and the crafted dataset to four\nstate-of-the-art LLMs for code generation. Our evaluation reveals severe bias\nin the LLM-generated code from all the subject LLMs. Furthermore, we explore\nseveral strategies for bias mitigation, including Chain-of-Thought (CoT)\nprompting, combining positive role-playing with CoT prompting and iterative\nprompting. Our experiments show that iterative prompting can effectively reduce\nsocial bias in LLM-generated code by up to 90%. Solar is highly extensible to\nevaluate new social problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced the field of\nautomated code generation. However, a notable research gap exists in the\nevaluation of social biases that may be present in the code produced by LLMs.\nTo solve this issue, we propose a novel fairness framework, i.e., Solar, to\nassess and mitigate the social biases of LLM-generated code. Specifically,\nSolar can automatically generate test cases for quantitatively uncovering\nsocial biases of the auto-generated code by LLMs. To quantify the severity of\nsocial biases in generated code, we develop a dataset that covers a diverse set\nof social problems. We applied Solar and the crafted dataset to four\nstate-of-the-art LLMs for code generation. Our evaluation reveals severe bias\nin the LLM-generated code from all the subject LLMs. Furthermore, we explore\nseveral strategies for bias mitigation, including Chain-of-Thought (CoT)\nprompting, combining positive role-playing with CoT prompting and iterative\nprompting. Our experiments show that iterative prompting can effectively reduce\nsocial bias in LLM-generated code by up to 90%. Solar is highly extensible to\nevaluate new social problems."
                },
                "authors": [
                    {
                        "name": "Lin Ling"
                    },
                    {
                        "name": "Fazle Rabbi"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Jinqiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiu Yang"
                },
                "author": "Jinqiu Yang",
                "arxiv_comment": "9pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04412v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04412v3",
                "updated": "2024-11-15T16:53:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    53,
                    18,
                    4,
                    320,
                    0
                ],
                "published": "2024-05-07T15:39:45Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    15,
                    39,
                    45,
                    1,
                    128,
                    0
                ],
                "title": "The Silicon Ceiling: Auditing GPT's Race and Gender Biases in Hiring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Silicon Ceiling: Auditing GPT's Race and Gender Biases in Hiring"
                },
                "summary": "Large language models (LLMs) are increasingly being introduced in workplace\nsettings, with the goals of improving efficiency and fairness. However,\nconcerns have arisen regarding these models' potential to reflect or exacerbate\nsocial biases and stereotypes. This study explores the potential impact of LLMs\non hiring practices. To do so, we conduct an AI audit of race and gender biases\nin one commonly-used LLM, OpenAI's GPT-3.5, taking inspiration from the history\nof traditional offline resume audits. We conduct two studies using names with\nvaried race and gender connotations: resume assessment (Study 1) and resume\ngeneration (Study 2). In Study 1, we ask GPT to score resumes with 32 different\nnames (4 names for each combination of the 2 gender and 4 racial groups) and\ntwo anonymous options across 10 occupations and 3 evaluation tasks (overall\nrating, willingness to interview, and hireability). We find that the model\nreflects some biases based on stereotypes. In Study 2, we prompt GPT to create\nresumes (10 for each name) for fictitious job candidates. When generating\nresumes, GPT reveals underlying biases; women's resumes had occupations with\nless experience, while Asian and Hispanic resumes had immigrant markers, such\nas non-native English and non-U.S. education and work experiences. Our findings\ncontribute to a growing body of literature on LLM biases, particularly in\nworkplace contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being introduced in workplace\nsettings, with the goals of improving efficiency and fairness. However,\nconcerns have arisen regarding these models' potential to reflect or exacerbate\nsocial biases and stereotypes. This study explores the potential impact of LLMs\non hiring practices. To do so, we conduct an AI audit of race and gender biases\nin one commonly-used LLM, OpenAI's GPT-3.5, taking inspiration from the history\nof traditional offline resume audits. We conduct two studies using names with\nvaried race and gender connotations: resume assessment (Study 1) and resume\ngeneration (Study 2). In Study 1, we ask GPT to score resumes with 32 different\nnames (4 names for each combination of the 2 gender and 4 racial groups) and\ntwo anonymous options across 10 occupations and 3 evaluation tasks (overall\nrating, willingness to interview, and hireability). We find that the model\nreflects some biases based on stereotypes. In Study 2, we prompt GPT to create\nresumes (10 for each name) for fictitious job candidates. When generating\nresumes, GPT reveals underlying biases; women's resumes had occupations with\nless experience, while Asian and Hispanic resumes had immigrant markers, such\nas non-native English and non-U.S. education and work experiences. Our findings\ncontribute to a growing body of literature on LLM biases, particularly in\nworkplace contexts."
                },
                "authors": [
                    {
                        "name": "Lena Armstrong"
                    },
                    {
                        "name": "Abbey Liu"
                    },
                    {
                        "name": "Stephen MacNeil"
                    },
                    {
                        "name": "Danaë Metaxa"
                    }
                ],
                "author_detail": {
                    "name": "Danaë Metaxa"
                },
                "author": "Danaë Metaxa",
                "arxiv_doi": "10.1145/3689904.3694699",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689904.3694699",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.04412v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04412v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10332v1",
                "updated": "2024-11-15T16:32:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    32,
                    34,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:32:34Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    32,
                    34,
                    4,
                    320,
                    0
                ],
                "title": "Number it: Temporal Grounding Videos like Flipping Manga",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Number it: Temporal Grounding Videos like Flipping Manga"
                },
                "summary": "Video Large Language Models (Vid-LLMs) have made remarkable advancements in\ncomprehending video content for QA dialogue. However, they struggle to extend\nthis visual understanding to tasks requiring precise temporal localization,\nknown as Video Temporal Grounding (VTG). To address this gap, we introduce\nNumber-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual\ncomprehension with temporal grounding by adding unique numerical identifiers to\neach video frame. Treating a video as a sequence of numbered frame images,\nNumPro transforms VTG into an intuitive process: flipping through manga panels\nin sequence. This allows Vid-LLMs to \"read\" event timelines, accurately linking\nvisual content with corresponding temporal information. Our experiments\ndemonstrate that NumPro significantly boosts VTG performance of top-tier\nVid-LLMs without additional computational cost. Furthermore, fine-tuning on a\nNumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing\nprevious top-performing methods by up to 6.9\\% in mIoU for moment retrieval and\n8.5\\% in mAP for highlight detection. The code will be available at\nhttps://github.com/yongliang-wu/NumPro.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (Vid-LLMs) have made remarkable advancements in\ncomprehending video content for QA dialogue. However, they struggle to extend\nthis visual understanding to tasks requiring precise temporal localization,\nknown as Video Temporal Grounding (VTG). To address this gap, we introduce\nNumber-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual\ncomprehension with temporal grounding by adding unique numerical identifiers to\neach video frame. Treating a video as a sequence of numbered frame images,\nNumPro transforms VTG into an intuitive process: flipping through manga panels\nin sequence. This allows Vid-LLMs to \"read\" event timelines, accurately linking\nvisual content with corresponding temporal information. Our experiments\ndemonstrate that NumPro significantly boosts VTG performance of top-tier\nVid-LLMs without additional computational cost. Furthermore, fine-tuning on a\nNumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing\nprevious top-performing methods by up to 6.9\\% in mIoU for moment retrieval and\n8.5\\% in mAP for highlight detection. The code will be available at\nhttps://github.com/yongliang-wu/NumPro."
                },
                "authors": [
                    {
                        "name": "Yongliang Wu"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Yuyang Sun"
                    },
                    {
                        "name": "Yizhou Zhou"
                    },
                    {
                        "name": "Wenbo Zhu"
                    },
                    {
                        "name": "Fengyun Rao"
                    },
                    {
                        "name": "Bernt Schiele"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05818v2",
                "updated": "2024-11-15T16:23:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    23,
                    17,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-02T12:02:09Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    12,
                    2,
                    9,
                    5,
                    307,
                    0
                ],
                "title": "Open LLMs are Necessary for Current Private Adaptations and Outperform\n  their Closed Alternatives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open LLMs are Necessary for Current Private Adaptations and Outperform\n  their Closed Alternatives"
                },
                "summary": "While open Large Language Models (LLMs) have made significant progress, they\nstill fall short of matching the performance of their closed, proprietary\ncounterparts, making the latter attractive even for the use on highly private\ndata. Recently, various new methods have been proposed to adapt closed LLMs to\nprivate data without leaking private information to third parties and/or the\nLLM provider. In this work, we analyze the privacy protection and performance\nof the four most recent methods for private adaptation of closed LLMs. By\nexamining their threat models and thoroughly comparing their performance under\ndifferent privacy levels according to differential privacy (DP), various LLM\narchitectures, and multiple datasets for classification and generation tasks,\nwe find that: (1) all the methods leak query data, i.e., the (potentially\nsensitive) user data that is queried at inference time, to the LLM provider,\n(2) three out of four methods also leak large fractions of private training\ndata to the LLM provider while the method that protects private data requires a\nlocal open LLM, (3) all the methods exhibit lower performance compared to three\nprivate gradient-based adaptation methods for local open LLMs, and (4) the\nprivate adaptation methods for closed LLMs incur higher monetary training and\nquery costs than running the alternative methods on local open LLMs. This\nyields the conclusion that, to achieve truly privacy-preserving LLM adaptations\nthat yield high performance and more privacy at lower costs, taking into\naccount current methods and models, one should use open LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While open Large Language Models (LLMs) have made significant progress, they\nstill fall short of matching the performance of their closed, proprietary\ncounterparts, making the latter attractive even for the use on highly private\ndata. Recently, various new methods have been proposed to adapt closed LLMs to\nprivate data without leaking private information to third parties and/or the\nLLM provider. In this work, we analyze the privacy protection and performance\nof the four most recent methods for private adaptation of closed LLMs. By\nexamining their threat models and thoroughly comparing their performance under\ndifferent privacy levels according to differential privacy (DP), various LLM\narchitectures, and multiple datasets for classification and generation tasks,\nwe find that: (1) all the methods leak query data, i.e., the (potentially\nsensitive) user data that is queried at inference time, to the LLM provider,\n(2) three out of four methods also leak large fractions of private training\ndata to the LLM provider while the method that protects private data requires a\nlocal open LLM, (3) all the methods exhibit lower performance compared to three\nprivate gradient-based adaptation methods for local open LLMs, and (4) the\nprivate adaptation methods for closed LLMs incur higher monetary training and\nquery costs than running the alternative methods on local open LLMs. This\nyields the conclusion that, to achieve truly privacy-preserving LLM adaptations\nthat yield high performance and more privacy at lower costs, taking into\naccount current methods and models, one should use open LLMs."
                },
                "authors": [
                    {
                        "name": "Vincent Hanke"
                    },
                    {
                        "name": "Tom Blanchard"
                    },
                    {
                        "name": "Franziska Boenisch"
                    },
                    {
                        "name": "Iyiola Emmanuel Olatunji"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Adam Dziedzic"
                    }
                ],
                "author_detail": {
                    "name": "Adam Dziedzic"
                },
                "author": "Adam Dziedzic",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10316v1",
                "updated": "2024-11-15T16:14:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    14,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:14:48Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    14,
                    48,
                    4,
                    320,
                    0
                ],
                "title": "M3TR: Generalist HD Map Construction with Variable Map Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M3TR: Generalist HD Map Construction with Variable Map Priors"
                },
                "summary": "Autonomous vehicles require road information for their operation, usually in\nform of HD maps. Since offline maps eventually become outdated or may only be\npartially available, online HD map construction methods have been proposed to\ninfer map information from live sensor data. A key issue remains how to exploit\nsuch partial or outdated map information as a prior. We introduce M3TR\n(Multi-Masking Map Transformer), a generalist approach for HD map construction\nboth with and without map priors. We address shortcomings in ground truth\ngeneration for Argoverse 2 and nuScenes and propose the first realistic\nscenarios with semantically diverse map priors. Examining various query\ndesigns, we use an improved method for integrating prior map elements into a HD\nmap construction model, increasing performance by +4.3 mAP. Finally, we show\nthat training across all prior scenarios yields a single Generalist model,\nwhose performance is on par with previous Expert models that can handle only\none specific type of map prior. M3TR thus is the first model capable of\nleveraging variable map priors, making it suitable for real-world deployment.\nCode is available at https://github.com/immel-f/m3tr",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous vehicles require road information for their operation, usually in\nform of HD maps. Since offline maps eventually become outdated or may only be\npartially available, online HD map construction methods have been proposed to\ninfer map information from live sensor data. A key issue remains how to exploit\nsuch partial or outdated map information as a prior. We introduce M3TR\n(Multi-Masking Map Transformer), a generalist approach for HD map construction\nboth with and without map priors. We address shortcomings in ground truth\ngeneration for Argoverse 2 and nuScenes and propose the first realistic\nscenarios with semantically diverse map priors. Examining various query\ndesigns, we use an improved method for integrating prior map elements into a HD\nmap construction model, increasing performance by +4.3 mAP. Finally, we show\nthat training across all prior scenarios yields a single Generalist model,\nwhose performance is on par with previous Expert models that can handle only\none specific type of map prior. M3TR thus is the first model capable of\nleveraging variable map priors, making it suitable for real-world deployment.\nCode is available at https://github.com/immel-f/m3tr"
                },
                "authors": [
                    {
                        "name": "Fabian Immel"
                    },
                    {
                        "name": "Richard Fehler"
                    },
                    {
                        "name": "Frank Bieder"
                    },
                    {
                        "name": "Jan-Hendrik Pauls"
                    },
                    {
                        "name": "Christoph Stiller"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Stiller"
                },
                "author": "Christoph Stiller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10294v1",
                "updated": "2024-11-15T15:52:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    52,
                    15,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T15:52:15Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    52,
                    15,
                    4,
                    320,
                    0
                ],
                "title": "Static network structure cannot stabilize cooperation among Large\n  Language Model agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static network structure cannot stabilize cooperation among Large\n  Language Model agents"
                },
                "summary": "Large language models (LLMs) are increasingly used to model human social\nbehavior, with recent research exploring their ability to simulate social\ndynamics. Here, we test whether LLMs mirror human behavior in social dilemmas,\nwhere individual and collective interests conflict. Humans generally cooperate\nmore than expected in laboratory settings, showing less cooperation in\nwell-mixed populations but more in fixed networks. In contrast, LLMs tend to\nexhibit greater cooperation in well-mixed settings. This raises a key question:\nAre LLMs about to emulate human behavior in cooperative dilemmas on networks?\nIn this study, we examine networked interactions where agents repeatedly engage\nin the Prisoner's Dilemma within both well-mixed and structured network\nconfigurations, aiming to identify parallels in cooperative behavior between\nLLMs and humans. Our findings indicate critical distinctions: while humans tend\nto cooperate more within structured networks, LLMs display increased\ncooperation mainly in well-mixed environments, with limited adjustment to\nnetworked contexts. Notably, LLM cooperation also varies across model types,\nillustrating the complexities of replicating human-like social adaptability in\nartificial agents. These results highlight a crucial gap: LLMs struggle to\nemulate the nuanced, adaptive social strategies humans deploy in fixed\nnetworks. Unlike human participants, LLMs do not alter their cooperative\nbehavior in response to network structures or evolving social contexts, missing\nthe reciprocity norms that humans adaptively employ. This limitation points to\na fundamental need in future LLM design -- to integrate a deeper comprehension\nof social norms, enabling more authentic modeling of human-like cooperation and\nadaptability in networked environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used to model human social\nbehavior, with recent research exploring their ability to simulate social\ndynamics. Here, we test whether LLMs mirror human behavior in social dilemmas,\nwhere individual and collective interests conflict. Humans generally cooperate\nmore than expected in laboratory settings, showing less cooperation in\nwell-mixed populations but more in fixed networks. In contrast, LLMs tend to\nexhibit greater cooperation in well-mixed settings. This raises a key question:\nAre LLMs about to emulate human behavior in cooperative dilemmas on networks?\nIn this study, we examine networked interactions where agents repeatedly engage\nin the Prisoner's Dilemma within both well-mixed and structured network\nconfigurations, aiming to identify parallels in cooperative behavior between\nLLMs and humans. Our findings indicate critical distinctions: while humans tend\nto cooperate more within structured networks, LLMs display increased\ncooperation mainly in well-mixed environments, with limited adjustment to\nnetworked contexts. Notably, LLM cooperation also varies across model types,\nillustrating the complexities of replicating human-like social adaptability in\nartificial agents. These results highlight a crucial gap: LLMs struggle to\nemulate the nuanced, adaptive social strategies humans deploy in fixed\nnetworks. Unlike human participants, LLMs do not alter their cooperative\nbehavior in response to network structures or evolving social contexts, missing\nthe reciprocity norms that humans adaptively employ. This limitation points to\na fundamental need in future LLM design -- to integrate a deeper comprehension\nof social norms, enabling more authentic modeling of human-like cooperation and\nadaptability in networked environments."
                },
                "authors": [
                    {
                        "name": "Jin Han"
                    },
                    {
                        "name": "Balaraju Battu"
                    },
                    {
                        "name": "Ivan Romić"
                    },
                    {
                        "name": "Talal Rahwan"
                    },
                    {
                        "name": "Petter Holme"
                    }
                ],
                "author_detail": {
                    "name": "Petter Holme"
                },
                "author": "Petter Holme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10285v1",
                "updated": "2024-11-15T15:40:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    40,
                    49,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T15:40:49Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    40,
                    49,
                    4,
                    320,
                    0
                ],
                "title": "Systolic Arrays and Structured Pruning Co-design for Efficient\n  Transformers in Edge Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systolic Arrays and Structured Pruning Co-design for Efficient\n  Transformers in Edge Systems"
                },
                "summary": "Efficient deployment of resource-intensive transformers on edge devices\nnecessitates cross-stack optimization. We thus study the interrelation between\nstructured pruning and systolic acceleration, matching the size of pruned\nblocks with the systolic array dimensions. In this setting, computations of\npruned weight blocks can be skipped, reducing run-time and energy consumption,\nbut potentially impacting quality of service (QoS). To evaluate the trade-offs\nbetween systolic array size and sparsity opportunities, we present a novel\nco-design framework that integrates algorithmic optimization, system\nsimulation, and hardware design. Targeting speech recognition using\ntransformers as a case study, we analyze how configuration choices across the\nstack affect performance metrics. Results demonstrate that structured pruning\non systems featuring systolic array acceleration can effectively increase\nperformance, while maintaining high QoS levels. Up to 26% system-wide speedups\ndue to structured pruning were measured, with only 1.4% word error rate\ndegradation on the standard Librispeech dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient deployment of resource-intensive transformers on edge devices\nnecessitates cross-stack optimization. We thus study the interrelation between\nstructured pruning and systolic acceleration, matching the size of pruned\nblocks with the systolic array dimensions. In this setting, computations of\npruned weight blocks can be skipped, reducing run-time and energy consumption,\nbut potentially impacting quality of service (QoS). To evaluate the trade-offs\nbetween systolic array size and sparsity opportunities, we present a novel\nco-design framework that integrates algorithmic optimization, system\nsimulation, and hardware design. Targeting speech recognition using\ntransformers as a case study, we analyze how configuration choices across the\nstack affect performance metrics. Results demonstrate that structured pruning\non systems featuring systolic array acceleration can effectively increase\nperformance, while maintaining high QoS levels. Up to 26% system-wide speedups\ndue to structured pruning were measured, with only 1.4% word error rate\ndegradation on the standard Librispeech dataset."
                },
                "authors": [
                    {
                        "name": "Pedro Palacios"
                    },
                    {
                        "name": "Rafael Medina"
                    },
                    {
                        "name": "Jean-Luc Rouas"
                    },
                    {
                        "name": "Giovanni Ansaloni"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; B.5.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10272v1",
                "updated": "2024-11-15T15:28:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    28,
                    42,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T15:28:42Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    28,
                    42,
                    4,
                    320,
                    0
                ],
                "title": "Scaling Law for Post-training after Model Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Law for Post-training after Model Pruning"
                },
                "summary": "Large language models (LLMs) based on the Transformer architecture are widely\nemployed across various domains and tasks. However, their increasing size\nimposes significant hardware demands, limiting practical deployment. To\nmitigate this, model pruning techniques have been developed to create more\nefficient models while maintaining high performance. Despite this,\npost-training after pruning is crucial for performance recovery and can be\nresource-intensive. This paper investigates the post-training requirements of\npruned LLMs and introduces a scaling law to determine the optimal amount of\npost-training data. Post-training experiments with the Llama-3 and Qwen-2.5\nseries models, pruned using depth pruning, width pruning, and 2:4\nsemi-structured pruning, show that higher pruning ratios necessitate more\npost-training data for performance recovery, whereas larger LLMs require less.\nThe proposed scaling law predicts a model's loss based on its parameter counts\nbefore and after pruning, as well as the post-training token counts.\nFurthermore, we find that the scaling law established from smaller LLMs can be\nreliably extrapolated to larger LLMs. This work provides valuable insights into\nthe post-training of pruned LLMs and offers a practical scaling law for\noptimizing post-training data usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on the Transformer architecture are widely\nemployed across various domains and tasks. However, their increasing size\nimposes significant hardware demands, limiting practical deployment. To\nmitigate this, model pruning techniques have been developed to create more\nefficient models while maintaining high performance. Despite this,\npost-training after pruning is crucial for performance recovery and can be\nresource-intensive. This paper investigates the post-training requirements of\npruned LLMs and introduces a scaling law to determine the optimal amount of\npost-training data. Post-training experiments with the Llama-3 and Qwen-2.5\nseries models, pruned using depth pruning, width pruning, and 2:4\nsemi-structured pruning, show that higher pruning ratios necessitate more\npost-training data for performance recovery, whereas larger LLMs require less.\nThe proposed scaling law predicts a model's loss based on its parameter counts\nbefore and after pruning, as well as the post-training token counts.\nFurthermore, we find that the scaling law established from smaller LLMs can be\nreliably extrapolated to larger LLMs. This work provides valuable insights into\nthe post-training of pruned LLMs and offers a practical scaling law for\noptimizing post-training data usage."
                },
                "authors": [
                    {
                        "name": "Xiaodong Chen"
                    },
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Cuiping Li"
                    },
                    {
                        "name": "Hong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hong Chen"
                },
                "author": "Hong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17710v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17710v3",
                "updated": "2024-11-15T14:57:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    14,
                    57,
                    28,
                    4,
                    320,
                    0
                ],
                "published": "2024-03-26T13:58:00Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    13,
                    58,
                    0,
                    1,
                    86,
                    0
                ],
                "title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge"
                },
                "summary": "LLM-as-a-Judge uses a large language model (LLM) to select the best response\nfrom a set of candidates for a given question. LLM-as-a-Judge has many\napplications such as LLM-powered search, reinforcement learning with AI\nfeedback (RLAIF), and tool selection. In this work, we propose JudgeDeceiver,\nan optimization-based prompt injection attack to LLM-as-a-Judge. JudgeDeceiver\ninjects a carefully crafted sequence into an attacker-controlled candidate\nresponse such that LLM-as-a-Judge selects the candidate response for an\nattacker-chosen question no matter what other candidate responses are.\nSpecifically, we formulate finding such sequence as an optimization problem and\npropose a gradient based method to approximately solve it. Our extensive\nevaluation shows that JudgeDeceive is highly effective, and is much more\neffective than existing prompt injection attacks that manually craft the\ninjected sequences and jailbreak attacks when extended to our problem. We also\nshow the effectiveness of JudgeDeceiver in three case studies, i.e.,\nLLM-powered search, RLAIF, and tool selection. Moreover, we consider defenses\nincluding known-answer detection, perplexity detection, and perplexity windowed\ndetection. Our results show these defenses are insufficient, highlighting the\nurgent need for developing new defense strategies. Our implementation is\navailable at this repository: https://github.com/ShiJiawenwen/JudgeDeceiver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge uses a large language model (LLM) to select the best response\nfrom a set of candidates for a given question. LLM-as-a-Judge has many\napplications such as LLM-powered search, reinforcement learning with AI\nfeedback (RLAIF), and tool selection. In this work, we propose JudgeDeceiver,\nan optimization-based prompt injection attack to LLM-as-a-Judge. JudgeDeceiver\ninjects a carefully crafted sequence into an attacker-controlled candidate\nresponse such that LLM-as-a-Judge selects the candidate response for an\nattacker-chosen question no matter what other candidate responses are.\nSpecifically, we formulate finding such sequence as an optimization problem and\npropose a gradient based method to approximately solve it. Our extensive\nevaluation shows that JudgeDeceive is highly effective, and is much more\neffective than existing prompt injection attacks that manually craft the\ninjected sequences and jailbreak attacks when extended to our problem. We also\nshow the effectiveness of JudgeDeceiver in three case studies, i.e.,\nLLM-powered search, RLAIF, and tool selection. Moreover, we consider defenses\nincluding known-answer detection, perplexity detection, and perplexity windowed\ndetection. Our results show these defenses are insufficient, highlighting the\nurgent need for developing new defense strategies. Our implementation is\navailable at this repository: https://github.com/ShiJiawenwen/JudgeDeceiver."
                },
                "authors": [
                    {
                        "name": "Jiawen Shi"
                    },
                    {
                        "name": "Zenghui Yuan"
                    },
                    {
                        "name": "Yinuo Liu"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Neil Zhenqiang Gong"
                },
                "author": "Neil Zhenqiang Gong",
                "arxiv_comment": "To appear in the Proceedings of The ACM Conference on Computer and\n  Communications Security (CCS), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17710v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17710v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10213v1",
                "updated": "2024-11-15T14:19:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    14,
                    19,
                    15,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T14:19:15Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    14,
                    19,
                    15,
                    4,
                    320,
                    0
                ],
                "title": "An Empirical Study on LLM-based Agents for Automated Bug Fixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on LLM-based Agents for Automated Bug Fixing"
                },
                "summary": "Large language models (LLMs) and LLM-based Agents have been applied to fix\nbugs automatically, demonstrating the capability in addressing software defects\nby engaging in development environment interaction, iterative validation and\ncode modification. However, systematic analysis of these agent and non-agent\nsystems remain limited, particularly regarding performance variations among\ntop-performing ones. In this paper, we examine seven proprietary and\nopen-source systems on the SWE-bench Lite benchmark for automated bug fixing.\nWe first assess each system's overall performance, noting instances solvable by\nall or none of these sytems, and explore why some instances are uniquely solved\nby specific system types. We also compare fault localization accuracy at file\nand line levels and evaluate bug reproduction capabilities, identifying\ninstances solvable only through dynamic reproduction. Through analysis, we\nconcluded that further optimization is needed in both the LLM itself and the\ndesign of Agentic flow to improve the effectiveness of the Agent in bug fixing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and LLM-based Agents have been applied to fix\nbugs automatically, demonstrating the capability in addressing software defects\nby engaging in development environment interaction, iterative validation and\ncode modification. However, systematic analysis of these agent and non-agent\nsystems remain limited, particularly regarding performance variations among\ntop-performing ones. In this paper, we examine seven proprietary and\nopen-source systems on the SWE-bench Lite benchmark for automated bug fixing.\nWe first assess each system's overall performance, noting instances solvable by\nall or none of these sytems, and explore why some instances are uniquely solved\nby specific system types. We also compare fault localization accuracy at file\nand line levels and evaluate bug reproduction capabilities, identifying\ninstances solvable only through dynamic reproduction. Through analysis, we\nconcluded that further optimization is needed in both the LLM itself and the\ndesign of Agentic flow to improve the effectiveness of the Agent in bug fixing."
                },
                "authors": [
                    {
                        "name": "Xiangxin Meng"
                    },
                    {
                        "name": "Zexiong Ma"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Chao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Chao Peng"
                },
                "author": "Chao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10184v1",
                "updated": "2024-11-15T13:33:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    13,
                    33,
                    10,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T13:33:10Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    13,
                    33,
                    10,
                    4,
                    320,
                    0
                ],
                "title": "Agentic LLMs in the Supply Chain: Towards Autonomous Multi-Agent\n  Consensus-Seeking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic LLMs in the Supply Chain: Towards Autonomous Multi-Agent\n  Consensus-Seeking"
                },
                "summary": "This paper explores how Large Language Models (LLMs) can automate\nconsensus-seeking in supply chain management (SCM), where frequent decisions on\nproblems such as inventory levels and delivery times require coordination among\ncompanies. Traditional SCM relies on human consensus in decision-making to\navoid emergent problems like the bullwhip effect. Some routine consensus\nprocesses, especially those that are time-intensive and costly, can be\nautomated. Existing solutions for automated coordination have faced challenges\ndue to high entry barriers locking out SMEs, limited capabilities, and limited\nadaptability in complex scenarios. However, recent advances in Generative AI,\nparticularly LLMs, show promise in overcoming these barriers. LLMs, trained on\nvast datasets can negotiate, reason, and plan, facilitating near-human-level\nconsensus at scale with minimal entry barriers. In this work, we identify key\nlimitations in existing approaches and propose autonomous LLM agents to address\nthese gaps. We introduce a series of novel, supply chain-specific\nconsensus-seeking frameworks tailored for LLM agents and validate the\neffectiveness of our approach through a case study in inventory management. To\naccelerate progress within the SCM community, we open-source our code,\nproviding a foundation for further advancements in LLM-powered autonomous\nsupply chain solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how Large Language Models (LLMs) can automate\nconsensus-seeking in supply chain management (SCM), where frequent decisions on\nproblems such as inventory levels and delivery times require coordination among\ncompanies. Traditional SCM relies on human consensus in decision-making to\navoid emergent problems like the bullwhip effect. Some routine consensus\nprocesses, especially those that are time-intensive and costly, can be\nautomated. Existing solutions for automated coordination have faced challenges\ndue to high entry barriers locking out SMEs, limited capabilities, and limited\nadaptability in complex scenarios. However, recent advances in Generative AI,\nparticularly LLMs, show promise in overcoming these barriers. LLMs, trained on\nvast datasets can negotiate, reason, and plan, facilitating near-human-level\nconsensus at scale with minimal entry barriers. In this work, we identify key\nlimitations in existing approaches and propose autonomous LLM agents to address\nthese gaps. We introduce a series of novel, supply chain-specific\nconsensus-seeking frameworks tailored for LLM agents and validate the\neffectiveness of our approach through a case study in inventory management. To\naccelerate progress within the SCM community, we open-source our code,\nproviding a foundation for further advancements in LLM-powered autonomous\nsupply chain solutions."
                },
                "authors": [
                    {
                        "name": "Valeria Jannelli"
                    },
                    {
                        "name": "Stefan Schoepf"
                    },
                    {
                        "name": "Matthias Bickel"
                    },
                    {
                        "name": "Torbjørn Netland"
                    },
                    {
                        "name": "Alexandra Brintrup"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Brintrup"
                },
                "author": "Alexandra Brintrup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06597v2",
                "updated": "2024-11-15T13:29:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    13,
                    29,
                    54,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-10T21:07:21Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    21,
                    7,
                    21,
                    6,
                    315,
                    0
                ],
                "title": "On-demand 5G Private Networks using a Mobile Cell",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-demand 5G Private Networks using a Mobile Cell"
                },
                "summary": "This paper proposes the Mobile Cell (MC) concept for on-demand 5G private\nnetworks. The MC is designed to extend, restore, and reinforce 5G wireless\ncoverage and network capacity on-demand, especially in areas with temporary\ncommunications needs or where it is costly or not possible to deploy a\npermanent fixed infrastructure. The design of the MC as well as the\ndevelopment, integration, and deployment in 5G private networks are discussed.\n  The Mobile Cell concept can be applied in multiple real-world environments,\nincluding seaports and application scenarios. Similarly to critical hubs in the\nglobal supply chain, seaports require reliable, high-performance wireless\ncommunications to increase efficiency and manage dynamic operations in\nreal-time. Current communications solutions in seaports typically rely on Wi-Fi\nand wired-based technologies. Wired-based technologies lack the necessary\nflexibility for dynamic environments. Wi-Fi is susceptible to interference from\nother systems operating in the same frequency bands. An MC operating in a\nlicensed, interference-free spectrum is a promising solution to overcome these\nlimitations and provide improved Quality of Service when using the 5G\ntechnology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes the Mobile Cell (MC) concept for on-demand 5G private\nnetworks. The MC is designed to extend, restore, and reinforce 5G wireless\ncoverage and network capacity on-demand, especially in areas with temporary\ncommunications needs or where it is costly or not possible to deploy a\npermanent fixed infrastructure. The design of the MC as well as the\ndevelopment, integration, and deployment in 5G private networks are discussed.\n  The Mobile Cell concept can be applied in multiple real-world environments,\nincluding seaports and application scenarios. Similarly to critical hubs in the\nglobal supply chain, seaports require reliable, high-performance wireless\ncommunications to increase efficiency and manage dynamic operations in\nreal-time. Current communications solutions in seaports typically rely on Wi-Fi\nand wired-based technologies. Wired-based technologies lack the necessary\nflexibility for dynamic environments. Wi-Fi is susceptible to interference from\nother systems operating in the same frequency bands. An MC operating in a\nlicensed, interference-free spectrum is a promising solution to overcome these\nlimitations and provide improved Quality of Service when using the 5G\ntechnology."
                },
                "authors": [
                    {
                        "name": "André Coelho"
                    },
                    {
                        "name": "José Ruela"
                    },
                    {
                        "name": "Gonçalo Queirós"
                    },
                    {
                        "name": "Ricardo Trancoso"
                    },
                    {
                        "name": "Paulo Furtado Correia"
                    },
                    {
                        "name": "Filipe Ribeiro"
                    },
                    {
                        "name": "Helder Fontes"
                    },
                    {
                        "name": "Rui Campos"
                    },
                    {
                        "name": "Manuel Ricardo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Ricardo"
                },
                "author": "Manuel Ricardo",
                "arxiv_comment": "Manuscript accepted at the NEXUS International Conference - DGTMP\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10178v1",
                "updated": "2024-11-15T13:28:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    13,
                    28,
                    17,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T13:28:17Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    13,
                    28,
                    17,
                    4,
                    320,
                    0
                ],
                "title": "Channel-Adaptive Wireless Image Semantic Transmission with Learnable\n  Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Channel-Adaptive Wireless Image Semantic Transmission with Learnable\n  Prompts"
                },
                "summary": "Recent developments in Deep learning based Joint Source-Channel Coding\n(DeepJSCC) have demonstrated impressive capabilities within wireless semantic\ncommunications system. However, existing DeepJSCC methodologies exhibit limited\ngeneralization ability across varying channel conditions, necessitating the\npreparation of multiple models. Optimal performance is only attained when the\nchannel status during testing aligns precisely with the training channel\nstatus, which is very inconvenient for real-life applications. In this paper,\nwe introduce a novel DeepJSCC framework, termed Prompt JSCC (PJSCC), which\nincorporates a learnable prompt to implicitly integrate the physical channel\nstate into the transmission system. Specifically, the Channel State Prompt\n(CSP) module is devised to generate prompts based on diverse SNR and channel\ndistribution models. Through the interaction of latent image features with\nchannel features derived from the CSP module, the DeepJSCC process dynamically\nadapts to varying channel conditions without necessitating retraining.\nComparative analyses against leading DeepJSCC methodologies and traditional\nseparate coding approaches reveal that the proposed PJSCC achieves optimal\nimage reconstruction performance across different SNR settings and various\nchannel models, as assessed by Peak Signal-to-Noise Ratio (PSNR) and\nLearning-based Perceptual Image Patch Similarity (LPIPS) metrics. Furthermore,\nin real-world scenarios, PJSCC shows excellent memory efficiency and\nscalability, rendering it readily deployable on resource-constrained platforms\nto facilitate semantic communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in Deep learning based Joint Source-Channel Coding\n(DeepJSCC) have demonstrated impressive capabilities within wireless semantic\ncommunications system. However, existing DeepJSCC methodologies exhibit limited\ngeneralization ability across varying channel conditions, necessitating the\npreparation of multiple models. Optimal performance is only attained when the\nchannel status during testing aligns precisely with the training channel\nstatus, which is very inconvenient for real-life applications. In this paper,\nwe introduce a novel DeepJSCC framework, termed Prompt JSCC (PJSCC), which\nincorporates a learnable prompt to implicitly integrate the physical channel\nstate into the transmission system. Specifically, the Channel State Prompt\n(CSP) module is devised to generate prompts based on diverse SNR and channel\ndistribution models. Through the interaction of latent image features with\nchannel features derived from the CSP module, the DeepJSCC process dynamically\nadapts to varying channel conditions without necessitating retraining.\nComparative analyses against leading DeepJSCC methodologies and traditional\nseparate coding approaches reveal that the proposed PJSCC achieves optimal\nimage reconstruction performance across different SNR settings and various\nchannel models, as assessed by Peak Signal-to-Noise Ratio (PSNR) and\nLearning-based Perceptual Image Patch Similarity (LPIPS) metrics. Furthermore,\nin real-world scenarios, PJSCC shows excellent memory efficiency and\nscalability, rendering it readily deployable on resource-constrained platforms\nto facilitate semantic communications."
                },
                "authors": [
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Danlan Huang"
                    },
                    {
                        "name": "Xinyi Zhou"
                    },
                    {
                        "name": "Feng Ding"
                    },
                    {
                        "name": "Sheng Wu"
                    },
                    {
                        "name": "Zhiqing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqing Wei"
                },
                "author": "Zhiqing Wei",
                "arxiv_journal_ref": "GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10168v1",
                "updated": "2024-11-15T13:16:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    13,
                    16,
                    11,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T13:16:11Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    13,
                    16,
                    11,
                    4,
                    320,
                    0
                ],
                "title": "Evaluating the role of `Constitutions' for learning from AI feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the role of `Constitutions' for learning from AI feedback"
                },
                "summary": "The growing capabilities of large language models (LLMs) have led to their\nuse as substitutes for human feedback for training and assessing other LLMs.\nThese methods often rely on `constitutions', written guidelines which a critic\nmodel uses to provide feedback and improve generations. We investigate how the\nchoice of constitution affects feedback quality by using four different\nconstitutions to improve patient-centered communication in medical interviews.\nIn pairwise comparisons conducted by 215 human raters, we found that detailed\nconstitutions led to better results regarding emotive qualities. However, none\nof the constitutions outperformed the baseline in learning more\npractically-oriented skills related to information gathering and provision. Our\nfindings indicate that while detailed constitutions should be prioritised,\nthere are possible limitations to the effectiveness of AI feedback as a reward\nsignal in certain areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing capabilities of large language models (LLMs) have led to their\nuse as substitutes for human feedback for training and assessing other LLMs.\nThese methods often rely on `constitutions', written guidelines which a critic\nmodel uses to provide feedback and improve generations. We investigate how the\nchoice of constitution affects feedback quality by using four different\nconstitutions to improve patient-centered communication in medical interviews.\nIn pairwise comparisons conducted by 215 human raters, we found that detailed\nconstitutions led to better results regarding emotive qualities. However, none\nof the constitutions outperformed the baseline in learning more\npractically-oriented skills related to information gathering and provision. Our\nfindings indicate that while detailed constitutions should be prioritised,\nthere are possible limitations to the effectiveness of AI feedback as a reward\nsignal in certain areas."
                },
                "authors": [
                    {
                        "name": "Saskia Redgate"
                    },
                    {
                        "name": "Andrew M. Bean"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "arxiv_comment": "4 pages, 2 figures. In NeurIPS 2024 Workshop on Language Gamification",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10163v1",
                "updated": "2024-11-15T13:12:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    13,
                    12,
                    29,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T13:12:29Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    13,
                    12,
                    29,
                    4,
                    320,
                    0
                ],
                "title": "Compound-QA: A Benchmark for Evaluating LLMs on Compound Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compound-QA: A Benchmark for Evaluating LLMs on Compound Questions"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance across\nvarious tasks, prompting researchers to develop diverse evaluation benchmarks.\nHowever, existing benchmarks typically measure the ability of LLMs to respond\nto individual questions, neglecting the complex interactions in real-world\napplications. In this paper, we introduce Compound Question Synthesis (CQ-Syn)\nto create the Compound-QA benchmark, focusing on compound questions with\nmultiple sub-questions. This benchmark is derived from existing QA datasets,\nannotated with proprietary LLMs and verified by humans for accuracy. It\nencompasses five categories: Factual-Statement, Cause-and-Effect,\nHypothetical-Analysis, Comparison-and-Selection, and Evaluation-and-Suggestion.\nIt evaluates the LLM capability in terms of three dimensions including\nunderstanding, reasoning, and knowledge. Our assessment of eight open-source\nLLMs using Compound-QA reveals distinct patterns in their responses to compound\nquestions, which are significantly poorer than those to non-compound questions.\nAdditionally, we investigate various methods to enhance LLMs performance on\ncompound questions. The results indicate that these approaches significantly\nimprove the models' comprehension and reasoning abilities on compound\nquestions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance across\nvarious tasks, prompting researchers to develop diverse evaluation benchmarks.\nHowever, existing benchmarks typically measure the ability of LLMs to respond\nto individual questions, neglecting the complex interactions in real-world\napplications. In this paper, we introduce Compound Question Synthesis (CQ-Syn)\nto create the Compound-QA benchmark, focusing on compound questions with\nmultiple sub-questions. This benchmark is derived from existing QA datasets,\nannotated with proprietary LLMs and verified by humans for accuracy. It\nencompasses five categories: Factual-Statement, Cause-and-Effect,\nHypothetical-Analysis, Comparison-and-Selection, and Evaluation-and-Suggestion.\nIt evaluates the LLM capability in terms of three dimensions including\nunderstanding, reasoning, and knowledge. Our assessment of eight open-source\nLLMs using Compound-QA reveals distinct patterns in their responses to compound\nquestions, which are significantly poorer than those to non-compound questions.\nAdditionally, we investigate various methods to enhance LLMs performance on\ncompound questions. The results indicate that these approaches significantly\nimprove the models' comprehension and reasoning abilities on compound\nquestions."
                },
                "authors": [
                    {
                        "name": "Yutao Hou"
                    },
                    {
                        "name": "Yajing Luo"
                    },
                    {
                        "name": "Zhiwen Ruan"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Weifeng Ge"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Guanhua Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guanhua Chen"
                },
                "author": "Guanhua Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14979v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14979v5",
                "updated": "2024-11-15T12:46:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    46,
                    30,
                    4,
                    320,
                    0
                ],
                "published": "2024-10-19T05:01:56Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    5,
                    1,
                    56,
                    5,
                    293,
                    0
                ],
                "title": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration From Cognitive Psychology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration From Cognitive Psychology"
                },
                "summary": "The cognitive mechanism by which Large Language Models (LLMs) solve\nmathematical problems remains a widely debated and unresolved issue. Currently,\nthere is little interpretable experimental evidence that connects LLMs'\nproblem-solving with human cognitive psychology.To determine if LLMs possess\nhuman-like mathematical reasoning, we modified the problems used in the human\nCognitive Reflection Test (CRT). Our results show that, even with the use of\nChains of Thought (CoT) prompts, mainstream LLMs, including the latest o1 model\n(noted for its reasoning capabilities), have a high error rate when solving\nthese modified CRT problems. Specifically, the average accuracy rate dropped by\nup to 50% compared to the original questions.Further analysis of LLMs'\nincorrect answers suggests that they primarily rely on pattern matching from\ntheir training data, which aligns more with human intuition (System 1 thinking)\nrather than with human-like reasoning (System 2 thinking). This finding\nchallenges the belief that LLMs have genuine mathematical reasoning abilities\ncomparable to humans. As a result, this work may adjust overly optimistic views\non LLMs' progress towards artificial general intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cognitive mechanism by which Large Language Models (LLMs) solve\nmathematical problems remains a widely debated and unresolved issue. Currently,\nthere is little interpretable experimental evidence that connects LLMs'\nproblem-solving with human cognitive psychology.To determine if LLMs possess\nhuman-like mathematical reasoning, we modified the problems used in the human\nCognitive Reflection Test (CRT). Our results show that, even with the use of\nChains of Thought (CoT) prompts, mainstream LLMs, including the latest o1 model\n(noted for its reasoning capabilities), have a high error rate when solving\nthese modified CRT problems. Specifically, the average accuracy rate dropped by\nup to 50% compared to the original questions.Further analysis of LLMs'\nincorrect answers suggests that they primarily rely on pattern matching from\ntheir training data, which aligns more with human intuition (System 1 thinking)\nrather than with human-like reasoning (System 2 thinking). This finding\nchallenges the belief that LLMs have genuine mathematical reasoning abilities\ncomparable to humans. As a result, this work may adjust overly optimistic views\non LLMs' progress towards artificial general intelligence."
                },
                "authors": [
                    {
                        "name": "Wei Xie"
                    },
                    {
                        "name": "Shuoyoucheng Ma"
                    },
                    {
                        "name": "Zhenhua Wang"
                    },
                    {
                        "name": "Enze Wang"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Xiaobing Sun"
                    },
                    {
                        "name": "Baosheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Baosheng Wang"
                },
                "author": "Baosheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14979v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14979v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10145v1",
                "updated": "2024-11-15T12:39:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    39,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T12:39:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    39,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "An Effective Framework to Help Large Language Models Handle\n  Numeric-involved Long-context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Effective Framework to Help Large Language Models Handle\n  Numeric-involved Long-context Tasks"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nhandling long texts and have almost perfect performance in traditional\nretrieval tasks. However, their performance significantly degrades when it\ncomes to numerical calculations in the long-context. Numeric-involved\nlong-context tasks typically cannot be addressed by current LLMs in normal\nsettings due to their inherent limitations in simultaneously handling complex\nand massive information. Some CoT like prompting methods can improve accuracy\nbut demands massive output tokens, which is costly and slow. To address this\nissue, we propose a workflow, which decompose a numeric-involved long-context\ntask into 4 low-level subtasks: judging, extracting and processing with code\nand conclusion. The former 2 subtasks is relatively simple, which allows us to\nuse smaller models for efficiently processing long context. When numerical\ncalculations are required, we use code generated by LLMs to avoid the\ndisadvantage of LLM not being good at calculations. The results in 2\nnumeric-involved long-context benchmarks demonstrate our workflow can not only\nimprove accuracy, but also significantly reduce the cost of API calls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nhandling long texts and have almost perfect performance in traditional\nretrieval tasks. However, their performance significantly degrades when it\ncomes to numerical calculations in the long-context. Numeric-involved\nlong-context tasks typically cannot be addressed by current LLMs in normal\nsettings due to their inherent limitations in simultaneously handling complex\nand massive information. Some CoT like prompting methods can improve accuracy\nbut demands massive output tokens, which is costly and slow. To address this\nissue, we propose a workflow, which decompose a numeric-involved long-context\ntask into 4 low-level subtasks: judging, extracting and processing with code\nand conclusion. The former 2 subtasks is relatively simple, which allows us to\nuse smaller models for efficiently processing long context. When numerical\ncalculations are required, we use code generated by LLMs to avoid the\ndisadvantage of LLM not being good at calculations. The results in 2\nnumeric-involved long-context benchmarks demonstrate our workflow can not only\nimprove accuracy, but also significantly reduce the cost of API calls."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yijiong Yu"
                },
                "author": "Yijiong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10137v1",
                "updated": "2024-11-15T12:23:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    23,
                    12,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T12:23:12Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    23,
                    12,
                    4,
                    320,
                    0
                ],
                "title": "Legal Evalutions and Challenges of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal Evalutions and Challenges of Large Language Models"
                },
                "summary": "In this paper, we review legal testing methods based on Large Language Models\n(LLMs), using the OPENAI o1 model as a case study to evaluate the performance\nof large models in applying legal provisions. We compare current\nstate-of-the-art LLMs, including open-source, closed-source, and legal-specific\nmodels trained specifically for the legal domain. Systematic tests are\nconducted on English and Chinese legal cases, and the results are analyzed in\ndepth. Through systematic testing of legal cases from common law systems and\nChina, this paper explores the strengths and weaknesses of LLMs in\nunderstanding and applying legal texts, reasoning through legal issues, and\npredicting judgments. The experimental results highlight both the potential and\nlimitations of LLMs in legal applications, particularly in terms of challenges\nrelated to the interpretation of legal language and the accuracy of legal\nreasoning. Finally, the paper provides a comprehensive analysis of the\nadvantages and disadvantages of various types of models, offering valuable\ninsights and references for the future application of AI in the legal field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we review legal testing methods based on Large Language Models\n(LLMs), using the OPENAI o1 model as a case study to evaluate the performance\nof large models in applying legal provisions. We compare current\nstate-of-the-art LLMs, including open-source, closed-source, and legal-specific\nmodels trained specifically for the legal domain. Systematic tests are\nconducted on English and Chinese legal cases, and the results are analyzed in\ndepth. Through systematic testing of legal cases from common law systems and\nChina, this paper explores the strengths and weaknesses of LLMs in\nunderstanding and applying legal texts, reasoning through legal issues, and\npredicting judgments. The experimental results highlight both the potential and\nlimitations of LLMs in legal applications, particularly in terms of challenges\nrelated to the interpretation of legal language and the accuracy of legal\nreasoning. Finally, the paper provides a comprehensive analysis of the\nadvantages and disadvantages of various types of models, offering valuable\ninsights and references for the future application of AI in the legal field."
                },
                "authors": [
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Huan Zhao"
                    },
                    {
                        "name": "Zhenyuan Yang"
                    },
                    {
                        "name": "Peng Shu"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Haobo Sun"
                    },
                    {
                        "name": "Ruixi Liang"
                    },
                    {
                        "name": "Shixin Li"
                    },
                    {
                        "name": "Pengcheng Shi"
                    },
                    {
                        "name": "Longjun Ma"
                    },
                    {
                        "name": "Zongjia Liu"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Tianyang Zhong"
                    },
                    {
                        "name": "Yutong Zhang"
                    },
                    {
                        "name": "Chong Ma"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Tuo Zhang"
                    },
                    {
                        "name": "Tianli Ding"
                    },
                    {
                        "name": "Yudan Ren"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Xi Jiang"
                    },
                    {
                        "name": "Shu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shu Zhang"
                },
                "author": "Shu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15527v2",
                "updated": "2024-11-15T12:05:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    5,
                    27,
                    4,
                    320,
                    0
                ],
                "published": "2024-07-22T10:32:48Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    10,
                    32,
                    48,
                    0,
                    204,
                    0
                ],
                "title": "Interpretable Concept-Based Memory Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Concept-Based Memory Reasoning"
                },
                "summary": "The lack of transparency in the decision-making processes of deep learning\nsystems presents a significant challenge in modern artificial intelligence\n(AI), as it impairs users' ability to rely on and verify these systems. To\naddress this challenge, Concept Bottleneck Models (CBMs) have made significant\nprogress by incorporating human-interpretable concepts into deep learning\narchitectures. This approach allows predictions to be traced back to specific\nconcept patterns that users can understand and potentially intervene on.\nHowever, existing CBMs' task predictors are not fully interpretable, preventing\na thorough analysis and any form of formal verification of their\ndecision-making process prior to deployment, thereby raising significant\nreliability concerns. To bridge this gap, we introduce Concept-based Memory\nReasoner (CMR), a novel CBM designed to provide a human-understandable and\nprovably-verifiable task prediction process. Our approach is to model each task\nprediction as a neural selection mechanism over a memory of learnable logic\nrules, followed by a symbolic evaluation of the selected rule. The presence of\nan explicit memory and the symbolic evaluation allow domain experts to inspect\nand formally verify the validity of certain global properties of interest for\nthe task prediction process. Experimental results demonstrate that CMR achieves\nbetter accuracy-interpretability trade-offs to state-of-the-art CBMs, discovers\nlogic rules consistent with ground truths, allows for rule interventions, and\nallows pre-deployment verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The lack of transparency in the decision-making processes of deep learning\nsystems presents a significant challenge in modern artificial intelligence\n(AI), as it impairs users' ability to rely on and verify these systems. To\naddress this challenge, Concept Bottleneck Models (CBMs) have made significant\nprogress by incorporating human-interpretable concepts into deep learning\narchitectures. This approach allows predictions to be traced back to specific\nconcept patterns that users can understand and potentially intervene on.\nHowever, existing CBMs' task predictors are not fully interpretable, preventing\na thorough analysis and any form of formal verification of their\ndecision-making process prior to deployment, thereby raising significant\nreliability concerns. To bridge this gap, we introduce Concept-based Memory\nReasoner (CMR), a novel CBM designed to provide a human-understandable and\nprovably-verifiable task prediction process. Our approach is to model each task\nprediction as a neural selection mechanism over a memory of learnable logic\nrules, followed by a symbolic evaluation of the selected rule. The presence of\nan explicit memory and the symbolic evaluation allow domain experts to inspect\nand formally verify the validity of certain global properties of interest for\nthe task prediction process. Experimental results demonstrate that CMR achieves\nbetter accuracy-interpretability trade-offs to state-of-the-art CBMs, discovers\nlogic rules consistent with ground truths, allows for rule interventions, and\nallows pre-deployment verification."
                },
                "authors": [
                    {
                        "name": "David Debot"
                    },
                    {
                        "name": "Pietro Barbiero"
                    },
                    {
                        "name": "Francesco Giannini"
                    },
                    {
                        "name": "Gabriele Ciravegna"
                    },
                    {
                        "name": "Michelangelo Diligenti"
                    },
                    {
                        "name": "Giuseppe Marra"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Marra"
                },
                "author": "Giuseppe Marra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10129v1",
                "updated": "2024-11-15T12:01:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    1,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T12:01:38Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    1,
                    38,
                    4,
                    320,
                    0
                ],
                "title": "Prompting and Fine-tuning Large Language Models for Automated Code\n  Review Comment Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting and Fine-tuning Large Language Models for Automated Code\n  Review Comment Generation"
                },
                "summary": "Generating accurate code review comments remains a significant challenge due\nto the inherently diverse and non-unique nature of the task output. Large\nlanguage models pretrained on both programming and natural language data tend\nto perform well in code-oriented tasks. However, large-scale pretraining is not\nalways feasible due to its environmental impact and project-specific\ngeneralizability issues. In this work, first we fine-tune open-source Large\nlanguage models (LLM) in parameter-efficient, quantized low-rank (QLoRA)\nfashion on consumer-grade hardware to improve review comment generation. Recent\nstudies demonstrate the efficacy of augmenting semantic metadata information\ninto prompts to boost performance in other code-related tasks. To explore this\nin code review activities, we also prompt proprietary, closed-source LLMs\naugmenting the input code patch with function call graphs and code summaries.\nBoth of our strategies improve the review comment generation performance, with\nfunction call graph augmented few-shot prompting on the GPT-3.5 model\nsurpassing the pretrained baseline by around 90% BLEU-4 score on the\nCodeReviewer dataset. Moreover, few-shot prompted Gemini-1.0 Pro, QLoRA\nfine-tuned Code Llama and Llama 3.1 models achieve competitive results (ranging\nfrom 25% to 83% performance improvement) on this task. An additional human\nevaluation study further validates our experimental findings, reflecting\nreal-world developers' perceptions of LLM-generated code review comments based\non relevant qualitative metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating accurate code review comments remains a significant challenge due\nto the inherently diverse and non-unique nature of the task output. Large\nlanguage models pretrained on both programming and natural language data tend\nto perform well in code-oriented tasks. However, large-scale pretraining is not\nalways feasible due to its environmental impact and project-specific\ngeneralizability issues. In this work, first we fine-tune open-source Large\nlanguage models (LLM) in parameter-efficient, quantized low-rank (QLoRA)\nfashion on consumer-grade hardware to improve review comment generation. Recent\nstudies demonstrate the efficacy of augmenting semantic metadata information\ninto prompts to boost performance in other code-related tasks. To explore this\nin code review activities, we also prompt proprietary, closed-source LLMs\naugmenting the input code patch with function call graphs and code summaries.\nBoth of our strategies improve the review comment generation performance, with\nfunction call graph augmented few-shot prompting on the GPT-3.5 model\nsurpassing the pretrained baseline by around 90% BLEU-4 score on the\nCodeReviewer dataset. Moreover, few-shot prompted Gemini-1.0 Pro, QLoRA\nfine-tuned Code Llama and Llama 3.1 models achieve competitive results (ranging\nfrom 25% to 83% performance improvement) on this task. An additional human\nevaluation study further validates our experimental findings, reflecting\nreal-world developers' perceptions of LLM-generated code review comments based\non relevant qualitative metrics."
                },
                "authors": [
                    {
                        "name": "Md. Asif Haider"
                    },
                    {
                        "name": "Ayesha Binte Mostofa"
                    },
                    {
                        "name": "Sk. Sabit Bin Mosaddek"
                    },
                    {
                        "name": "Anindya Iqbal"
                    },
                    {
                        "name": "Toufique Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Toufique Ahmed"
                },
                "author": "Toufique Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09510v2",
                "updated": "2024-11-15T10:47:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    10,
                    47,
                    37,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-14T15:19:01Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    19,
                    1,
                    3,
                    319,
                    0
                ],
                "title": "Communication Compression for Tensor Parallel LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication Compression for Tensor Parallel LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have pushed the frontier of artificial\nintelligence but are comprised of hundreds of billions of parameters and\noperations. For faster inference latency, LLMs are deployed on multiple\nhardware accelerators through various Model Parallelism strategies. Our paper\nlooks into the details on one such strategy - Tensor Parallel - and proposes to\nreduce latency by compressing inter-accelerator communication. We leverage fine\ngrained quantization techniques to compress selected activations by 3.5 - 4.5x.\nOur proposed method leads up to 2x reduction of time-to-first-token (TTFT) with\nnegligible model performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have pushed the frontier of artificial\nintelligence but are comprised of hundreds of billions of parameters and\noperations. For faster inference latency, LLMs are deployed on multiple\nhardware accelerators through various Model Parallelism strategies. Our paper\nlooks into the details on one such strategy - Tensor Parallel - and proposes to\nreduce latency by compressing inter-accelerator communication. We leverage fine\ngrained quantization techniques to compress selected activations by 3.5 - 4.5x.\nOur proposed method leads up to 2x reduction of time-to-first-token (TTFT) with\nnegligible model performance degradation."
                },
                "authors": [
                    {
                        "name": "Jan Hansen-Palmus"
                    },
                    {
                        "name": "Michael Truong Le"
                    },
                    {
                        "name": "Oliver Hausdörfer"
                    },
                    {
                        "name": "Alok Verma"
                    }
                ],
                "author_detail": {
                    "name": "Alok Verma"
                },
                "author": "Alok Verma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10083v1",
                "updated": "2024-11-15T10:01:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    10,
                    1,
                    52,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T10:01:52Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    10,
                    1,
                    52,
                    4,
                    320,
                    0
                ],
                "title": "Xmodel-1.5: An 1B-scale Multilingual LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Xmodel-1.5: An 1B-scale Multilingual LLM"
                },
                "summary": "We introduce Xmodel-1.5, a novel 1-billion-parameter multilingual large model\npretrained on approximately 2 trillion tokens. The model demonstrates strong\nperformance across several languages, with particularly notable results in\nThai, Arabic, and French, alongside its effectiveness in Chinese and English.\nIn addition, we contribute to the research community by releasing a Thai\nevaluation dataset, which includes hundreds of questions annotated by students\nfrom Chulalongkorn University's School of Integrated Innovation. While the\nresults are promising, we acknowledge that there is still room for improvement.\nWe hope this work advances ongoing efforts in multilingual AI research and\npromotes better cross-linguistic understanding in various natural language\nprocessing tasks. Our models and code are publicly available on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Xmodel-1.5, a novel 1-billion-parameter multilingual large model\npretrained on approximately 2 trillion tokens. The model demonstrates strong\nperformance across several languages, with particularly notable results in\nThai, Arabic, and French, alongside its effectiveness in Chinese and English.\nIn addition, we contribute to the research community by releasing a Thai\nevaluation dataset, which includes hundreds of questions annotated by students\nfrom Chulalongkorn University's School of Integrated Innovation. While the\nresults are promising, we acknowledge that there is still room for improvement.\nWe hope this work advances ongoing efforts in multilingual AI research and\npromotes better cross-linguistic understanding in various natural language\nprocessing tasks. Our models and code are publicly available on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM."
                },
                "authors": [
                    {
                        "name": "Wang Qun"
                    },
                    {
                        "name": "Liu Yang"
                    },
                    {
                        "name": "Lin Qingquan"
                    },
                    {
                        "name": "Jiang Ling"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Ling"
                },
                "author": "Jiang Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10080v1",
                "updated": "2024-11-15T09:50:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    50,
                    27,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T09:50:27Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    50,
                    27,
                    4,
                    320,
                    0
                ],
                "title": "Understanding The Effect Of Temperature On Alignment With Human Opinions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding The Effect Of Temperature On Alignment With Human Opinions"
                },
                "summary": "With the increasing capabilities of LLMs, recent studies focus on\nunderstanding whose opinions are represented by them and how to effectively\nextract aligned opinion distributions. We conducted an empirical analysis of\nthree straightforward methods for obtaining distributions and evaluated the\nresults across a variety of metrics. Our findings suggest that sampling and\nlog-probability approaches with simple parameter adjustments can return better\naligned outputs in subjective tasks compared to direct prompting. Yet, assuming\nmodels reflect human opinions may be limiting, highlighting the need for\nfurther research on how human subjectivity affects model uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing capabilities of LLMs, recent studies focus on\nunderstanding whose opinions are represented by them and how to effectively\nextract aligned opinion distributions. We conducted an empirical analysis of\nthree straightforward methods for obtaining distributions and evaluated the\nresults across a variety of metrics. Our findings suggest that sampling and\nlog-probability approaches with simple parameter adjustments can return better\naligned outputs in subjective tasks compared to direct prompting. Yet, assuming\nmodels reflect human opinions may be limiting, highlighting the need for\nfurther research on how human subjectivity affects model uncertainty."
                },
                "authors": [
                    {
                        "name": "Maja Pavlovic"
                    },
                    {
                        "name": "Massimo Poesio"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Poesio"
                },
                "author": "Massimo Poesio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10069v1",
                "updated": "2024-11-15T09:33:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    33,
                    47,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T09:33:47Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    33,
                    47,
                    4,
                    320,
                    0
                ],
                "title": "Layer Importance and Hallucination Analysis in Large Language Models via\n  Enhanced Activation Variance-Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer Importance and Hallucination Analysis in Large Language Models via\n  Enhanced Activation Variance-Sparsity"
                },
                "summary": "Evaluating the importance of different layers in large language models (LLMs)\nis crucial for optimizing model performance and interpretability. This paper\nfirst explores layer importance using the Activation Variance-Sparsity Score\n(AVSS), which combines normalized activation variance and sparsity to quantify\neach layer's contribution to overall model performance. By ranking layers based\non AVSS and pruning the least impactful 25\\%, our experiments on tasks such as\nquestion answering, language modeling, and sentiment classification show that\nover 90\\% of the original performance is retained, highlighting potential\nredundancies in LLM architectures. Building on AVSS, we propose an enhanced\nversion tailored to assess hallucination propensity across layers (EAVSS). This\nimproved approach introduces Hallucination-Specific Activation Variance (HSAV)\nand Hallucination-Specific Sparsity (HSS) metrics, allowing precise\nidentification of hallucination-prone layers. By incorporating contrastive\nlearning on these layers, we effectively mitigate hallucination generation,\ncontributing to more robust and efficient LLMs(The maximum performance\nimprovement is 12\\%). Our results on the NQ, SciQ, TriviaQA, TruthfulQA, and\nWikiQA datasets demonstrate the efficacy of this method, offering a\ncomprehensive framework for both layer importance evaluation and hallucination\nmitigation in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the importance of different layers in large language models (LLMs)\nis crucial for optimizing model performance and interpretability. This paper\nfirst explores layer importance using the Activation Variance-Sparsity Score\n(AVSS), which combines normalized activation variance and sparsity to quantify\neach layer's contribution to overall model performance. By ranking layers based\non AVSS and pruning the least impactful 25\\%, our experiments on tasks such as\nquestion answering, language modeling, and sentiment classification show that\nover 90\\% of the original performance is retained, highlighting potential\nredundancies in LLM architectures. Building on AVSS, we propose an enhanced\nversion tailored to assess hallucination propensity across layers (EAVSS). This\nimproved approach introduces Hallucination-Specific Activation Variance (HSAV)\nand Hallucination-Specific Sparsity (HSS) metrics, allowing precise\nidentification of hallucination-prone layers. By incorporating contrastive\nlearning on these layers, we effectively mitigate hallucination generation,\ncontributing to more robust and efficient LLMs(The maximum performance\nimprovement is 12\\%). Our results on the NQ, SciQ, TriviaQA, TruthfulQA, and\nWikiQA datasets demonstrate the efficacy of this method, offering a\ncomprehensive framework for both layer importance evaluation and hallucination\nmitigation in LLMs."
                },
                "authors": [
                    {
                        "name": "Zichen Song"
                    },
                    {
                        "name": "Sitan Huang"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Zhongfeng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Kang"
                },
                "author": "Zhongfeng Kang",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10057v1",
                "updated": "2024-11-15T09:20:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    20,
                    46,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T09:20:46Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    20,
                    46,
                    4,
                    320,
                    0
                ],
                "title": "KuaiFormer: Transformer-Based Retrieval at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KuaiFormer: Transformer-Based Retrieval at Kuaishou"
                },
                "summary": "In large-scale content recommendation systems, retrieval serves as the\ninitial stage in the pipeline, responsible for selecting thousands of candidate\nitems from billions of options to pass on to ranking modules. Traditionally,\nthe dominant retrieval method has been Embedding-Based Retrieval (EBR) using a\nDeep Neural Network (DNN) dual-tower structure. However, applying transformer\nin retrieval tasks has been the focus of recent research, though real-world\nindustrial deployment still presents significant challenges. In this paper, we\nintroduce KuaiFormer, a novel transformer-based retrieval framework deployed in\na large-scale content recommendation system. KuaiFormer fundamentally redefines\nthe retrieval process by shifting from conventional score estimation tasks\n(such as click-through rate estimate) to a transformer-driven Next Action\nPrediction paradigm. This shift enables more effective real-time interest\nacquisition and multi-interest extraction, significantly enhancing retrieval\nperformance. KuaiFormer has been successfully integrated into Kuaishou App's\nshort-video recommendation system since May 2024, serving over 400 million\ndaily active users and resulting in a marked increase in average daily usage\ntime of Kuaishou users. We provide insights into both the technical and\nbusiness aspects of deploying transformer in large-scale recommendation\nsystems, addressing practical challenges encountered during industrial\nimplementation. Our findings offer valuable guidance for engineers and\nresearchers aiming to leverage transformer models to optimize large-scale\ncontent recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-scale content recommendation systems, retrieval serves as the\ninitial stage in the pipeline, responsible for selecting thousands of candidate\nitems from billions of options to pass on to ranking modules. Traditionally,\nthe dominant retrieval method has been Embedding-Based Retrieval (EBR) using a\nDeep Neural Network (DNN) dual-tower structure. However, applying transformer\nin retrieval tasks has been the focus of recent research, though real-world\nindustrial deployment still presents significant challenges. In this paper, we\nintroduce KuaiFormer, a novel transformer-based retrieval framework deployed in\na large-scale content recommendation system. KuaiFormer fundamentally redefines\nthe retrieval process by shifting from conventional score estimation tasks\n(such as click-through rate estimate) to a transformer-driven Next Action\nPrediction paradigm. This shift enables more effective real-time interest\nacquisition and multi-interest extraction, significantly enhancing retrieval\nperformance. KuaiFormer has been successfully integrated into Kuaishou App's\nshort-video recommendation system since May 2024, serving over 400 million\ndaily active users and resulting in a marked increase in average daily usage\ntime of Kuaishou users. We provide insights into both the technical and\nbusiness aspects of deploying transformer in large-scale recommendation\nsystems, addressing practical challenges encountered during industrial\nimplementation. Our findings offer valuable guidance for engineers and\nresearchers aiming to leverage transformer models to optimize large-scale\ncontent recommendation systems."
                },
                "authors": [
                    {
                        "name": "Chi Liu"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Qiang Luo"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10055v1",
                "updated": "2024-11-15T09:17:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    17,
                    40,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T09:17:40Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    17,
                    40,
                    4,
                    320,
                    0
                ],
                "title": "Towards unearthing neglected climate innovations from scientific\n  literature using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards unearthing neglected climate innovations from scientific\n  literature using Large Language Models"
                },
                "summary": "Climate change poses an urgent global threat, needing the rapid\nidentification and deployment of innovative solutions. We hypothesise that many\nof these solutions already exist within scientific literature but remain\nunderutilised. To address this gap, this study employs a curated dataset\nsourced from OpenAlex, a comprehensive repository of scientific papers.\nUtilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate\ntitle-abstract pairs from scientific papers on seven dimensions, covering\nclimate change mitigation potential, stage of technological development, and\nreadiness for deployment. The outputs of the language models are then compared\nwith human evaluations to assess their effectiveness in identifying promising\nyet overlooked climate innovations. Our findings suggest that these LLM-based\nmodels can effectively augment human expertise, uncovering climate solutions\nthat are potentially impactful but with far greater speed, throughput and\nconsistency. Here, we focused on UK-based solutions, but the workflow is\nregion-agnostic. This work contributes to the discovery of neglected\ninnovations in scientific literature and demonstrates the potential of AI in\nenhancing climate action strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climate change poses an urgent global threat, needing the rapid\nidentification and deployment of innovative solutions. We hypothesise that many\nof these solutions already exist within scientific literature but remain\nunderutilised. To address this gap, this study employs a curated dataset\nsourced from OpenAlex, a comprehensive repository of scientific papers.\nUtilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate\ntitle-abstract pairs from scientific papers on seven dimensions, covering\nclimate change mitigation potential, stage of technological development, and\nreadiness for deployment. The outputs of the language models are then compared\nwith human evaluations to assess their effectiveness in identifying promising\nyet overlooked climate innovations. Our findings suggest that these LLM-based\nmodels can effectively augment human expertise, uncovering climate solutions\nthat are potentially impactful but with far greater speed, throughput and\nconsistency. Here, we focused on UK-based solutions, but the workflow is\nregion-agnostic. This work contributes to the discovery of neglected\ninnovations in scientific literature and demonstrates the potential of AI in\nenhancing climate action strategies."
                },
                "authors": [
                    {
                        "name": "César Quilodrán-Casas"
                    },
                    {
                        "name": "Christopher Waite"
                    },
                    {
                        "name": "Nicole Alhadeff"
                    },
                    {
                        "name": "Diyona Dsouza"
                    },
                    {
                        "name": "Cathal Hughes"
                    },
                    {
                        "name": "Larissa Kunstel-Tabet"
                    },
                    {
                        "name": "Alyssa Gilbert"
                    }
                ],
                "author_detail": {
                    "name": "Alyssa Gilbert"
                },
                "author": "Alyssa Gilbert",
                "arxiv_comment": "10 pages. Accepted in the LatinX in AI workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10032v1",
                "updated": "2024-11-15T08:20:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    8,
                    20,
                    26,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T08:20:26Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    8,
                    20,
                    26,
                    4,
                    320,
                    0
                ],
                "title": "VMID: A Multimodal Fusion LLM Framework for Detecting and Identifying\n  Misinformation of Short Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VMID: A Multimodal Fusion LLM Framework for Detecting and Identifying\n  Misinformation of Short Videos"
                },
                "summary": "Short video platforms have become important channels for news dissemination,\noffering a highly engaging and immediate way for users to access current events\nand share information. However, these platforms have also emerged as\nsignificant conduits for the rapid spread of misinformation, as fake news and\nrumors can leverage the visual appeal and wide reach of short videos to\ncirculate extensively among audiences. Existing fake news detection methods\nmainly rely on single-modal information, such as text or images, or apply only\nbasic fusion techniques, limiting their ability to handle the complex,\nmulti-layered information inherent in short videos. To address these\nlimitations, this paper presents a novel fake news detection method based on\nmultimodal information, designed to identify misinformation through a\nmulti-level analysis of video content. This approach effectively utilizes\ndifferent modal representations to generate a unified textual description,\nwhich is then fed into a large language model for comprehensive evaluation. The\nproposed framework successfully integrates multimodal features within videos,\nsignificantly enhancing the accuracy and reliability of fake news detection.\nExperimental results demonstrate that the proposed approach outperforms\nexisting models in terms of accuracy, robustness, and utilization of multimodal\ninformation, achieving an accuracy of 90.93%, which is significantly higher\nthan the best baseline model (SV-FEND) at 81.05%. Furthermore, case studies\nprovide additional evidence of the effectiveness of the approach in accurately\ndistinguishing between fake news, debunking content, and real incidents,\nhighlighting its reliability and robustness in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Short video platforms have become important channels for news dissemination,\noffering a highly engaging and immediate way for users to access current events\nand share information. However, these platforms have also emerged as\nsignificant conduits for the rapid spread of misinformation, as fake news and\nrumors can leverage the visual appeal and wide reach of short videos to\ncirculate extensively among audiences. Existing fake news detection methods\nmainly rely on single-modal information, such as text or images, or apply only\nbasic fusion techniques, limiting their ability to handle the complex,\nmulti-layered information inherent in short videos. To address these\nlimitations, this paper presents a novel fake news detection method based on\nmultimodal information, designed to identify misinformation through a\nmulti-level analysis of video content. This approach effectively utilizes\ndifferent modal representations to generate a unified textual description,\nwhich is then fed into a large language model for comprehensive evaluation. The\nproposed framework successfully integrates multimodal features within videos,\nsignificantly enhancing the accuracy and reliability of fake news detection.\nExperimental results demonstrate that the proposed approach outperforms\nexisting models in terms of accuracy, robustness, and utilization of multimodal\ninformation, achieving an accuracy of 90.93%, which is significantly higher\nthan the best baseline model (SV-FEND) at 81.05%. Furthermore, case studies\nprovide additional evidence of the effectiveness of the approach in accurately\ndistinguishing between fake news, debunking content, and real incidents,\nhighlighting its reliability and robustness in real-world applications."
                },
                "authors": [
                    {
                        "name": "Weihao Zhong"
                    },
                    {
                        "name": "Yinhao Xiao"
                    },
                    {
                        "name": "Minghui Xu"
                    },
                    {
                        "name": "Xiuzhen Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xiuzhen Cheng"
                },
                "author": "Xiuzhen Cheng",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2211.10973 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10020v1",
                "updated": "2024-11-15T07:54:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    54,
                    19,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T07:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    54,
                    19,
                    4,
                    320,
                    0
                ],
                "title": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?"
                },
                "summary": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BiomedBERT in terms of performance, generalizability, computational\nresources, and throughput to BiomedBERT. Results: LLaMA models outperformed\nBiomedBERT across datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BiomedBERT by 7%\n(F1) on NER and 4% on RE. However, LLaMA models required more computing\nresources and ran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE\npackage featuring both models, available at https://kiwi.clinicalnlp.org/.\nConclusion: This study is among the first to develop and evaluate a\ncomprehensive clinical IE system using open-source LLMs. Results indicate that\nLLaMA models outperform BiomedBERT for clinical NER and RE but with higher\ncomputational costs and lower throughputs. These findings highlight that\nchoosing between LLMs and traditional deep learning methods for clinical IE\napplications should remain task-specific, taking into account both performance\nmetrics and practical considerations such as available computing resources and\nthe intended use case scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BiomedBERT in terms of performance, generalizability, computational\nresources, and throughput to BiomedBERT. Results: LLaMA models outperformed\nBiomedBERT across datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BiomedBERT by 7%\n(F1) on NER and 4% on RE. However, LLaMA models required more computing\nresources and ran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE\npackage featuring both models, available at https://kiwi.clinicalnlp.org/.\nConclusion: This study is among the first to develop and evaluate a\ncomprehensive clinical IE system using open-source LLMs. Results indicate that\nLLaMA models outperform BiomedBERT for clinical NER and RE but with higher\ncomputational costs and lower throughputs. These findings highlight that\nchoosing between LLMs and traditional deep learning methods for clinical IE\napplications should remain task-specific, taking into account both performance\nmetrics and practical considerations such as available computing resources and\nthe intended use case scenarios."
                },
                "authors": [
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Xu Zuo"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Vipina K. Keloth"
                    },
                    {
                        "name": "Vincent J. Zhang"
                    },
                    {
                        "name": "Ruey-Ling Weng"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Xiaoqian Jiang"
                    },
                    {
                        "name": "Kirk E. Roberts"
                    },
                    {
                        "name": "Hua Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xu"
                },
                "author": "Hua Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10006v1",
                "updated": "2024-11-15T07:35:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    35,
                    47,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T07:35:47Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    35,
                    47,
                    4,
                    320,
                    0
                ],
                "title": "Orca: Enhancing Role-Playing Abilities of Large Language Models by\n  Integrating Personality Traits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orca: Enhancing Role-Playing Abilities of Large Language Models by\n  Integrating Personality Traits"
                },
                "summary": "Large language models has catalyzed the development of personalized dialogue\nsystems, numerous role-playing conversational agents have emerged. While\nprevious research predominantly focused on enhancing the model's capability to\nfollow instructions by designing character profiles, neglecting the\npsychological factors that drive human conversations. In this paper, we propose\nOrca, a framework for data processing and training LLMs of custom characters by\nintegrating personality traits. Orca comprises four stages: (1) Personality\ntraits inferring, leverage LLMs to infer user's BigFive personality trait\nreports and scores. (2) Data Augment, simulate user's profile, background\nstory, and psychological activities. (3) Dataset construction,\npersonality-conditioned instruction prompting (PCIP) to stimulate LLMs. (4)\nModeling and Training, personality-conditioned instruction tuning (PTIT and\nPSIT), using the generated data to enhance existing open-source LLMs. We\nintroduce OrcaBench, the first benchmark for evaluating the quality of content\ngenerated by LLMs on social platforms across multiple scales. Our experiments\ndemonstrate that our proposed model achieves superior performance on this\nbenchmark, demonstrating its excellence and effectiveness in perceiving\npersonality traits that significantly improve role-playing abilities. Our Code\nis available at https://github.com/Aipura/Orca.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models has catalyzed the development of personalized dialogue\nsystems, numerous role-playing conversational agents have emerged. While\nprevious research predominantly focused on enhancing the model's capability to\nfollow instructions by designing character profiles, neglecting the\npsychological factors that drive human conversations. In this paper, we propose\nOrca, a framework for data processing and training LLMs of custom characters by\nintegrating personality traits. Orca comprises four stages: (1) Personality\ntraits inferring, leverage LLMs to infer user's BigFive personality trait\nreports and scores. (2) Data Augment, simulate user's profile, background\nstory, and psychological activities. (3) Dataset construction,\npersonality-conditioned instruction prompting (PCIP) to stimulate LLMs. (4)\nModeling and Training, personality-conditioned instruction tuning (PTIT and\nPSIT), using the generated data to enhance existing open-source LLMs. We\nintroduce OrcaBench, the first benchmark for evaluating the quality of content\ngenerated by LLMs on social platforms across multiple scales. Our experiments\ndemonstrate that our proposed model achieves superior performance on this\nbenchmark, demonstrating its excellence and effectiveness in perceiving\npersonality traits that significantly improve role-playing abilities. Our Code\nis available at https://github.com/Aipura/Orca."
                },
                "authors": [
                    {
                        "name": "Yuxuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Huang"
                },
                "author": "Yuxuan Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04422v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04422v5",
                "updated": "2024-11-15T07:07:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    7,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-10-06T09:29:19Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    9,
                    29,
                    19,
                    6,
                    280,
                    0
                ],
                "title": "Hyper-multi-step: The Truth Behind Difficult Long-context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyper-multi-step: The Truth Behind Difficult Long-context Tasks"
                },
                "summary": "Long-context language models (LCLM), characterized by their extensive context\nwindow, is becoming increasingly popular. Meanwhile, many long-context\nbenchmarks present challenging tasks that even the most advanced LCLMs struggle\nto complete. However, the underlying sources of various challenging\nlong-context tasks have seldom been studied. To bridge this gap, we conduct\nexperiments to indicate their difficulty stems primarily from two basic issues:\n\"multi-matching retrieval,\" which requires the simultaneous retrieval of\nmultiple items, and \"logic-based retrieval,\" which necessitates logical\njudgment within retrieval criteria. These two problems, while seemingly\nstraightforward, actually exceed the capabilities of LCLMs because they are\nproven to be hyper-multi-step (demanding numerous steps to solve) in nature.\nThis finding could explain why LLMs struggle with more advanced long-context\ntasks, providing a more accurate perspective for rethinking solutions for them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models (LCLM), characterized by their extensive context\nwindow, is becoming increasingly popular. Meanwhile, many long-context\nbenchmarks present challenging tasks that even the most advanced LCLMs struggle\nto complete. However, the underlying sources of various challenging\nlong-context tasks have seldom been studied. To bridge this gap, we conduct\nexperiments to indicate their difficulty stems primarily from two basic issues:\n\"multi-matching retrieval,\" which requires the simultaneous retrieval of\nmultiple items, and \"logic-based retrieval,\" which necessitates logical\njudgment within retrieval criteria. These two problems, while seemingly\nstraightforward, actually exceed the capabilities of LCLMs because they are\nproven to be hyper-multi-step (demanding numerous steps to solve) in nature.\nThis finding could explain why LLMs struggle with more advanced long-context\ntasks, providing a more accurate perspective for rethinking solutions for them."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Ma Xiufa"
                    },
                    {
                        "name": "Fang Jianwei"
                    },
                    {
                        "name": "Zhi Xu"
                    },
                    {
                        "name": "Su Guangyao"
                    },
                    {
                        "name": "Wang Jiancheng"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Zhixiao Qi"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Weifeng Liu"
                    },
                    {
                        "name": "Ran Chen"
                    },
                    {
                        "name": "Ji Pei"
                    }
                ],
                "author_detail": {
                    "name": "Ji Pei"
                },
                "author": "Ji Pei",
                "arxiv_comment": "Our code is publicly available at\n  https://github.com/yuyijiong/hard_retrieval_for_llm and the datasets is at\n  https://huggingface.co/datasets/yuyijiong/difficult_retrieval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04422v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04422v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09995v1",
                "updated": "2024-11-15T06:59:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    59,
                    41,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T06:59:41Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    59,
                    41,
                    4,
                    320,
                    0
                ],
                "title": "Strategic Roadmap for Quantum- Resistant Security: A Framework for\n  Preparing Industries for the Quantum Threat",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Roadmap for Quantum- Resistant Security: A Framework for\n  Preparing Industries for the Quantum Threat"
                },
                "summary": "As quantum computing continues to advance, its ability to compromise widely\nused cryptographic systems projects a significant challenge to modern\ncybersecurity. This paper outlines a strategic roadmap for industries to\nanticipate and mitigate the risks posed by quantum attacks. Our study explores\nthe development of a quantum-resistant cryptographic solutioning framework for\nthe industry, offering a practical and strategic approach to mitigating quantum\nattacks. We, here, propose a novel strategic framework, coined name\nSTL-QCRYPTO, outlines tailored, industry-specific methodologies to implement\nquantum-safe security systems, ensuring long-term protection against the\ndisruptive potential of quantum computing. The following fourteen high-risk\nsectors: Financial Services, Banking, Healthcare, Critical Infrastructure,\nGovernment & Defence, E-commerce, Energy & Utilities, Automotive &\nTransportation, Cloud Computing & Data Storage, Insurance, Internet &\nTelecommunications, Blockchain Applications, Metaverse Applications, and\nMultiagent AI Systems - are critically assessed for their vulnerability to\nquantum threats. The evaluation emphasizes practical approaches for the\ndeployment of quantum-safe security systems to safeguard these industries\nagainst emerging quantum-enabled cyber risks. Additionally, the paper addresses\nthe technical, operational, and regulatory hurdles associated with adopting\nquantum-resistant technologies. By presenting a structured timeline and\nactionable recommendations, this roadmap with proposed framework prepares\nindustries with the essential strategy to safeguard their potential security\nthreats in the quantum computing era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As quantum computing continues to advance, its ability to compromise widely\nused cryptographic systems projects a significant challenge to modern\ncybersecurity. This paper outlines a strategic roadmap for industries to\nanticipate and mitigate the risks posed by quantum attacks. Our study explores\nthe development of a quantum-resistant cryptographic solutioning framework for\nthe industry, offering a practical and strategic approach to mitigating quantum\nattacks. We, here, propose a novel strategic framework, coined name\nSTL-QCRYPTO, outlines tailored, industry-specific methodologies to implement\nquantum-safe security systems, ensuring long-term protection against the\ndisruptive potential of quantum computing. The following fourteen high-risk\nsectors: Financial Services, Banking, Healthcare, Critical Infrastructure,\nGovernment & Defence, E-commerce, Energy & Utilities, Automotive &\nTransportation, Cloud Computing & Data Storage, Insurance, Internet &\nTelecommunications, Blockchain Applications, Metaverse Applications, and\nMultiagent AI Systems - are critically assessed for their vulnerability to\nquantum threats. The evaluation emphasizes practical approaches for the\ndeployment of quantum-safe security systems to safeguard these industries\nagainst emerging quantum-enabled cyber risks. Additionally, the paper addresses\nthe technical, operational, and regulatory hurdles associated with adopting\nquantum-resistant technologies. By presenting a structured timeline and\nactionable recommendations, this roadmap with proposed framework prepares\nindustries with the essential strategy to safeguard their potential security\nthreats in the quantum computing era."
                },
                "authors": [
                    {
                        "name": "Arit Kumar Bishwas"
                    },
                    {
                        "name": "Mousumi Sen"
                    }
                ],
                "author_detail": {
                    "name": "Mousumi Sen"
                },
                "author": "Mousumi Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11282v3",
                "updated": "2024-11-15T06:48:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    48,
                    58,
                    4,
                    320,
                    0
                ],
                "published": "2023-12-18T15:23:06Z",
                "published_parsed": [
                    2023,
                    12,
                    18,
                    15,
                    23,
                    6,
                    0,
                    352,
                    0
                ],
                "title": "Evaluating and Enhancing Large Language Models for Conversational\n  Reasoning on Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Enhancing Large Language Models for Conversational\n  Reasoning on Knowledge Graphs"
                },
                "summary": "The development of large language models (LLMs) has been catalyzed by\nadvancements in pre-training techniques. These models have demonstrated robust\nreasoning capabilities through manually designed prompts. In this work, we\nevaluate the conversational reasoning capabilities of the current\nstate-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the\nperformance of LLMs is constrained due to a lack of KG environment awareness\nand the difficulties in developing effective optimization mechanisms for\nintermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG\nreasoning agent designed to deliver precise and adaptable predictions on KG\npaths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate\nstate information within each reasoning step. We reframe the challenge of\nmulti-hop reasoning on the KG as a sequential decision-making task. Utilizing\nthe Proximal Policy Optimization (PPO) online policy gradient reinforcement\nlearning algorithm, our model is optimized to learn from rich reward signals.\nAdditionally, we conduct an evaluation of our model and GPT-4 on the OpenDialKG\ndataset. The experimental results reveal that LLaMA-2-7B-ARK outperforms the\ncurrent state-of-the-art model by 5.28 percentage points, with a performance\nrate of 36.39% on the target@1 evaluation metric. Meanwhile, GPT-4 scored\n14.91%, further demonstrating the effectiveness of our method. Our code is\navailable on GitHub (https://github.com/Aipura/LLM-ARK) for further access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has been catalyzed by\nadvancements in pre-training techniques. These models have demonstrated robust\nreasoning capabilities through manually designed prompts. In this work, we\nevaluate the conversational reasoning capabilities of the current\nstate-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the\nperformance of LLMs is constrained due to a lack of KG environment awareness\nand the difficulties in developing effective optimization mechanisms for\nintermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG\nreasoning agent designed to deliver precise and adaptable predictions on KG\npaths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate\nstate information within each reasoning step. We reframe the challenge of\nmulti-hop reasoning on the KG as a sequential decision-making task. Utilizing\nthe Proximal Policy Optimization (PPO) online policy gradient reinforcement\nlearning algorithm, our model is optimized to learn from rich reward signals.\nAdditionally, we conduct an evaluation of our model and GPT-4 on the OpenDialKG\ndataset. The experimental results reveal that LLaMA-2-7B-ARK outperforms the\ncurrent state-of-the-art model by 5.28 percentage points, with a performance\nrate of 36.39% on the target@1 evaluation metric. Meanwhile, GPT-4 scored\n14.91%, further demonstrating the effectiveness of our method. Our code is\navailable on GitHub (https://github.com/Aipura/LLM-ARK) for further access."
                },
                "authors": [
                    {
                        "name": "Yuxuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Huang"
                },
                "author": "Yuxuan Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09978v1",
                "updated": "2024-11-15T06:21:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    21,
                    13,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T06:21:13Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    21,
                    13,
                    4,
                    320,
                    0
                ],
                "title": "HistoLens: An LLM-Powered Framework for Multi-Layered Analysis of\n  Historical Texts -- A Case Application of Yantie Lun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HistoLens: An LLM-Powered Framework for Multi-Layered Analysis of\n  Historical Texts -- A Case Application of Yantie Lun"
                },
                "summary": "This paper proposes HistoLens, a multi-layered analysis framework for\nhistorical texts based on Large Language Models (LLMs). Using the important\nWestern Han dynasty text \"Yantie Lun\" as a case study, we demonstrate the\nframework's potential applications in historical research and education.\nHistoLens integrates NLP technology (especially LLMs), including named entity\nrecognition, knowledge graph construction, and geographic information\nvisualization. The paper showcases how HistoLens explores Western Han culture\nin \"Yantie Lun\" through multi-dimensional, visual, and quantitative methods,\nfocusing particularly on the influence of Confucian and Legalist thoughts on\npolitical, economic, military, and ethnic. We also demonstrate how HistoLens\nconstructs a machine teaching scenario using LLMs for explainable analysis,\nbased on a dataset of Confucian and Legalist ideas extracted with LLM\nassistance. This approach offers novel and diverse perspectives for studying\nhistorical texts like \"Yantie Lun\" and provides new auxiliary tools for history\neducation. The framework aims to equip historians and learners with\nLLM-assisted tools to facilitate in-depth, multi-layered analysis of historical\ntexts and foster innovation in historical education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes HistoLens, a multi-layered analysis framework for\nhistorical texts based on Large Language Models (LLMs). Using the important\nWestern Han dynasty text \"Yantie Lun\" as a case study, we demonstrate the\nframework's potential applications in historical research and education.\nHistoLens integrates NLP technology (especially LLMs), including named entity\nrecognition, knowledge graph construction, and geographic information\nvisualization. The paper showcases how HistoLens explores Western Han culture\nin \"Yantie Lun\" through multi-dimensional, visual, and quantitative methods,\nfocusing particularly on the influence of Confucian and Legalist thoughts on\npolitical, economic, military, and ethnic. We also demonstrate how HistoLens\nconstructs a machine teaching scenario using LLMs for explainable analysis,\nbased on a dataset of Confucian and Legalist ideas extracted with LLM\nassistance. This approach offers novel and diverse perspectives for studying\nhistorical texts like \"Yantie Lun\" and provides new auxiliary tools for history\neducation. The framework aims to equip historians and learners with\nLLM-assisted tools to facilitate in-depth, multi-layered analysis of historical\ntexts and foster innovation in historical education."
                },
                "authors": [
                    {
                        "name": "Yifan Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Zeng"
                },
                "author": "Yifan Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09974v1",
                "updated": "2024-11-15T06:08:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    8,
                    57,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T06:08:57Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    8,
                    57,
                    4,
                    320,
                    0
                ],
                "title": "Experiences from Using LLMs for Repository Mining Studies in Empirical\n  Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experiences from Using LLMs for Repository Mining Studies in Empirical\n  Software Engineering"
                },
                "summary": "Context: The emergence of Large Language Models (LLMs) has significantly\ntransformed Software Engineering (SE) by providing innovative methods for\nanalyzing software repositories. Objectives: Our objective is to establish a\npractical framework for future SE researchers needing to enhance the data\ncollection and dataset while conducting software repository mining studies\nusing LLMs. Method: This experience report shares insights from two previous\nrepository mining studies, focusing on the methodologies used for creating,\nrefining, and validating prompts that enhance the output of LLMs, particularly\nin the context of data collection in empirical studies. Results: Our research\npackages a framework, coined Prompt Refinement and Insights for Mining\nEmpirical Software repositories (PRIMES), consisting of a checklist that can\nimprove LLM usage performance, enhance output quality, and minimize errors\nthrough iterative processes and comparisons among different LLMs. We also\nemphasize the significance of reproducibility by implementing mechanisms for\ntracking model results. Conclusion: Our findings indicate that standardizing\nprompt engineering and using PRIMES can enhance the reliability and\nreproducibility of studies utilizing LLMs. Ultimately, this work calls for\nfurther research to address challenges like hallucinations, model biases, and\ncost-effectiveness in integrating LLMs into workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: The emergence of Large Language Models (LLMs) has significantly\ntransformed Software Engineering (SE) by providing innovative methods for\nanalyzing software repositories. Objectives: Our objective is to establish a\npractical framework for future SE researchers needing to enhance the data\ncollection and dataset while conducting software repository mining studies\nusing LLMs. Method: This experience report shares insights from two previous\nrepository mining studies, focusing on the methodologies used for creating,\nrefining, and validating prompts that enhance the output of LLMs, particularly\nin the context of data collection in empirical studies. Results: Our research\npackages a framework, coined Prompt Refinement and Insights for Mining\nEmpirical Software repositories (PRIMES), consisting of a checklist that can\nimprove LLM usage performance, enhance output quality, and minimize errors\nthrough iterative processes and comparisons among different LLMs. We also\nemphasize the significance of reproducibility by implementing mechanisms for\ntracking model results. Conclusion: Our findings indicate that standardizing\nprompt engineering and using PRIMES can enhance the reliability and\nreproducibility of studies utilizing LLMs. Ultimately, this work calls for\nfurther research to address challenges like hallucinations, model biases, and\ncost-effectiveness in integrating LLMs into workflows."
                },
                "authors": [
                    {
                        "name": "Vincenzo de Martino"
                    },
                    {
                        "name": "Joel Castaño"
                    },
                    {
                        "name": "Fabio Palomba"
                    },
                    {
                        "name": "Xavier Franch"
                    },
                    {
                        "name": "Silverio Martínez-Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Silverio Martínez-Fernández"
                },
                "author": "Silverio Martínez-Fernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09972v1",
                "updated": "2024-11-15T06:05:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    5,
                    45,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T06:05:45Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    6,
                    5,
                    45,
                    4,
                    320,
                    0
                ],
                "title": "Large Language Models as User-Agents for Evaluating\n  Task-Oriented-Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as User-Agents for Evaluating\n  Task-Oriented-Dialogue Systems"
                },
                "summary": "Traditionally, offline datasets have been used to evaluate task-oriented\ndialogue (TOD) models. These datasets lack context awareness, making them\nsuboptimal benchmarks for conversational systems. In contrast, user-agents,\nwhich are context-aware, can simulate the variability and unpredictability of\nhuman conversations, making them better alternatives as evaluators. Prior\nresearch has utilized large language models (LLMs) to develop user-agents. Our\nwork builds upon this by using LLMs to create user-agents for the evaluation of\nTOD systems. This involves prompting an LLM, using in-context examples as\nguidance, and tracking the user-goal state. Our evaluation of diversity and\ntask completion metrics for the user-agents shows improved performance with the\nuse of better prompts. Additionally, we propose methodologies for the automatic\nevaluation of TOD models within this dynamic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditionally, offline datasets have been used to evaluate task-oriented\ndialogue (TOD) models. These datasets lack context awareness, making them\nsuboptimal benchmarks for conversational systems. In contrast, user-agents,\nwhich are context-aware, can simulate the variability and unpredictability of\nhuman conversations, making them better alternatives as evaluators. Prior\nresearch has utilized large language models (LLMs) to develop user-agents. Our\nwork builds upon this by using LLMs to create user-agents for the evaluation of\nTOD systems. This involves prompting an LLM, using in-context examples as\nguidance, and tracking the user-goal state. Our evaluation of diversity and\ntask completion metrics for the user-agents shows improved performance with the\nuse of better prompts. Additionally, we propose methodologies for the automatic\nevaluation of TOD models within this dynamic framework."
                },
                "authors": [
                    {
                        "name": "Taaha Kazi"
                    },
                    {
                        "name": "Ruiliang Lyu"
                    },
                    {
                        "name": "Sizhe Zhou"
                    },
                    {
                        "name": "Dilek Hakkani-Tur"
                    },
                    {
                        "name": "Gokhan Tur"
                    }
                ],
                "author_detail": {
                    "name": "Gokhan Tur"
                },
                "author": "Gokhan Tur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09969v1",
                "updated": "2024-11-15T05:55:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    5,
                    55,
                    23,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T05:55:23Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    5,
                    55,
                    23,
                    4,
                    320,
                    0
                ],
                "title": "Steering AI-Driven Personalization of Scientific Text for General\n  Audiences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering AI-Driven Personalization of Scientific Text for General\n  Audiences"
                },
                "summary": "Digital media platforms (e.g., social media, science blogs) offer\nopportunities to communicate scientific content to general audiences at scale.\nHowever, these audiences vary in their scientific expertise, literacy levels,\nand personal backgrounds, making effective science communication challenging.\nTo address this challenge, we designed TranSlider, an AI-powered tool that\ngenerates personalized translations of scientific text based on individual user\nprofiles (e.g., hobbies, location, and education). Our tool features an\ninteractive slider that allows users to steer the degree of personalization\nfrom 0 (weakly relatable) to 100 (strongly relatable), leveraging LLMs to\ngenerate the translations with given degrees. Through an exploratory study with\n15 participants, we investigated both the utility of these AI-personalized\ntranslations and how interactive reading features influenced users'\nunderstanding and reading experiences. We found that participants who preferred\nhigher degrees of personalization appreciated the relatable and contextual\ntranslations, while those who preferred lower degrees valued concise\ntranslations with subtle contextualization. Furthermore, participants reported\nthe compounding effect of multiple translations on their understanding of\nscientific content. Given these findings, we discuss several implications of\nAI-personalized translation tools in facilitating communication in\ncollaborative contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital media platforms (e.g., social media, science blogs) offer\nopportunities to communicate scientific content to general audiences at scale.\nHowever, these audiences vary in their scientific expertise, literacy levels,\nand personal backgrounds, making effective science communication challenging.\nTo address this challenge, we designed TranSlider, an AI-powered tool that\ngenerates personalized translations of scientific text based on individual user\nprofiles (e.g., hobbies, location, and education). Our tool features an\ninteractive slider that allows users to steer the degree of personalization\nfrom 0 (weakly relatable) to 100 (strongly relatable), leveraging LLMs to\ngenerate the translations with given degrees. Through an exploratory study with\n15 participants, we investigated both the utility of these AI-personalized\ntranslations and how interactive reading features influenced users'\nunderstanding and reading experiences. We found that participants who preferred\nhigher degrees of personalization appreciated the relatable and contextual\ntranslations, while those who preferred lower degrees valued concise\ntranslations with subtle contextualization. Furthermore, participants reported\nthe compounding effect of multiple translations on their understanding of\nscientific content. Given these findings, we discuss several implications of\nAI-personalized translation tools in facilitating communication in\ncollaborative contexts."
                },
                "authors": [
                    {
                        "name": "Taewook Kim"
                    },
                    {
                        "name": "Dhruv Agarwal"
                    },
                    {
                        "name": "Jordan Ackerman"
                    },
                    {
                        "name": "Manaswi Saha"
                    }
                ],
                "author_detail": {
                    "name": "Manaswi Saha"
                },
                "author": "Manaswi Saha",
                "arxiv_comment": "23 pages, 5 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09955v1",
                "updated": "2024-11-15T05:18:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    5,
                    18,
                    15,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T05:18:15Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    5,
                    18,
                    15,
                    4,
                    320,
                    0
                ],
                "title": "Instruction-Guided Editing Controls for Images and Multimedia: A Survey\n  in LLM era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-Guided Editing Controls for Images and Multimedia: A Survey\n  in LLM era"
                },
                "summary": "The rapid advancement of large language models (LLMs) and multimodal learning\nhas transformed digital content creation and manipulation. Traditional visual\nediting tools require significant expertise, limiting accessibility. Recent\nstrides in instruction-based editing have enabled intuitive interaction with\nvisual content, using natural language as a bridge between user intent and\ncomplex editing operations. This survey provides an overview of these\ntechniques, focusing on how LLMs and multimodal models empower users to achieve\nprecise visual modifications without deep technical knowledge. By synthesizing\nover 100 publications, we explore methods from generative adversarial networks\nto diffusion models, examining multimodal integration for fine-grained content\ncontrol. We discuss practical applications across domains such as fashion, 3D\nscene manipulation, and video synthesis, highlighting increased accessibility\nand alignment with human intuition. Our survey compares existing literature,\nemphasizing LLM-empowered editing, and identifies key challenges to stimulate\nfurther research. We aim to democratize powerful visual editing across various\nindustries, from entertainment to education. Interested readers are encouraged\nto access our repository at\nhttps://github.com/tamlhp/awesome-instruction-editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) and multimodal learning\nhas transformed digital content creation and manipulation. Traditional visual\nediting tools require significant expertise, limiting accessibility. Recent\nstrides in instruction-based editing have enabled intuitive interaction with\nvisual content, using natural language as a bridge between user intent and\ncomplex editing operations. This survey provides an overview of these\ntechniques, focusing on how LLMs and multimodal models empower users to achieve\nprecise visual modifications without deep technical knowledge. By synthesizing\nover 100 publications, we explore methods from generative adversarial networks\nto diffusion models, examining multimodal integration for fine-grained content\ncontrol. We discuss practical applications across domains such as fashion, 3D\nscene manipulation, and video synthesis, highlighting increased accessibility\nand alignment with human intuition. Our survey compares existing literature,\nemphasizing LLM-empowered editing, and identifies key challenges to stimulate\nfurther research. We aim to democratize powerful visual editing across various\nindustries, from entertainment to education. Interested readers are encouraged\nto access our repository at\nhttps://github.com/tamlhp/awesome-instruction-editing."
                },
                "authors": [
                    {
                        "name": "Thanh Tam Nguyen"
                    },
                    {
                        "name": "Zhao Ren"
                    },
                    {
                        "name": "Trinh Pham"
                    },
                    {
                        "name": "Phi Le Nguyen"
                    },
                    {
                        "name": "Hongzhi Yin"
                    },
                    {
                        "name": "Quoc Viet Hung Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Quoc Viet Hung Nguyen"
                },
                "author": "Quoc Viet Hung Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16842v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16842v2",
                "updated": "2024-11-15T05:08:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    5,
                    8,
                    56,
                    4,
                    320,
                    0
                ],
                "published": "2024-08-29T18:10:36Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    18,
                    10,
                    36,
                    3,
                    242,
                    0
                ],
                "title": "AdapShare: An RL-Based Dynamic Spectrum Sharing Solution for O-RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapShare: An RL-Based Dynamic Spectrum Sharing Solution for O-RAN"
                },
                "summary": "The Open Radio Access Network (O-RAN) initiative, characterized by open\ninterfaces and AI/ML-capable RAN Intelligent Controller (RIC), facilitates\neffective spectrum sharing among RANs. In this context, we introduce AdapShare,\nan ORAN-compatible solution leveraging Reinforcement Learning (RL) for\nintent-based spectrum management, with the primary goal of minimizing resource\nsurpluses or deficits in RANs. By employing RL agents, AdapShare intelligently\nlearns network demand patterns and uses them to allocate resources. We\ndemonstrate the efficacy of AdapShare in the spectrum sharing scenario between\nLTE and NR networks, incorporating real-world LTE resource usage data and\nsynthetic NR usage data to demonstrate its practical use. We use the average\nsurplus or deficit and fairness index to measure the system's performance in\nvarious scenarios. AdapShare outperforms a quasi-static resource allocation\nscheme based on long-term network demand statistics, particularly when\navailable resources are scarce or exceed the aggregate demand from the\nnetworks. Lastly, we present a high-level O-RAN compatible architecture using\nRL agents, which demonstrates the seamless integration of AdapShare into\nreal-world deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open Radio Access Network (O-RAN) initiative, characterized by open\ninterfaces and AI/ML-capable RAN Intelligent Controller (RIC), facilitates\neffective spectrum sharing among RANs. In this context, we introduce AdapShare,\nan ORAN-compatible solution leveraging Reinforcement Learning (RL) for\nintent-based spectrum management, with the primary goal of minimizing resource\nsurpluses or deficits in RANs. By employing RL agents, AdapShare intelligently\nlearns network demand patterns and uses them to allocate resources. We\ndemonstrate the efficacy of AdapShare in the spectrum sharing scenario between\nLTE and NR networks, incorporating real-world LTE resource usage data and\nsynthetic NR usage data to demonstrate its practical use. We use the average\nsurplus or deficit and fairness index to measure the system's performance in\nvarious scenarios. AdapShare outperforms a quasi-static resource allocation\nscheme based on long-term network demand statistics, particularly when\navailable resources are scarce or exceed the aggregate demand from the\nnetworks. Lastly, we present a high-level O-RAN compatible architecture using\nRL agents, which demonstrates the seamless integration of AdapShare into\nreal-world deployment scenarios."
                },
                "authors": [
                    {
                        "name": "Sneihil Gopal"
                    },
                    {
                        "name": "David Griffith"
                    },
                    {
                        "name": "Richard A. Rouil"
                    },
                    {
                        "name": "Chunmei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chunmei Liu"
                },
                "author": "Chunmei Liu",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2404.09110",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16842v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10347v2",
                "updated": "2024-11-15T04:44:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    44,
                    40,
                    4,
                    320,
                    0
                ],
                "published": "2024-05-16T02:00:44Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    2,
                    0,
                    44,
                    3,
                    137,
                    0
                ],
                "title": "Networking Systems for Video Anomaly Detection: A Tutorial and Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Networking Systems for Video Anomaly Detection: A Tutorial and Survey"
                },
                "summary": "The increasing utilization of surveillance cameras in smart cities, coupled\nwith the surge of online video applications, has heightened concerns regarding\npublic security and privacy protection, which propelled automated Video Anomaly\nDetection (VAD) into a fundamental research task within the Artificial\nIntelligence (AI) community. With the advancements in deep learning and edge\ncomputing, VAD has made significant progress and advances synergized with\nemerging applications in smart cities and video internet, which has moved\nbeyond the conventional research scope of algorithm engineering to deployable\nNetworking Systems for VAD (NSVAD), a practical hotspot for intersection\nexploration in the AI, IoVT, and computing fields. In this article, we\ndelineate the foundational assumptions, learning frameworks, and applicable\nscenarios of various deep learning-driven VAD routes, offering an exhaustive\ntutorial for novices in NSVAD. This article elucidates core concepts by\nreviewing recent advances and typical solutions and aggregating available\nresearch resources accessible at https://github.com/fdjingliu/NSVAD.\nAdditionally, we showcase our latest NSVAD research in industrial IoT and smart\ncities, along with an end-cloud collaborative architecture for deployable\nNSVAD. Lastly, this article projects future development trends and discusses\nhow the integration of AI and computing technologies can address existing\nresearch challenges and promote open opportunities, serving as an insightful\nguide for prospective researchers and engineers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing utilization of surveillance cameras in smart cities, coupled\nwith the surge of online video applications, has heightened concerns regarding\npublic security and privacy protection, which propelled automated Video Anomaly\nDetection (VAD) into a fundamental research task within the Artificial\nIntelligence (AI) community. With the advancements in deep learning and edge\ncomputing, VAD has made significant progress and advances synergized with\nemerging applications in smart cities and video internet, which has moved\nbeyond the conventional research scope of algorithm engineering to deployable\nNetworking Systems for VAD (NSVAD), a practical hotspot for intersection\nexploration in the AI, IoVT, and computing fields. In this article, we\ndelineate the foundational assumptions, learning frameworks, and applicable\nscenarios of various deep learning-driven VAD routes, offering an exhaustive\ntutorial for novices in NSVAD. This article elucidates core concepts by\nreviewing recent advances and typical solutions and aggregating available\nresearch resources accessible at https://github.com/fdjingliu/NSVAD.\nAdditionally, we showcase our latest NSVAD research in industrial IoT and smart\ncities, along with an end-cloud collaborative architecture for deployable\nNSVAD. Lastly, this article projects future development trends and discusses\nhow the integration of AI and computing technologies can address existing\nresearch challenges and promote open opportunities, serving as an insightful\nguide for prospective researchers and engineers."
                },
                "authors": [
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jieyu Lin"
                    },
                    {
                        "name": "Jielin Li"
                    },
                    {
                        "name": "Liang Cao"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Bo Hu"
                    },
                    {
                        "name": "Liang Song"
                    },
                    {
                        "name": "Azzedine Boukerche"
                    },
                    {
                        "name": "Victor C. M. Leung"
                    }
                ],
                "author_detail": {
                    "name": "Victor C. M. Leung"
                },
                "author": "Victor C. M. Leung",
                "arxiv_comment": "Revised to ACM Computing Surveys, under review, for more information\n  and supplementary material, please see https://github.com/fdjingliu/NSVAD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09944v1",
                "updated": "2024-11-15T04:44:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    44,
                    34,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T04:44:34Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    44,
                    34,
                    4,
                    320,
                    0
                ],
                "title": "SlimLM: An Efficient Small Language Model for On-Device Document\n  Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimLM: An Efficient Small Language Model for On-Device Document\n  Assistance"
                },
                "summary": "While small language models (SLMs) show promises for mobile deployment, their\nreal-world performance and applications on smartphones remains underexplored.\nWe present SlimLM, a series of SLMs optimized for document assistance tasks on\nmobile devices. Through extensive experiments on a Samsung Galaxy S24, we\nidentify the optimal trade-offs between model size (ranging from 125M to 7B\nparameters), context length, and inference time for efficient on-device\nprocessing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on\nDocAssist, our constructed dataset for summarization, question answering and\nsuggestion tasks. Our smallest model demonstrates efficient performance on S24,\nwhile larger variants offer enhanced capabilities within mobile constraints. We\nevaluate SlimLM against existing SLMs, showing comparable or superior\nperformance and offering a benchmark for future research in on-device language\nmodels. We also provide an Android application, offering practical insights\ninto SLM deployment. Our findings provide valuable insights and illuminate the\ncapabilities of running advanced language models on high-end smartphones,\npotentially reducing server costs and enhancing privacy through on-device\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While small language models (SLMs) show promises for mobile deployment, their\nreal-world performance and applications on smartphones remains underexplored.\nWe present SlimLM, a series of SLMs optimized for document assistance tasks on\nmobile devices. Through extensive experiments on a Samsung Galaxy S24, we\nidentify the optimal trade-offs between model size (ranging from 125M to 7B\nparameters), context length, and inference time for efficient on-device\nprocessing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on\nDocAssist, our constructed dataset for summarization, question answering and\nsuggestion tasks. Our smallest model demonstrates efficient performance on S24,\nwhile larger variants offer enhanced capabilities within mobile constraints. We\nevaluate SlimLM against existing SLMs, showing comparable or superior\nperformance and offering a benchmark for future research in on-device language\nmodels. We also provide an Android application, offering practical insights\ninto SLM deployment. Our findings provide valuable insights and illuminate the\ncapabilities of running advanced language models on high-end smartphones,\npotentially reducing server costs and enhancing privacy through on-device\nprocessing."
                },
                "authors": [
                    {
                        "name": "Thang M. Pham"
                    },
                    {
                        "name": "Phat T. Nguyen"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Trung Bui"
                    }
                ],
                "author_detail": {
                    "name": "Trung Bui"
                },
                "author": "Trung Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02170v2",
                "updated": "2024-11-15T04:30:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    30,
                    4,
                    4,
                    320,
                    0
                ],
                "published": "2023-10-03T16:05:48Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    16,
                    5,
                    48,
                    1,
                    276,
                    0
                ],
                "title": "A Dynamic LLM-Powered Agent Network for Task-Oriented Agent\n  Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dynamic LLM-Powered Agent Network for Task-Oriented Agent\n  Collaboration"
                },
                "summary": "Recent studies show that collaborating multiple large language model (LLM)\npowered agents is a promising way for task solving. However, current approaches\nare constrained by using a fixed number of agents and static communication\nstructures. In this work, we propose automatically selecting a team of agents\nfrom candidates to collaborate in a dynamic communication structure toward\ndifferent tasks and domains. Specifically, we build a framework named Dynamic\nLLM-Powered Agent Network ($\\textbf{DyLAN}$) for LLM-powered agent\ncollaboration, operating a two-stage paradigm: (1) Team Optimization and (2)\nTask Solving. During the first stage, we utilize an $\\textit{agent selection}$\nalgorithm, based on an unsupervised metric called $\\textit{Agent Importance\nScore}$, enabling the selection of best agents according to their contributions\nin a preliminary trial, oriented to the given task. Then, in the second stage,\nthe selected agents collaborate dynamically according to the query.\nEmpirically, we demonstrate that DyLAN outperforms strong baselines in code\ngeneration, decision-making, general reasoning, and arithmetic reasoning tasks\nwith moderate computational cost. On specific subjects in MMLU, selecting a\nteam of agents in the team optimization stage improves accuracy by up to 25.0%\nin DyLAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that collaborating multiple large language model (LLM)\npowered agents is a promising way for task solving. However, current approaches\nare constrained by using a fixed number of agents and static communication\nstructures. In this work, we propose automatically selecting a team of agents\nfrom candidates to collaborate in a dynamic communication structure toward\ndifferent tasks and domains. Specifically, we build a framework named Dynamic\nLLM-Powered Agent Network ($\\textbf{DyLAN}$) for LLM-powered agent\ncollaboration, operating a two-stage paradigm: (1) Team Optimization and (2)\nTask Solving. During the first stage, we utilize an $\\textit{agent selection}$\nalgorithm, based on an unsupervised metric called $\\textit{Agent Importance\nScore}$, enabling the selection of best agents according to their contributions\nin a preliminary trial, oriented to the given task. Then, in the second stage,\nthe selected agents collaborate dynamically according to the query.\nEmpirically, we demonstrate that DyLAN outperforms strong baselines in code\ngeneration, decision-making, general reasoning, and arithmetic reasoning tasks\nwith moderate computational cost. On specific subjects in MMLU, selecting a\nteam of agents in the team optimization stage improves accuracy by up to 25.0%\nin DyLAN."
                },
                "authors": [
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Yanzhe Zhang"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "Published in COLM2024. Code Repo: https://github.com/SALT-NLP/DyLAN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09937v1",
                "updated": "2024-11-15T04:22:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    22,
                    21,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T04:22:21Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    22,
                    21,
                    4,
                    320,
                    0
                ],
                "title": "Refined and Segmented Price Sentiment Indices from Survey Comments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refined and Segmented Price Sentiment Indices from Survey Comments"
                },
                "summary": "We aim to enhance a price sentiment index and to more precisely understand\nprice trends from the perspective of not only consumers but also businesses. We\nextract comments related to prices from the Economy Watchers Survey conducted\nby the Cabinet Office of Japan and classify price trends using a large language\nmodel (LLM). We classify whether the survey sample reflects the perspective of\nconsumers or businesses, and whether the comments pertain to goods or services\nby utilizing information on the fields of comments and the industries of\nrespondents included in the Economy Watchers Survey. From these classified\nprice-related comments, we construct price sentiment indices not only for a\ngeneral purpose but also for more specific objectives by combining perspectives\non consumers and prices, as well as goods and services. It becomes possible to\nachieve a more accurate classification of price directions by employing a LLM\nfor classification. Furthermore, integrating the outputs of multiple LLMs\nsuggests the potential for the better performance of the classification. The\nuse of more accurately classified comments allows for the construction of an\nindex with a higher correlation to existing indices than previous studies. We\ndemonstrate that the correlation of the price index for consumers, which has a\nlarger sample size, is further enhanced by selecting comments for aggregation\nbased on the industry of the survey respondents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We aim to enhance a price sentiment index and to more precisely understand\nprice trends from the perspective of not only consumers but also businesses. We\nextract comments related to prices from the Economy Watchers Survey conducted\nby the Cabinet Office of Japan and classify price trends using a large language\nmodel (LLM). We classify whether the survey sample reflects the perspective of\nconsumers or businesses, and whether the comments pertain to goods or services\nby utilizing information on the fields of comments and the industries of\nrespondents included in the Economy Watchers Survey. From these classified\nprice-related comments, we construct price sentiment indices not only for a\ngeneral purpose but also for more specific objectives by combining perspectives\non consumers and prices, as well as goods and services. It becomes possible to\nachieve a more accurate classification of price directions by employing a LLM\nfor classification. Furthermore, integrating the outputs of multiple LLMs\nsuggests the potential for the better performance of the classification. The\nuse of more accurately classified comments allows for the construction of an\nindex with a higher correlation to existing indices than previous studies. We\ndemonstrate that the correlation of the price index for consumers, which has a\nlarger sample size, is further enhanced by selecting comments for aggregation\nbased on the industry of the survey respondents."
                },
                "authors": [
                    {
                        "name": "Masahiro Suzuki"
                    },
                    {
                        "name": "Hiroki Sakaji"
                    }
                ],
                "author_detail": {
                    "name": "Hiroki Sakaji"
                },
                "author": "Hiroki Sakaji",
                "arxiv_comment": "Accepted to IEEE BigData 2024. 9 pages, 11 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02827v3",
                "updated": "2024-11-15T04:17:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    17,
                    35,
                    4,
                    320,
                    0
                ],
                "published": "2024-04-03T15:59:42Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    15,
                    59,
                    42,
                    2,
                    94,
                    0
                ],
                "title": "BAdam: A Memory Efficient Full Parameter Optimization Method for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BAdam: A Memory Efficient Full Parameter Optimization Method for Large\n  Language Models"
                },
                "summary": "This work presents BAdam, an optimization method that leverages the block\ncoordinate descent (BCD) framework with Adam's update rule. BAdam offers a\nmemory efficient approach to the full parameter finetuning of large language\nmodels. We conduct a theoretical convergence analysis for BAdam in the\ndeterministic case. Experimentally, we apply BAdam to finetune the Llama 3-8B\nand Llama 3-70B models using a single RTX3090-24GB GPU and 4 A100-80GB GPUs,\nrespectively. The results confirm BAdam's efficiency in terms of memory usage,\nrunning time, and optimization capability. Furthermore, the downstream\nperformance evaluation based on MT-bench and math benchmarks shows that BAdam\noutperforms existing memory efficient baselines such as LoRA. It also\ndemonstrates that BAdam can achieve comparable or even superior performance\ncompared to Adam. Finally, the ablation study using SGD's update rule\nillustrates the suitability of BCD for finetuning LLMs. Our code can be easily\nintegrated into any PyTorch-based codebase and is available at\nhttps://github.com/Ledzy/BAdam.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents BAdam, an optimization method that leverages the block\ncoordinate descent (BCD) framework with Adam's update rule. BAdam offers a\nmemory efficient approach to the full parameter finetuning of large language\nmodels. We conduct a theoretical convergence analysis for BAdam in the\ndeterministic case. Experimentally, we apply BAdam to finetune the Llama 3-8B\nand Llama 3-70B models using a single RTX3090-24GB GPU and 4 A100-80GB GPUs,\nrespectively. The results confirm BAdam's efficiency in terms of memory usage,\nrunning time, and optimization capability. Furthermore, the downstream\nperformance evaluation based on MT-bench and math benchmarks shows that BAdam\noutperforms existing memory efficient baselines such as LoRA. It also\ndemonstrates that BAdam can achieve comparable or even superior performance\ncompared to Adam. Finally, the ablation study using SGD's update rule\nillustrates the suitability of BCD for finetuning LLMs. Our code can be easily\nintegrated into any PyTorch-based codebase and is available at\nhttps://github.com/Ledzy/BAdam."
                },
                "authors": [
                    {
                        "name": "Qijun Luo"
                    },
                    {
                        "name": "Hengxu Yu"
                    },
                    {
                        "name": "Xiao Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Li"
                },
                "author": "Xiao Li",
                "arxiv_comment": "Accepted for Publication in Conference on Neural Information\n  Processing Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09933v1",
                "updated": "2024-11-15T04:16:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    16,
                    50,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T04:16:50Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    16,
                    50,
                    4,
                    320,
                    0
                ],
                "title": "JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by\n  Evolutionary Optimization of Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by\n  Evolutionary Optimization of Model Merging"
                },
                "summary": "With the rapid advancement of large language models (LLMs), foundational\nmodels (FMs) have seen significant advancements. Healthcare is one of the most\ncrucial application areas for these FMs, given the significant time and effort\nrequired for physicians to analyze large volumes of patient data. Recent\nefforts have focused on adapting multimodal FMs to the medical domain through\ntechniques like instruction-tuning, leading to the development of medical\nfoundation models (MFMs). However, these approaches typically require large\namounts of training data to effectively adapt models to the medical field.\nMoreover, most existing models are trained on English datasets, limiting their\npracticality in non-English-speaking regions where healthcare professionals and\npatients are not always fluent in English. The need for translation introduces\nadditional costs and inefficiencies. To address these challenges, we propose a\n\\textbf{J}apanese \\textbf{Radi}ology report generation model enhanced by\n\\textbf{Evo}lutionary optimization of model merging (JRadiEvo). This is the\nfirst attempt to extend a non-medical vision-language foundation model to the\nmedical domain through evolutionary optimization of model merging. We\nsuccessfully created a model that generates accurate Japanese reports from\nX-ray images using only 50 translated samples from publicly available data.\nThis model, developed with highly efficient use of limited data, outperformed\nleading models from recent research trained on much larger datasets.\nAdditionally, with only 8 billion parameters, this relatively compact\nfoundation model can be deployed locally within hospitals, making it a\npractical solution for environments where APIs and other external services\ncannot be used due to strict privacy and security requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large language models (LLMs), foundational\nmodels (FMs) have seen significant advancements. Healthcare is one of the most\ncrucial application areas for these FMs, given the significant time and effort\nrequired for physicians to analyze large volumes of patient data. Recent\nefforts have focused on adapting multimodal FMs to the medical domain through\ntechniques like instruction-tuning, leading to the development of medical\nfoundation models (MFMs). However, these approaches typically require large\namounts of training data to effectively adapt models to the medical field.\nMoreover, most existing models are trained on English datasets, limiting their\npracticality in non-English-speaking regions where healthcare professionals and\npatients are not always fluent in English. The need for translation introduces\nadditional costs and inefficiencies. To address these challenges, we propose a\n\\textbf{J}apanese \\textbf{Radi}ology report generation model enhanced by\n\\textbf{Evo}lutionary optimization of model merging (JRadiEvo). This is the\nfirst attempt to extend a non-medical vision-language foundation model to the\nmedical domain through evolutionary optimization of model merging. We\nsuccessfully created a model that generates accurate Japanese reports from\nX-ray images using only 50 translated samples from publicly available data.\nThis model, developed with highly efficient use of limited data, outperformed\nleading models from recent research trained on much larger datasets.\nAdditionally, with only 8 billion parameters, this relatively compact\nfoundation model can be deployed locally within hospitals, making it a\npractical solution for environments where APIs and other external services\ncannot be used due to strict privacy and security requirements."
                },
                "authors": [
                    {
                        "name": "Kaito Baba"
                    },
                    {
                        "name": "Ryota Yagi"
                    },
                    {
                        "name": "Junichiro Takahashi"
                    },
                    {
                        "name": "Risa Kishikawa"
                    },
                    {
                        "name": "Satoshi Kodera"
                    }
                ],
                "author_detail": {
                    "name": "Satoshi Kodera"
                },
                "author": "Satoshi Kodera",
                "arxiv_comment": "Accepted by NeurIPS'24 Workshop on AIM-FM: Advancements In Medical\n  Foundation Models: Explainability, Robustness, Security, and Beyond",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10942v3",
                "updated": "2024-11-15T03:48:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    48,
                    7,
                    4,
                    320,
                    0
                ],
                "published": "2024-06-16T13:44:41Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    13,
                    44,
                    41,
                    6,
                    168,
                    0
                ],
                "title": "Effective Generative AI: The Human-Algorithm Centaur",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective Generative AI: The Human-Algorithm Centaur"
                },
                "summary": "Advanced analytics science methods have enabled combining the power of\nartificial and human intelligence, creating \\textit{centaurs} that allow\nsuperior decision-making. Centaurs are hybrid human-algorithm models that\ncombine both formal analytics and human intuition in a symbiotic manner within\ntheir learning and reasoning process. We argue that the future of AI\ndevelopment and use in many domains needs to focus more on centaurs as opposed\nto other AI approaches. This paradigm shift towards centaur-based AI methods\nraises some fundamental questions: How are centaurs different from other\nhuman-in-the-loop methods? What are the most effective methods for creating\ncentaurs? When should centaurs be used, and when should the lead be given to\npure AI models? Doesn't the incorporation of human intuition -- which at times\ncan be misleading -- in centaurs' decision-making process degrade its\nperformance compared to pure AI methods? This work aims to address these\nfundamental questions, focusing on recent advancements in generative AI, and\nespecially in Large Language Models (LLMs), as a main case study to illustrate\ncentaurs' critical essentiality to future AI endeavors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced analytics science methods have enabled combining the power of\nartificial and human intelligence, creating \\textit{centaurs} that allow\nsuperior decision-making. Centaurs are hybrid human-algorithm models that\ncombine both formal analytics and human intuition in a symbiotic manner within\ntheir learning and reasoning process. We argue that the future of AI\ndevelopment and use in many domains needs to focus more on centaurs as opposed\nto other AI approaches. This paradigm shift towards centaur-based AI methods\nraises some fundamental questions: How are centaurs different from other\nhuman-in-the-loop methods? What are the most effective methods for creating\ncentaurs? When should centaurs be used, and when should the lead be given to\npure AI models? Doesn't the incorporation of human intuition -- which at times\ncan be misleading -- in centaurs' decision-making process degrade its\nperformance compared to pure AI methods? This work aims to address these\nfundamental questions, focusing on recent advancements in generative AI, and\nespecially in Large Language Models (LLMs), as a main case study to illustrate\ncentaurs' critical essentiality to future AI endeavors."
                },
                "authors": [
                    {
                        "name": "Soroush Saghafian"
                    },
                    {
                        "name": "Lihi Idan"
                    }
                ],
                "author_detail": {
                    "name": "Lihi Idan"
                },
                "author": "Lihi Idan",
                "arxiv_doi": "10.1162/99608f92.19d78478",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1162/99608f92.19d78478",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To Appear in SI: Future Shock, Harvard Data Science Review\n  (https://hdsr.mitpress.mit.edu/specialissue5)",
                "arxiv_journal_ref": "Harvard Data Science Review (2024)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09921v1",
                "updated": "2024-11-15T03:45:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    45,
                    9,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T03:45:09Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    45,
                    9,
                    4,
                    320,
                    0
                ],
                "title": "Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at\n  Pixel Level",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at\n  Pixel Level"
                },
                "summary": "In this paper, we introduce Motion-Grounded Video Reasoning, a new motion\nunderstanding task that requires generating visual answers (video segmentation\nmasks) according to the input question, and hence needs implicit spatiotemporal\nreasoning and grounding. This task extends existing spatiotemporal grounding\nwork focusing on explicit action/motion grounding, to a more general format by\nenabling implicit reasoning via questions. To facilitate the development of the\nnew task, we collect a large-scale dataset called GROUNDMORE, which comprises\n1,715 video clips, 249K object masks that are deliberately designed with 4\nquestion types (Causal, Sequential, Counterfactual, and Descriptive) for\nbenchmarking deep and comprehensive motion reasoning abilities. GROUNDMORE\nuniquely requires models to generate visual answers, providing a more concrete\nand visually interpretable response than plain texts. It evaluates models on\nboth spatiotemporal grounding and reasoning, fostering to address complex\nchallenges in motion-related video reasoning, temporal perception, and\npixel-level understanding. Furthermore, we introduce a novel baseline model\nnamed Motion-Grounded Video Reasoning Assistant (MORA). MORA incorporates the\nmultimodal reasoning ability from the Multimodal LLM, the pixel-level\nperception capability from the grounding model (SAM), and the temporal\nperception ability from a lightweight localization head. MORA achieves\nrespectable performance on GROUNDMORE outperforming the best existing visual\ngrounding baseline model by an average of 21.5% relatively. We hope this novel\nand challenging task will pave the way for future advancements in robust and\ngeneral motion understanding via video reasoning segmentation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Motion-Grounded Video Reasoning, a new motion\nunderstanding task that requires generating visual answers (video segmentation\nmasks) according to the input question, and hence needs implicit spatiotemporal\nreasoning and grounding. This task extends existing spatiotemporal grounding\nwork focusing on explicit action/motion grounding, to a more general format by\nenabling implicit reasoning via questions. To facilitate the development of the\nnew task, we collect a large-scale dataset called GROUNDMORE, which comprises\n1,715 video clips, 249K object masks that are deliberately designed with 4\nquestion types (Causal, Sequential, Counterfactual, and Descriptive) for\nbenchmarking deep and comprehensive motion reasoning abilities. GROUNDMORE\nuniquely requires models to generate visual answers, providing a more concrete\nand visually interpretable response than plain texts. It evaluates models on\nboth spatiotemporal grounding and reasoning, fostering to address complex\nchallenges in motion-related video reasoning, temporal perception, and\npixel-level understanding. Furthermore, we introduce a novel baseline model\nnamed Motion-Grounded Video Reasoning Assistant (MORA). MORA incorporates the\nmultimodal reasoning ability from the Multimodal LLM, the pixel-level\nperception capability from the grounding model (SAM), and the temporal\nperception ability from a lightweight localization head. MORA achieves\nrespectable performance on GROUNDMORE outperforming the best existing visual\ngrounding baseline model by an average of 21.5% relatively. We hope this novel\nand challenging task will pave the way for future advancements in robust and\ngeneral motion understanding via video reasoning segmentation"
                },
                "authors": [
                    {
                        "name": "Andong Deng"
                    },
                    {
                        "name": "Tongjia Chen"
                    },
                    {
                        "name": "Shoubin Yu"
                    },
                    {
                        "name": "Taojiannan Yang"
                    },
                    {
                        "name": "Lincoln Spencer"
                    },
                    {
                        "name": "Yapeng Tian"
                    },
                    {
                        "name": "Ajmal Saeed Mian"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Chen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chen Chen"
                },
                "author": "Chen Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09916v1",
                "updated": "2024-11-15T03:29:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    29,
                    41,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T03:29:41Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    29,
                    41,
                    4,
                    320,
                    0
                ],
                "title": "LLMs are Imperfect, Then What? An Empirical Study on LLM Failures in\n  Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Imperfect, Then What? An Empirical Study on LLM Failures in\n  Software Engineering"
                },
                "summary": "Software engineers are integrating AI assistants into their workflows to\nenhance productivity and reduce cognitive strain. However, experiences vary\nsignificantly, with some engineers finding large language models (LLMs), like\nChatGPT, beneficial, while others consider them counterproductive. Researchers\nalso found that ChatGPT's answers included incorrect information. Given the\nfact that LLMs are still imperfect, it is important to understand how to best\nincorporate LLMs into the workflow for software engineering (SE) task\ncompletion. Therefore, we conducted an observational study with 22 participants\nusing ChatGPT as a coding assistant in a non-trivial SE task to understand the\npractices, challenges, and opportunities for using LLMs for SE tasks. We\nidentified the cases where ChatGPT failed, their root causes, and the\ncorresponding mitigation solutions used by users. These findings contribute to\nthe overall understanding and strategies for human-AI interaction on SE tasks.\nOur study also highlights future research and tooling support directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software engineers are integrating AI assistants into their workflows to\nenhance productivity and reduce cognitive strain. However, experiences vary\nsignificantly, with some engineers finding large language models (LLMs), like\nChatGPT, beneficial, while others consider them counterproductive. Researchers\nalso found that ChatGPT's answers included incorrect information. Given the\nfact that LLMs are still imperfect, it is important to understand how to best\nincorporate LLMs into the workflow for software engineering (SE) task\ncompletion. Therefore, we conducted an observational study with 22 participants\nusing ChatGPT as a coding assistant in a non-trivial SE task to understand the\npractices, challenges, and opportunities for using LLMs for SE tasks. We\nidentified the cases where ChatGPT failed, their root causes, and the\ncorresponding mitigation solutions used by users. These findings contribute to\nthe overall understanding and strategies for human-AI interaction on SE tasks.\nOur study also highlights future research and tooling support directions."
                },
                "authors": [
                    {
                        "name": "Jiessie Tie"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Tianshi Li"
                    },
                    {
                        "name": "Syed Ishtiaque Ahmed"
                    },
                    {
                        "name": "Dakuo Wang"
                    },
                    {
                        "name": "Shurui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shurui Zhou"
                },
                "author": "Shurui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17743v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17743v3",
                "updated": "2024-11-15T03:25:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    25,
                    40,
                    4,
                    320,
                    0
                ],
                "published": "2024-05-28T01:55:35Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    1,
                    55,
                    35,
                    1,
                    149,
                    0
                ],
                "title": "ORLM: A Customizable Framework in Training Large Models for Automated\n  Optimization Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORLM: A Customizable Framework in Training Large Models for Automated\n  Optimization Modeling"
                },
                "summary": "Optimization modeling and solving play a critical role in the application of\nOperations Research (OR) tools to address real-world problems, yet they pose\nchallenges and require extensive expertise from OR experts. With the advent of\nlarge language models (LLMs), new opportunities have emerged to streamline and\nautomate these tasks. However, current research predominantly relies on\nclosed-source LLMs such as GPT-4, along with extensive prompt engineering\ntechniques. This reliance stems from the scarcity of high-quality training\ndatasets for optimization modeling, resulting in elevated costs, prolonged\nprocessing times, and privacy concerns. To address these challenges, our work\nis the first to propose a viable path for training open-source LLMs that are\ncapable of optimization modeling as well as developing and executing solver\ncodes, eventually leading to a superior ability for automating optimization\nmodeling and solving. Particularly, we introduce a semi-automated data\nsynthesis framework designed for optimization modeling issues, named\nOR-Instruct. This framework merges the training data requirements of large\nmodels with the unique characteristics of optimization modeling problems, and\nallows for customizable enhancements tailored to specific scenarios or modeling\ntypes. To evaluate the performance of our proposed framework, we present the\nIndustryOR benchmark, the inaugural industrial standard for evaluating LLMs in\nsolving practical OR problems. Utilizing data synthesized through OR-Instruct,\nwe train various open-source LLMs with a capacity of 7 billion parameters\n(dubbed ORLMs). The resulting model demonstrates significantly enhanced\noptimization modeling capabilities, achieving state-of-the-art performance\nacross the NL4OPT, MAMO, and IndustryOR benchmarks. Our code and data are\navailable at \\url{https://github.com/Cardinal-Operations/ORLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization modeling and solving play a critical role in the application of\nOperations Research (OR) tools to address real-world problems, yet they pose\nchallenges and require extensive expertise from OR experts. With the advent of\nlarge language models (LLMs), new opportunities have emerged to streamline and\nautomate these tasks. However, current research predominantly relies on\nclosed-source LLMs such as GPT-4, along with extensive prompt engineering\ntechniques. This reliance stems from the scarcity of high-quality training\ndatasets for optimization modeling, resulting in elevated costs, prolonged\nprocessing times, and privacy concerns. To address these challenges, our work\nis the first to propose a viable path for training open-source LLMs that are\ncapable of optimization modeling as well as developing and executing solver\ncodes, eventually leading to a superior ability for automating optimization\nmodeling and solving. Particularly, we introduce a semi-automated data\nsynthesis framework designed for optimization modeling issues, named\nOR-Instruct. This framework merges the training data requirements of large\nmodels with the unique characteristics of optimization modeling problems, and\nallows for customizable enhancements tailored to specific scenarios or modeling\ntypes. To evaluate the performance of our proposed framework, we present the\nIndustryOR benchmark, the inaugural industrial standard for evaluating LLMs in\nsolving practical OR problems. Utilizing data synthesized through OR-Instruct,\nwe train various open-source LLMs with a capacity of 7 billion parameters\n(dubbed ORLMs). The resulting model demonstrates significantly enhanced\noptimization modeling capabilities, achieving state-of-the-art performance\nacross the NL4OPT, MAMO, and IndustryOR benchmarks. Our code and data are\navailable at \\url{https://github.com/Cardinal-Operations/ORLM}."
                },
                "authors": [
                    {
                        "name": "Chenyu Huang"
                    },
                    {
                        "name": "Zhengyang Tang"
                    },
                    {
                        "name": "Dongdong Ge"
                    },
                    {
                        "name": "Shixi Hu"
                    },
                    {
                        "name": "Ruoqing Jiang"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Zizhuo Wang"
                    },
                    {
                        "name": "Xin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zheng"
                },
                "author": "Xin Zheng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17743v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17743v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07571v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07571v2",
                "updated": "2024-11-15T03:20:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    20,
                    57,
                    4,
                    320,
                    0
                ],
                "published": "2024-10-10T03:12:03Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    12,
                    3,
                    3,
                    284,
                    0
                ],
                "title": "How Does Vision-Language Adaptation Impact the Safety of Vision Language\n  Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Does Vision-Language Adaptation Impact the Safety of Vision Language\n  Models?"
                },
                "summary": "Vision-Language adaptation (VL adaptation) transforms Large Language Models\n(LLMs) into Large Vision-Language Models (LVLMs) for multimodal tasks, but this\nprocess often compromises the inherent safety capabilities embedded in the\noriginal LLMs. Despite potential harmfulness due to weakened safety measures,\nin-depth analysis on the effects of VL adaptation on safety remains\nunder-explored. This study examines how VL adaptation influences safety and\nevaluates the impact of safety fine-tuning methods. Our analysis reveals that\nsafety degradation occurs during VL adaptation, even when the training data is\nsafe. While safety tuning techniques like supervised fine-tuning with safety\ndatasets or reinforcement learning from human feedback mitigate some risks,\nthey still lead to safety degradation and a reduction in helpfulness due to\nover-rejection issues. Further analysis of internal model weights suggests that\nVL adaptation may impact certain safety-related layers, potentially lowering\noverall safety levels. Additionally, our findings demonstrate that the\nobjectives of VL adaptation and safety tuning are divergent, which often\nresults in their simultaneous application being suboptimal. To address this, we\nsuggest the weight merging approach as an optimal solution effectively reducing\nsafety degradation while maintaining helpfulness. These insights help guide the\ndevelopment of more reliable and secure LVLMs for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language adaptation (VL adaptation) transforms Large Language Models\n(LLMs) into Large Vision-Language Models (LVLMs) for multimodal tasks, but this\nprocess often compromises the inherent safety capabilities embedded in the\noriginal LLMs. Despite potential harmfulness due to weakened safety measures,\nin-depth analysis on the effects of VL adaptation on safety remains\nunder-explored. This study examines how VL adaptation influences safety and\nevaluates the impact of safety fine-tuning methods. Our analysis reveals that\nsafety degradation occurs during VL adaptation, even when the training data is\nsafe. While safety tuning techniques like supervised fine-tuning with safety\ndatasets or reinforcement learning from human feedback mitigate some risks,\nthey still lead to safety degradation and a reduction in helpfulness due to\nover-rejection issues. Further analysis of internal model weights suggests that\nVL adaptation may impact certain safety-related layers, potentially lowering\noverall safety levels. Additionally, our findings demonstrate that the\nobjectives of VL adaptation and safety tuning are divergent, which often\nresults in their simultaneous application being suboptimal. To address this, we\nsuggest the weight merging approach as an optimal solution effectively reducing\nsafety degradation while maintaining helpfulness. These insights help guide the\ndevelopment of more reliable and secure LVLMs for real-world applications."
                },
                "authors": [
                    {
                        "name": "Seongyun Lee"
                    },
                    {
                        "name": "Geewook Kim"
                    },
                    {
                        "name": "Jiyeon Kim"
                    },
                    {
                        "name": "Hyunji Lee"
                    },
                    {
                        "name": "Hoyeon Chang"
                    },
                    {
                        "name": "Sue Hyun Park"
                    },
                    {
                        "name": "Minjoon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Minjoon Seo"
                },
                "author": "Minjoon Seo",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07571v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07571v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09913v1",
                "updated": "2024-11-15T03:20:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    20,
                    24,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T03:20:24Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    20,
                    24,
                    4,
                    320,
                    0
                ],
                "title": "A Graph-based Strategic Sensor Deployment Approach for k-coverage in WSN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Graph-based Strategic Sensor Deployment Approach for k-coverage in WSN"
                },
                "summary": "This paper studies a graph-based sensor deployment approach in wireless\nsensor networks (WSNs). Specifically, in today's world, where sensors are\neverywhere, detecting various attributes like temperature and movement, their\ndeteriorating lifetime is indeed a very concerning issue. In many scenarios,\nthese sensors are placed in extremely remote areas, where maintenance becomes\nchallenging. As a result, it is not very wise to depend on a single sensor to\nobtain data from a particular terrain or place. Hence, multiple sensors are\ndeployed in these places, such that no problem arises if one or few of them\nfail. In this work, this problem of intelligent placement of sensors is\nmodelled from the graph theoretic point of view. We propose a new sensor\ndeployment approach here, which results in lesser sensor density per unit area\nand less number of sensors as compared to the existing benchmark schemes.\nFinally, the numerical results also support our claims and provide insights\nregarding the selection of parameters that enhance the system performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a graph-based sensor deployment approach in wireless\nsensor networks (WSNs). Specifically, in today's world, where sensors are\neverywhere, detecting various attributes like temperature and movement, their\ndeteriorating lifetime is indeed a very concerning issue. In many scenarios,\nthese sensors are placed in extremely remote areas, where maintenance becomes\nchallenging. As a result, it is not very wise to depend on a single sensor to\nobtain data from a particular terrain or place. Hence, multiple sensors are\ndeployed in these places, such that no problem arises if one or few of them\nfail. In this work, this problem of intelligent placement of sensors is\nmodelled from the graph theoretic point of view. We propose a new sensor\ndeployment approach here, which results in lesser sensor density per unit area\nand less number of sensors as compared to the existing benchmark schemes.\nFinally, the numerical results also support our claims and provide insights\nregarding the selection of parameters that enhance the system performance."
                },
                "authors": [
                    {
                        "name": "Lakshmikanta Sau"
                    },
                    {
                        "name": "Priyadarshi Mukherjee"
                    },
                    {
                        "name": "Sasthi C. Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Sasthi C. Ghosh"
                },
                "author": "Sasthi C. Ghosh",
                "arxiv_comment": "Submitted for a possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09909v1",
                "updated": "2024-11-15T03:11:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    11,
                    19,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T03:11:19Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    11,
                    19,
                    4,
                    320,
                    0
                ],
                "title": "AMXFP4: Taming Activation Outliers with Asymmetric Microscaling\n  Floating-Point for 4-bit LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMXFP4: Taming Activation Outliers with Asymmetric Microscaling\n  Floating-Point for 4-bit LLM Inference"
                },
                "summary": "Scaling Large Language Models (LLMs) with extended context lengths has\nincreased the need for efficient low-bit quantization to manage their\nsubstantial computational demands. However, reducing precision to 4 bits\nfrequently degrades performance due to activation outliers. To address this, we\npropose Asymmetric Microscaling 4-bit Floating-Point (AMXFP4) for efficient LLM\ninference. This novel data format leverages asymmetric shared scales to\nmitigate outliers while naturally capturing the asymmetry introduced by\ngroup-wise quantization. Unlike conventional 4-bit quantization methods that\nrely on data rotation and costly calibration, AMXFP4 uses asymmetric shared\nscales for direct 4-bit casting, achieving near-ideal quantization accuracy\nacross various LLM tasks, including multi-turn conversations, long-context\nreasoning, and visual question answering. Our AMXFP4 format significantly\noutperforms MXFP4 and other leading quantization techniques, enabling robust,\ncalibration-free 4-bit inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Large Language Models (LLMs) with extended context lengths has\nincreased the need for efficient low-bit quantization to manage their\nsubstantial computational demands. However, reducing precision to 4 bits\nfrequently degrades performance due to activation outliers. To address this, we\npropose Asymmetric Microscaling 4-bit Floating-Point (AMXFP4) for efficient LLM\ninference. This novel data format leverages asymmetric shared scales to\nmitigate outliers while naturally capturing the asymmetry introduced by\ngroup-wise quantization. Unlike conventional 4-bit quantization methods that\nrely on data rotation and costly calibration, AMXFP4 uses asymmetric shared\nscales for direct 4-bit casting, achieving near-ideal quantization accuracy\nacross various LLM tasks, including multi-turn conversations, long-context\nreasoning, and visual question answering. Our AMXFP4 format significantly\noutperforms MXFP4 and other leading quantization techniques, enabling robust,\ncalibration-free 4-bit inference."
                },
                "authors": [
                    {
                        "name": "Janghwan Lee"
                    },
                    {
                        "name": "Jiwoong Park"
                    },
                    {
                        "name": "Jinseok Kim"
                    },
                    {
                        "name": "Yongjik Kim"
                    },
                    {
                        "name": "Jungju Oh"
                    },
                    {
                        "name": "Jinwook Oh"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09410v2",
                "updated": "2024-11-15T02:53:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    2,
                    53,
                    36,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-14T13:00:23Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    0,
                    23,
                    3,
                    319,
                    0
                ],
                "title": "LLM-assisted Explicit and Implicit Multi-interest Learning Framework for\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-assisted Explicit and Implicit Multi-interest Learning Framework for\n  Sequential Recommendation"
                },
                "summary": "Multi-interest modeling in current recommender systems (RS) is mainly based\non user behavioral data, capturing user interest preferences from multiple\ndimensions. However, since behavioral data is implicit and often highly sparse,\nit is challenging to understand users' complex and diverse interests. Recent\nstudies have shown that the rich semantic information in the text can\neffectively supplement the deficiencies of behavioral data. Despite this, it is\nstill difficult for small models to directly extract semantic features\nassociated with users' deep interests. That is, how to effectively align\nsemantics with behavioral information to form a more comprehensive and accurate\nunderstanding of user interests has become a critical research problem. To\naddress this, we propose an LLM-assisted explicit and implicit multi-interest\nlearning framework (named EIMF) to model user interests on two levels: behavior\nand semantics. The framework consists of two parts: Implicit Behavioral\nInterest Module (IBIM) and Explicit Semantic Interest Module (ESIM). The\ntraditional multi-interest RS model in IBIM can learn users' implicit\nbehavioral interests from interactions with items. In ESIM, we first adopt a\nclustering algorithm to select typical samples and design a prompting strategy\non LLM to obtain explicit semantic interests. Furthermore, in the training\nphase, the semantic interests of typical samples can enhance the representation\nlearning of behavioral interests based on the multi-task learning on semantic\nprediction and modality alignment. Therefore, in the inference stage, accurate\nrecommendations can be achieved with only the user's behavioral data. Extensive\nexperiments on real-world datasets demonstrate the effectiveness of the\nproposed EIMF framework, which effectively and efficiently combines small\nmodels with LLM to improve the accuracy of multi-interest modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-interest modeling in current recommender systems (RS) is mainly based\non user behavioral data, capturing user interest preferences from multiple\ndimensions. However, since behavioral data is implicit and often highly sparse,\nit is challenging to understand users' complex and diverse interests. Recent\nstudies have shown that the rich semantic information in the text can\neffectively supplement the deficiencies of behavioral data. Despite this, it is\nstill difficult for small models to directly extract semantic features\nassociated with users' deep interests. That is, how to effectively align\nsemantics with behavioral information to form a more comprehensive and accurate\nunderstanding of user interests has become a critical research problem. To\naddress this, we propose an LLM-assisted explicit and implicit multi-interest\nlearning framework (named EIMF) to model user interests on two levels: behavior\nand semantics. The framework consists of two parts: Implicit Behavioral\nInterest Module (IBIM) and Explicit Semantic Interest Module (ESIM). The\ntraditional multi-interest RS model in IBIM can learn users' implicit\nbehavioral interests from interactions with items. In ESIM, we first adopt a\nclustering algorithm to select typical samples and design a prompting strategy\non LLM to obtain explicit semantic interests. Furthermore, in the training\nphase, the semantic interests of typical samples can enhance the representation\nlearning of behavioral interests based on the multi-task learning on semantic\nprediction and modality alignment. Therefore, in the inference stage, accurate\nrecommendations can be achieved with only the user's behavioral data. Extensive\nexperiments on real-world datasets demonstrate the effectiveness of the\nproposed EIMF framework, which effectively and efficiently combines small\nmodels with LLM to improve the accuracy of multi-interest modeling."
                },
                "authors": [
                    {
                        "name": "Shutong Qiao"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09318v2",
                "updated": "2024-11-15T02:42:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    2,
                    42,
                    59,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-14T10:00:33Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    0,
                    33,
                    3,
                    319,
                    0
                ],
                "title": "DriveThru: a Document Extraction Platform and Benchmark Datasets for\n  Indonesian Local Language Archives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriveThru: a Document Extraction Platform and Benchmark Datasets for\n  Indonesian Local Language Archives"
                },
                "summary": "Indonesia is one of the most diverse countries linguistically. However,\ndespite this linguistic diversity, Indonesian languages remain underrepresented\nin Natural Language Processing (NLP) research and technologies. In the past two\nyears, several efforts have been conducted to construct NLP resources for\nIndonesian languages. However, most of these efforts have been focused on\ncreating manual resources thus difficult to scale to more languages. Although\nmany Indonesian languages do not have a web presence, locally there are\nresources that document these languages well in printed forms such as books,\nmagazines, and newspapers. Digitizing these existing resources will enable\nscaling of Indonesian language resource construction to many more languages. In\nthis paper, we propose an alternative method of creating datasets by digitizing\ndocuments, which have not previously been used to build digital language\nresources in Indonesia. DriveThru is a platform for extracting document content\nutilizing Optical Character Recognition (OCR) techniques in its system to\nprovide language resource building with less manual effort and cost. This paper\nalso studies the utility of current state-of-the-art LLM for post-OCR\ncorrection to show the capability of increasing the character accuracy rate\n(CAR) and word accuracy rate (WAR) compared to off-the-shelf OCR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indonesia is one of the most diverse countries linguistically. However,\ndespite this linguistic diversity, Indonesian languages remain underrepresented\nin Natural Language Processing (NLP) research and technologies. In the past two\nyears, several efforts have been conducted to construct NLP resources for\nIndonesian languages. However, most of these efforts have been focused on\ncreating manual resources thus difficult to scale to more languages. Although\nmany Indonesian languages do not have a web presence, locally there are\nresources that document these languages well in printed forms such as books,\nmagazines, and newspapers. Digitizing these existing resources will enable\nscaling of Indonesian language resource construction to many more languages. In\nthis paper, we propose an alternative method of creating datasets by digitizing\ndocuments, which have not previously been used to build digital language\nresources in Indonesia. DriveThru is a platform for extracting document content\nutilizing Optical Character Recognition (OCR) techniques in its system to\nprovide language resource building with less manual effort and cost. This paper\nalso studies the utility of current state-of-the-art LLM for post-OCR\ncorrection to show the capability of increasing the character accuracy rate\n(CAR) and word accuracy rate (WAR) compared to off-the-shelf OCR."
                },
                "authors": [
                    {
                        "name": "Mohammad Rifqi Farhansyah"
                    },
                    {
                        "name": "Muhammad Zuhdi Fikri Johari"
                    },
                    {
                        "name": "Afinzaki Amiral"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    },
                    {
                        "name": "Kumara Ari Yuana"
                    },
                    {
                        "name": "Derry Tanti Wijaya"
                    }
                ],
                "author_detail": {
                    "name": "Derry Tanti Wijaya"
                },
                "author": "Derry Tanti Wijaya",
                "arxiv_comment": "12 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09891v1",
                "updated": "2024-11-15T02:35:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    2,
                    35,
                    20,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T02:35:20Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    2,
                    35,
                    20,
                    4,
                    320,
                    0
                ],
                "title": "Off-Dynamics Reinforcement Learning via Domain Adaptation and Reward\n  Augmented Imitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off-Dynamics Reinforcement Learning via Domain Adaptation and Reward\n  Augmented Imitation"
                },
                "summary": "Training a policy in a source domain for deployment in the target domain\nunder a dynamics shift can be challenging, often resulting in performance\ndegradation. Previous work tackles this challenge by training on the source\ndomain with modified rewards derived by matching distributions between the\nsource and the target optimal trajectories. However, pure modified rewards only\nensure the behavior of the learned policy in the source domain resembles\ntrajectories produced by the target optimal policies, which does not guarantee\noptimal performance when the learned policy is actually deployed to the target\ndomain. In this work, we propose to utilize imitation learning to transfer the\npolicy learned from the reward modification to the target domain so that the\nnew policy can generate the same trajectories in the target domain. Our\napproach, Domain Adaptation and Reward Augmented Imitation Learning (DARAIL),\nutilizes the reward modification for domain adaptation and follows the general\nframework of generative adversarial imitation learning from observation (GAIfO)\nby applying a reward augmented estimator for the policy optimization step.\nTheoretically, we present an error bound for our method under a mild assumption\nregarding the dynamics shift to justify the motivation of our method.\nEmpirically, our method outperforms the pure modified reward method without\nimitation learning and also outperforms other baselines in benchmark\noff-dynamics environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training a policy in a source domain for deployment in the target domain\nunder a dynamics shift can be challenging, often resulting in performance\ndegradation. Previous work tackles this challenge by training on the source\ndomain with modified rewards derived by matching distributions between the\nsource and the target optimal trajectories. However, pure modified rewards only\nensure the behavior of the learned policy in the source domain resembles\ntrajectories produced by the target optimal policies, which does not guarantee\noptimal performance when the learned policy is actually deployed to the target\ndomain. In this work, we propose to utilize imitation learning to transfer the\npolicy learned from the reward modification to the target domain so that the\nnew policy can generate the same trajectories in the target domain. Our\napproach, Domain Adaptation and Reward Augmented Imitation Learning (DARAIL),\nutilizes the reward modification for domain adaptation and follows the general\nframework of generative adversarial imitation learning from observation (GAIfO)\nby applying a reward augmented estimator for the policy optimization step.\nTheoretically, we present an error bound for our method under a mild assumption\nregarding the dynamics shift to justify the motivation of our method.\nEmpirically, our method outperforms the pure modified reward method without\nimitation learning and also outperforms other baselines in benchmark\noff-dynamics environments."
                },
                "authors": [
                    {
                        "name": "Yihong Guo"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Yuanyuan Shi"
                    },
                    {
                        "name": "Pan Xu"
                    },
                    {
                        "name": "Anqi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Anqi Liu"
                },
                "author": "Anqi Liu",
                "arxiv_comment": "Published at Neurips 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18027v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18027v2",
                "updated": "2024-11-15T02:07:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    2,
                    7,
                    34,
                    4,
                    320,
                    0
                ],
                "published": "2024-06-26T02:49:28Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    2,
                    49,
                    28,
                    2,
                    178,
                    0
                ],
                "title": "Automated Clinical Data Extraction with Knowledge Conditioned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Clinical Data Extraction with Knowledge Conditioned LLMs"
                },
                "summary": "The extraction of lung lesion information from clinical and medical imaging\nreports is crucial for research on and clinical care of lung-related diseases.\nLarge language models (LLMs) can be effective at interpreting unstructured text\nin reports, but they often hallucinate due to a lack of domain-specific\nknowledge, leading to reduced accuracy and posing challenges for use in\nclinical settings. To address this, we propose a novel framework that aligns\ngenerated internal knowledge with external knowledge through in-context\nlearning (ICL). Our framework employs a retriever to identify relevant units of\ninternal or external knowledge and a grader to evaluate the truthfulness and\nhelpfulness of the retrieved internal-knowledge rules, to align and update the\nknowledge bases. Experiments with expert-curated test datasets demonstrate that\nthis ICL approach can increase the F1 score for key fields (lesion size, margin\nand solidity) by an average of 12.9% over existing ICL methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extraction of lung lesion information from clinical and medical imaging\nreports is crucial for research on and clinical care of lung-related diseases.\nLarge language models (LLMs) can be effective at interpreting unstructured text\nin reports, but they often hallucinate due to a lack of domain-specific\nknowledge, leading to reduced accuracy and posing challenges for use in\nclinical settings. To address this, we propose a novel framework that aligns\ngenerated internal knowledge with external knowledge through in-context\nlearning (ICL). Our framework employs a retriever to identify relevant units of\ninternal or external knowledge and a grader to evaluate the truthfulness and\nhelpfulness of the retrieved internal-knowledge rules, to align and update the\nknowledge bases. Experiments with expert-curated test datasets demonstrate that\nthis ICL approach can increase the F1 score for key fields (lesion size, margin\nand solidity) by an average of 12.9% over existing ICL methods."
                },
                "authors": [
                    {
                        "name": "Diya Li"
                    },
                    {
                        "name": "Asim Kadav"
                    },
                    {
                        "name": "Aijing Gao"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Richard Bourgon"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bourgon"
                },
                "author": "Richard Bourgon",
                "arxiv_comment": "COLING25 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18027v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18027v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09874v1",
                "updated": "2024-11-15T01:49:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    1,
                    49,
                    17,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T01:49:17Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    1,
                    49,
                    17,
                    4,
                    320,
                    0
                ],
                "title": "A Hybrid Artificial Intelligence System for Automated EEG Background\n  Analysis and Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid Artificial Intelligence System for Automated EEG Background\n  Analysis and Report Generation"
                },
                "summary": "Electroencephalography (EEG) plays a crucial role in the diagnosis of various\nneurological disorders. However, small hospitals and clinics often lack\nadvanced EEG signal analysis systems and are prone to misinterpretation in\nmanual EEG reading. This study proposes an innovative hybrid artificial\nintelligence (AI) system for automatic interpretation of EEG background\nactivity and report generation. The system combines deep learning models for\nposterior dominant rhythm (PDR) prediction, unsupervised artifact removal, and\nexpert-designed algorithms for abnormality detection. For PDR prediction, 1530\nlabeled EEGs were used, and the best ensemble model achieved a mean absolute\nerror (MAE) of 0.237, a root mean square error (RMSE) of 0.359, an accuracy of\n91.8% within a 0.6Hz error, and an accuracy of 99% within a 1.2Hz error. The AI\nsystem significantly outperformed neurologists in detecting generalized\nbackground slowing (p = 0.02; F1: AI 0.93, neurologists 0.82) and demonstrated\nimproved focal abnormality detection, although not statistically significant (p\n= 0.79; F1: AI 0.71, neurologists 0.55). Validation on both an internal dataset\nand the Temple University Abnormal EEG Corpus showed consistent performance\n(F1: 0.884 and 0.835, respectively; p = 0.66), demonstrating generalizability.\nThe use of large language models (LLMs) for report generation demonstrated 100%\naccuracy, verified by three other independent LLMs. This hybrid AI system\nprovides an easily scalable and accurate solution for EEG interpretation in\nresource-limited settings, assisting neurologists in improving diagnostic\naccuracy and reducing misdiagnosis rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electroencephalography (EEG) plays a crucial role in the diagnosis of various\nneurological disorders. However, small hospitals and clinics often lack\nadvanced EEG signal analysis systems and are prone to misinterpretation in\nmanual EEG reading. This study proposes an innovative hybrid artificial\nintelligence (AI) system for automatic interpretation of EEG background\nactivity and report generation. The system combines deep learning models for\nposterior dominant rhythm (PDR) prediction, unsupervised artifact removal, and\nexpert-designed algorithms for abnormality detection. For PDR prediction, 1530\nlabeled EEGs were used, and the best ensemble model achieved a mean absolute\nerror (MAE) of 0.237, a root mean square error (RMSE) of 0.359, an accuracy of\n91.8% within a 0.6Hz error, and an accuracy of 99% within a 1.2Hz error. The AI\nsystem significantly outperformed neurologists in detecting generalized\nbackground slowing (p = 0.02; F1: AI 0.93, neurologists 0.82) and demonstrated\nimproved focal abnormality detection, although not statistically significant (p\n= 0.79; F1: AI 0.71, neurologists 0.55). Validation on both an internal dataset\nand the Temple University Abnormal EEG Corpus showed consistent performance\n(F1: 0.884 and 0.835, respectively; p = 0.66), demonstrating generalizability.\nThe use of large language models (LLMs) for report generation demonstrated 100%\naccuracy, verified by three other independent LLMs. This hybrid AI system\nprovides an easily scalable and accurate solution for EEG interpretation in\nresource-limited settings, assisting neurologists in improving diagnostic\naccuracy and reducing misdiagnosis rates."
                },
                "authors": [
                    {
                        "name": "Chin-Sung Tung"
                    },
                    {
                        "name": "Sheng-Fu Liang"
                    },
                    {
                        "name": "Shu-Feng Chang"
                    },
                    {
                        "name": "Chung-Ping Young"
                    }
                ],
                "author_detail": {
                    "name": "Chung-Ping Young"
                },
                "author": "Chung-Ping Young",
                "arxiv_doi": "10.1109/JBHI.2024.3496996",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JBHI.2024.3496996",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.09874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Example code available at https://github.com/tcs211/AI_EEEG_REPORT",
                "arxiv_journal_ref": "IEEE Journal of Biomedical and Health Informatics (2024)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09873v1",
                "updated": "2024-11-15T01:48:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    1,
                    48,
                    8,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T01:48:08Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    1,
                    48,
                    8,
                    4,
                    320,
                    0
                ],
                "title": "LLM-Powered AI Tutors with Personas for d/Deaf and Hard-of-Hearing\n  Online Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered AI Tutors with Personas for d/Deaf and Hard-of-Hearing\n  Online Learners"
                },
                "summary": "Intelligent tutoring systems (ITS) using artificial intelligence (AI)\ntechnology have shown promise in supporting learners with diverse abilities;\nhowever, they often fail to meet the specific communication needs and cultural\nnuances needed by d/Deaf and Hard-of-Hearing (DHH) learners. As large language\nmodels (LLMs) provide new opportunities to incorporate personas to AI-based\ntutors and support dynamic interactive dialogue, this paper explores how DHH\nlearners perceive LLM-powered ITS with different personas and identified design\nsuggestions for improving the interaction. We developed an interface that\nallows DHH learners to interact with ChatGPT and three LLM-powered AI tutors\nwith different experiences in DHH education while the learners watch an\neducational video. A user study with 16 DHH participants showed that they\nperceived conversations with the AI tutors who had DHH education experiences to\nbe more human-like and trustworthy due to the tutors' cultural knowledge of DHH\ncommunities. Participants also suggested providing more transparency regarding\nthe tutors' background information to clarify each AI tutor's position within\nthe DHH community. We discuss design implications for more inclusive LLM-based\nsystems, such as supports for the multimodality of sign language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent tutoring systems (ITS) using artificial intelligence (AI)\ntechnology have shown promise in supporting learners with diverse abilities;\nhowever, they often fail to meet the specific communication needs and cultural\nnuances needed by d/Deaf and Hard-of-Hearing (DHH) learners. As large language\nmodels (LLMs) provide new opportunities to incorporate personas to AI-based\ntutors and support dynamic interactive dialogue, this paper explores how DHH\nlearners perceive LLM-powered ITS with different personas and identified design\nsuggestions for improving the interaction. We developed an interface that\nallows DHH learners to interact with ChatGPT and three LLM-powered AI tutors\nwith different experiences in DHH education while the learners watch an\neducational video. A user study with 16 DHH participants showed that they\nperceived conversations with the AI tutors who had DHH education experiences to\nbe more human-like and trustworthy due to the tutors' cultural knowledge of DHH\ncommunities. Participants also suggested providing more transparency regarding\nthe tutors' background information to clarify each AI tutor's position within\nthe DHH community. We discuss design implications for more inclusive LLM-based\nsystems, such as supports for the multimodality of sign language."
                },
                "authors": [
                    {
                        "name": "Haocong Cheng"
                    },
                    {
                        "name": "Si Chen"
                    },
                    {
                        "name": "Christopher Perdriau"
                    },
                    {
                        "name": "Yun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yun Huang"
                },
                "author": "Yun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14567v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14567v3",
                "updated": "2024-11-15T00:24:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    24,
                    0,
                    4,
                    320,
                    0
                ],
                "published": "2024-05-23T13:43:29Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    13,
                    43,
                    29,
                    3,
                    144,
                    0
                ],
                "title": "EHRMamba: Towards Generalizable and Scalable Foundation Models for\n  Electronic Health Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EHRMamba: Towards Generalizable and Scalable Foundation Models for\n  Electronic Health Records"
                },
                "summary": "Transformers have significantly advanced the modeling of Electronic Health\nRecords (EHR), yet their deployment in real-world healthcare is limited by\nseveral key challenges. Firstly, the quadratic computational cost and\ninsufficient context length of these models hinder hospitals' ability in\nprocessing the extensive medical histories typical in EHR data. Additionally,\nexisting models employ separate finetuning for each clinical task, complicating\nmaintenance in healthcare environments. Moreover, these models focus\nexclusively on either clinical prediction or EHR forecasting, lacking\nproficiency in both tasks. To overcome these limitations, we introduce\nEHRMamba, a robust foundation model built on the Mamba architecture. EHRMamba\ncan process sequences up to 300% longer than previous models due to its linear\ncomputational cost. We also introduce a novel approach to Multitask Prompted\nFinetuning (MPF) for EHR data, which enables EHRMamba to simultaneously learn\nmultiple clinical tasks in a single finetuning phase, significantly enhancing\ndeployment and cross-task generalization. Furthermore, our model leverages the\nHL7 FHIR data standard to simplify integration into existing hospital systems.\nAlongside EHRMamba, we open-source Odyssey, a toolkit designed to support the\ndevelopment and deployment of EHR foundation models, with an emphasis on data\nstandardization and interpretability. Our evaluations on the MIMIC-IV dataset\ndemonstrate that EHRMamba advances state-of-the-art performance across 6 major\nclinical tasks and excels in EHR forecasting, marking a significant leap\nforward in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have significantly advanced the modeling of Electronic Health\nRecords (EHR), yet their deployment in real-world healthcare is limited by\nseveral key challenges. Firstly, the quadratic computational cost and\ninsufficient context length of these models hinder hospitals' ability in\nprocessing the extensive medical histories typical in EHR data. Additionally,\nexisting models employ separate finetuning for each clinical task, complicating\nmaintenance in healthcare environments. Moreover, these models focus\nexclusively on either clinical prediction or EHR forecasting, lacking\nproficiency in both tasks. To overcome these limitations, we introduce\nEHRMamba, a robust foundation model built on the Mamba architecture. EHRMamba\ncan process sequences up to 300% longer than previous models due to its linear\ncomputational cost. We also introduce a novel approach to Multitask Prompted\nFinetuning (MPF) for EHR data, which enables EHRMamba to simultaneously learn\nmultiple clinical tasks in a single finetuning phase, significantly enhancing\ndeployment and cross-task generalization. Furthermore, our model leverages the\nHL7 FHIR data standard to simplify integration into existing hospital systems.\nAlongside EHRMamba, we open-source Odyssey, a toolkit designed to support the\ndevelopment and deployment of EHR foundation models, with an emphasis on data\nstandardization and interpretability. Our evaluations on the MIMIC-IV dataset\ndemonstrate that EHRMamba advances state-of-the-art performance across 6 major\nclinical tasks and excels in EHR forecasting, marking a significant leap\nforward in the field."
                },
                "authors": [
                    {
                        "name": "Adibvafa Fallahpour"
                    },
                    {
                        "name": "Mahshid Alinoori"
                    },
                    {
                        "name": "Wenqian Ye"
                    },
                    {
                        "name": "Xu Cao"
                    },
                    {
                        "name": "Arash Afkanpour"
                    },
                    {
                        "name": "Amrit Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "Amrit Krishnan"
                },
                "author": "Amrit Krishnan",
                "arxiv_comment": "17 Pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14567v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14567v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18856v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18856v2",
                "updated": "2024-11-15T00:15:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    15,
                    18,
                    4,
                    320,
                    0
                ],
                "published": "2024-10-24T15:41:56Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    41,
                    56,
                    3,
                    298,
                    0
                ],
                "title": "Demystifying Large Language Models for Medicine: A Primer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Large Language Models for Medicine: A Primer"
                },
                "summary": "Large language models (LLMs) represent a transformative class of AI tools\ncapable of revolutionizing various aspects of healthcare by generating\nhuman-like responses across diverse contexts and adapting to novel tasks\nfollowing human instructions. Their potential application spans a broad range\nof medical tasks, such as clinical documentation, matching patients to clinical\ntrials, and answering medical questions. In this primer paper, we propose an\nactionable guideline to help healthcare professionals more efficiently utilize\nLLMs in their work, along with a set of best practices. This approach consists\nof several main phases, including formulating the task, choosing LLMs, prompt\nengineering, fine-tuning, and deployment. We start with the discussion of\ncritical considerations in identifying healthcare tasks that align with the\ncore capabilities of LLMs and selecting models based on the selected task and\ndata, performance requirements, and model interface. We then review the\nstrategies, such as prompt engineering and fine-tuning, to adapt standard LLMs\nto specialized medical tasks. Deployment considerations, including regulatory\ncompliance, ethical guidelines, and continuous monitoring for fairness and\nbias, are also discussed. By providing a structured step-by-step methodology,\nthis tutorial aims to equip healthcare professionals with the tools necessary\nto effectively integrate LLMs into clinical practice, ensuring that these\npowerful technologies are applied in a safe, reliable, and impactful manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a transformative class of AI tools\ncapable of revolutionizing various aspects of healthcare by generating\nhuman-like responses across diverse contexts and adapting to novel tasks\nfollowing human instructions. Their potential application spans a broad range\nof medical tasks, such as clinical documentation, matching patients to clinical\ntrials, and answering medical questions. In this primer paper, we propose an\nactionable guideline to help healthcare professionals more efficiently utilize\nLLMs in their work, along with a set of best practices. This approach consists\nof several main phases, including formulating the task, choosing LLMs, prompt\nengineering, fine-tuning, and deployment. We start with the discussion of\ncritical considerations in identifying healthcare tasks that align with the\ncore capabilities of LLMs and selecting models based on the selected task and\ndata, performance requirements, and model interface. We then review the\nstrategies, such as prompt engineering and fine-tuning, to adapt standard LLMs\nto specialized medical tasks. Deployment considerations, including regulatory\ncompliance, ethical guidelines, and continuous monitoring for fairness and\nbias, are also discussed. By providing a structured step-by-step methodology,\nthis tutorial aims to equip healthcare professionals with the tools necessary\nto effectively integrate LLMs into clinical practice, ensuring that these\npowerful technologies are applied in a safe, reliable, and impactful manner."
                },
                "authors": [
                    {
                        "name": "Qiao Jin"
                    },
                    {
                        "name": "Nicholas Wan"
                    },
                    {
                        "name": "Robert Leaman"
                    },
                    {
                        "name": "Shubo Tian"
                    },
                    {
                        "name": "Zhizheng Wang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Guangzhi Xiong"
                    },
                    {
                        "name": "Po-Ting Lai"
                    },
                    {
                        "name": "Qingqing Zhu"
                    },
                    {
                        "name": "Benjamin Hou"
                    },
                    {
                        "name": "Maame Sarfo-Gyamfi"
                    },
                    {
                        "name": "Gongbo Zhang"
                    },
                    {
                        "name": "Aidan Gilson"
                    },
                    {
                        "name": "Balu Bhasuran"
                    },
                    {
                        "name": "Zhe He"
                    },
                    {
                        "name": "Aidong Zhang"
                    },
                    {
                        "name": "Jimeng Sun"
                    },
                    {
                        "name": "Chunhua Weng"
                    },
                    {
                        "name": "Ronald M. Summers"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Zhiyong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Lu"
                },
                "author": "Zhiyong Lu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18856v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18856v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10930v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10930v3",
                "updated": "2024-11-15T00:09:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    9,
                    44,
                    4,
                    320,
                    0
                ],
                "published": "2024-01-31T17:52:52Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    17,
                    52,
                    52,
                    2,
                    31,
                    0
                ],
                "title": "ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters"
                },
                "summary": "The self-attention mechanism distinguishes transformer-based large language\nmodels (LLMs) apart from convolutional and recurrent neural networks. Despite\nthe performance improvement, achieving real-time LLM inference on silicon\nremains challenging due to the extensive use of Softmax in self-attention. In\naddition to the non-linearity, the low arithmetic intensity significantly\nlimits processing parallelism, especially when working with longer contexts. To\naddress this challenge, we propose Constant Softmax (ConSmax), a\nsoftware-hardware co-design that serves as an efficient alternative to Softmax.\nConSmax utilizes differentiable normalization parameters to eliminate the need\nfor maximum searching and denominator summation in Softmax. This approach\nenables extensive parallelization while still executing the essential functions\nof Softmax. Moreover, a scalable ConSmax hardware design with a bitwidth-split\nlook-up table (LUT) can achieve lossless non-linear operations and support\nmixed-precision computing. Experimental results show that ConSmax achieves a\nminuscule power consumption of 0.2mW and an area of 0.0008mm^2 at 1250MHz\nworking frequency in 16nm FinFET technology. For open-source contribution, we\nfurther implement our design with the OpenROAD toolchain under SkyWater's 130nm\nCMOS technology. The corresponding power is 2.69mW and the area is 0.007mm^2.\nConSmax achieves 3.35x power savings and 2.75x area savings in 16nm technology,\nand 3.15x power savings and 4.14x area savings with the open-source EDA\ntoolchain. In the meantime, it also maintains comparable accuracy on the GPT-2\nmodel and the WikiText103 dataset. The project is available at\nhttps://github.com/ReaLLMASIC/ConSmax",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The self-attention mechanism distinguishes transformer-based large language\nmodels (LLMs) apart from convolutional and recurrent neural networks. Despite\nthe performance improvement, achieving real-time LLM inference on silicon\nremains challenging due to the extensive use of Softmax in self-attention. In\naddition to the non-linearity, the low arithmetic intensity significantly\nlimits processing parallelism, especially when working with longer contexts. To\naddress this challenge, we propose Constant Softmax (ConSmax), a\nsoftware-hardware co-design that serves as an efficient alternative to Softmax.\nConSmax utilizes differentiable normalization parameters to eliminate the need\nfor maximum searching and denominator summation in Softmax. This approach\nenables extensive parallelization while still executing the essential functions\nof Softmax. Moreover, a scalable ConSmax hardware design with a bitwidth-split\nlook-up table (LUT) can achieve lossless non-linear operations and support\nmixed-precision computing. Experimental results show that ConSmax achieves a\nminuscule power consumption of 0.2mW and an area of 0.0008mm^2 at 1250MHz\nworking frequency in 16nm FinFET technology. For open-source contribution, we\nfurther implement our design with the OpenROAD toolchain under SkyWater's 130nm\nCMOS technology. The corresponding power is 2.69mW and the area is 0.007mm^2.\nConSmax achieves 3.35x power savings and 2.75x area savings in 16nm technology,\nand 3.15x power savings and 4.14x area savings with the open-source EDA\ntoolchain. In the meantime, it also maintains comparable accuracy on the GPT-2\nmodel and the WikiText103 dataset. The project is available at\nhttps://github.com/ReaLLMASIC/ConSmax"
                },
                "authors": [
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Guanchen Tao"
                    },
                    {
                        "name": "Yifei Zou"
                    },
                    {
                        "name": "Derek Chow"
                    },
                    {
                        "name": "Zichen Fan"
                    },
                    {
                        "name": "Kauna Lei"
                    },
                    {
                        "name": "Bangfei Pan"
                    },
                    {
                        "name": "Dennis Sylvester"
                    },
                    {
                        "name": "Gregory Kielian"
                    },
                    {
                        "name": "Mehdi Saligane"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Saligane"
                },
                "author": "Mehdi Saligane",
                "arxiv_journal_ref": "International Conference on Computer-Aided Design 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10930v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10930v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01603v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01603v3",
                "updated": "2024-11-14T23:56:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    23,
                    56,
                    22,
                    3,
                    319,
                    0
                ],
                "published": "2024-06-26T17:33:21Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    17,
                    33,
                    21,
                    2,
                    178,
                    0
                ],
                "title": "A Review of Large Language Models and Autonomous Agents in Chemistry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of Large Language Models and Autonomous Agents in Chemistry"
                },
                "summary": "Large language models (LLMs) have emerged as powerful tools in chemistry,\nsignificantly impacting molecule design, property prediction, and synthesis\noptimization. This review highlights LLM capabilities in these domains and\ntheir potential to accelerate scientific discovery through automation. We also\nreview LLM-based autonomous agents: LLMs with a broader set of tools to\ninteract with their surrounding environment. These agents perform diverse tasks\nsuch as paper scraping, interfacing with automated laboratories, and synthesis\nplanning. As agents are an emerging topic, we extend the scope of our review of\nagents beyond chemistry and discuss across any scientific domains. This review\ncovers the recent history, current capabilities, and design of LLMs and\nautonomous agents, addressing specific challenges, opportunities, and future\ndirections in chemistry. Key challenges include data quality and integration,\nmodel interpretability, and the need for standard benchmarks, while future\ndirections point towards more sophisticated multi-modal agents and enhanced\ncollaboration between agents and experimental methods. Due to the quick pace of\nthis field, a repository has been built to keep track of the latest studies:\nhttps://github.com/ur-whitelab/LLMs-in-science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have emerged as powerful tools in chemistry,\nsignificantly impacting molecule design, property prediction, and synthesis\noptimization. This review highlights LLM capabilities in these domains and\ntheir potential to accelerate scientific discovery through automation. We also\nreview LLM-based autonomous agents: LLMs with a broader set of tools to\ninteract with their surrounding environment. These agents perform diverse tasks\nsuch as paper scraping, interfacing with automated laboratories, and synthesis\nplanning. As agents are an emerging topic, we extend the scope of our review of\nagents beyond chemistry and discuss across any scientific domains. This review\ncovers the recent history, current capabilities, and design of LLMs and\nautonomous agents, addressing specific challenges, opportunities, and future\ndirections in chemistry. Key challenges include data quality and integration,\nmodel interpretability, and the need for standard benchmarks, while future\ndirections point towards more sophisticated multi-modal agents and enhanced\ncollaboration between agents and experimental methods. Due to the quick pace of\nthis field, a repository has been built to keep track of the latest studies:\nhttps://github.com/ur-whitelab/LLMs-in-science."
                },
                "authors": [
                    {
                        "name": "Mayk Caldas Ramos"
                    },
                    {
                        "name": "Christopher J. Collison"
                    },
                    {
                        "name": "Andrew D. White"
                    }
                ],
                "author_detail": {
                    "name": "Andrew D. White"
                },
                "author": "Andrew D. White",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01603v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01603v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13800v2",
                "updated": "2024-11-14T23:53:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    23,
                    53,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-05-22T16:25:03Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    16,
                    25,
                    3,
                    2,
                    143,
                    0
                ],
                "title": "Dense Connector for MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dense Connector for MLLMs"
                },
                "summary": "Do we fully leverage the potential of visual encoder in Multimodal Large\nLanguage Models (MLLMs)? The recent outstanding performance of MLLMs in\nmultimodal understanding has garnered broad attention from both academia and\nindustry. In the current MLLM rat race, the focus seems to be predominantly on\nthe linguistic side. We witness the rise of larger and higher-quality\ninstruction datasets, as well as the involvement of larger-sized LLMs. Yet,\nscant attention has been directed towards the visual signals utilized by MLLMs,\noften assumed to be the final high-level features extracted by a frozen visual\nencoder. In this paper, we introduce the Dense Connector - a simple, effective,\nand plug-and-play vision-language connector that significantly enhances\nexisting MLLMs by leveraging multi-layer visual features, with minimal\nadditional computational overhead. Building on this, we also propose the\nEfficient Dense Connector, which achieves performance comparable to LLaVA-v1.5\nwith only 25% of the visual tokens. Furthermore, our model, trained solely on\nimages, showcases remarkable zero-shot capabilities in video understanding as\nwell. Experimental results across various vision encoders, image resolutions,\ntraining dataset scales, varying sizes of LLMs (2.7B->70B), and diverse\narchitectures of MLLMs (e.g., LLaVA-v1.5, LLaVA-NeXT and Mini-Gemini) validate\nthe versatility and scalability of our approach, achieving state-of-the-art\nperformance across 19 image and video benchmarks. We hope that this work will\nprovide valuable experience and serve as a basic module for future MLLM\ndevelopment. Code is available at https://github.com/HJYao00/DenseConnector .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do we fully leverage the potential of visual encoder in Multimodal Large\nLanguage Models (MLLMs)? The recent outstanding performance of MLLMs in\nmultimodal understanding has garnered broad attention from both academia and\nindustry. In the current MLLM rat race, the focus seems to be predominantly on\nthe linguistic side. We witness the rise of larger and higher-quality\ninstruction datasets, as well as the involvement of larger-sized LLMs. Yet,\nscant attention has been directed towards the visual signals utilized by MLLMs,\noften assumed to be the final high-level features extracted by a frozen visual\nencoder. In this paper, we introduce the Dense Connector - a simple, effective,\nand plug-and-play vision-language connector that significantly enhances\nexisting MLLMs by leveraging multi-layer visual features, with minimal\nadditional computational overhead. Building on this, we also propose the\nEfficient Dense Connector, which achieves performance comparable to LLaVA-v1.5\nwith only 25% of the visual tokens. Furthermore, our model, trained solely on\nimages, showcases remarkable zero-shot capabilities in video understanding as\nwell. Experimental results across various vision encoders, image resolutions,\ntraining dataset scales, varying sizes of LLMs (2.7B->70B), and diverse\narchitectures of MLLMs (e.g., LLaVA-v1.5, LLaVA-NeXT and Mini-Gemini) validate\nthe versatility and scalability of our approach, achieving state-of-the-art\nperformance across 19 image and video benchmarks. We hope that this work will\nprovide valuable experience and serve as a basic module for future MLLM\ndevelopment. Code is available at https://github.com/HJYao00/DenseConnector ."
                },
                "authors": [
                    {
                        "name": "Huanjin Yao"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Taojiannan Yang"
                    },
                    {
                        "name": "YuXin Song"
                    },
                    {
                        "name": "Mengxi Zhang"
                    },
                    {
                        "name": "Haocheng Feng"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Zhiheng Li"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Jingdong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingdong Wang"
                },
                "author": "Jingdong Wang",
                "arxiv_comment": "27 pages, NeurIPS 2024",
                "arxiv_journal_ref": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09838v1",
                "updated": "2024-11-14T23:11:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    23,
                    11,
                    45,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T23:11:45Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    23,
                    11,
                    45,
                    3,
                    319,
                    0
                ],
                "title": "OneNet: A Channel-Wise 1D Convolutional U-Net",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneNet: A Channel-Wise 1D Convolutional U-Net"
                },
                "summary": "Many state-of-the-art computer vision architectures leverage U-Net for its\nadaptability and efficient feature extraction. However, the multi-resolution\nconvolutional design often leads to significant computational demands, limiting\ndeployment on edge devices. We present a streamlined alternative: a 1D\nconvolutional encoder that retains accuracy while enhancing its suitability for\nedge applications. Our novel encoder architecture achieves semantic\nsegmentation through channel-wise 1D convolutions combined with pixel-unshuffle\noperations. By incorporating PixelShuffle, known for improving accuracy in\nsuper-resolution tasks while reducing computational load, OneNet captures\nspatial relationships without requiring 2D convolutions, reducing parameters by\nup to 47%. Additionally, we explore a fully 1D encoder-decoder that achieves a\n71% reduction in size, albeit with some accuracy loss. We benchmark our\napproach against U-Net variants across diverse mask-generation tasks,\ndemonstrating that it preserves accuracy effectively. Although focused on image\nsegmentation, this architecture is adaptable to other convolutional\napplications. Code for the project is available at\nhttps://github.com/shbyun080/OneNet .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many state-of-the-art computer vision architectures leverage U-Net for its\nadaptability and efficient feature extraction. However, the multi-resolution\nconvolutional design often leads to significant computational demands, limiting\ndeployment on edge devices. We present a streamlined alternative: a 1D\nconvolutional encoder that retains accuracy while enhancing its suitability for\nedge applications. Our novel encoder architecture achieves semantic\nsegmentation through channel-wise 1D convolutions combined with pixel-unshuffle\noperations. By incorporating PixelShuffle, known for improving accuracy in\nsuper-resolution tasks while reducing computational load, OneNet captures\nspatial relationships without requiring 2D convolutions, reducing parameters by\nup to 47%. Additionally, we explore a fully 1D encoder-decoder that achieves a\n71% reduction in size, albeit with some accuracy loss. We benchmark our\napproach against U-Net variants across diverse mask-generation tasks,\ndemonstrating that it preserves accuracy effectively. Although focused on image\nsegmentation, this architecture is adaptable to other convolutional\napplications. Code for the project is available at\nhttps://github.com/shbyun080/OneNet ."
                },
                "authors": [
                    {
                        "name": "Sanghyun Byun"
                    },
                    {
                        "name": "Kayvan Shah"
                    },
                    {
                        "name": "Ayushi Gang"
                    },
                    {
                        "name": "Christopher Apton"
                    },
                    {
                        "name": "Jacob Song"
                    },
                    {
                        "name": "Woo Seong Chung"
                    }
                ],
                "author_detail": {
                    "name": "Woo Seong Chung"
                },
                "author": "Woo Seong Chung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09837v1",
                "updated": "2024-11-14T23:02:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    23,
                    2,
                    30,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T23:02:30Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    23,
                    2,
                    30,
                    3,
                    319,
                    0
                ],
                "title": "Real-time Adapting Routing (RAR): Improving Efficiency Through\n  Continuous Learning in Software Powered by Layered Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Adapting Routing (RAR): Improving Efficiency Through\n  Continuous Learning in Software Powered by Layered Foundation Models"
                },
                "summary": "To balance the quality and inference cost of a Foundation Model (FM, such as\nlarge language models (LLMs)) powered software, people often opt to train a\nrouting model that routes requests to FMs with different sizes and\ncapabilities. Existing routing models rely on learning the optimal routing\ndecision from carefully curated data, require complex computations to be\nupdated, and do not consider the potential evolution of weaker FMs. In this\npaper, we propose Real-time Adaptive Routing (RAR), an approach to continuously\nadapt FM routing decisions while using guided in-context learning to enhance\nthe capabilities of weaker FM. The goal is to reduce reliance on stronger, more\nexpensive FMs. We evaluate our approach on different subsets of the popular\nMMLU benchmark. Over time, our approach routes 50.2% fewer requests to\ncomputationally expensive models while maintaining around 90.5% of the general\nresponse quality. In addition, the guides generated from stronger models have\nshown intra-domain generalization and led to a better quality of responses\ncompared to an equivalent approach with a standalone weaker FM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To balance the quality and inference cost of a Foundation Model (FM, such as\nlarge language models (LLMs)) powered software, people often opt to train a\nrouting model that routes requests to FMs with different sizes and\ncapabilities. Existing routing models rely on learning the optimal routing\ndecision from carefully curated data, require complex computations to be\nupdated, and do not consider the potential evolution of weaker FMs. In this\npaper, we propose Real-time Adaptive Routing (RAR), an approach to continuously\nadapt FM routing decisions while using guided in-context learning to enhance\nthe capabilities of weaker FM. The goal is to reduce reliance on stronger, more\nexpensive FMs. We evaluate our approach on different subsets of the popular\nMMLU benchmark. Over time, our approach routes 50.2% fewer requests to\ncomputationally expensive models while maintaining around 90.5% of the general\nresponse quality. In addition, the guides generated from stronger models have\nshown intra-domain generalization and led to a better quality of responses\ncompared to an equivalent approach with a standalone weaker FM."
                },
                "authors": [
                    {
                        "name": "Kirill Vasilevski"
                    },
                    {
                        "name": "Dayi Lin"
                    },
                    {
                        "name": "Ahmed Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Hassan"
                },
                "author": "Ahmed Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09834v1",
                "updated": "2024-11-14T22:54:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    22,
                    54,
                    38,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T22:54:38Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    22,
                    54,
                    38,
                    3,
                    319,
                    0
                ],
                "title": "A Benchmark for Long-Form Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Benchmark for Long-Form Medical Question Answering"
                },
                "summary": "There is a lack of benchmarks for evaluating large language models (LLMs) in\nlong-form medical question answering (QA). Most existing medical QA evaluation\nbenchmarks focus on automatic metrics and multiple-choice questions. While\nvaluable, these benchmarks fail to fully capture or assess the complexities of\nreal-world clinical applications where LLMs are being deployed. Furthermore,\nexisting studies on evaluating long-form answer generation in medical QA are\nprimarily closed-source, lacking access to human medical expert annotations,\nwhich makes it difficult to reproduce results and enhance existing baselines.\nIn this work, we introduce a new publicly available benchmark featuring\nreal-world consumer medical questions with long-form answer evaluations\nannotated by medical doctors. We performed pairwise comparisons of responses\nfrom various open and closed-source medical and general-purpose LLMs based on\ncriteria such as correctness, helpfulness, harmfulness, and bias. Additionally,\nwe performed a comprehensive LLM-as-a-judge analysis to study the alignment\nbetween human judgments and LLMs. Our preliminary results highlight the strong\npotential of open LLMs in medical QA compared to leading closed models. Code &\nData: https://github.com/lavita-ai/medical-eval-sphere",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a lack of benchmarks for evaluating large language models (LLMs) in\nlong-form medical question answering (QA). Most existing medical QA evaluation\nbenchmarks focus on automatic metrics and multiple-choice questions. While\nvaluable, these benchmarks fail to fully capture or assess the complexities of\nreal-world clinical applications where LLMs are being deployed. Furthermore,\nexisting studies on evaluating long-form answer generation in medical QA are\nprimarily closed-source, lacking access to human medical expert annotations,\nwhich makes it difficult to reproduce results and enhance existing baselines.\nIn this work, we introduce a new publicly available benchmark featuring\nreal-world consumer medical questions with long-form answer evaluations\nannotated by medical doctors. We performed pairwise comparisons of responses\nfrom various open and closed-source medical and general-purpose LLMs based on\ncriteria such as correctness, helpfulness, harmfulness, and bias. Additionally,\nwe performed a comprehensive LLM-as-a-judge analysis to study the alignment\nbetween human judgments and LLMs. Our preliminary results highlight the strong\npotential of open LLMs in medical QA compared to leading closed models. Code &\nData: https://github.com/lavita-ai/medical-eval-sphere"
                },
                "authors": [
                    {
                        "name": "Pedram Hosseini"
                    },
                    {
                        "name": "Jessica M. Sin"
                    },
                    {
                        "name": "Bing Ren"
                    },
                    {
                        "name": "Bryceton G. Thomas"
                    },
                    {
                        "name": "Elnaz Nouri"
                    },
                    {
                        "name": "Ali Farahanchi"
                    },
                    {
                        "name": "Saeed Hassanpour"
                    }
                ],
                "author_detail": {
                    "name": "Saeed Hassanpour"
                },
                "author": "Saeed Hassanpour",
                "arxiv_comment": "AIM-FM: Advancements in Medical Foundation Models Workshop, 38th\n  Conference on Neural Information Processing Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05250v2",
                "updated": "2024-11-14T22:51:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    22,
                    51,
                    46,
                    3,
                    319,
                    0
                ],
                "published": "2024-07-07T03:41:51Z",
                "published_parsed": [
                    2024,
                    7,
                    7,
                    3,
                    41,
                    51,
                    6,
                    189,
                    0
                ],
                "title": "CLIMB: A Benchmark of Clinical Bias in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIMB: A Benchmark of Clinical Bias in Large Language Models"
                },
                "summary": "Large language models (LLMs) are increasingly applied to clinical\ndecision-making. However, their potential to exhibit bias poses significant\nrisks to clinical equity. Currently, there is a lack of benchmarks that\nsystematically evaluate such clinical bias in LLMs. While in downstream tasks,\nsome biases of LLMs can be avoided such as by instructing the model to answer\n\"I'm not sure...\", the internal bias hidden within the model still lacks deep\nstudies. We introduce CLIMB (shorthand for A Benchmark of Clinical Bias in\nLarge Language Models), a pioneering comprehensive benchmark to evaluate both\nintrinsic (within LLMs) and extrinsic (on downstream tasks) bias in LLMs for\nclinical decision tasks. Notably, for intrinsic bias, we introduce a novel\nmetric, AssocMAD, to assess the disparities of LLMs across multiple demographic\ngroups. Additionally, we leverage counterfactual intervention to evaluate\nextrinsic bias in a task of clinical diagnosis prediction. Our experiments\nacross popular and medically adapted LLMs, particularly from the Mistral and\nLLaMA families, unveil prevalent behaviors with both intrinsic and extrinsic\nbias. This work underscores the critical need to mitigate clinical bias and\nsets a new standard for future evaluations of LLMs' clinical bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly applied to clinical\ndecision-making. However, their potential to exhibit bias poses significant\nrisks to clinical equity. Currently, there is a lack of benchmarks that\nsystematically evaluate such clinical bias in LLMs. While in downstream tasks,\nsome biases of LLMs can be avoided such as by instructing the model to answer\n\"I'm not sure...\", the internal bias hidden within the model still lacks deep\nstudies. We introduce CLIMB (shorthand for A Benchmark of Clinical Bias in\nLarge Language Models), a pioneering comprehensive benchmark to evaluate both\nintrinsic (within LLMs) and extrinsic (on downstream tasks) bias in LLMs for\nclinical decision tasks. Notably, for intrinsic bias, we introduce a novel\nmetric, AssocMAD, to assess the disparities of LLMs across multiple demographic\ngroups. Additionally, we leverage counterfactual intervention to evaluate\nextrinsic bias in a task of clinical diagnosis prediction. Our experiments\nacross popular and medically adapted LLMs, particularly from the Mistral and\nLLaMA families, unveil prevalent behaviors with both intrinsic and extrinsic\nbias. This work underscores the critical need to mitigate clinical bias and\nsets a new standard for future evaluations of LLMs' clinical bias."
                },
                "authors": [
                    {
                        "name": "Yubo Zhang"
                    },
                    {
                        "name": "Shudi Hou"
                    },
                    {
                        "name": "Mingyu Derek Ma"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Jieyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieyu Zhao"
                },
                "author": "Jieyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09826v1",
                "updated": "2024-11-14T22:23:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    22,
                    23,
                    13,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T22:23:13Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    22,
                    23,
                    13,
                    3,
                    319,
                    0
                ],
                "title": "Evaluating Gender Bias in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Gender Bias in Large Language Models"
                },
                "summary": "Gender bias in artificial intelligence has become an important issue,\nparticularly in the context of language models used in communication-oriented\napplications. This study examines the extent to which Large Language Models\n(LLMs) exhibit gender bias in pronoun selection in occupational contexts. The\nanalysis evaluates the models GPT-4, GPT-4o, PaLM 2 Text Bison and Gemini 1.0\nPro using a self-generated dataset. The jobs considered include a range of\noccupations, from those with a significant male presence to those with a\nnotable female concentration, as well as jobs with a relatively equal gender\ndistribution. Three different sentence processing methods were used to assess\npotential gender bias: masked tokens, unmasked sentences, and sentence\ncompletion. In addition, the LLMs suggested names of individuals in specific\noccupations, which were then examined for gender distribution. The results show\na positive correlation between the models' pronoun choices and the gender\ndistribution present in U.S. labor force data. Female pronouns were more often\nassociated with female-dominated occupations, while male pronouns were more\noften associated with male-dominated occupations. Sentence completion showed\nthe strongest correlation with actual gender distribution, while name\ngeneration resulted in a more balanced 'politically correct' gender\ndistribution, albeit with notable variations in predominantly male or female\noccupations. Overall, the prompting method had a greater impact on gender\ndistribution than the model selection itself, highlighting the complexity of\naddressing gender bias in LLMs. The findings highlight the importance of\nprompting in gender mapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gender bias in artificial intelligence has become an important issue,\nparticularly in the context of language models used in communication-oriented\napplications. This study examines the extent to which Large Language Models\n(LLMs) exhibit gender bias in pronoun selection in occupational contexts. The\nanalysis evaluates the models GPT-4, GPT-4o, PaLM 2 Text Bison and Gemini 1.0\nPro using a self-generated dataset. The jobs considered include a range of\noccupations, from those with a significant male presence to those with a\nnotable female concentration, as well as jobs with a relatively equal gender\ndistribution. Three different sentence processing methods were used to assess\npotential gender bias: masked tokens, unmasked sentences, and sentence\ncompletion. In addition, the LLMs suggested names of individuals in specific\noccupations, which were then examined for gender distribution. The results show\na positive correlation between the models' pronoun choices and the gender\ndistribution present in U.S. labor force data. Female pronouns were more often\nassociated with female-dominated occupations, while male pronouns were more\noften associated with male-dominated occupations. Sentence completion showed\nthe strongest correlation with actual gender distribution, while name\ngeneration resulted in a more balanced 'politically correct' gender\ndistribution, albeit with notable variations in predominantly male or female\noccupations. Overall, the prompting method had a greater impact on gender\ndistribution than the model selection itself, highlighting the complexity of\naddressing gender bias in LLMs. The findings highlight the importance of\nprompting in gender mapping."
                },
                "authors": [
                    {
                        "name": "Michael Döll"
                    },
                    {
                        "name": "Markus Döhring"
                    },
                    {
                        "name": "Andreas Müller"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Müller"
                },
                "author": "Andreas Müller",
                "arxiv_comment": "13 pages, 12 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00888v2",
                "updated": "2024-11-14T22:20:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    22,
                    20,
                    49,
                    3,
                    319,
                    0
                ],
                "published": "2024-01-30T04:00:54Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    4,
                    0,
                    54,
                    1,
                    30,
                    0
                ],
                "title": "Security and Privacy Challenges of Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security and Privacy Challenges of Large Language Models: A Survey"
                },
                "summary": "Large Language Models (LLMs) have demonstrated extraordinary capabilities and\ncontributed to multiple fields, such as generating and summarizing text,\nlanguage translation, and question-answering. Nowadays, LLM is becoming a very\npopular tool in computerized language processing tasks, with the capability to\nanalyze complicated linguistic patterns and provide relevant and appropriate\nresponses depending on the context. While offering significant advantages,\nthese models are also vulnerable to security and privacy attacks, such as\njailbreaking attacks, data poisoning attacks, and Personally Identifiable\nInformation (PII) leakage attacks. This survey provides a thorough review of\nthe security and privacy challenges of LLMs for both training data and users,\nalong with the application-based risks in various domains, such as\ntransportation, education, and healthcare. We assess the extent of LLM\nvulnerabilities, investigate emerging security and privacy attacks for LLMs,\nand review the potential defense mechanisms. Additionally, the survey outlines\nexisting research gaps in this domain and highlights future research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated extraordinary capabilities and\ncontributed to multiple fields, such as generating and summarizing text,\nlanguage translation, and question-answering. Nowadays, LLM is becoming a very\npopular tool in computerized language processing tasks, with the capability to\nanalyze complicated linguistic patterns and provide relevant and appropriate\nresponses depending on the context. While offering significant advantages,\nthese models are also vulnerable to security and privacy attacks, such as\njailbreaking attacks, data poisoning attacks, and Personally Identifiable\nInformation (PII) leakage attacks. This survey provides a thorough review of\nthe security and privacy challenges of LLMs for both training data and users,\nalong with the application-based risks in various domains, such as\ntransportation, education, and healthcare. We assess the extent of LLM\nvulnerabilities, investigate emerging security and privacy attacks for LLMs,\nand review the potential defense mechanisms. Additionally, the survey outlines\nexisting research gaps in this domain and highlights future research\ndirections."
                },
                "authors": [
                    {
                        "name": "Badhan Chandra Das"
                    },
                    {
                        "name": "M. Hadi Amini"
                    },
                    {
                        "name": "Yanzhao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhao Wu"
                },
                "author": "Yanzhao Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09823v1",
                "updated": "2024-11-14T22:15:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    22,
                    15,
                    48,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T22:15:48Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    22,
                    15,
                    48,
                    3,
                    319,
                    0
                ],
                "title": "Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical\n  2D Inpainting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical\n  2D Inpainting"
                },
                "summary": "Creating large-scale interactive 3D environments is essential for the\ndevelopment of Robotics and Embodied AI research. Current methods, including\nmanual design, procedural generation, diffusion-based scene generation, and\nlarge language model (LLM) guided scene design, are hindered by limitations\nsuch as excessive human effort, reliance on predefined rules or training\ndatasets, and limited 3D spatial reasoning ability. Since pre-trained 2D image\ngenerative models better capture scene and object configuration than LLMs, we\naddress these challenges by introducing Architect, a generative framework that\ncreates complex and realistic 3D embodied environments leveraging\ndiffusion-based 2D image inpainting. In detail, we utilize foundation visual\nperception models to obtain each generated object from the image and leverage\npre-trained depth estimation models to lift the generated 2D image to 3D space.\nOur pipeline is further extended to a hierarchical and iterative inpainting\nprocess to continuously generate placement of large furniture and small objects\nto enrich the scene. This iterative structure brings the flexibility for our\nmethod to generate or refine scenes from various starting points, such as text,\nfloor plans, or pre-arranged environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating large-scale interactive 3D environments is essential for the\ndevelopment of Robotics and Embodied AI research. Current methods, including\nmanual design, procedural generation, diffusion-based scene generation, and\nlarge language model (LLM) guided scene design, are hindered by limitations\nsuch as excessive human effort, reliance on predefined rules or training\ndatasets, and limited 3D spatial reasoning ability. Since pre-trained 2D image\ngenerative models better capture scene and object configuration than LLMs, we\naddress these challenges by introducing Architect, a generative framework that\ncreates complex and realistic 3D embodied environments leveraging\ndiffusion-based 2D image inpainting. In detail, we utilize foundation visual\nperception models to obtain each generated object from the image and leverage\npre-trained depth estimation models to lift the generated 2D image to 3D space.\nOur pipeline is further extended to a hierarchical and iterative inpainting\nprocess to continuously generate placement of large furniture and small objects\nto enrich the scene. This iterative structure brings the flexibility for our\nmethod to generate or refine scenes from various starting points, such as text,\nfloor plans, or pre-arranged environments."
                },
                "authors": [
                    {
                        "name": "Yian Wang"
                    },
                    {
                        "name": "Xiaowen Qiu"
                    },
                    {
                        "name": "Jiageng Liu"
                    },
                    {
                        "name": "Zhehuan Chen"
                    },
                    {
                        "name": "Jiting Cai"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Tsun-Hsuan Wang"
                    },
                    {
                        "name": "Zhou Xian"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09816v1",
                "updated": "2024-11-14T21:29:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    29,
                    58,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T21:29:58Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    29,
                    58,
                    3,
                    319,
                    0
                ],
                "title": "Learning Parameter Sharing with Tensor Decompositions and Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Parameter Sharing with Tensor Decompositions and Sparsity"
                },
                "summary": "Large neural networks achieve remarkable performance, but their size hinders\ndeployment on resource-constrained devices. While various compression\ntechniques exist, parameter sharing remains relatively unexplored. This paper\nintroduces Fine-grained Parameter Sharing (FiPS), a novel algorithm that\nleverages the relationship between parameter sharing, tensor decomposition, and\nsparsity to efficiently compress large vision transformer models. FiPS employs\na shared base and sparse factors to represent shared neurons across multi-layer\nperception (MLP) modules. Shared parameterization is initialized via Singular\nValue Decomposition (SVD) and optimized by minimizing block-wise reconstruction\nerror. Experiments demonstrate that FiPS compresses DeiT-B and Swin-L MLPs to\n25-40% of their original parameter count while maintaining accuracy within 1\npercentage point of the original models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large neural networks achieve remarkable performance, but their size hinders\ndeployment on resource-constrained devices. While various compression\ntechniques exist, parameter sharing remains relatively unexplored. This paper\nintroduces Fine-grained Parameter Sharing (FiPS), a novel algorithm that\nleverages the relationship between parameter sharing, tensor decomposition, and\nsparsity to efficiently compress large vision transformer models. FiPS employs\na shared base and sparse factors to represent shared neurons across multi-layer\nperception (MLP) modules. Shared parameterization is initialized via Singular\nValue Decomposition (SVD) and optimized by minimizing block-wise reconstruction\nerror. Experiments demonstrate that FiPS compresses DeiT-B and Swin-L MLPs to\n25-40% of their original parameter count while maintaining accuracy within 1\npercentage point of the original models."
                },
                "authors": [
                    {
                        "name": "Cem Üyük"
                    },
                    {
                        "name": "Mike Lasby"
                    },
                    {
                        "name": "Mohamed Yassin"
                    },
                    {
                        "name": "Utku Evci"
                    },
                    {
                        "name": "Yani Ioannou"
                    }
                ],
                "author_detail": {
                    "name": "Yani Ioannou"
                },
                "author": "Yani Ioannou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08123v2",
                "updated": "2024-11-14T19:53:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    19,
                    53,
                    49,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-12T19:06:33Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    19,
                    6,
                    33,
                    1,
                    317,
                    0
                ],
                "title": "Exploring the Role of LLMs for Supporting Older Adults: Opportunities\n  and Concerns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Role of LLMs for Supporting Older Adults: Opportunities\n  and Concerns"
                },
                "summary": "We explore some of the existing research in HCI around technology for older\nadults and examine the role of LLMs in enhancing it. We also discuss the\ndigital divide and emphasize the need for inclusive technology design. At the\nsame time, we also surface concerns regarding privacy, security, and the\naccuracy of information provided by LLMs, alongside the importance of\nuser-centered design to make technology accessible and effective for the\nelderly. We show the transformative possibilities of LLM-supported interactions\nat the intersection of aging, technology, and human-computer interaction,\nadvocating for further research and development in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore some of the existing research in HCI around technology for older\nadults and examine the role of LLMs in enhancing it. We also discuss the\ndigital divide and emphasize the need for inclusive technology design. At the\nsame time, we also surface concerns regarding privacy, security, and the\naccuracy of information provided by LLMs, alongside the importance of\nuser-centered design to make technology accessible and effective for the\nelderly. We show the transformative possibilities of LLM-supported interactions\nat the intersection of aging, technology, and human-computer interaction,\nadvocating for further research and development in this area."
                },
                "authors": [
                    {
                        "name": "Sidharth Kaliappan"
                    },
                    {
                        "name": "Abhay Sheel Anand"
                    },
                    {
                        "name": "Koustuv Saha"
                    },
                    {
                        "name": "Ravi Karkar"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Karkar"
                },
                "author": "Ravi Karkar",
                "arxiv_comment": "This short paper was accepted at CHI 2024 Workshop on HCI and Aging:\n  New Directions, New Principles",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09772v1",
                "updated": "2024-11-14T19:33:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    19,
                    33,
                    8,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T19:33:08Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    19,
                    33,
                    8,
                    3,
                    319,
                    0
                ],
                "title": "Beyond Static Tools: Evaluating Large Language Models for Cryptographic\n  Misuse Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Static Tools: Evaluating Large Language Models for Cryptographic\n  Misuse Detection"
                },
                "summary": "The use of Large Language Models (LLMs) in software development is rapidly\ngrowing, with developers increasingly relying on these models for coding\nassistance, including security-critical tasks. Our work presents a\ncomprehensive comparison between traditional static analysis tools for\ncryptographic API misuse detection-CryptoGuard, CogniCrypt, and Snyk Code-and\nthe LLMs-GPT and Gemini. Using benchmark datasets (OWASP, CryptoAPI, and MASC),\nwe evaluate the effectiveness of each tool in identifying cryptographic\nmisuses. Our findings show that GPT 4-o-mini surpasses current state-of-the-art\nstatic analysis tools on the CryptoAPI and MASC datasets, though it lags on the\nOWASP dataset. Additionally, we assess the quality of LLM responses to\ndetermine which models provide actionable and accurate advice, giving\ndevelopers insights into their practical utility for secure coding. This study\nhighlights the comparative strengths and limitations of static analysis versus\nLLM-driven approaches, offering valuable insights into the evolving role of AI\nin advancing software security practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) in software development is rapidly\ngrowing, with developers increasingly relying on these models for coding\nassistance, including security-critical tasks. Our work presents a\ncomprehensive comparison between traditional static analysis tools for\ncryptographic API misuse detection-CryptoGuard, CogniCrypt, and Snyk Code-and\nthe LLMs-GPT and Gemini. Using benchmark datasets (OWASP, CryptoAPI, and MASC),\nwe evaluate the effectiveness of each tool in identifying cryptographic\nmisuses. Our findings show that GPT 4-o-mini surpasses current state-of-the-art\nstatic analysis tools on the CryptoAPI and MASC datasets, though it lags on the\nOWASP dataset. Additionally, we assess the quality of LLM responses to\ndetermine which models provide actionable and accurate advice, giving\ndevelopers insights into their practical utility for secure coding. This study\nhighlights the comparative strengths and limitations of static analysis versus\nLLM-driven approaches, offering valuable insights into the evolving role of AI\nin advancing software security practices."
                },
                "authors": [
                    {
                        "name": "Zohaib Masood"
                    },
                    {
                        "name": "Miguel Vargas Martin"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Vargas Martin"
                },
                "arxiv_affiliation": "Ontario Tech University",
                "author": "Miguel Vargas Martin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09763v1",
                "updated": "2024-11-14T19:20:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    19,
                    20,
                    33,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T19:20:33Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    19,
                    20,
                    33,
                    3,
                    319,
                    0
                ],
                "title": "Evaluating the Predictive Capacity of ChatGPT for Academic Peer Review\n  Outcomes Across Multiple Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Predictive Capacity of ChatGPT for Academic Peer Review\n  Outcomes Across Multiple Platforms"
                },
                "summary": "While previous studies have demonstrated that Large Language Models (LLMs)\ncan predict peer review outcomes to some extent, this paper builds on that by\nintroducing two new contexts and employing a more robust method - averaging\nmultiple ChatGPT scores. The findings that averaging 30 ChatGPT predictions,\nbased on reviewer guidelines and using only the submitted titles and abstracts,\nfailed to predict peer review outcomes for F1000Research (Spearman's rho=0.00).\nHowever, it produced mostly weak positive correlations with the quality\ndimensions of SciPost Physics (rho=0.25 for validity, rho=0.25 for originality,\nrho=0.20 for significance, and rho = 0.08 for clarity) and a moderate positive\ncorrelation for papers from the International Conference on Learning\nRepresentations (ICLR) (rho=0.38). Including the full text of articles\nsignificantly increased the correlation for ICLR (rho=0.46) and slightly\nimproved it for F1000Research (rho=0.09), while it had variable effects on the\nfour quality dimension correlations for SciPost LaTeX files. The use of\nchain-of-thought system prompts slightly increased the correlation for\nF1000Research (rho=0.10), marginally reduced it for ICLR (rho=0.37), and\nfurther decreased it for SciPost Physics (rho=0.16 for validity, rho=0.18 for\noriginality, rho=0.18 for significance, and rho=0.05 for clarity). Overall, the\nresults suggest that in some contexts, ChatGPT can produce weak pre-publication\nquality assessments. However, the effectiveness of these assessments and the\noptimal strategies for employing them vary considerably across different\nplatforms, journals, and conferences. Additionally, the most suitable inputs\nfor ChatGPT appear to differ depending on the platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While previous studies have demonstrated that Large Language Models (LLMs)\ncan predict peer review outcomes to some extent, this paper builds on that by\nintroducing two new contexts and employing a more robust method - averaging\nmultiple ChatGPT scores. The findings that averaging 30 ChatGPT predictions,\nbased on reviewer guidelines and using only the submitted titles and abstracts,\nfailed to predict peer review outcomes for F1000Research (Spearman's rho=0.00).\nHowever, it produced mostly weak positive correlations with the quality\ndimensions of SciPost Physics (rho=0.25 for validity, rho=0.25 for originality,\nrho=0.20 for significance, and rho = 0.08 for clarity) and a moderate positive\ncorrelation for papers from the International Conference on Learning\nRepresentations (ICLR) (rho=0.38). Including the full text of articles\nsignificantly increased the correlation for ICLR (rho=0.46) and slightly\nimproved it for F1000Research (rho=0.09), while it had variable effects on the\nfour quality dimension correlations for SciPost LaTeX files. The use of\nchain-of-thought system prompts slightly increased the correlation for\nF1000Research (rho=0.10), marginally reduced it for ICLR (rho=0.37), and\nfurther decreased it for SciPost Physics (rho=0.16 for validity, rho=0.18 for\noriginality, rho=0.18 for significance, and rho=0.05 for clarity). Overall, the\nresults suggest that in some contexts, ChatGPT can produce weak pre-publication\nquality assessments. However, the effectiveness of these assessments and the\noptimal strategies for employing them vary considerably across different\nplatforms, journals, and conferences. Additionally, the most suitable inputs\nfor ChatGPT appear to differ depending on the platform."
                },
                "authors": [
                    {
                        "name": "Mike Thelwall"
                    },
                    {
                        "name": "Abdullah Yaghi"
                    }
                ],
                "author_detail": {
                    "name": "Abdullah Yaghi"
                },
                "author": "Abdullah Yaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09689v1",
                "updated": "2024-11-14T18:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    55,
                    26,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    55,
                    26,
                    3,
                    319,
                    0
                ],
                "title": "LLM Hallucination Reasoning with Zero-shot Knowledge Test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Hallucination Reasoning with Zero-shot Knowledge Test"
                },
                "summary": "LLM hallucination, where LLMs occasionally generate unfaithful text, poses\nsignificant challenges for their practical applications. Most existing\ndetection methods rely on external knowledge, LLM fine-tuning, or\nhallucination-labeled datasets, and they do not distinguish between different\ntypes of hallucinations, which are crucial for improving detection performance.\nWe introduce a new task, Hallucination Reasoning, which classifies\nLLM-generated text into one of three categories: aligned, misaligned, and\nfabricated. Our novel zero-shot method assesses whether LLM has enough\nknowledge about a given prompt and text. Our experiments conducted on new\ndatasets demonstrate the effectiveness of our method in hallucination reasoning\nand underscore its importance for enhancing detection performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM hallucination, where LLMs occasionally generate unfaithful text, poses\nsignificant challenges for their practical applications. Most existing\ndetection methods rely on external knowledge, LLM fine-tuning, or\nhallucination-labeled datasets, and they do not distinguish between different\ntypes of hallucinations, which are crucial for improving detection performance.\nWe introduce a new task, Hallucination Reasoning, which classifies\nLLM-generated text into one of three categories: aligned, misaligned, and\nfabricated. Our novel zero-shot method assesses whether LLM has enough\nknowledge about a given prompt and text. Our experiments conducted on new\ndatasets demonstrate the effectiveness of our method in hallucination reasoning\nand underscore its importance for enhancing detection performance."
                },
                "authors": [
                    {
                        "name": "Seongmin Lee"
                    },
                    {
                        "name": "Hsiang Hsu"
                    },
                    {
                        "name": "Chun-Fu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Fu Chen"
                },
                "author": "Chun-Fu Chen",
                "arxiv_comment": "12 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v1",
                "updated": "2024-11-14T18:54:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05777v2",
                "updated": "2024-11-14T18:35:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    35,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-08T18:43:15Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    43,
                    15,
                    4,
                    313,
                    0
                ],
                "title": "Quantitative Assessment of Intersectional Empathetic Bias and\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative Assessment of Intersectional Empathetic Bias and\n  Understanding"
                },
                "summary": "A growing amount of literature critiques the current operationalizations of\nempathy based on loose definitions of the construct. Such definitions\nnegatively affect dataset quality, model robustness, and evaluation\nreliability. We propose an empathy evaluation framework that operationalizes\nempathy close to its psychological origins. The framework measures the variance\nin responses of LLMs to prompts using existing metrics for empathy and\nemotional valence. The variance is introduced through the controlled generation\nof the prompts by varying social biases affecting context understanding, thus\nimpacting empathetic understanding. The control over generation ensures high\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\nhigh-quality translation, especially into languages that currently have\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\nempathy evaluation with the framework, including multiple-choice answers and\nfree generation. The variance in our initial evaluation sample is small and we\nwere unable to measure convincing differences between the empathetic\nunderstanding in contexts given by different social groups. However, the\nresults are promising because the models showed significant alterations their\nreasoning chains needed to capture the relatively subtle changes in the\nprompts. This provides the basis for future research into the construction of\nthe evaluation sample and statistical methods for measuring the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing amount of literature critiques the current operationalizations of\nempathy based on loose definitions of the construct. Such definitions\nnegatively affect dataset quality, model robustness, and evaluation\nreliability. We propose an empathy evaluation framework that operationalizes\nempathy close to its psychological origins. The framework measures the variance\nin responses of LLMs to prompts using existing metrics for empathy and\nemotional valence. The variance is introduced through the controlled generation\nof the prompts by varying social biases affecting context understanding, thus\nimpacting empathetic understanding. The control over generation ensures high\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\nhigh-quality translation, especially into languages that currently have\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\nempathy evaluation with the framework, including multiple-choice answers and\nfree generation. The variance in our initial evaluation sample is small and we\nwere unable to measure convincing differences between the empathetic\nunderstanding in contexts given by different social groups. However, the\nresults are promising because the models showed significant alterations their\nreasoning chains needed to capture the relatively subtle changes in the\nprompts. This provides the basis for future research into the construction of\nthe evaluation sample and statistical methods for measuring the results."
                },
                "authors": [
                    {
                        "name": "Vojtech Formanek"
                    },
                    {
                        "name": "Ondrej Sotolar"
                    }
                ],
                "author_detail": {
                    "name": "Ondrej Sotolar"
                },
                "author": "Ondrej Sotolar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09660v1",
                "updated": "2024-11-14T18:31:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    31,
                    16,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:31:16Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    31,
                    16,
                    3,
                    319,
                    0
                ],
                "title": "Capacity and Power Consumption of Multi-Layer 6G Networks Using the\n  Upper Mid-Band",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capacity and Power Consumption of Multi-Layer 6G Networks Using the\n  Upper Mid-Band"
                },
                "summary": "This paper presents a new system model to evaluate the capacity and power\nconsumption of multi-layer 6G networks utilising the upper mid-band (FR3). The\nmodel captures heterogeneous 4G, 5G, and 6G deployments, analyzing their\nperformance under different deployment strategies. Our results show that\nstrategic 6G deployments, non-co-located with existing 5G sites, significantly\nenhance throughput, with median and peak user rates of 300 Mbps and exceeding 1\nGbps, respectively. We also emphasize the importance of priority-based cell\nreselection and beam configuration to fully leverage 6G capabilities. While 6G\nimplementation increases power consumption by 33%, non-colocated deployments\nstrike a balance between performance and power consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new system model to evaluate the capacity and power\nconsumption of multi-layer 6G networks utilising the upper mid-band (FR3). The\nmodel captures heterogeneous 4G, 5G, and 6G deployments, analyzing their\nperformance under different deployment strategies. Our results show that\nstrategic 6G deployments, non-co-located with existing 5G sites, significantly\nenhance throughput, with median and peak user rates of 300 Mbps and exceeding 1\nGbps, respectively. We also emphasize the importance of priority-based cell\nreselection and beam configuration to fully leverage 6G capabilities. While 6G\nimplementation increases power consumption by 33%, non-colocated deployments\nstrike a balance between performance and power consumption."
                },
                "authors": [
                    {
                        "name": "David López-Pérez"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Giovanni Geraci"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Geraci"
                },
                "author": "Giovanni Geraci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03862v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03862v3",
                "updated": "2024-11-14T18:27:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    27,
                    39,
                    3,
                    319,
                    0
                ],
                "published": "2024-04-05T02:27:09Z",
                "published_parsed": [
                    2024,
                    4,
                    5,
                    2,
                    27,
                    9,
                    4,
                    96,
                    0
                ],
                "title": "Verifiable by Design: Aligning Language Models to Quote from\n  Pre-Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifiable by Design: Aligning Language Models to Quote from\n  Pre-Training Data"
                },
                "summary": "To trust the fluent generations of large language models (LLMs), humans must\nbe able to verify their correctness against trusted, external sources. Recent\nefforts, such as providing citations via retrieved documents or post-hoc\nprovenance, enhance verifiability but provide no guarantees on their\ncorrectness. To address these limitations, we tackle the verifiability goal\nwith a different philosophy: trivializing the verification process by\ndeveloping models that quote verbatim statements from trusted sources in their\npre-training data. We propose Quote-Tuning, which demonstrates the feasibility\nof aligning models to quote. The core of Quote-Tuning is a fast membership\ninference function that efficiently verifies text against trusted corpora. We\nleverage this tool to design a reward function to quantify quotes in model\nresponses, and curate datasets for preference learning. Experiments show that\nQuote-Tuning significantly increases verbatim quotes from high-quality\ndocuments by up to 130% relative to base models while maintaining response\nquality. Quote-Tuning is applicable in different tasks, generalizes to\nout-of-domain data and diverse model families, and provides additional benefits\nto truthfulness. Our method not only serves as a hassle-free method to increase\nquoting but also opens up avenues for improving LLM trustworthiness through\nbetter verifiability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To trust the fluent generations of large language models (LLMs), humans must\nbe able to verify their correctness against trusted, external sources. Recent\nefforts, such as providing citations via retrieved documents or post-hoc\nprovenance, enhance verifiability but provide no guarantees on their\ncorrectness. To address these limitations, we tackle the verifiability goal\nwith a different philosophy: trivializing the verification process by\ndeveloping models that quote verbatim statements from trusted sources in their\npre-training data. We propose Quote-Tuning, which demonstrates the feasibility\nof aligning models to quote. The core of Quote-Tuning is a fast membership\ninference function that efficiently verifies text against trusted corpora. We\nleverage this tool to design a reward function to quantify quotes in model\nresponses, and curate datasets for preference learning. Experiments show that\nQuote-Tuning significantly increases verbatim quotes from high-quality\ndocuments by up to 130% relative to base models while maintaining response\nquality. Quote-Tuning is applicable in different tasks, generalizes to\nout-of-domain data and diverse model families, and provides additional benefits\nto truthfulness. Our method not only serves as a hassle-free method to increase\nquoting but also opens up avenues for improving LLM trustworthiness through\nbetter verifiability."
                },
                "authors": [
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Marc Marone"
                    },
                    {
                        "name": "Tianjian Li"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03862v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03862v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09648v1",
                "updated": "2024-11-14T18:17:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    17,
                    30,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:17:30Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    17,
                    30,
                    3,
                    319,
                    0
                ],
                "title": "Med-Bot: An AI-Powered Assistant to Provide Accurate and Reliable\n  Medical Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-Bot: An AI-Powered Assistant to Provide Accurate and Reliable\n  Medical Information"
                },
                "summary": "This paper introduces Med-Bot, an AI-powered chatbot designed to provide\nusers with accurate and reliable medical information. Utilizing advanced\nlibraries and frameworks such as PyTorch, Chromadb, Langchain and Autogptq,\nMed-Bot is built to handle the complexities of natural language understanding\nin a healthcare context. The integration of llamaassisted data processing and\nAutoGPT-Q provides enhanced performance in processing and responding to queries\nbased on PDFs of medical literature, ensuring that users receive precise and\ntrustworthy information. This research details the methodologies employed in\ndeveloping Med-Bot and evaluates its effectiveness in disseminating healthcare\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Med-Bot, an AI-powered chatbot designed to provide\nusers with accurate and reliable medical information. Utilizing advanced\nlibraries and frameworks such as PyTorch, Chromadb, Langchain and Autogptq,\nMed-Bot is built to handle the complexities of natural language understanding\nin a healthcare context. The integration of llamaassisted data processing and\nAutoGPT-Q provides enhanced performance in processing and responding to queries\nbased on PDFs of medical literature, ensuring that users receive precise and\ntrustworthy information. This research details the methodologies employed in\ndeveloping Med-Bot and evaluates its effectiveness in disseminating healthcare\ninformation."
                },
                "authors": [
                    {
                        "name": "Ahan Bhatt"
                    },
                    {
                        "name": "Nandan Vaghela"
                    }
                ],
                "author_detail": {
                    "name": "Nandan Vaghela"
                },
                "author": "Nandan Vaghela",
                "arxiv_comment": "3 figures, 5 pages Keywords-LLM, AI-powered healthcare, Medical\n  chatbot, Context-based interaction, Llama-assisted data processing,\n  AutoGPT-Q, PyTorch, TensorFlow, Reliable medical information, Machine\n  learning in healthcare, Conversational AI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09645v1",
                "updated": "2024-11-14T18:14:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    14,
                    32,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:14:32Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    14,
                    32,
                    3,
                    319,
                    0
                ],
                "title": "How do Machine Learning Models Change?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do Machine Learning Models Change?"
                },
                "summary": "The proliferation of Machine Learning (ML) models and their open-source\nimplementations has transformed Artificial Intelligence research and\napplications. Platforms like Hugging Face (HF) enable the development, sharing,\nand deployment of these models, fostering an evolving ecosystem. While previous\nstudies have examined aspects of models hosted on platforms like HF, a\ncomprehensive longitudinal study of how these models change remains\nunderexplored. This study addresses this gap by utilizing both repository\nmining and longitudinal analysis methods to examine over 200,000 commits and\n1,200 releases from over 50,000 models on HF. We replicate and extend an ML\nchange taxonomy for classifying commits and utilize Bayesian networks to\nuncover patterns in commit and release activities over time. Our findings\nindicate that commit activities align with established data science\nmethodologies, such as CRISP-DM, emphasizing iterative refinement and\ncontinuous improvement. Additionally, release patterns tend to consolidate\nsignificant updates, particularly in documentation, distinguishing between\ngranular changes and milestone-based releases. Furthermore, projects with\nhigher popularity prioritize infrastructure enhancements early in their\nlifecycle, and those with intensive collaboration practices exhibit improved\ndocumentation standards. These and other insights enhance the understanding of\nmodel changes on community platforms and provide valuable guidance for best\npractices in model maintenance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Machine Learning (ML) models and their open-source\nimplementations has transformed Artificial Intelligence research and\napplications. Platforms like Hugging Face (HF) enable the development, sharing,\nand deployment of these models, fostering an evolving ecosystem. While previous\nstudies have examined aspects of models hosted on platforms like HF, a\ncomprehensive longitudinal study of how these models change remains\nunderexplored. This study addresses this gap by utilizing both repository\nmining and longitudinal analysis methods to examine over 200,000 commits and\n1,200 releases from over 50,000 models on HF. We replicate and extend an ML\nchange taxonomy for classifying commits and utilize Bayesian networks to\nuncover patterns in commit and release activities over time. Our findings\nindicate that commit activities align with established data science\nmethodologies, such as CRISP-DM, emphasizing iterative refinement and\ncontinuous improvement. Additionally, release patterns tend to consolidate\nsignificant updates, particularly in documentation, distinguishing between\ngranular changes and milestone-based releases. Furthermore, projects with\nhigher popularity prioritize infrastructure enhancements early in their\nlifecycle, and those with intensive collaboration practices exhibit improved\ndocumentation standards. These and other insights enhance the understanding of\nmodel changes on community platforms and provide valuable guidance for best\npractices in model maintenance."
                },
                "authors": [
                    {
                        "name": "Joel Castaño"
                    },
                    {
                        "name": "Rafael Cabañas"
                    },
                    {
                        "name": "Antonio Salmerón"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Silverio Martínez-Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Silverio Martínez-Fernández"
                },
                "author": "Silverio Martínez-Fernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04783v2",
                "updated": "2024-11-14T18:14:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    14,
                    0,
                    3,
                    319,
                    0
                ],
                "published": "2024-03-02T16:52:22Z",
                "published_parsed": [
                    2024,
                    3,
                    2,
                    16,
                    52,
                    22,
                    5,
                    62,
                    0
                ],
                "title": "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks"
                },
                "summary": "Despite extensive pre-training in moral alignment to prevent generating\nharmful information, large language models (LLMs) remain vulnerable to\njailbreak attacks. In this paper, we propose AutoDefense, a multi-agent defense\nframework that filters harmful responses from LLMs. With the response-filtering\nmechanism, our framework is robust against different jailbreak attack prompts,\nand can be used to defend different victim models. AutoDefense assigns\ndifferent roles to LLM agents and employs them to complete the defense task\ncollaboratively. The division in tasks enhances the overall\ninstruction-following of LLMs and enables the integration of other defense\ncomponents as tools. With AutoDefense, small open-source LMs can serve as\nagents and defend larger models against jailbreak attacks. Our experiments show\nthat AutoDefense can effectively defense against different jailbreak attacks,\nwhile maintaining the performance at normal user request. For example, we\nreduce the attack success rate on GPT-3.5 from 55.74% to 7.95% using\nLLaMA-2-13b with a 3-agent system. Our code and data are publicly available at\nhttps://github.com/XHMY/AutoDefense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite extensive pre-training in moral alignment to prevent generating\nharmful information, large language models (LLMs) remain vulnerable to\njailbreak attacks. In this paper, we propose AutoDefense, a multi-agent defense\nframework that filters harmful responses from LLMs. With the response-filtering\nmechanism, our framework is robust against different jailbreak attack prompts,\nand can be used to defend different victim models. AutoDefense assigns\ndifferent roles to LLM agents and employs them to complete the defense task\ncollaboratively. The division in tasks enhances the overall\ninstruction-following of LLMs and enables the integration of other defense\ncomponents as tools. With AutoDefense, small open-source LMs can serve as\nagents and defend larger models against jailbreak attacks. Our experiments show\nthat AutoDefense can effectively defense against different jailbreak attacks,\nwhile maintaining the performance at normal user request. For example, we\nreduce the attack success rate on GPT-3.5 from 55.74% to 7.95% using\nLLaMA-2-13b with a 3-agent system. Our code and data are publicly available at\nhttps://github.com/XHMY/AutoDefense."
                },
                "authors": [
                    {
                        "name": "Yifan Zeng"
                    },
                    {
                        "name": "Yiran Wu"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Huazheng Wang"
                    },
                    {
                        "name": "Qingyun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyun Wu"
                },
                "author": "Qingyun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04573v2",
                "updated": "2024-11-14T18:01:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    1,
                    10,
                    3,
                    319,
                    0
                ],
                "published": "2024-07-05T15:08:44Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    15,
                    8,
                    44,
                    4,
                    187,
                    0
                ],
                "title": "VRSD: Rethinking Similarity and Diversity for Retrieval in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VRSD: Rethinking Similarity and Diversity for Retrieval in Large\n  Language Models"
                },
                "summary": "Vector retrieval algorithms are essential for semantic queries within the\nrapidly evolving landscape of Large Language Models (LLMs). The ability to\nretrieve vectors that satisfy both similarity and diversity criteria\nsubstantially enhances the performance of LLMs. Although Maximal Marginal\nRelevance (MMR) is widely employed in retrieval scenarios requiring relevance\nand diversity, variations in the parameter $\\lambda$ lead to fluctuations that\ncomplicate the optimization trajectory in vector spaces. This obscures the\ndirection of improvement and highlights the lack of a robust theoretical\nanalysis regarding similarity and diversity constraints in retrieval processes.\nTo address these challenges, this paper introduces a novel approach that\ncharacterizes both constraints through the relationship between the sum vector\nand the query vector. The proximity of these vectors ensures the similarity\nconstraint, while requiring individual vectors within the sum vector to diverge\nin their alignment with the query vector satisfies the diversity constraint. We\nfirst formulate a new combinatorial optimization problem, selecting k vectors\nfrom a candidate set such that their sum vector maximally aligns with the query\nvector, and demonstrate that this problem is NP-complete. This result\nunderscores the inherent difficulty of simultaneously achieving similarity and\ndiversity in vector retrieval, thereby providing a theoretical foundation for\nfuture research. Subsequently, we present the heuristic algorithm Vectors\nRetrieval with Similarity and Diversity, VRSD, which features a clear\noptimization objective and eliminates the need for preset parameters. VRSD also\nachieves a modest reduction in time complexity compared to MMR. Empirical\nvalidation confirms that VRSD significantly outperforms MMR across various\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector retrieval algorithms are essential for semantic queries within the\nrapidly evolving landscape of Large Language Models (LLMs). The ability to\nretrieve vectors that satisfy both similarity and diversity criteria\nsubstantially enhances the performance of LLMs. Although Maximal Marginal\nRelevance (MMR) is widely employed in retrieval scenarios requiring relevance\nand diversity, variations in the parameter $\\lambda$ lead to fluctuations that\ncomplicate the optimization trajectory in vector spaces. This obscures the\ndirection of improvement and highlights the lack of a robust theoretical\nanalysis regarding similarity and diversity constraints in retrieval processes.\nTo address these challenges, this paper introduces a novel approach that\ncharacterizes both constraints through the relationship between the sum vector\nand the query vector. The proximity of these vectors ensures the similarity\nconstraint, while requiring individual vectors within the sum vector to diverge\nin their alignment with the query vector satisfies the diversity constraint. We\nfirst formulate a new combinatorial optimization problem, selecting k vectors\nfrom a candidate set such that their sum vector maximally aligns with the query\nvector, and demonstrate that this problem is NP-complete. This result\nunderscores the inherent difficulty of simultaneously achieving similarity and\ndiversity in vector retrieval, thereby providing a theoretical foundation for\nfuture research. Subsequently, we present the heuristic algorithm Vectors\nRetrieval with Similarity and Diversity, VRSD, which features a clear\noptimization objective and eliminates the need for preset parameters. VRSD also\nachieves a modest reduction in time complexity compared to MMR. Empirical\nvalidation confirms that VRSD significantly outperforms MMR across various\ndatasets."
                },
                "authors": [
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09625v1",
                "updated": "2024-11-14T17:49:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    49,
                    27,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T17:49:27Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    49,
                    27,
                    3,
                    319,
                    0
                ],
                "title": "Local deployment of large-scale music AI models on commodity hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local deployment of large-scale music AI models on commodity hardware"
                },
                "summary": "We present the MIDInfinite, a web application capable of generating symbolic\nmusic using a large-scale generative AI model locally on commodity hardware.\nCreating this demo involved porting the Anticipatory Music Transformer, a large\nlanguage model (LLM) pre-trained on the Lakh MIDI dataset, to the Machine\nLearning Compilation (MLC) framework. Once the model is ported, MLC facilitates\ninference on a variety of runtimes including C++, mobile, and the browser. We\nenvision that MLC has the potential to bridge the gap between the landscape of\nincreasingly capable music AI models and technology more familiar to music\nsoftware developers. As a proof of concept, we build a web application that\nallows users to generate endless streams of multi-instrumental MIDI in the\nbrowser, either from scratch or conditioned on a prompt. On commodity hardware\n(an M3 Macbook Pro), our demo can generate 51 notes per second, which is faster\nthan real-time playback for 72.9% of generations, and increases to 86.3% with 2\nseconds of upfront buffering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the MIDInfinite, a web application capable of generating symbolic\nmusic using a large-scale generative AI model locally on commodity hardware.\nCreating this demo involved porting the Anticipatory Music Transformer, a large\nlanguage model (LLM) pre-trained on the Lakh MIDI dataset, to the Machine\nLearning Compilation (MLC) framework. Once the model is ported, MLC facilitates\ninference on a variety of runtimes including C++, mobile, and the browser. We\nenvision that MLC has the potential to bridge the gap between the landscape of\nincreasingly capable music AI models and technology more familiar to music\nsoftware developers. As a proof of concept, we build a web application that\nallows users to generate endless streams of multi-instrumental MIDI in the\nbrowser, either from scratch or conditioned on a prompt. On commodity hardware\n(an M3 Macbook Pro), our demo can generate 51 notes per second, which is faster\nthan real-time playback for 72.9% of generations, and increases to 86.3% with 2\nseconds of upfront buffering."
                },
                "authors": [
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Charlie Ruan"
                    },
                    {
                        "name": "Zihe Zhao"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Chris Donahue"
                    }
                ],
                "author_detail": {
                    "name": "Chris Donahue"
                },
                "author": "Chris Donahue",
                "arxiv_comment": "2 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09613v1",
                "updated": "2024-11-14T17:33:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    33,
                    36,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T17:33:36Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    33,
                    36,
                    3,
                    319,
                    0
                ],
                "title": "PTR: Precision-Driven Tool Recommendation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PTR: Precision-Driven Tool Recommendation for Large Language Models"
                },
                "summary": "By augmenting Large Language Models (LLMs) with external tools, their\ncapacity to solve complex problems has been significantly enhanced. However,\ndespite ongoing advancements in the parsing capabilities of LLMs, incorporating\nall available tools simultaneously in the prompt remains impractical due to the\nvast number of external tools. Consequently, it is essential to provide LLMs\nwith a precise set of tools tailored to the specific task, considering both\nquantity and quality. Current tool retrieval methods primarily focus on\nrefining the ranking list of tools and directly packaging a fixed number of\ntop-ranked tools as the tool set. However, these approaches often fail to equip\nLLMs with the optimal set of tools prior to execution, since the optimal number\nof tools for different tasks could be different, resulting in inefficiencies\nsuch as redundant or unsuitable tools, which impede immediate access to the\nmost relevant tools. This paper addresses the challenge of recommending precise\ntoolsets for LLMs. We introduce the problem of tool recommendation, define its\nscope, and propose a novel Precision-driven Tool Recommendation (PTR) approach.\nPTR captures an initial, concise set of tools by leveraging historical tool\nbundle usage and dynamically adjusts the tool set by performing tool matching,\nculminating in a multi-view-based tool addition. Additionally, we present a new\ndataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness\nof tool recommendation for LLMs. We further validate our design choices through\ncomprehensive experiments, demonstrating promising accuracy across two open\nbenchmarks and our RecTools dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By augmenting Large Language Models (LLMs) with external tools, their\ncapacity to solve complex problems has been significantly enhanced. However,\ndespite ongoing advancements in the parsing capabilities of LLMs, incorporating\nall available tools simultaneously in the prompt remains impractical due to the\nvast number of external tools. Consequently, it is essential to provide LLMs\nwith a precise set of tools tailored to the specific task, considering both\nquantity and quality. Current tool retrieval methods primarily focus on\nrefining the ranking list of tools and directly packaging a fixed number of\ntop-ranked tools as the tool set. However, these approaches often fail to equip\nLLMs with the optimal set of tools prior to execution, since the optimal number\nof tools for different tasks could be different, resulting in inefficiencies\nsuch as redundant or unsuitable tools, which impede immediate access to the\nmost relevant tools. This paper addresses the challenge of recommending precise\ntoolsets for LLMs. We introduce the problem of tool recommendation, define its\nscope, and propose a novel Precision-driven Tool Recommendation (PTR) approach.\nPTR captures an initial, concise set of tools by leveraging historical tool\nbundle usage and dynamically adjusts the tool set by performing tool matching,\nculminating in a multi-view-based tool addition. Additionally, we present a new\ndataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness\nof tool recommendation for LLMs. We further validate our design choices through\ncomprehensive experiments, demonstrating promising accuracy across two open\nbenchmarks and our RecTools dataset."
                },
                "authors": [
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09601v1",
                "updated": "2024-11-14T17:21:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    21,
                    2,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T17:21:02Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    21,
                    2,
                    3,
                    319,
                    0
                ],
                "title": "Accelerating Knowledge Graph and Ontology Engineering with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Knowledge Graph and Ontology Engineering with Large\n  Language Models"
                },
                "summary": "Large Language Models bear the promise of significant acceleration of key\nKnowledge Graph and Ontology Engineering tasks, including ontology modeling,\nextension, modification, population, alignment, as well as entity\ndisambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering\nas a new and coming area of research, and argue that modular approaches to\nontologies will be of central importance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models bear the promise of significant acceleration of key\nKnowledge Graph and Ontology Engineering tasks, including ontology modeling,\nextension, modification, population, alignment, as well as entity\ndisambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering\nas a new and coming area of research, and argue that modular approaches to\nontologies will be of central importance."
                },
                "authors": [
                    {
                        "name": "Cogan Shimizu"
                    },
                    {
                        "name": "Pascal Hitzler"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Hitzler"
                },
                "author": "Pascal Hitzler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09595v1",
                "updated": "2024-11-14T17:08:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    8,
                    23,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T17:08:23Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    8,
                    23,
                    3,
                    319,
                    0
                ],
                "title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models"
                },
                "summary": "This work explores expanding the capabilities of large language models (LLMs)\npretrained on text to generate 3D meshes within a unified model. This offers\nkey advantages of (1) leveraging spatial knowledge already embedded in LLMs,\nderived from textual sources like 3D tutorials, and (2) enabling conversational\n3D generation and mesh understanding. A primary challenge is effectively\ntokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly.\nTo address this, we introduce LLaMA-Mesh, a novel approach that represents the\nvertex coordinates and face definitions of 3D meshes as plain text, allowing\ndirect integration with LLMs without expanding the vocabulary. We construct a\nsupervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate\n3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs\nas required, and (3) understand and interpret 3D meshes. Our work is the first\nto demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge\nfor 3D mesh generation in a text-based format, effectively unifying the 3D and\ntext modalities. LLaMA-Mesh achieves mesh generation quality on par with models\ntrained from scratch while maintaining strong text generation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores expanding the capabilities of large language models (LLMs)\npretrained on text to generate 3D meshes within a unified model. This offers\nkey advantages of (1) leveraging spatial knowledge already embedded in LLMs,\nderived from textual sources like 3D tutorials, and (2) enabling conversational\n3D generation and mesh understanding. A primary challenge is effectively\ntokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly.\nTo address this, we introduce LLaMA-Mesh, a novel approach that represents the\nvertex coordinates and face definitions of 3D meshes as plain text, allowing\ndirect integration with LLMs without expanding the vocabulary. We construct a\nsupervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate\n3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs\nas required, and (3) understand and interpret 3D meshes. Our work is the first\nto demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge\nfor 3D mesh generation in a text-based format, effectively unifying the 3D and\ntext modalities. LLaMA-Mesh achieves mesh generation quality on par with models\ntrained from scratch while maintaining strong text generation performance."
                },
                "authors": [
                    {
                        "name": "Zhengyi Wang"
                    },
                    {
                        "name": "Jonathan Lorraine"
                    },
                    {
                        "name": "Yikai Wang"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Xiaohui Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohui Zeng"
                },
                "author": "Xiaohui Zeng",
                "arxiv_comment": "See the project website at\n  https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5; I.2.10; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09590v1",
                "updated": "2024-11-14T17:01:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    1,
                    24,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T17:01:24Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    1,
                    24,
                    3,
                    319,
                    0
                ],
                "title": "Adopting RAG for LLM-Aided Future Vehicle Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adopting RAG for LLM-Aided Future Vehicle Design"
                },
                "summary": "In this paper, we explore the integration of Large Language Models (LLMs)\nwith Retrieval-Augmented Generation (RAG) to enhance automated design and\nsoftware development in the automotive industry. We present two case studies: a\nstandardization compliance chatbot and a design copilot, both utilizing RAG to\nprovide accurate, context-aware responses. We evaluate four LLMs-GPT-4o,\nLLAMA3, Mistral, and Mixtral -- comparing their answering accuracy and\nexecution time. Our results demonstrate that while GPT-4 offers superior\nperformance, LLAMA3 and Mistral also show promising capabilities for local\ndeployment, addressing data privacy concerns in automotive applications. This\nstudy highlights the potential of RAG-augmented LLMs in improving design\nworkflows and compliance in automotive engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the integration of Large Language Models (LLMs)\nwith Retrieval-Augmented Generation (RAG) to enhance automated design and\nsoftware development in the automotive industry. We present two case studies: a\nstandardization compliance chatbot and a design copilot, both utilizing RAG to\nprovide accurate, context-aware responses. We evaluate four LLMs-GPT-4o,\nLLAMA3, Mistral, and Mixtral -- comparing their answering accuracy and\nexecution time. Our results demonstrate that while GPT-4 offers superior\nperformance, LLAMA3 and Mistral also show promising capabilities for local\ndeployment, addressing data privacy concerns in automotive applications. This\nstudy highlights the potential of RAG-augmented LLMs in improving design\nworkflows and compliance in automotive engineering."
                },
                "authors": [
                    {
                        "name": "Vahid Zolfaghari"
                    },
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Fengjunjie Pan"
                    },
                    {
                        "name": "Krzysztof Lebioda"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "arxiv_comment": "Conference paper accepted in IEEE FLLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09439v1",
                "updated": "2024-11-14T16:58:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    58,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:58:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    58,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Spider: Any-to-Many Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spider: Any-to-Many Multimodal LLM"
                },
                "summary": "Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models\n(LLMs), enabling the integration of various modalities. However, Any-to-Any\nMLLMs are limited to generating pairwise modalities 'Text + X' within a single\nresponse, such as Text + {Image or Audio or Video}. To address this limitation,\nwe introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG)\nframework, which can generate an arbitrary combination of modalities 'Text +\nXs', such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our\nSpider integrates three core components: a Base Model for basic X-to-X (i.e.,\nAny-to-Any) modality processing, a novel Efficient Decoders-Controller for\ncontrolling multimodal Decoders to generate Xs (many-modal) contents, and an\nAny-to-Many Instruction Template designed for producing Xs signal prompts. To\ntrain Spider, we constructed a novel Text-formatted Many-Modal (TMM) dataset,\nwhich facilitates the learning of the X-to-Xs (i.e., Any-to-Many) capability\nnecessary for AMMG. Ultimately, the well-trained Spider generates a pseudo\nX-to-Xs dataset, the first-ever X-to-Xs many-modal dataset, enhancing the\npotential for AMMG task in future research. Overall, this work not only pushes\nthe boundary of multimodal interaction but also provides rich data support for\nadvancing the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models\n(LLMs), enabling the integration of various modalities. However, Any-to-Any\nMLLMs are limited to generating pairwise modalities 'Text + X' within a single\nresponse, such as Text + {Image or Audio or Video}. To address this limitation,\nwe introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG)\nframework, which can generate an arbitrary combination of modalities 'Text +\nXs', such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our\nSpider integrates three core components: a Base Model for basic X-to-X (i.e.,\nAny-to-Any) modality processing, a novel Efficient Decoders-Controller for\ncontrolling multimodal Decoders to generate Xs (many-modal) contents, and an\nAny-to-Many Instruction Template designed for producing Xs signal prompts. To\ntrain Spider, we constructed a novel Text-formatted Many-Modal (TMM) dataset,\nwhich facilitates the learning of the X-to-Xs (i.e., Any-to-Many) capability\nnecessary for AMMG. Ultimately, the well-trained Spider generates a pseudo\nX-to-Xs dataset, the first-ever X-to-Xs many-modal dataset, enhancing the\npotential for AMMG task in future research. Overall, this work not only pushes\nthe boundary of multimodal interaction but also provides rich data support for\nadvancing the field."
                },
                "authors": [
                    {
                        "name": "Jinxiang Lai"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Xiaocheng Lu"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09580v1",
                "updated": "2024-11-14T16:42:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    42,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:42:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    42,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Software Performance Engineering for Foundation Model-Powered Software\n  (FMware)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Performance Engineering for Foundation Model-Powered Software\n  (FMware)"
                },
                "summary": "The rise of Foundation Models (FMs) like Large Language Models (LLMs) is\nrevolutionizing software development. Despite the impressive prototypes,\ntransforming FMware into production-ready products demands complex engineering\nacross various domains. A critical but overlooked aspect is performance\nengineering, which aims at ensuring FMware meets performance goals such as\nthroughput and latency to avoid user dissatisfaction and financial loss. Often,\nperformance considerations are an afterthought, leading to costly optimization\nefforts post-deployment. FMware's high computational resource demands highlight\nthe need for efficient hardware use. Continuous performance engineering is\nessential to prevent degradation. This paper highlights the significance of\nSoftware Performance Engineering (SPE) in FMware, identifying four key\nchallenges: cognitive architecture design, communication protocols, tuning and\noptimization, and deployment. These challenges are based on literature surveys\nand experiences from developing an in-house FMware system. We discuss problems,\ncurrent practices, and innovative paths for the software engineering community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Foundation Models (FMs) like Large Language Models (LLMs) is\nrevolutionizing software development. Despite the impressive prototypes,\ntransforming FMware into production-ready products demands complex engineering\nacross various domains. A critical but overlooked aspect is performance\nengineering, which aims at ensuring FMware meets performance goals such as\nthroughput and latency to avoid user dissatisfaction and financial loss. Often,\nperformance considerations are an afterthought, leading to costly optimization\nefforts post-deployment. FMware's high computational resource demands highlight\nthe need for efficient hardware use. Continuous performance engineering is\nessential to prevent degradation. This paper highlights the significance of\nSoftware Performance Engineering (SPE) in FMware, identifying four key\nchallenges: cognitive architecture design, communication protocols, tuning and\noptimization, and deployment. These challenges are based on literature surveys\nand experiences from developing an in-house FMware system. We discuss problems,\ncurrent practices, and innovative paths for the software engineering community."
                },
                "authors": [
                    {
                        "name": "Haoxiang Zhang"
                    },
                    {
                        "name": "Shi Chang"
                    },
                    {
                        "name": "Arthur Leung"
                    },
                    {
                        "name": "Kishanthan Thangarajah"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Hanan Lutfiyya"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22980v2",
                "updated": "2024-11-14T16:40:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    40,
                    0,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-30T12:45:12Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    45,
                    12,
                    2,
                    304,
                    0
                ],
                "title": "Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices\n  with Hierarchical Heatmaps and Feature Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices\n  with Hierarchical Heatmaps and Feature Propagation"
                },
                "summary": "6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments. Benefiting from our end-to-end\nmethodology and efficient network design, our approach surpasses previous\nmethods in model inference efficiency and achieves real-time 6-Dof grasp\ndetection on edge devices. Furthermore, real-world experiments validate the\neffectiveness of our method, achieving a satisfactory 94% object grasping\nsuccess rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments. Benefiting from our end-to-end\nmethodology and efficient network design, our approach surpasses previous\nmethods in model inference efficiency and achieves real-time 6-Dof grasp\ndetection on edge devices. Furthermore, real-world experiments validate the\neffectiveness of our method, achieving a satisfactory 94% object grasping\nsuccess rate."
                },
                "authors": [
                    {
                        "name": "Kaiqin Yang"
                    },
                    {
                        "name": "Yixiang Dai"
                    },
                    {
                        "name": "Guijin Wang"
                    },
                    {
                        "name": "Siang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siang Chen"
                },
                "author": "Siang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09553v1",
                "updated": "2024-11-14T16:06:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    6,
                    30,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:06:30Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    6,
                    30,
                    3,
                    319,
                    0
                ],
                "title": "OOD-SEG: Out-Of-Distribution detection for image SEGmentation with\n  sparse multi-class positive-only annotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OOD-SEG: Out-Of-Distribution detection for image SEGmentation with\n  sparse multi-class positive-only annotations"
                },
                "summary": "Despite significant advancements, segmentation based on deep neural networks\nin medical and surgical imaging faces several challenges, two of which we aim\nto address in this work. First, acquiring complete pixel-level segmentation\nlabels for medical images is time-consuming and requires domain expertise.\nSecond, typical segmentation pipelines cannot detect out-of-distribution (OOD)\npixels, leaving them prone to spurious outputs during deployment. In this work,\nwe propose a novel segmentation approach exploiting OOD detection that learns\nonly from sparsely annotated pixels from multiple positive-only classes. %but\n\\emph{no background class} annotation. These multi-class positive annotations\nnaturally fall within the in-distribution (ID) set. Unlabelled pixels may\ncontain positive classes but also negative ones, including what is typically\nreferred to as \\emph{background} in standard segmentation formulations. Here,\nwe forgo the need for background annotation and consider these together with\nany other unseen classes as part of the OOD set. Our framework can integrate,\nat a pixel-level, any OOD detection approaches designed for classification\ntasks. To address the lack of existing OOD datasets and established evaluation\nmetric for medical image segmentation, we propose a cross-validation strategy\nthat treats held-out labelled classes as OOD. Extensive experiments on both\nmulti-class hyperspectral and RGB surgical imaging datasets demonstrate the\nrobustness and generalisation capability of our proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements, segmentation based on deep neural networks\nin medical and surgical imaging faces several challenges, two of which we aim\nto address in this work. First, acquiring complete pixel-level segmentation\nlabels for medical images is time-consuming and requires domain expertise.\nSecond, typical segmentation pipelines cannot detect out-of-distribution (OOD)\npixels, leaving them prone to spurious outputs during deployment. In this work,\nwe propose a novel segmentation approach exploiting OOD detection that learns\nonly from sparsely annotated pixels from multiple positive-only classes. %but\n\\emph{no background class} annotation. These multi-class positive annotations\nnaturally fall within the in-distribution (ID) set. Unlabelled pixels may\ncontain positive classes but also negative ones, including what is typically\nreferred to as \\emph{background} in standard segmentation formulations. Here,\nwe forgo the need for background annotation and consider these together with\nany other unseen classes as part of the OOD set. Our framework can integrate,\nat a pixel-level, any OOD detection approaches designed for classification\ntasks. To address the lack of existing OOD datasets and established evaluation\nmetric for medical image segmentation, we propose a cross-validation strategy\nthat treats held-out labelled classes as OOD. Extensive experiments on both\nmulti-class hyperspectral and RGB surgical imaging datasets demonstrate the\nrobustness and generalisation capability of our proposed framework."
                },
                "authors": [
                    {
                        "name": "Junwen Wang"
                    },
                    {
                        "name": "Zhonghao Wang"
                    },
                    {
                        "name": "Oscar MacCormac"
                    },
                    {
                        "name": "Jonathan Shapey"
                    },
                    {
                        "name": "Tom Vercauteren"
                    }
                ],
                "author_detail": {
                    "name": "Tom Vercauteren"
                },
                "author": "Tom Vercauteren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09539v1",
                "updated": "2024-11-14T15:55:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    55,
                    37,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T15:55:37Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    55,
                    37,
                    3,
                    319,
                    0
                ],
                "title": "A Practical Guide to Fine-tuning Language Models with Limited Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practical Guide to Fine-tuning Language Models with Limited Data"
                },
                "summary": "Employing pre-trained Large Language Models (LLMs) has become the de facto\nstandard in Natural Language Processing (NLP) despite their extensive data\nrequirements. Motivated by the recent surge in research focused on training\nLLMs with limited data, particularly in low-resource domains and languages,\nthis paper surveys recent transfer learning approaches to optimize model\nperformance in downstream tasks where data is scarce. We first address initial\nand continued pre-training strategies to better leverage prior knowledge in\nunseen domains and languages. We then examine how to maximize the utility of\nlimited data during fine-tuning and few-shot learning. The final section takes\na task-specific perspective, reviewing models and methods suited for different\nlevels of data scarcity. Our goal is to provide practitioners with practical\nguidelines for overcoming the challenges posed by constrained data while also\nhighlighting promising directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing pre-trained Large Language Models (LLMs) has become the de facto\nstandard in Natural Language Processing (NLP) despite their extensive data\nrequirements. Motivated by the recent surge in research focused on training\nLLMs with limited data, particularly in low-resource domains and languages,\nthis paper surveys recent transfer learning approaches to optimize model\nperformance in downstream tasks where data is scarce. We first address initial\nand continued pre-training strategies to better leverage prior knowledge in\nunseen domains and languages. We then examine how to maximize the utility of\nlimited data during fine-tuning and few-shot learning. The final section takes\na task-specific perspective, reviewing models and methods suited for different\nlevels of data scarcity. Our goal is to provide practitioners with practical\nguidelines for overcoming the challenges posed by constrained data while also\nhighlighting promising directions for future research."
                },
                "authors": [
                    {
                        "name": "Márton Szép"
                    },
                    {
                        "name": "Daniel Rueckert"
                    },
                    {
                        "name": "Rüdiger von Eisenhart-Rothe"
                    },
                    {
                        "name": "Florian Hinterwimmer"
                    }
                ],
                "author_detail": {
                    "name": "Florian Hinterwimmer"
                },
                "author": "Florian Hinterwimmer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09534v1",
                "updated": "2024-11-14T15:52:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    52,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T15:52:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    52,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Enabling Efficient Wearables: An Analysis of Low-Power Microcontrollers\n  for Biomedical Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Wearables: An Analysis of Low-Power Microcontrollers\n  for Biomedical Applications"
                },
                "summary": "Breakthroughs in ultra-low-power chip technology are transforming biomedical\nwearables, making it possible to monitor patients in real time with devices\noperating on mere {\\mu}W. Although many studies have examined the power\nperformance of commercial microcontrollers, it remains unclear which ones\nperform best across diverse application profiles and which hardware features\nare most crucial for minimizing energy consumption under varying computational\nloads. Identifying these features for typical wearable applications and\nunderstanding their effects on performance and energy efficiency are essential\nfor optimizing deployment strategies and informing future hardware designs. In\nthis work, we conduct an in-depth study of state-of-the-art (SoA)\nmicro-controller units(MCUs) in terms of processing capability and energy\nefficiency using representative end-to-end SoA wearable applications. We\nsystematically benchmark each platform across three primary application phases:\nidle, data acquisition, and processing, allowing a holistic assessment of the\nplatform processing capability and overall energy efficiency across varying\npatient-monitoring application profiles. Our detailed analysis of performance\nand energy discrepancies across different platforms reveals key strengths and\nlimitations of the current low-power hardware design and pinpoints the\nstrengths and weaknesses of SoA MCUs. We conclude with actionable insights for\nwearable application designers and hardware engineers, aiming to inform future\nhardware design improvements and support optimal platform selection for\nenergy-constrained biomedical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breakthroughs in ultra-low-power chip technology are transforming biomedical\nwearables, making it possible to monitor patients in real time with devices\noperating on mere {\\mu}W. Although many studies have examined the power\nperformance of commercial microcontrollers, it remains unclear which ones\nperform best across diverse application profiles and which hardware features\nare most crucial for minimizing energy consumption under varying computational\nloads. Identifying these features for typical wearable applications and\nunderstanding their effects on performance and energy efficiency are essential\nfor optimizing deployment strategies and informing future hardware designs. In\nthis work, we conduct an in-depth study of state-of-the-art (SoA)\nmicro-controller units(MCUs) in terms of processing capability and energy\nefficiency using representative end-to-end SoA wearable applications. We\nsystematically benchmark each platform across three primary application phases:\nidle, data acquisition, and processing, allowing a holistic assessment of the\nplatform processing capability and overall energy efficiency across varying\npatient-monitoring application profiles. Our detailed analysis of performance\nand energy discrepancies across different platforms reveals key strengths and\nlimitations of the current low-power hardware design and pinpoints the\nstrengths and weaknesses of SoA MCUs. We conclude with actionable insights for\nwearable application designers and hardware engineers, aiming to inform future\nhardware design improvements and support optimal platform selection for\nenergy-constrained biomedical applications."
                },
                "authors": [
                    {
                        "name": "Dimitrios Samakovlis"
                    },
                    {
                        "name": "Stefano Albini"
                    },
                    {
                        "name": "Rubén Rodríguez Álvarez"
                    },
                    {
                        "name": "Denisa-Andreea Constantinescu"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Miguel Peón-Quirós"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "arxiv_comment": "21 pages, 6 Figures, 6 Tables, Submitted to ACM TECS journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]