[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v1",
                "updated": "2025-02-20T18:59:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v1",
                "updated": "2025-02-20T18:50:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v3",
                "updated": "2025-02-20T16:01:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    1,
                    34,
                    3,
                    51,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14504v1",
                "updated": "2025-02-20T12:31:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:31:31Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "title": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Chenran Huang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Xiaoping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoping Zhang"
                },
                "author": "Xiaoping Zhang",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v2",
                "updated": "2025-02-20T12:14:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    14,
                    49,
                    3,
                    51,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v1",
                "updated": "2025-02-20T12:09:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "18 pages, 6 figures, submitted to SEA 2025, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v2",
                "updated": "2025-02-20T09:03:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    9,
                    3,
                    5,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14347v1",
                "updated": "2025-02-20T08:00:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T08:00:25Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "title": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure"
                },
                "summary": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved."
                },
                "authors": [
                    {
                        "name": "Zheyu Wang"
                    },
                    {
                        "name": "Lingfei Wang"
                    },
                    {
                        "name": "King Yau Yip"
                    },
                    {
                        "name": "Ying Kit Tsui"
                    },
                    {
                        "name": "Tsz Fung Poon"
                    },
                    {
                        "name": "Wenyan Wang"
                    },
                    {
                        "name": "Chun Wai Tsang"
                    },
                    {
                        "name": "Shanmin Wang"
                    },
                    {
                        "name": "David Graf"
                    },
                    {
                        "name": "Alexandre Pourret"
                    },
                    {
                        "name": "Gabriel Seyfarth"
                    },
                    {
                        "name": "Georg Knebel"
                    },
                    {
                        "name": "Kwing To Lai"
                    },
                    {
                        "name": "Wing Chi Yu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Swee K. Goh"
                    }
                ],
                "author_detail": {
                    "name": "Swee K. Goh"
                },
                "author": "Swee K. Goh",
                "arxiv_comment": "10 pages, 5 figures. Advanced Science (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v1",
                "updated": "2025-02-20T07:10:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "We will release the code soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14307v1",
                "updated": "2025-02-20T06:42:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T06:42:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "RL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning"
                },
                "summary": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach."
                },
                "authors": [
                    {
                        "name": "M. Caner Tol"
                    },
                    {
                        "name": "Kemal Derya"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v4",
                "updated": "2025-02-20T06:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    7,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14280v1",
                "updated": "2025-02-20T05:41:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T05:41:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks."
                },
                "authors": [
                    {
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Sarathkrishna Swaminathan"
                    },
                    {
                        "name": "Georgios Kollias"
                    },
                    {
                        "name": "Elliot Nelson"
                    },
                    {
                        "name": "Khushbu Pahwa"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Igor Melnyk"
                    },
                    {
                        "name": "Matthew Riemer"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Riemer"
                },
                "author": "Matthew Riemer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14220v1",
                "updated": "2025-02-20T03:27:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T03:27:00Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "title": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table"
                },
                "summary": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Qingcai Jiang"
                    },
                    {
                        "name": "Buxin Tu"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v1",
                "updated": "2025-02-19T19:12:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v4",
                "updated": "2025-02-19T17:53:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    11,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning"
                },
                "summary": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v1",
                "updated": "2025-02-19T16:54:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v2",
                "updated": "2025-02-19T11:10:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v4",
                "updated": "2025-02-19T10:39:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    39,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ToCa is honored to be accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v1",
                "updated": "2025-02-19T09:30:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13542v1",
                "updated": "2025-02-19T08:50:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T08:50:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference"
                },
                "summary": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiaqi Tang"
                    },
                    {
                        "name": "Shuangyin Li"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13502v1",
                "updated": "2025-02-19T07:43:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    43,
                    36,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T07:43:36Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    43,
                    36,
                    2,
                    50,
                    0
                ],
                "title": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference"
                },
                "summary": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache."
                },
                "authors": [
                    {
                        "name": "Burc Gokden"
                    }
                ],
                "author_detail": {
                    "name": "Burc Gokden"
                },
                "author": "Burc Gokden",
                "arxiv_comment": "15 pages, 1 figure, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v1",
                "updated": "2025-02-18T18:59:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v1",
                "updated": "2025-02-18T17:08:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12875v1",
                "updated": "2025-02-18T14:05:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:05:12Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "title": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations"
                },
                "summary": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Shaoxin Cui"
                    },
                    {
                        "name": "Wen Qiu"
                    },
                    {
                        "name": "Zhiqiang He"
                    },
                    {
                        "name": "Zhi Liu"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Bomin Mao"
                    },
                    {
                        "name": "Nei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Nei Kato"
                },
                "author": "Nei Kato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v1",
                "updated": "2025-02-18T09:11:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v2",
                "updated": "2025-02-18T07:58:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    7,
                    58,
                    29,
                    1,
                    49,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective"
                },
                "summary": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12574v1",
                "updated": "2025-02-18T06:26:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T06:26:05Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods."
                },
                "authors": [
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13176v1",
                "updated": "2025-02-18T04:08:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T04:08:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference"
                },
                "summary": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels."
                },
                "authors": [
                    {
                        "name": "Ahmed Burak Gulhan"
                    },
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Mahmut Kandemir"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v1",
                "updated": "2025-02-17T14:54:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v2",
                "updated": "2025-02-17T14:34:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    34,
                    58,
                    0,
                    48,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12216v1",
                "updated": "2025-02-17T08:39:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T08:39:43Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "title": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs"
                },
                "summary": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Qinyu Xu"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Zhichen Zeng"
                    },
                    {
                        "name": "Rohan Kadekodi"
                    },
                    {
                        "name": "Liangyu Zhao"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Baris Kasikci"
                    }
                ],
                "author_detail": {
                    "name": "Baris Kasikci"
                },
                "author": "Baris Kasikci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11501v1",
                "updated": "2025-02-17T07:05:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T07:05:36Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "title": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?"
                },
                "summary": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11444v1",
                "updated": "2025-02-17T05:02:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T05:02:25Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "title": "Does RAG Really Perform Bad For Long-Context Processing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does RAG Really Perform Bad For Long-Context Processing?"
                },
                "summary": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension."
                },
                "authors": [
                    {
                        "name": "Kun Luo"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09383v2",
                "updated": "2025-02-16T18:31:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    18,
                    31,
                    10,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-13T14:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    59,
                    3,
                    3,
                    44,
                    0
                ],
                "title": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic"
                },
                "summary": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality."
                },
                "authors": [
                    {
                        "name": "Naomi Muggleton"
                    },
                    {
                        "name": "Charles Rahal"
                    },
                    {
                        "name": "Aaron Reeves"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Reeves"
                },
                "author": "Aaron Reeves",
                "arxiv_doi": "10.1007/s42001-025-00360-4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s42001-025-00360-4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Computational Social Science, 8(2), 1-29 (2025)",
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v2",
                "updated": "2025-02-16T16:41:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    16,
                    41,
                    43,
                    6,
                    47,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v2",
                "updated": "2025-02-16T14:50:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    50,
                    0,
                    6,
                    47,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11147v1",
                "updated": "2025-02-16T14:28:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T14:28:52Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "title": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11101v1",
                "updated": "2025-02-16T12:33:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T12:33:16Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "title": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation."
                },
                "authors": [
                    {
                        "name": "Kun-Hui Lee"
                    },
                    {
                        "name": "Eunhwan Park"
                    },
                    {
                        "name": "Donghoon Han"
                    },
                    {
                        "name": "Seung-Hoon Na"
                    }
                ],
                "author_detail": {
                    "name": "Seung-Hoon Na"
                },
                "author": "Seung-Hoon Na",
                "arxiv_comment": "11 pages (Work in progress)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11083v1",
                "updated": "2025-02-16T11:37:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T11:37:14Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "title": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks"
                },
                "summary": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency."
                },
                "authors": [
                    {
                        "name": "Yuanjie Lyu"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yuhao Chen"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Tong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Tong Xu"
                },
                "author": "Tong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v1",
                "updated": "2025-02-16T09:08:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05231v2",
                "updated": "2025-02-15T23:54:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    23,
                    54,
                    38,
                    5,
                    46,
                    0
                ],
                "published": "2024-05-08T17:27:11Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    17,
                    27,
                    11,
                    2,
                    129,
                    0
                ],
                "title": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training"
                },
                "summary": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy."
                },
                "authors": [
                    {
                        "name": "Renjie Liu"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Haitian Jiang"
                    },
                    {
                        "name": "Zhenkun Cai"
                    },
                    {
                        "name": "Minjie Wang"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01939v2",
                "updated": "2025-02-15T18:09:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    18,
                    9,
                    50,
                    5,
                    46,
                    0
                ],
                "published": "2024-06-04T03:48:08Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    3,
                    48,
                    8,
                    1,
                    156,
                    0
                ],
                "title": "Speeding up Policy Simulation in Supply Chain RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speeding up Policy Simulation in Supply Chain RL"
                },
                "summary": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments."
                },
                "authors": [
                    {
                        "name": "Vivek Farias"
                    },
                    {
                        "name": "Joren Gijsbrechts"
                    },
                    {
                        "name": "Aryan Khojandi"
                    },
                    {
                        "name": "Tianyi Peng"
                    },
                    {
                        "name": "Andrew Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zheng"
                },
                "author": "Andrew Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10659v1",
                "updated": "2025-02-15T03:56:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "published": "2025-02-15T03:56:22Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "title": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA"
                },
                "summary": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design."
                },
                "authors": [
                    {
                        "name": "Jindong Li"
                    },
                    {
                        "name": "Tenglong Li"
                    },
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "arxiv_comment": "Accepted by DATE2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10389v1",
                "updated": "2025-02-14T18:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "title": "Region-Adaptive Sampling for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Region-Adaptive Sampling for Diffusion Transformers"
                },
                "summary": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications."
                },
                "authors": [
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yiqi Zhang"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Yuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Yang"
                },
                "author": "Yuqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v2",
                "updated": "2025-02-14T17:17:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    17,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_doi": "10.1145/3701716.3715192",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715192",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.09057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by WWW 2025 (Demo Track)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10220v1",
                "updated": "2025-02-14T15:14:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:14:53Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "title": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem"
                },
                "summary": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network."
                },
                "authors": [
                    {
                        "name": "Hugo Rodrigues de Brito"
                    },
                    {
                        "name": "Daniel Simon Baltensperger"
                    },
                    {
                        "name": "Kjetil Obstfelder Uhlen"
                    }
                ],
                "author_detail": {
                    "name": "Kjetil Obstfelder Uhlen"
                },
                "author": "Kjetil Obstfelder Uhlen",
                "arxiv_comment": "11 pages, 8 figures, CIGRE Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v1",
                "updated": "2025-02-14T13:55:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hlscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Joo Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jrg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jrgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Teich"
                },
                "author": "Jrgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09921v1",
                "updated": "2025-02-14T05:19:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T05:19:46Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "title": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing"
                },
                "summary": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption."
                },
                "authors": [
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Siung Noh"
                    },
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaewon Jung"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v1",
                "updated": "2025-02-14T03:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law"
                },
                "summary": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Fangjian Li"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09726v1",
                "updated": "2025-02-13T19:16:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:16:39Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "title": "Analysis of Robust and Secure DNS Protocols for IoT Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Robust and Secure DNS Protocols for IoT Devices"
                },
                "summary": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices."
                },
                "authors": [
                    {
                        "name": "Abdullah Aydeger"
                    },
                    {
                        "name": "Sanzida Hoque"
                    },
                    {
                        "name": "Engin Zeydan"
                    },
                    {
                        "name": "Kapal Dev"
                    }
                ],
                "author_detail": {
                    "name": "Kapal Dev"
                },
                "author": "Kapal Dev",
                "arxiv_comment": "6 pages, 2 tables, 2 figures. This paper has been accepted in the\n  2025 IEEE International Conference on Communications (ICC): SAC Cloud\n  Computing, Networking, and Storage Track. The final version will be published\n  in the IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v1",
                "updated": "2025-02-13T19:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v2",
                "updated": "2025-02-13T18:07:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    7,
                    4,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09541v1",
                "updated": "2025-02-13T17:57:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T17:57:05Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "title": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics"
                },
                "summary": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline."
                },
                "authors": [
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Advait Iyer"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v2",
                "updated": "2025-02-13T12:54:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    54,
                    36,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v1",
                "updated": "2025-02-13T06:44:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang"
                    },
                    {
                        "name": "Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "arxiv_affiliation": "Katie",
                "author": "Mingyi Hong",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08982v1",
                "updated": "2025-02-13T05:40:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T05:40:28Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "title": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory"
                },
                "summary": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Minghao Xie"
                    },
                    {
                        "name": "Shouqian Shi"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Heiner Litz"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "arxiv_doi": "10.14778/3705829.3705849",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3705829.3705849",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.08982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "PVLDB, 18(2): 335-348, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08910v1",
                "updated": "2025-02-13T02:52:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T02:52:01Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU"
                },
                "summary": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02690v2",
                "updated": "2025-02-12T14:32:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2024-04-03T12:37:34Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    12,
                    37,
                    34,
                    2,
                    94,
                    0
                ],
                "title": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse"
                },
                "summary": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths."
                },
                "authors": [
                    {
                        "name": "Yichuan Deng"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chiwun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chiwun Yang"
                },
                "author": "Chiwun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05431v2",
                "updated": "2025-02-12T13:54:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    54,
                    1,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-08T03:41:16Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    41,
                    16,
                    5,
                    39,
                    0
                ],
                "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding"
                },
                "summary": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08363v1",
                "updated": "2025-02-12T12:50:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:50:15Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "title": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding"
                },
                "summary": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores."
                },
                "authors": [
                    {
                        "name": "Konstantin Berestizshevsky"
                    },
                    {
                        "name": "Renzo Andri"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "arxiv_comment": "8 pages, 11 figures, work under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v2",
                "updated": "2025-02-12T11:05:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    5,
                    5,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v3",
                "updated": "2025-02-12T07:02:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    2,
                    6,
                    2,
                    43,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07903v1",
                "updated": "2025-02-11T19:17:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T19:17:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment"
                },
                "summary": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget."
                },
                "authors": [
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v1",
                "updated": "2025-02-11T18:58:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v2",
                "updated": "2025-02-11T17:48:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    48,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v2",
                "updated": "2025-02-11T17:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    36,
                    32,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference"
                },
                "summary": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07861v1",
                "updated": "2025-02-11T17:18:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:18:17Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "title": "BalanceKV: KV Cache Compression through Discrepancy Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BalanceKV: KV Cache Compression through Discrepancy Theory"
                },
                "summary": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Michael Kapralov"
                    },
                    {
                        "name": "Ekaterina Kochetkova"
                    },
                    {
                        "name": "Kshiteej Sheth"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03736v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03736v3",
                "updated": "2025-02-11T15:42:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    42,
                    19,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-06T04:22:11Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    4,
                    22,
                    11,
                    3,
                    158,
                    0
                ],
                "title": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data"
                },
                "summary": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD."
                },
                "authors": [
                    {
                        "name": "Jingyang Ou"
                    },
                    {
                        "name": "Shen Nie"
                    },
                    {
                        "name": "Kaiwen Xue"
                    },
                    {
                        "name": "Fengqi Zhu"
                    },
                    {
                        "name": "Jiacheng Sun"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Chongxuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongxuan Li"
                },
                "author": "Chongxuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03736v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03736v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v1",
                "updated": "2025-02-11T14:25:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v2",
                "updated": "2025-02-10T18:34:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    34,
                    53,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v2",
                "updated": "2025-02-10T17:19:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    19,
                    21,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v2",
                "updated": "2025-02-10T15:17:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    17,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06327v1",
                "updated": "2025-02-10T10:28:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T10:28:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Prompt-Driven Continual Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Driven Continual Graph Learning"
                },
                "summary": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Tianfei Zhou"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Rui Mao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Mao"
                },
                "author": "Rui Mao",
                "arxiv_comment": "12 pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06166v1",
                "updated": "2025-02-10T05:33:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T05:33:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators"
                },
                "summary": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices."
                },
                "authors": [
                    {
                        "name": "Qi Shao"
                    },
                    {
                        "name": "Xin-Jun Liu"
                    },
                    {
                        "name": "Huichan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Huichan Zhao"
                },
                "author": "Huichan Zhao",
                "arxiv_comment": "7 pages, 10 figures, accepted by ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v2",
                "updated": "2025-02-09T20:52:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    52,
                    26,
                    6,
                    40,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_doi": "10.1103/PhysRevD.111.032007",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.032007",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.04603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 14 figures",
                "arxiv_journal_ref": "Phys. Rev. D 111, 032007 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06901v1",
                "updated": "2025-02-09T20:02:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    2,
                    5,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T20:02:05Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    2,
                    5,
                    6,
                    40,
                    0
                ],
                "title": "Enabling Autoregressive Models to Fill In Masked Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Autoregressive Models to Fill In Masked Tokens"
                },
                "summary": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    }
                ],
                "author_detail": {
                    "name": "Guy Van den Broeck"
                },
                "author": "Guy Van den Broeck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05960v1",
                "updated": "2025-02-09T17:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T17:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "title": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4"
                },
                "summary": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Ping Liu"
                    },
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05859v1",
                "updated": "2025-02-09T11:36:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T11:36:45Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "title": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion"
                },
                "summary": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image."
                },
                "authors": [
                    {
                        "name": "Qingsong Yan"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Kaiyong Zhao"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Fei Deng"
                    }
                ],
                "author_detail": {
                    "name": "Fei Deng"
                },
                "author": "Fei Deng",
                "arxiv_comment": "3DV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05763v1",
                "updated": "2025-02-09T03:49:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T03:49:52Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "title": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay"
                },
                "summary": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance."
                },
                "authors": [
                    {
                        "name": "Nicholas Kernan"
                    },
                    {
                        "name": "Joey Li"
                    },
                    {
                        "name": "Rami Al-Dalky"
                    },
                    {
                        "name": "Michael Rabinovich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Rabinovich"
                },
                "author": "Michael Rabinovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v2",
                "updated": "2025-02-08T21:44:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    21,
                    44,
                    24,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Seluk Kse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v2",
                "updated": "2025-02-08T14:11:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    14,
                    11,
                    25,
                    5,
                    39,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v2",
                "updated": "2025-02-08T11:51:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    11,
                    51,
                    57,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05511v1",
                "updated": "2025-02-08T10:14:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T10:14:21Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "title": "New and Improved Bounds for Markov Paging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New and Improved Bounds for Markov Paging"
                },
                "summary": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown."
                },
                "authors": [
                    {
                        "name": "Chirag Pabbaraju"
                    },
                    {
                        "name": "Ali Vakilian"
                    }
                ],
                "author_detail": {
                    "name": "Ali Vakilian"
                },
                "author": "Ali Vakilian",
                "arxiv_comment": "26 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05433v1",
                "updated": "2025-02-08T03:46:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:46:28Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "title": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection"
                },
                "summary": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow."
                },
                "authors": [
                    {
                        "name": "Shuheng Zhang"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "Hongbo Zhou"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05429v1",
                "updated": "2025-02-08T03:35:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:35:55Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "title": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts"
                },
                "summary": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats."
                },
                "authors": [
                    {
                        "name": "Seonghun Son"
                    },
                    {
                        "name": "Daniel Moghimi"
                    },
                    {
                        "name": "Berk Gulmezoglu"
                    }
                ],
                "author_detail": {
                    "name": "Berk Gulmezoglu"
                },
                "author": "Berk Gulmezoglu",
                "arxiv_doi": "10.1145/3676641.3716274",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716274",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.05429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Proceedings of the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS) accepted",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12304v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12304v4",
                "updated": "2025-02-07T23:14:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    23,
                    14,
                    10,
                    4,
                    38,
                    0
                ],
                "published": "2024-05-20T18:11:45Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    18,
                    11,
                    45,
                    0,
                    141,
                    0
                ],
                "title": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach"
                },
                "summary": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated."
                },
                "authors": [
                    {
                        "name": "Stphane Pouget"
                    },
                    {
                        "name": "Louis-Nol Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3711847",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711847",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.12304v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12304v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05370v1",
                "updated": "2025-02-07T22:51:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T22:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving"
                },
                "summary": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Hanfei Yu"
                    },
                    {
                        "name": "Xingqi Cui"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v2",
                "updated": "2025-02-07T22:00:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    0,
                    48,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04923v1",
                "updated": "2025-02-07T13:41:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:41:51Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "title": "Cached Multi-Lora Composition for Multi-Concept Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Multi-Lora Composition for Multi-Concept Image Generation"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch."
                },
                "authors": [
                    {
                        "name": "Xiandong Zou"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Christos-Savvas Bouganis"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v2",
                "updated": "2025-02-07T13:09:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    9,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "arxiv_comment": "AAAI 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v1",
                "updated": "2025-02-07T08:48:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v2",
                "updated": "2025-02-06T20:26:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    20,
                    26,
                    24,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v1",
                "updated": "2025-02-06T15:26:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v1",
                "updated": "2025-02-06T13:41:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v4",
                "updated": "2025-02-06T12:32:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    32,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v1",
                "updated": "2025-02-06T12:19:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keon Vin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01449v2",
                "updated": "2025-02-06T08:36:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    36,
                    44,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-03T15:38:53Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    38,
                    53,
                    0,
                    34,
                    0
                ],
                "title": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies"
                },
                "summary": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs."
                },
                "authors": [
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Benigna Bruggmann"
                    },
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03805v1",
                "updated": "2025-02-06T06:31:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T06:31:47Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "title": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective"
                },
                "summary": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v1",
                "updated": "2025-02-06T04:16:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04393v1",
                "updated": "2025-02-06T03:56:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T03:56:11Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "title": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation"
                },
                "summary": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Wenzhang Sun"
                    },
                    {
                        "name": "Qirui Hou"
                    },
                    {
                        "name": "Donglin Di"
                    },
                    {
                        "name": "Jiahui Yang"
                    },
                    {
                        "name": "Yongjia Ma"
                    },
                    {
                        "name": "Jianxun Cui"
                    }
                ],
                "author_detail": {
                    "name": "Jianxun Cui"
                },
                "author": "Jianxun Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v1",
                "updated": "2025-02-20T18:59:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14860v1",
                "updated": "2025-02-20T18:59:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:59:31Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    31,
                    3,
                    51,
                    0
                ],
                "title": "Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning"
                },
                "summary": "Large language models (LLMs) often fail to ask effective questions under\nuncertainty, making them unreliable in domains where proactive\ninformation-gathering is essential for decisionmaking. We present ALFA, a\nframework that improves LLM question-asking by (i) decomposing the notion of a\n\"good\" question into a set of theory-grounded attributes (e.g., clarity,\nrelevance), (ii) controllably synthesizing attribute-specific question\nvariations, and (iii) aligning models via preference-based optimization to\nexplicitly learn to ask better questions along these fine-grained attributes.\nFocusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs\ndataset, composed of 17k real-world clinical interactions augmented with 80k\nattribute-specific preference pairs of follow-up questions, as well as a novel\nexpert-annotated interactive healthcare QA task to evaluate question-asking\nabilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on\nMediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level\nwin-rate of 64.4% and strong generalizability. Our findings suggest that\nexplicitly guiding question-asking with structured, fine-grained attributes\noffers a scalable path to improve LLMs, especially in expert application\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often fail to ask effective questions under\nuncertainty, making them unreliable in domains where proactive\ninformation-gathering is essential for decisionmaking. We present ALFA, a\nframework that improves LLM question-asking by (i) decomposing the notion of a\n\"good\" question into a set of theory-grounded attributes (e.g., clarity,\nrelevance), (ii) controllably synthesizing attribute-specific question\nvariations, and (iii) aligning models via preference-based optimization to\nexplicitly learn to ask better questions along these fine-grained attributes.\nFocusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs\ndataset, composed of 17k real-world clinical interactions augmented with 80k\nattribute-specific preference pairs of follow-up questions, as well as a novel\nexpert-annotated interactive healthcare QA task to evaluate question-asking\nabilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on\nMediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level\nwin-rate of 64.4% and strong generalizability. Our findings suggest that\nexplicitly guiding question-asking with structured, fine-grained attributes\noffers a scalable path to improve LLMs, especially in expert application\ndomains."
                },
                "authors": [
                    {
                        "name": "Shuyue Stella Li"
                    },
                    {
                        "name": "Jimin Mun"
                    },
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Jonathan S. Ilgen"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "22 pages, 8 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14856v1",
                "updated": "2025-02-20T18:58:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    58,
                    10,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:58:10Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    58,
                    10,
                    3,
                    51,
                    0
                ],
                "title": "FR-Spec: Accelerating Large-Vocabulary Language Models via\n  Frequency-Ranked Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FR-Spec: Accelerating Large-Vocabulary Language Models via\n  Frequency-Ranked Speculative Sampling"
                },
                "summary": "Speculative sampling has emerged as an important technique for accelerating\nthe auto-regressive generation process of large language models (LLMs) by\nutilizing a draft-then-verify mechanism to produce multiple tokens per forward\npass. While state-of-the-art speculative sampling methods use only a single\nlayer and a language modeling (LM) head as the draft model to achieve\nimpressive layer compression, their efficiency gains are substantially reduced\nfor large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.\nTo address this, we present FR-Spec, a frequency-ranked speculative sampling\nframework that optimizes draft candidate selection through vocabulary space\ncompression. By constraining the draft search to a frequency-prioritized token\nsubset, our method reduces LM Head computation overhead by 75% while ensuring\nthe equivalence of the final output distribution. Experiments across multiple\ndatasets demonstrate an average of 1.12$\\times$ speedup over the\nstate-of-the-art speculative sampling method EAGLE-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling has emerged as an important technique for accelerating\nthe auto-regressive generation process of large language models (LLMs) by\nutilizing a draft-then-verify mechanism to produce multiple tokens per forward\npass. While state-of-the-art speculative sampling methods use only a single\nlayer and a language modeling (LM) head as the draft model to achieve\nimpressive layer compression, their efficiency gains are substantially reduced\nfor large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.\nTo address this, we present FR-Spec, a frequency-ranked speculative sampling\nframework that optimizes draft candidate selection through vocabulary space\ncompression. By constraining the draft search to a frequency-prioritized token\nsubset, our method reduces LM Head computation overhead by 75% while ensuring\nthe equivalence of the final output distribution. Experiments across multiple\ndatasets demonstrate an average of 1.12$\\times$ speedup over the\nstate-of-the-art speculative sampling method EAGLE-2."
                },
                "authors": [
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Tengyu Pan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yudi Zhang"
                    },
                    {
                        "name": "Ao Sun"
                    },
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Kaihuo Zhang"
                    },
                    {
                        "name": "Weilun Zhao"
                    },
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14855v1",
                "updated": "2025-02-20T18:58:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    58,
                    7,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:58:07Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    58,
                    7,
                    3,
                    51,
                    0
                ],
                "title": "Prompt-to-Leaderboard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-to-Leaderboard"
                },
                "summary": "Large language model (LLM) evaluations typically rely on aggregated metrics\nlike accuracy or human preference, averaging across users and prompts. This\naveraging obscures user- and prompt-specific variations in model performance.\nTo address this, we propose Prompt-to-Leaderboard (P2L), a method that produces\nleaderboards specific to a prompt. The core idea is to train an LLM taking\nnatural language prompts as input to output a vector of Bradley-Terry\ncoefficients which are then used to predict the human preference vote. The\nresulting prompt-dependent leaderboards allow for unsupervised task-specific\nevaluation, optimal routing of queries to models, personalization, and\nautomated evaluation of model strengths and weaknesses. Data from Chatbot Arena\nsuggest that P2L better captures the nuanced landscape of language model\nperformance than the averaged leaderboard. Furthermore, our findings suggest\nthat P2L's ability to produce prompt-specific evaluations follows a power law\nscaling similar to that observed in LLMs themselves. In January 2025, the\nrouter we trained based on this methodology achieved the \\#1 spot in the\nChatbot Arena leaderboard. Our code is available at this GitHub link:\nhttps://github.com/lmarena/p2l.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) evaluations typically rely on aggregated metrics\nlike accuracy or human preference, averaging across users and prompts. This\naveraging obscures user- and prompt-specific variations in model performance.\nTo address this, we propose Prompt-to-Leaderboard (P2L), a method that produces\nleaderboards specific to a prompt. The core idea is to train an LLM taking\nnatural language prompts as input to output a vector of Bradley-Terry\ncoefficients which are then used to predict the human preference vote. The\nresulting prompt-dependent leaderboards allow for unsupervised task-specific\nevaluation, optimal routing of queries to models, personalization, and\nautomated evaluation of model strengths and weaknesses. Data from Chatbot Arena\nsuggest that P2L better captures the nuanced landscape of language model\nperformance than the averaged leaderboard. Furthermore, our findings suggest\nthat P2L's ability to produce prompt-specific evaluations follows a power law\nscaling similar to that observed in LLMs themselves. In January 2025, the\nrouter we trained based on this methodology achieved the \\#1 spot in the\nChatbot Arena leaderboard. Our code is available at this GitHub link:\nhttps://github.com/lmarena/p2l."
                },
                "authors": [
                    {
                        "name": "Evan Frick"
                    },
                    {
                        "name": "Connor Chen"
                    },
                    {
                        "name": "Joseph Tennyson"
                    },
                    {
                        "name": "Tianle Li"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Anastasios N. Angelopoulos"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14854v1",
                "updated": "2025-02-20T18:58:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    58,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:58:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    58,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "CLIPPER: Compression enables long-context synthetic data generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIPPER: Compression enables long-context synthetic data generation"
                },
                "summary": "LLM developers are increasingly reliant on synthetic data, but generating\nhigh-quality data for complex long-context reasoning tasks remains challenging.\nWe introduce CLIPPER, a compression-based approach for generating synthetic\ndata tailored to narrative claim verification - a task that requires reasoning\nover a book to verify a given claim. Instead of generating claims directly from\nthe raw text of the book, which results in artifact-riddled claims, CLIPPER\nfirst compresses the book into chapter outlines and book summaries and then\nuses these intermediate representations to generate complex claims and\ncorresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces\nclaims that are more valid, grounded, and complex. Using CLIPPER, we construct\na dataset of 19K synthetic book claims paired with their source texts and\nchain-of-thought reasoning, and use it to fine-tune three open-weight models.\nOur best model achieves breakthrough results on narrative claim verification\n(from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for\nsub-10B models on the NoCha leaderboard. Further analysis shows that our models\ngenerate more detailed and grounded chain-of-thought reasoning while also\nimproving performance on other narrative understanding tasks (e.g.,\nNarrativeQA).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM developers are increasingly reliant on synthetic data, but generating\nhigh-quality data for complex long-context reasoning tasks remains challenging.\nWe introduce CLIPPER, a compression-based approach for generating synthetic\ndata tailored to narrative claim verification - a task that requires reasoning\nover a book to verify a given claim. Instead of generating claims directly from\nthe raw text of the book, which results in artifact-riddled claims, CLIPPER\nfirst compresses the book into chapter outlines and book summaries and then\nuses these intermediate representations to generate complex claims and\ncorresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces\nclaims that are more valid, grounded, and complex. Using CLIPPER, we construct\na dataset of 19K synthetic book claims paired with their source texts and\nchain-of-thought reasoning, and use it to fine-tune three open-weight models.\nOur best model achieves breakthrough results on narrative claim verification\n(from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for\nsub-10B models on the NoCha leaderboard. Further analysis shows that our models\ngenerate more detailed and grounded chain-of-thought reasoning while also\nimproving performance on other narrative understanding tasks (e.g.,\nNarrativeQA)."
                },
                "authors": [
                    {
                        "name": "Chau Minh Pham"
                    },
                    {
                        "name": "Yapei Chang"
                    },
                    {
                        "name": "Mohit Iyyer"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Iyyer"
                },
                "author": "Mohit Iyyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14851v1",
                "updated": "2025-02-20T18:57:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    57,
                    6,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:57:06Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    57,
                    6,
                    3,
                    51,
                    0
                ],
                "title": "Fast Generation of Weak Lensing Maps in Modified Gravity with COLA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Generation of Weak Lensing Maps in Modified Gravity with COLA"
                },
                "summary": "Accurate predictions of weak lensing observables are essential for\nunderstanding the large-scale structure of the Universe and probing the nature\nof gravity. In this work, we present a lightcone implementation to generate\nmaps of the weak lensing convergence field using the COmoving Lagrangian\nAcceleration (COLA) method. The lightcone is constructed in spherical shells\nfrom the source to the observer following an onion representation of the\nUniverse.\n  We validate the COLA-generated convergence maps in General Relativity by\ncomparing five statistics to those of maps obtained with the high-resolution\n$N$-body simulations presented in Takahashi $et\\ al.$ (2017): the power\nspectrum, bispectrum, probability distribution function, peak counts and\nMinkowski functionals. The convergence power spectrum is accurate to within\n$5\\%$ up to $\\ell\\sim500$ and to within $10\\%$ up to $\\ell\\sim750$, confirming\nthe accuracy of this method on both linear and non-linear scales. For the\nprobability distribution function, peak counts and Minkowski functionals, we\ndetermine the map pixel resolution required for COLA to capture the statistical\nfeatures of the $N$-body convergence maps.\n  Our validation tests provide a baseline for the convergence map\nspecifications at which we can trust COLA for each statistic considered. Using\nthese map specifications, we extend our analyses to two representative theories\nof Modified Gravity, and demonstrate their imprints on the five convergence\nstatistics considered. This work represents a step towards precise weak lensing\npredictions under both General Relativity and Modified Gravity with reduced\ncomputational cost, providing a robust framework to explore the nature of\ngravity using field-level inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate predictions of weak lensing observables are essential for\nunderstanding the large-scale structure of the Universe and probing the nature\nof gravity. In this work, we present a lightcone implementation to generate\nmaps of the weak lensing convergence field using the COmoving Lagrangian\nAcceleration (COLA) method. The lightcone is constructed in spherical shells\nfrom the source to the observer following an onion representation of the\nUniverse.\n  We validate the COLA-generated convergence maps in General Relativity by\ncomparing five statistics to those of maps obtained with the high-resolution\n$N$-body simulations presented in Takahashi $et\\ al.$ (2017): the power\nspectrum, bispectrum, probability distribution function, peak counts and\nMinkowski functionals. The convergence power spectrum is accurate to within\n$5\\%$ up to $\\ell\\sim500$ and to within $10\\%$ up to $\\ell\\sim750$, confirming\nthe accuracy of this method on both linear and non-linear scales. For the\nprobability distribution function, peak counts and Minkowski functionals, we\ndetermine the map pixel resolution required for COLA to capture the statistical\nfeatures of the $N$-body convergence maps.\n  Our validation tests provide a baseline for the convergence map\nspecifications at which we can trust COLA for each statistic considered. Using\nthese map specifications, we extend our analyses to two representative theories\nof Modified Gravity, and demonstrate their imprints on the five convergence\nstatistics considered. This work represents a step towards precise weak lensing\npredictions under both General Relativity and Modified Gravity with reduced\ncomputational cost, providing a robust framework to explore the nature of\ngravity using field-level inference."
                },
                "authors": [
                    {
                        "name": "Sophie Hoyland"
                    },
                    {
                        "name": "Hans A. Winther"
                    },
                    {
                        "name": "Daniela Saadeh"
                    },
                    {
                        "name": "Kazuya Koyama"
                    },
                    {
                        "name": "Albert Izard"
                    }
                ],
                "author_detail": {
                    "name": "Albert Izard"
                },
                "author": "Albert Izard",
                "arxiv_comment": "18 pages, 12 figures; to be submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14848v1",
                "updated": "2025-02-20T18:56:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    56,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:56:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    56,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "GATE: Graph-based Adaptive Tool Evolution Across Diverse Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Graph-based Adaptive Tool Evolution Across Diverse Tasks"
                },
                "summary": "Large Language Models (LLMs) have shown great promise in tool-making, yet\nexisting frameworks often struggle to efficiently construct reliable toolsets\nand are limited to single-task settings. To address these challenges, we\npropose GATE (Graph-based Adaptive Tool Evolution), an adaptive framework that\ndynamically constructs and evolves a hierarchical graph of reusable tools\nacross multiple scenarios. We evaluate GATE on open-ended tasks (Minecraft),\nagent-based tasks (TextCraft, DABench), and code generation tasks (MATH, Date,\nTabMWP). Our results show that GATE achieves up to 4.3x faster milestone\ncompletion in Minecraft compared to the previous SOTA, and provides an average\nimprovement of 9.23% over existing tool-making methods in code generation tasks\nand 10.03% in agent tasks. GATE demonstrates the power of adaptive evolution,\nbalancing tool quantity, complexity, and functionality while maintaining high\nefficiency. Code and data are available at\n\\url{https://github.com/ayanami2003/GATE}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown great promise in tool-making, yet\nexisting frameworks often struggle to efficiently construct reliable toolsets\nand are limited to single-task settings. To address these challenges, we\npropose GATE (Graph-based Adaptive Tool Evolution), an adaptive framework that\ndynamically constructs and evolves a hierarchical graph of reusable tools\nacross multiple scenarios. We evaluate GATE on open-ended tasks (Minecraft),\nagent-based tasks (TextCraft, DABench), and code generation tasks (MATH, Date,\nTabMWP). Our results show that GATE achieves up to 4.3x faster milestone\ncompletion in Minecraft compared to the previous SOTA, and provides an average\nimprovement of 9.23% over existing tool-making methods in code generation tasks\nand 10.03% in agent tasks. GATE demonstrates the power of adaptive evolution,\nbalancing tool quantity, complexity, and functionality while maintaining high\nefficiency. Code and data are available at\n\\url{https://github.com/ayanami2003/GATE}."
                },
                "authors": [
                    {
                        "name": "Jianwen Luo"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Jinxiang Meng"
                    },
                    {
                        "name": "Fangyu Lei"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Shanshan Jiang"
                    },
                    {
                        "name": "Bin Dong"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "arxiv_comment": "8 pages of main text, 38 pages of appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14847v1",
                "updated": "2025-02-20T18:55:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    55,
                    39,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:55:39Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    55,
                    39,
                    3,
                    51,
                    0
                ],
                "title": "Red-Teaming LLM Multi-Agent Systems via Communication Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red-Teaming LLM Multi-Agent Systems via Communication Attacks"
                },
                "summary": "Large Language Model-based Multi-Agent Systems (LLM-MAS) have revolutionized\ncomplex problem-solving capability by enabling sophisticated agent\ncollaboration through message-based communications. While the communication\nframework is crucial for agent coordination, it also introduces a critical yet\nunexplored security vulnerability. In this work, we introduce\nAgent-in-the-Middle (AiTM), a novel attack that exploits the fundamental\ncommunication mechanisms in LLM-MAS by intercepting and manipulating\ninter-agent messages. Unlike existing attacks that compromise individual\nagents, AiTM demonstrates how an adversary can compromise entire multi-agent\nsystems by only manipulating the messages passing between agents. To enable the\nattack under the challenges of limited control and role-restricted\ncommunication format, we develop an LLM-powered adversarial agent with a\nreflection mechanism that generates contextually-aware malicious instructions.\nOur comprehensive evaluation across various frameworks, communication\nstructures, and real-world applications demonstrates that LLM-MAS is vulnerable\nto communication-based attacks, highlighting the need for robust security\nmeasures in multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Multi-Agent Systems (LLM-MAS) have revolutionized\ncomplex problem-solving capability by enabling sophisticated agent\ncollaboration through message-based communications. While the communication\nframework is crucial for agent coordination, it also introduces a critical yet\nunexplored security vulnerability. In this work, we introduce\nAgent-in-the-Middle (AiTM), a novel attack that exploits the fundamental\ncommunication mechanisms in LLM-MAS by intercepting and manipulating\ninter-agent messages. Unlike existing attacks that compromise individual\nagents, AiTM demonstrates how an adversary can compromise entire multi-agent\nsystems by only manipulating the messages passing between agents. To enable the\nattack under the challenges of limited control and role-restricted\ncommunication format, we develop an LLM-powered adversarial agent with a\nreflection mechanism that generates contextually-aware malicious instructions.\nOur comprehensive evaluation across various frameworks, communication\nstructures, and real-world applications demonstrates that LLM-MAS is vulnerable\nto communication-based attacks, highlighting the need for robust security\nmeasures in multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Pengfei He"
                    },
                    {
                        "name": "Yupin Lin"
                    },
                    {
                        "name": "Shen Dong"
                    },
                    {
                        "name": "Han Xu"
                    },
                    {
                        "name": "Yue Xing"
                    },
                    {
                        "name": "Hui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Liu"
                },
                "author": "Hui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14846v1",
                "updated": "2025-02-20T18:55:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    55,
                    30,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:55:30Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    55,
                    30,
                    3,
                    51,
                    0
                ],
                "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data Generation"
                },
                "summary": "Reasoning about images with rich text, such as charts and documents, is a\ncritical application of vision-language models (VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically create synthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-quality instruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-language instruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developing multimodal agents capable of acting in real-world\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning about images with rich text, such as charts and documents, is a\ncritical application of vision-language models (VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically create synthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-quality instruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-language instruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developing multimodal agents capable of acting in real-world\nenvironments."
                },
                "authors": [
                    {
                        "name": "Yue Yang"
                    },
                    {
                        "name": "Ajay Patel"
                    },
                    {
                        "name": "Matt Deitke"
                    },
                    {
                        "name": "Tanmay Gupta"
                    },
                    {
                        "name": "Luca Weihs"
                    },
                    {
                        "name": "Andrew Head"
                    },
                    {
                        "name": "Mark Yatskar"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Aniruddha Kembhavi"
                    },
                    {
                        "name": "Christopher Clark"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Clark"
                },
                "author": "Christopher Clark",
                "arxiv_comment": "20 pages, 19 figures, 9 tables, website:\n  https://yueyang1996.github.io/cosyn/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01839v2",
                "updated": "2025-02-20T18:52:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    52,
                    18,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-03T21:31:07Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    21,
                    31,
                    7,
                    0,
                    34,
                    0
                ],
                "title": "Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling\n  Verification"
                },
                "summary": "Sampling-based search, a simple paradigm for utilizing test-time compute,\ninvolves generating multiple candidate responses and selecting the best one --\ntypically by having models self-verify each response for correctness. In this\npaper, we study the scaling trends governing sampling-based search. Among our\nfindings is that simply scaling up a minimalist implementation of\nsampling-based search, using only random sampling and direct self-verification,\nprovides a practical inference method that, for example, elevates the reasoning\ncapabilities of Gemini v1.5 Pro above that of o1-Preview on popular benchmarks.\nWe partially attribute the scalability of sampling-based search to a phenomenon\nof implicit scaling, where sampling a larger pool of responses in turn improves\nself-verification accuracy. We further identify two useful principles for\nimproving self-verification capabilities with test-time compute: (1) comparing\nacross responses provides helpful signals about the locations of errors and\nhallucinations, and (2) different model output styles are useful for different\ncontexts -- chains of thought are useful for reasoning but harder to verify. We\nalso find that, though accurate verification can be elicited, frontier models\ndemonstrate remarkably weak out-of-box verification capabilities and introduce\na benchmark to measure progress on these deficiencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sampling-based search, a simple paradigm for utilizing test-time compute,\ninvolves generating multiple candidate responses and selecting the best one --\ntypically by having models self-verify each response for correctness. In this\npaper, we study the scaling trends governing sampling-based search. Among our\nfindings is that simply scaling up a minimalist implementation of\nsampling-based search, using only random sampling and direct self-verification,\nprovides a practical inference method that, for example, elevates the reasoning\ncapabilities of Gemini v1.5 Pro above that of o1-Preview on popular benchmarks.\nWe partially attribute the scalability of sampling-based search to a phenomenon\nof implicit scaling, where sampling a larger pool of responses in turn improves\nself-verification accuracy. We further identify two useful principles for\nimproving self-verification capabilities with test-time compute: (1) comparing\nacross responses provides helpful signals about the locations of errors and\nhallucinations, and (2) different model output styles are useful for different\ncontexts -- chains of thought are useful for reasoning but harder to verify. We\nalso find that, though accurate verification can be elicited, frontier models\ndemonstrate remarkably weak out-of-box verification capabilities and introduce\na benchmark to measure progress on these deficiencies."
                },
                "authors": [
                    {
                        "name": "Eric Zhao"
                    },
                    {
                        "name": "Pranjal Awasthi"
                    },
                    {
                        "name": "Sreenivas Gollapudi"
                    }
                ],
                "author_detail": {
                    "name": "Sreenivas Gollapudi"
                },
                "author": "Sreenivas Gollapudi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14838v1",
                "updated": "2025-02-20T18:51:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    51,
                    12,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:51:12Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    51,
                    12,
                    3,
                    51,
                    0
                ],
                "title": "Revealing and Mitigating Over-Attention in Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing and Mitigating Over-Attention in Knowledge Editing"
                },
                "summary": "Large Language Models have demonstrated superior performance across a wide\nrange of tasks, but they still exhibit undesirable errors due to incorrect\nknowledge learned from the training data. To avoid this, knowledge editing\nmethods emerged to precisely edit the specific model knowledge via efficiently\nmodifying a very small percentage of parameters. % However, those methods can\nlead to the problem of Specificity Failure: when the content related to the\nedited knowledge occurs in the context, it can inadvertently corrupt other\npre-existing knowledge. However, those methods can lead to the problem of\nSpecificity Failure, where the existing knowledge and capabilities are severely\ndegraded due to editing. Our preliminary indicates that Specificity Failure\nprimarily stems from the model's attention heads assigning excessive attention\nscores to entities related to the edited knowledge, thereby unduly focusing on\nspecific snippets within the context, which we denote as the Attention Drift\nphenomenon. To mitigate such Attention Drift issue, we introduce a simple yet\neffective method Selective Attention Drift Restriction}(SADR), which introduces\nan additional regularization term during the knowledge editing process to\nrestrict changes in the attention weight distribution, thereby preventing undue\nfocus on the edited entity. Experiments on five frequently used strong LLMs\ndemonstrate the effectiveness of our method, where SADR can significantly\nmitigate Specificity Failure in the predominant knowledge editing tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have demonstrated superior performance across a wide\nrange of tasks, but they still exhibit undesirable errors due to incorrect\nknowledge learned from the training data. To avoid this, knowledge editing\nmethods emerged to precisely edit the specific model knowledge via efficiently\nmodifying a very small percentage of parameters. % However, those methods can\nlead to the problem of Specificity Failure: when the content related to the\nedited knowledge occurs in the context, it can inadvertently corrupt other\npre-existing knowledge. However, those methods can lead to the problem of\nSpecificity Failure, where the existing knowledge and capabilities are severely\ndegraded due to editing. Our preliminary indicates that Specificity Failure\nprimarily stems from the model's attention heads assigning excessive attention\nscores to entities related to the edited knowledge, thereby unduly focusing on\nspecific snippets within the context, which we denote as the Attention Drift\nphenomenon. To mitigate such Attention Drift issue, we introduce a simple yet\neffective method Selective Attention Drift Restriction}(SADR), which introduces\nan additional regularization term during the knowledge editing process to\nrestrict changes in the attention weight distribution, thereby preventing undue\nfocus on the edited entity. Experiments on five frequently used strong LLMs\ndemonstrate the effectiveness of our method, where SADR can significantly\nmitigate Specificity Failure in the predominant knowledge editing tasks."
                },
                "authors": [
                    {
                        "name": "Pinzheng Wang"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Keyan Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qiaoming Zhu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v1",
                "updated": "2025-02-20T18:50:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07752v2",
                "updated": "2025-02-20T18:48:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    48,
                    58,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-11T18:27:19Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    27,
                    19,
                    1,
                    42,
                    0
                ],
                "title": "Towards Efficient Optimizer Design for LLM via Structured Fisher\n  Approximation with a Low-Rank Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Optimizer Design for LLM via Structured Fisher\n  Approximation with a Low-Rank Extension"
                },
                "summary": "Designing efficient optimizers for large language models (LLMs) with\nlow-memory requirements and fast convergence is an important and challenging\nproblem. This paper makes a step towards the systematic design of such\noptimizers through the lens of structured Fisher information matrix (FIM)\napproximation. We show that many state-of-the-art efficient optimizers can be\nviewed as solutions to FIM approximation (under the Frobenius norm) with\nspecific structural assumptions. Building on these insights, we propose two\ndesign recommendations of practical efficient optimizers for LLMs, involving\nthe careful selection of structural assumptions to balance generality and\nefficiency, and enhancing memory efficiency of optimizers with general\nstructures through a novel low-rank extension framework. We demonstrate how to\nuse each design approach by deriving new memory-efficient optimizers: Row and\nColumn Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation\n(Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the\neffectiveness, showing faster and better convergence than existing\nmemory-efficient baselines and Adam with little memory overhead. Notably, Alice\nachieves better than 2x faster convergence over Adam, while RACS delivers\nstrong performance on the 1B model with SGD-like memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing efficient optimizers for large language models (LLMs) with\nlow-memory requirements and fast convergence is an important and challenging\nproblem. This paper makes a step towards the systematic design of such\noptimizers through the lens of structured Fisher information matrix (FIM)\napproximation. We show that many state-of-the-art efficient optimizers can be\nviewed as solutions to FIM approximation (under the Frobenius norm) with\nspecific structural assumptions. Building on these insights, we propose two\ndesign recommendations of practical efficient optimizers for LLMs, involving\nthe careful selection of structural assumptions to balance generality and\nefficiency, and enhancing memory efficiency of optimizers with general\nstructures through a novel low-rank extension framework. We demonstrate how to\nuse each design approach by deriving new memory-efficient optimizers: Row and\nColumn Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation\n(Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the\neffectiveness, showing faster and better convergence than existing\nmemory-efficient baselines and Adam with little memory overhead. Notably, Alice\nachieves better than 2x faster convergence over Adam, while RACS delivers\nstrong performance on the 1B model with SGD-like memory."
                },
                "authors": [
                    {
                        "name": "Wenbo Gong"
                    },
                    {
                        "name": "Meyer Scetbon"
                    },
                    {
                        "name": "Chao Ma"
                    },
                    {
                        "name": "Edward Meeds"
                    }
                ],
                "author_detail": {
                    "name": "Edward Meeds"
                },
                "author": "Edward Meeds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13433v2",
                "updated": "2025-02-20T18:47:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    47,
                    24,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-19T05:07:56Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    5,
                    7,
                    56,
                    2,
                    50,
                    0
                ],
                "title": "MATS: An Audio Language Model under Text-only Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATS: An Audio Language Model under Text-only Supervision"
                },
                "summary": "Large audio-language models (LALMs), built upon powerful Large Language\nModels (LLMs), have exhibited remarkable audio comprehension and reasoning\ncapabilities. However, the training of LALMs demands a large corpus of\naudio-language pairs, which requires substantial costs in both data collection\nand training resources. In this paper, we propose MATS, an audio-language\nmultimodal LLM designed to handle Multiple Audio task using solely Text-only\nSupervision. By leveraging pre-trained audio-language alignment models such as\nCLAP, we develop a text-only training strategy that projects the shared\naudio-language latent space into LLM latent space, endowing the LLM with audio\ncomprehension capabilities without relying on audio data during training. To\nfurther bridge the modality gap between audio and language embeddings within\nCLAP, we propose the Strongly-related noisy text with audio (Santa) mechanism.\nSanta maps audio embeddings into CLAP language embedding space while preserving\nessential information from the audio input. Extensive experiments demonstrate\nthat MATS, despite being trained exclusively on text data, achieves competitive\nperformance compared to recent LALMs trained on large-scale audio-language\npairs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large audio-language models (LALMs), built upon powerful Large Language\nModels (LLMs), have exhibited remarkable audio comprehension and reasoning\ncapabilities. However, the training of LALMs demands a large corpus of\naudio-language pairs, which requires substantial costs in both data collection\nand training resources. In this paper, we propose MATS, an audio-language\nmultimodal LLM designed to handle Multiple Audio task using solely Text-only\nSupervision. By leveraging pre-trained audio-language alignment models such as\nCLAP, we develop a text-only training strategy that projects the shared\naudio-language latent space into LLM latent space, endowing the LLM with audio\ncomprehension capabilities without relying on audio data during training. To\nfurther bridge the modality gap between audio and language embeddings within\nCLAP, we propose the Strongly-related noisy text with audio (Santa) mechanism.\nSanta maps audio embeddings into CLAP language embedding space while preserving\nessential information from the audio input. Extensive experiments demonstrate\nthat MATS, despite being trained exclusively on text data, achieves competitive\nperformance compared to recent LALMs trained on large-scale audio-language\npairs."
                },
                "authors": [
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Ruibing Hou"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Shiguang Shan"
                    },
                    {
                        "name": "Xilin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xilin Chen"
                },
                "author": "Xilin Chen",
                "arxiv_comment": "19 pages,11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14830v1",
                "updated": "2025-02-20T18:45:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    45,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:45:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    45,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in\n  Fine-Tuned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in\n  Fine-Tuned LLMs"
                },
                "summary": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align)."
                },
                "authors": [
                    {
                        "name": "Danni Liu"
                    },
                    {
                        "name": "Jan Niehues"
                    }
                ],
                "author_detail": {
                    "name": "Jan Niehues"
                },
                "author": "Jan Niehues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14828v1",
                "updated": "2025-02-20T18:45:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    45,
                    1,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:45:01Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    45,
                    1,
                    3,
                    51,
                    0
                ],
                "title": "Fundamental Limitations in Defending LLM Finetuning APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limitations in Defending LLM Finetuning APIs"
                },
                "summary": "LLM developers have imposed technical interventions to prevent fine-tuning\nmisuse attacks, attacks where adversaries evade safeguards by fine-tuning the\nmodel using a public API. Previous work has established several successful\nattacks against specific fine-tuning API defences. In this work, we show that\ndefences of fine-tuning APIs that seek to detect individual harmful training or\ninference samples ('pointwise' detection) are fundamentally limited in their\nability to prevent fine-tuning attacks. We construct 'pointwise-undetectable'\nattacks that repurpose entropy in benign model outputs (e.g. semantic or\nsyntactic variations) to covertly transmit dangerous knowledge. Our attacks are\ncomposed solely of unsuspicious benign samples that can be collected from the\nmodel before fine-tuning, meaning training and inference samples are all\nindividually benign and low-perplexity. We test our attacks against the OpenAI\nfine-tuning API, finding they succeed in eliciting answers to harmful\nmultiple-choice questions, and that they evade an enhanced monitoring system we\ndesign that successfully detects other fine-tuning attacks. We encourage the\ncommunity to develop defences that tackle the fundamental limitations we\nuncover in pointwise fine-tuning API defences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM developers have imposed technical interventions to prevent fine-tuning\nmisuse attacks, attacks where adversaries evade safeguards by fine-tuning the\nmodel using a public API. Previous work has established several successful\nattacks against specific fine-tuning API defences. In this work, we show that\ndefences of fine-tuning APIs that seek to detect individual harmful training or\ninference samples ('pointwise' detection) are fundamentally limited in their\nability to prevent fine-tuning attacks. We construct 'pointwise-undetectable'\nattacks that repurpose entropy in benign model outputs (e.g. semantic or\nsyntactic variations) to covertly transmit dangerous knowledge. Our attacks are\ncomposed solely of unsuspicious benign samples that can be collected from the\nmodel before fine-tuning, meaning training and inference samples are all\nindividually benign and low-perplexity. We test our attacks against the OpenAI\nfine-tuning API, finding they succeed in eliciting answers to harmful\nmultiple-choice questions, and that they evade an enhanced monitoring system we\ndesign that successfully detects other fine-tuning attacks. We encourage the\ncommunity to develop defences that tackle the fundamental limitations we\nuncover in pointwise fine-tuning API defences."
                },
                "authors": [
                    {
                        "name": "Xander Davies"
                    },
                    {
                        "name": "Eric Winsor"
                    },
                    {
                        "name": "Tomek Korbak"
                    },
                    {
                        "name": "Alexandra Souly"
                    },
                    {
                        "name": "Robert Kirk"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "Yarin Gal"
                    }
                ],
                "author_detail": {
                    "name": "Yarin Gal"
                },
                "author": "Yarin Gal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14822v1",
                "updated": "2025-02-20T18:42:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    42,
                    58,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:42:58Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    42,
                    58,
                    3,
                    51,
                    0
                ],
                "title": "A Survey of Model Architectures in Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Model Architectures in Information Retrieval"
                },
                "summary": "This survey examines the evolution of model architectures in information\nretrieval (IR), focusing on two key aspects: backbone models for feature\nextraction and end-to-end system architectures for relevance estimation. The\nreview intentionally separates architectural considerations from training\nmethodologies to provide a focused analysis of structural innovations in IR\nsystems.We trace the development from traditional term-based methods to modern\nneural approaches, particularly highlighting the impact of transformer-based\nmodels and subsequent large language models (LLMs). We conclude by discussing\nemerging challenges and future directions, including architectural\noptimizations for performance and scalability, handling of multimodal,\nmultilingual data, and adaptation to novel application domains beyond\ntraditional search paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey examines the evolution of model architectures in information\nretrieval (IR), focusing on two key aspects: backbone models for feature\nextraction and end-to-end system architectures for relevance estimation. The\nreview intentionally separates architectural considerations from training\nmethodologies to provide a focused analysis of structural innovations in IR\nsystems.We trace the development from traditional term-based methods to modern\nneural approaches, particularly highlighting the impact of transformer-based\nmodels and subsequent large language models (LLMs). We conclude by discussing\nemerging challenges and future directions, including architectural\noptimizations for performance and scalability, handling of multimodal,\nmultilingual data, and adaptation to novel application domains beyond\ntraditional search paradigms."
                },
                "authors": [
                    {
                        "name": "Zhichao Xu"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Crystina Zhang"
                    },
                    {
                        "name": "Puxuan Yu"
                    },
                    {
                        "name": "Bei Wang"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Vivek Srikumar"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Srikumar"
                },
                "author": "Vivek Srikumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04370v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04370v3",
                "updated": "2025-02-20T18:42:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    42,
                    41,
                    3,
                    51,
                    0
                ],
                "published": "2024-06-01T02:08:44Z",
                "published_parsed": [
                    2024,
                    6,
                    1,
                    2,
                    8,
                    44,
                    5,
                    153,
                    0
                ],
                "title": "Large Language Model Confidence Estimation via Black-Box Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Confidence Estimation via Black-Box Access"
                },
                "summary": "Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\\&A tasks as well\nas of Pegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\\&A tasks as well\nas of Pegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset."
                },
                "authors": [
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    },
                    {
                        "name": "Soumya Ghosh"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Prasanna Sattigeri"
                    }
                ],
                "author_detail": {
                    "name": "Prasanna Sattigeri"
                },
                "author": "Prasanna Sattigeri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04370v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04370v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14820v1",
                "updated": "2025-02-20T18:41:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    41,
                    48,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:41:48Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    41,
                    48,
                    3,
                    51,
                    0
                ],
                "title": "eC-Tab2Text: Aspect-Based Text Generation from e-Commerce Product Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eC-Tab2Text: Aspect-Based Text Generation from e-Commerce Product Tables"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional versatility across\ndiverse domains, yet their application in e-commerce remains underexplored due\nto a lack of domain-specific datasets. To address this gap, we introduce\neC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce,\nincluding detailed product attributes and user-specific queries. Leveraging\neC-Tab2Text, we focus on text generation from product tables, enabling LLMs to\nproduce high-quality, attribute-specific product reviews from structured\ntabular data. Fine-tuned models were rigorously evaluated using standard\nTable2Text metrics, alongside correctness, faithfulness, and fluency\nassessments. Our results demonstrate substantial improvements in generating\ncontextually accurate reviews, highlighting the transformative potential of\ntailored datasets and fine-tuning methodologies in optimizing e-commerce\nworkflows. This work highlights the potential of LLMs in e-commerce workflows\nand the essential role of domain-specific datasets in tailoring them to\nindustry-specific challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional versatility across\ndiverse domains, yet their application in e-commerce remains underexplored due\nto a lack of domain-specific datasets. To address this gap, we introduce\neC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce,\nincluding detailed product attributes and user-specific queries. Leveraging\neC-Tab2Text, we focus on text generation from product tables, enabling LLMs to\nproduce high-quality, attribute-specific product reviews from structured\ntabular data. Fine-tuned models were rigorously evaluated using standard\nTable2Text metrics, alongside correctness, faithfulness, and fluency\nassessments. Our results demonstrate substantial improvements in generating\ncontextually accurate reviews, highlighting the transformative potential of\ntailored datasets and fine-tuning methodologies in optimizing e-commerce\nworkflows. This work highlights the potential of LLMs in e-commerce workflows\nand the essential role of domain-specific datasets in tailoring them to\nindustry-specific challenges."
                },
                "authors": [
                    {
                        "name": "Luis Antonio Gutirrez Guanilo"
                    },
                    {
                        "name": "Mir Tafseer Nayeem"
                    },
                    {
                        "name": "Cristian Lpez"
                    },
                    {
                        "name": "Davood Rafiei"
                    }
                ],
                "author_detail": {
                    "name": "Davood Rafiei"
                },
                "author": "Davood Rafiei",
                "arxiv_comment": "NAACL 2025 (Industry Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14816v1",
                "updated": "2025-02-20T18:37:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    37,
                    32,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:37:32Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    37,
                    32,
                    3,
                    51,
                    0
                ],
                "title": "Dynamic Low-Rank Sparse Adaptation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Low-Rank Sparse Adaptation for Large Language Models"
                },
                "summary": "Despite the efficacy of network sparsity in alleviating the deployment strain\nof Large Language Models (LLMs), it endures significant performance\ndegradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs\noffers an intuitive approach to counter this predicament, while it holds\nshortcomings include: 1) The inability to integrate LoRA weights into sparse\nLLMs post-training, and 2) Insufficient performance recovery at high sparsity\nratios. In this paper, we introduce dynamic Low-rank Sparse Adaptation (LoSA),\na novel method that seamlessly integrates low-rank adaptation into LLM sparsity\nwithin a unified framework, thereby enhancing the performance of sparse LLMs\nwithout increasing the inference latency. In particular, LoSA dynamically\nsparsifies the LoRA outcomes based on the corresponding sparse weights during\nfine-tuning, thus guaranteeing that the LoRA module can be integrated into the\nsparse LLMs post-training. Besides, LoSA leverages Representation Mutual\nInformation (RMI) as an indicator to determine the importance of layers,\nthereby efficiently determining the layer-wise sparsity rates during\nfine-tuning. Predicated on this, LoSA adjusts the rank of the LoRA module based\non the variability in layer-wise reconstruction errors, allocating an\nappropriate fine-tuning for each layer to reduce the output discrepancies\nbetween dense and sparse LLMs. Extensive experiments tell that LoSA can\nefficiently boost the efficacy of sparse LLMs within a few hours, without\nintroducing any additional inferential burden. For example, LoSA reduced the\nperplexity of sparse LLaMA-2-7B by 68.73 and increased zero-shot accuracy by\n16.32$\\%$, achieving a 2.60$\\times$ speedup on CPU and 2.23$\\times$ speedup on\nGPU, requiring only 45 minutes of fine-tuning on a single NVIDIA A100 80GB GPU.\nCode is available at https://github.com/wzhuang-xmu/LoSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the efficacy of network sparsity in alleviating the deployment strain\nof Large Language Models (LLMs), it endures significant performance\ndegradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs\noffers an intuitive approach to counter this predicament, while it holds\nshortcomings include: 1) The inability to integrate LoRA weights into sparse\nLLMs post-training, and 2) Insufficient performance recovery at high sparsity\nratios. In this paper, we introduce dynamic Low-rank Sparse Adaptation (LoSA),\na novel method that seamlessly integrates low-rank adaptation into LLM sparsity\nwithin a unified framework, thereby enhancing the performance of sparse LLMs\nwithout increasing the inference latency. In particular, LoSA dynamically\nsparsifies the LoRA outcomes based on the corresponding sparse weights during\nfine-tuning, thus guaranteeing that the LoRA module can be integrated into the\nsparse LLMs post-training. Besides, LoSA leverages Representation Mutual\nInformation (RMI) as an indicator to determine the importance of layers,\nthereby efficiently determining the layer-wise sparsity rates during\nfine-tuning. Predicated on this, LoSA adjusts the rank of the LoRA module based\non the variability in layer-wise reconstruction errors, allocating an\nappropriate fine-tuning for each layer to reduce the output discrepancies\nbetween dense and sparse LLMs. Extensive experiments tell that LoSA can\nefficiently boost the efficacy of sparse LLMs within a few hours, without\nintroducing any additional inferential burden. For example, LoSA reduced the\nperplexity of sparse LLaMA-2-7B by 68.73 and increased zero-shot accuracy by\n16.32$\\%$, achieving a 2.60$\\times$ speedup on CPU and 2.23$\\times$ speedup on\nGPU, requiring only 45 minutes of fine-tuning on a single NVIDIA A100 80GB GPU.\nCode is available at https://github.com/wzhuang-xmu/LoSA."
                },
                "authors": [
                    {
                        "name": "Weizhong Huang"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14815v1",
                "updated": "2025-02-20T18:36:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    36,
                    25,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:36:25Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    36,
                    25,
                    3,
                    51,
                    0
                ],
                "title": "Optimizing Model Selection for Compound AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Model Selection for Compound AI Systems"
                },
                "summary": "Compound AI systems that combine multiple LLM calls, such as self-refine and\nmulti-agent-debate, achieve strong performance on many AI tasks. We address a\ncore question in optimizing compound systems: for each LLM call or module in\nthe system, how should one decide which LLM to use? We show that these LLM\nchoices have a large effect on quality, but the search space is exponential. We\npropose LLMSelector, an efficient framework for model selection in compound\nsystems, which leverages two key empirical insights: (i) end-to-end performance\nis often monotonic in how well each module performs, with all other modules\nheld fixed, and (ii) per-module performance can be estimated accurately by an\nLLM. Building upon these insights, LLMSelector iteratively selects one module\nand allocates to it the model with the highest module-wise performance, as\nestimated by an LLM, until no further gain is possible. LLMSelector is\napplicable to any compound system with a bounded number of modules, and its\nnumber of API calls scales linearly with the number of modules, achieving\nhigh-quality model allocation both empirically and theoretically. Experiments\nwith popular compound systems such as multi-agent debate and self-refine using\nLLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector\nconfers 5%-70% accuracy gains compared to using the same LLM for all modules.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compound AI systems that combine multiple LLM calls, such as self-refine and\nmulti-agent-debate, achieve strong performance on many AI tasks. We address a\ncore question in optimizing compound systems: for each LLM call or module in\nthe system, how should one decide which LLM to use? We show that these LLM\nchoices have a large effect on quality, but the search space is exponential. We\npropose LLMSelector, an efficient framework for model selection in compound\nsystems, which leverages two key empirical insights: (i) end-to-end performance\nis often monotonic in how well each module performs, with all other modules\nheld fixed, and (ii) per-module performance can be estimated accurately by an\nLLM. Building upon these insights, LLMSelector iteratively selects one module\nand allocates to it the model with the highest module-wise performance, as\nestimated by an LLM, until no further gain is possible. LLMSelector is\napplicable to any compound system with a bounded number of modules, and its\nnumber of API calls scales linearly with the number of modules, achieving\nhigh-quality model allocation both empirically and theoretically. Experiments\nwith popular compound systems such as multi-agent debate and self-refine using\nLLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector\nconfers 5%-70% accuracy gains compared to using the same LLM for all modules."
                },
                "authors": [
                    {
                        "name": "Lingjiao Chen"
                    },
                    {
                        "name": "Jared Quincy Davis"
                    },
                    {
                        "name": "Boris Hanin"
                    },
                    {
                        "name": "Peter Bailis"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14802v1",
                "updated": "2025-02-20T18:26:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    26,
                    2,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:26:02Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    26,
                    2,
                    3,
                    51,
                    0
                ],
                "title": "From RAG to Memory: Non-Parametric Continual Learning for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From RAG to Memory: Non-Parametric Continual Learning for Large Language\n  Models"
                },
                "summary": "Our ability to continuously acquire, organize, and leverage knowledge is a\nkey feature of human intelligence that AI systems must approximate to unlock\ntheir full potential. Given the challenges in continual learning with large\nlanguage models (LLMs), retrieval-augmented generation (RAG) has become the\ndominant way to introduce new information. However, its reliance on vector\nretrieval hinders its ability to mimic the dynamic and interconnected nature of\nhuman long-term memory. Recent RAG approaches augment vector embeddings with\nvarious structures like knowledge graphs to address some of these gaps, namely\nsense-making and associativity. However, their performance on more basic\nfactual memory tasks drops considerably below standard RAG. We address this\nunintended deterioration and propose HippoRAG 2, a framework that outperforms\nstandard RAG comprehensively on factual, sense-making, and associative memory\ntasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in\nHippoRAG and enhances it with deeper passage integration and more effective\nonline use of an LLM. This combination pushes this RAG system closer to the\neffectiveness of human long-term memory, achieving a 7% improvement in\nassociative memory tasks over the state-of-the-art embedding model while also\nexhibiting superior factual knowledge and sense-making memory capabilities.\nThis work paves the way for non-parametric continual learning for LLMs. Our\ncode and data will be released at https://github.com/OSU-NLP-Group/HippoRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our ability to continuously acquire, organize, and leverage knowledge is a\nkey feature of human intelligence that AI systems must approximate to unlock\ntheir full potential. Given the challenges in continual learning with large\nlanguage models (LLMs), retrieval-augmented generation (RAG) has become the\ndominant way to introduce new information. However, its reliance on vector\nretrieval hinders its ability to mimic the dynamic and interconnected nature of\nhuman long-term memory. Recent RAG approaches augment vector embeddings with\nvarious structures like knowledge graphs to address some of these gaps, namely\nsense-making and associativity. However, their performance on more basic\nfactual memory tasks drops considerably below standard RAG. We address this\nunintended deterioration and propose HippoRAG 2, a framework that outperforms\nstandard RAG comprehensively on factual, sense-making, and associative memory\ntasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in\nHippoRAG and enhances it with deeper passage integration and more effective\nonline use of an LLM. This combination pushes this RAG system closer to the\neffectiveness of human long-term memory, achieving a 7% improvement in\nassociative memory tasks over the state-of-the-art embedding model while also\nexhibiting superior factual knowledge and sense-making memory capabilities.\nThis work paves the way for non-parametric continual learning for LLMs. Our\ncode and data will be released at https://github.com/OSU-NLP-Group/HippoRAG."
                },
                "authors": [
                    {
                        "name": "Bernal Jimnez Gutirrez"
                    },
                    {
                        "name": "Yiheng Shu"
                    },
                    {
                        "name": "Weijian Qi"
                    },
                    {
                        "name": "Sizhe Zhou"
                    },
                    {
                        "name": "Yu Su"
                    }
                ],
                "author_detail": {
                    "name": "Yu Su"
                },
                "author": "Yu Su",
                "arxiv_comment": "Code and data to be released at:\n  https://github.com/OSU-NLP-Group/HippoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14796v1",
                "updated": "2025-02-20T18:17:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    17,
                    26,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:17:26Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    17,
                    26,
                    3,
                    51,
                    0
                ],
                "title": "A Multi-Agent Perspective on Modern Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Agent Perspective on Modern Information Retrieval"
                },
                "summary": "The rise of large language models (LLMs) has introduced a new era in\ninformation retrieval (IR), where queries and documents that were once assumed\nto be generated exclusively by humans can now also be created by automated\nagents. These agents can formulate queries, generate documents, and perform\nranking. This shift challenges some long-standing IR paradigms and calls for a\nreassessment of both theoretical frameworks and practical methodologies. We\nadvocate for a multi-agent perspective to better capture the complex\ninteractions between query agents, document agents, and ranker agents. Through\nempirical exploration of various multi-agent retrieval settings, we reveal the\nsignificant impact of these interactions on system performance. Our findings\nunderscore the need to revisit classical IR paradigms and develop new\nframeworks for more effective modeling and evaluation of modern retrieval\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has introduced a new era in\ninformation retrieval (IR), where queries and documents that were once assumed\nto be generated exclusively by humans can now also be created by automated\nagents. These agents can formulate queries, generate documents, and perform\nranking. This shift challenges some long-standing IR paradigms and calls for a\nreassessment of both theoretical frameworks and practical methodologies. We\nadvocate for a multi-agent perspective to better capture the complex\ninteractions between query agents, document agents, and ranker agents. Through\nempirical exploration of various multi-agent retrieval settings, we reveal the\nsignificant impact of these interactions on system performance. Our findings\nunderscore the need to revisit classical IR paradigms and develop new\nframeworks for more effective modeling and evaluation of modern retrieval\nsystems."
                },
                "authors": [
                    {
                        "name": "Haya Nachimovsky"
                    },
                    {
                        "name": "Moshe Tennenholtz"
                    },
                    {
                        "name": "Oren Kurland"
                    }
                ],
                "author_detail": {
                    "name": "Oren Kurland"
                },
                "author": "Oren Kurland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14791v1",
                "updated": "2025-02-20T18:11:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    11,
                    38,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:11:38Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    11,
                    38,
                    3,
                    51,
                    0
                ],
                "title": "Rapid Word Learning Through Meta In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid Word Learning Through Meta In-Context Learning"
                },
                "summary": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks."
                },
                "authors": [
                    {
                        "name": "Wentao Wang"
                    },
                    {
                        "name": "Guangyuan Jiang"
                    },
                    {
                        "name": "Tal Linzen"
                    },
                    {
                        "name": "Brenden M. Lake"
                    }
                ],
                "author_detail": {
                    "name": "Brenden M. Lake"
                },
                "author": "Brenden M. Lake",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13491v2",
                "updated": "2025-02-20T18:09:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    9,
                    53,
                    3,
                    51,
                    0
                ],
                "published": "2024-03-20T10:48:00Z",
                "published_parsed": [
                    2024,
                    3,
                    20,
                    10,
                    48,
                    0,
                    2,
                    80,
                    0
                ],
                "title": "Dimensionality-Reduction Techniques for Approximate Nearest Neighbor\n  Search: A Survey and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dimensionality-Reduction Techniques for Approximate Nearest Neighbor\n  Search: A Survey and Evaluation"
                },
                "summary": "Approximate Nearest Neighbor Search (ANNS) on high-dimensional vectors has\nbecome a fundamental and essential component in various machine learning tasks.\nRecently, with the rapid development of deep learning models and the\napplications of Large Language Models (LLMs), the dimensionality of the vectors\nkeeps growing in order to accommodate a richer semantic representation. This\nposes a major challenge to the ANNS solutions since distance calculation cost\nin ANNS grows linearly with the dimensionality of vectors. To overcome this\nchallenge, dimensionality-reduction techniques can be leveraged to accelerate\nthe distance calculation in the search process. In this paper, we investigate\nsix dimensionality-reduction techniques that have the potential to improve ANNS\nsolutions, including classical algorithms such as PCA and vector quantization,\nas well as algorithms based on deep learning approaches. We further describe\ntwo frameworks to apply these techniques in the ANNS workflow, and\ntheoretically analyze the time and space costs, as well as the beneficial\nthreshold for the pruning ratio of these techniques. The surveyed techniques\nare evaluated on six public datasets. The analysis of the results reveals the\ncharacteristics of the different families of techniques and provides insights\ninto the promising future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest Neighbor Search (ANNS) on high-dimensional vectors has\nbecome a fundamental and essential component in various machine learning tasks.\nRecently, with the rapid development of deep learning models and the\napplications of Large Language Models (LLMs), the dimensionality of the vectors\nkeeps growing in order to accommodate a richer semantic representation. This\nposes a major challenge to the ANNS solutions since distance calculation cost\nin ANNS grows linearly with the dimensionality of vectors. To overcome this\nchallenge, dimensionality-reduction techniques can be leveraged to accelerate\nthe distance calculation in the search process. In this paper, we investigate\nsix dimensionality-reduction techniques that have the potential to improve ANNS\nsolutions, including classical algorithms such as PCA and vector quantization,\nas well as algorithms based on deep learning approaches. We further describe\ntwo frameworks to apply these techniques in the ANNS workflow, and\ntheoretically analyze the time and space costs, as well as the beneficial\nthreshold for the pruning ratio of these techniques. The surveyed techniques\nare evaluated on six public datasets. The analysis of the results reveals the\ncharacteristics of the different families of techniques and provides insights\ninto the promising future research directions."
                },
                "authors": [
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Haoran Xiong"
                    },
                    {
                        "name": "Qitong Wang"
                    },
                    {
                        "name": "Zhenying He"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Themis Palpanas"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14788v1",
                "updated": "2025-02-20T18:09:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    9,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:09:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    9,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "Ray-Tracing for Conditionally Activated Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ray-Tracing for Conditionally Activated Neural Networks"
                },
                "summary": "In this paper, we introduce a novel architecture for conditionally activated\nneural networks combining a hierarchical construction of multiple Mixture of\nExperts (MoEs) layers with a sampling mechanism that progressively converges to\nan optimized configuration of expert activation. This methodology enables the\ndynamic unfolding of the network's architecture, facilitating efficient\npath-specific training. Experimental results demonstrate that this approach\nachieves competitive accuracy compared to conventional baselines while\nsignificantly reducing the parameter count required for inference. Notably,\nthis parameter reduction correlates with the complexity of the input patterns,\na property naturally emerging from the network's operational dynamics without\nnecessitating explicit auxiliary penalty functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a novel architecture for conditionally activated\nneural networks combining a hierarchical construction of multiple Mixture of\nExperts (MoEs) layers with a sampling mechanism that progressively converges to\nan optimized configuration of expert activation. This methodology enables the\ndynamic unfolding of the network's architecture, facilitating efficient\npath-specific training. Experimental results demonstrate that this approach\nachieves competitive accuracy compared to conventional baselines while\nsignificantly reducing the parameter count required for inference. Notably,\nthis parameter reduction correlates with the complexity of the input patterns,\na property naturally emerging from the network's operational dynamics without\nnecessitating explicit auxiliary penalty functions."
                },
                "authors": [
                    {
                        "name": "Claudio Gallicchio"
                    },
                    {
                        "name": "Giuseppe Nuti"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Nuti"
                },
                "author": "Giuseppe Nuti",
                "arxiv_comment": "submitted to workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14786v1",
                "updated": "2025-02-20T18:08:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    8,
                    29,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:08:29Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    8,
                    29,
                    3,
                    51,
                    0
                ],
                "title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic\n  Understanding, Localization, and Dense Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic\n  Understanding, Localization, and Dense Features"
                },
                "summary": "We introduce SigLIP 2, a family of new multilingual vision-language encoders\nthat build on the success of the original SigLIP. In this second iteration, we\nextend the original image-text training objective with several prior,\nindependently developed techniques into a unified recipe -- this includes\ncaptioning-based pretraining, self-supervised losses (self-distillation, masked\nprediction) and online data curation. With these changes, SigLIP 2 models\noutperform their SigLIP counterparts at all model scales in core capabilities,\nincluding zero-shot classification, image-text retrieval, and transfer\nperformance when extracting visual representations for Vision-Language Models\n(VLMs). Furthermore, the new training recipe leads to significant improvements\non localization and dense prediction tasks. We also train variants which\nsupport multiple resolutions and preserve the input's native aspect ratio.\nFinally, we train on a more diverse data-mixture that includes de-biasing\ntechniques, leading to much better multilingual understanding and improved\nfairness. To allow users to trade off inference cost with performance, we\nrelease model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M),\nand g (1B).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SigLIP 2, a family of new multilingual vision-language encoders\nthat build on the success of the original SigLIP. In this second iteration, we\nextend the original image-text training objective with several prior,\nindependently developed techniques into a unified recipe -- this includes\ncaptioning-based pretraining, self-supervised losses (self-distillation, masked\nprediction) and online data curation. With these changes, SigLIP 2 models\noutperform their SigLIP counterparts at all model scales in core capabilities,\nincluding zero-shot classification, image-text retrieval, and transfer\nperformance when extracting visual representations for Vision-Language Models\n(VLMs). Furthermore, the new training recipe leads to significant improvements\non localization and dense prediction tasks. We also train variants which\nsupport multiple resolutions and preserve the input's native aspect ratio.\nFinally, we train on a more diverse data-mixture that includes de-biasing\ntechniques, leading to much better multilingual understanding and improved\nfairness. To allow users to trade off inference cost with performance, we\nrelease model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M),\nand g (1B)."
                },
                "authors": [
                    {
                        "name": "Michael Tschannen"
                    },
                    {
                        "name": "Alexey Gritsenko"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Muhammad Ferjad Naeem"
                    },
                    {
                        "name": "Ibrahim Alabdulmohsin"
                    },
                    {
                        "name": "Nikhil Parthasarathy"
                    },
                    {
                        "name": "Talfan Evans"
                    },
                    {
                        "name": "Lucas Beyer"
                    },
                    {
                        "name": "Ye Xia"
                    },
                    {
                        "name": "Basil Mustafa"
                    },
                    {
                        "name": "Olivier Hnaff"
                    },
                    {
                        "name": "Jeremiah Harmsen"
                    },
                    {
                        "name": "Andreas Steiner"
                    },
                    {
                        "name": "Xiaohua Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Zhai"
                },
                "author": "Xiaohua Zhai",
                "arxiv_comment": "Model checkpoints are available at\n  https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/image_text/README_siglip2.md",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14776v1",
                "updated": "2025-02-20T17:59:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    59,
                    45,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:59:45Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    59,
                    45,
                    3,
                    51,
                    0
                ],
                "title": "SurveyX: Academic Survey Automation via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurveyX: Academic Survey Automation via Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional comprehension\ncapabilities and a vast knowledge base, suggesting that LLMs can serve as\nefficient tools for automated survey generation. However, recent research\nrelated to automated survey generation remains constrained by some critical\nlimitations like finite context window, lack of in-depth content discussion,\nand absence of systematic evaluation frameworks. Inspired by human writing\nprocesses, we propose SurveyX, an efficient and organized system for automated\nsurvey generation that decomposes the survey composing process into two phases:\nthe Preparation and Generation phases. By innovatively introducing online\nreference retrieval, a pre-processing method called AttributeTree, and a\nre-polishing process, SurveyX significantly enhances the efficacy of survey\ncomposition. Experimental evaluation results show that SurveyX outperforms\nexisting automated survey generation systems in content quality (0.259\nimprovement) and citation quality (1.76 enhancement), approaching human expert\nperformance across multiple evaluation dimensions. Examples of surveys\ngenerated by SurveyX are available on www.surveyx.cn",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional comprehension\ncapabilities and a vast knowledge base, suggesting that LLMs can serve as\nefficient tools for automated survey generation. However, recent research\nrelated to automated survey generation remains constrained by some critical\nlimitations like finite context window, lack of in-depth content discussion,\nand absence of systematic evaluation frameworks. Inspired by human writing\nprocesses, we propose SurveyX, an efficient and organized system for automated\nsurvey generation that decomposes the survey composing process into two phases:\nthe Preparation and Generation phases. By innovatively introducing online\nreference retrieval, a pre-processing method called AttributeTree, and a\nre-polishing process, SurveyX significantly enhances the efficacy of survey\ncomposition. Experimental evaluation results show that SurveyX outperforms\nexisting automated survey generation systems in content quality (0.259\nimprovement) and citation quality (1.76 enhancement), approaching human expert\nperformance across multiple evaluation dimensions. Examples of surveys\ngenerated by SurveyX are available on www.surveyx.cn"
                },
                "authors": [
                    {
                        "name": "Xun Liang"
                    },
                    {
                        "name": "Jiawei Yang"
                    },
                    {
                        "name": "Yezhaohui Wang"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Keming Mao"
                    },
                    {
                        "name": "Zhiyu li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu li"
                },
                "author": "Zhiyu li",
                "arxiv_comment": "15 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05287v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05287v2",
                "updated": "2025-02-20T17:55:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    55,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2024-08-09T18:16:47Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    18,
                    16,
                    47,
                    4,
                    222,
                    0
                ],
                "title": "Constraining twin stars with cold neutron star cooling data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining twin stars with cold neutron star cooling data"
                },
                "summary": "We investigate the influence of a phase transition from hadronic matter to a\ndeconfined quark phase inside a neutron star on its cooling behaviour including\nthe appearance of twin star solutions in the mass-radius diagram. We find that\nwhile the inferred neutrino luminosity of cold transiently-accreting star in\nMXB $1659\\text{-}29$ is reproduced in all of the constructed twin star models,\nthe luminosity of a colder source, the neutron star in SAX\nJ$1808.4\\text{-}3658$, cannot be described by equations of state with\nquark-hadron transition densities below $1.7$ saturation density, suggesting\nthat twin stars with such low density transitions to the quark phase are not\nrealized in nature. We also discuss how constraints to the quark-hadron phase\ntransition density are strongly dependent on the cooling effectiveness of\nneutrino reactions in the quark phase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the influence of a phase transition from hadronic matter to a\ndeconfined quark phase inside a neutron star on its cooling behaviour including\nthe appearance of twin star solutions in the mass-radius diagram. We find that\nwhile the inferred neutrino luminosity of cold transiently-accreting star in\nMXB $1659\\text{-}29$ is reproduced in all of the constructed twin star models,\nthe luminosity of a colder source, the neutron star in SAX\nJ$1808.4\\text{-}3658$, cannot be described by equations of state with\nquark-hadron transition densities below $1.7$ saturation density, suggesting\nthat twin stars with such low density transitions to the quark phase are not\nrealized in nature. We also discuss how constraints to the quark-hadron phase\ntransition density are strongly dependent on the cooling effectiveness of\nneutrino reactions in the quark phase."
                },
                "authors": [
                    {
                        "name": "Melissa Mendes"
                    },
                    {
                        "name": "Jan-Erik Christian"
                    },
                    {
                        "name": "Farrukh J. Fattoyev"
                    },
                    {
                        "name": "Jrgen Schaffner-Bielich"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Schaffner-Bielich"
                },
                "author": "Jrgen Schaffner-Bielich",
                "arxiv_comment": "11 pages, 6 figures. Accepted for publication in PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05287v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05287v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14772v1",
                "updated": "2025-02-20T17:53:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    53,
                    13,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:53:13Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    53,
                    13,
                    3,
                    51,
                    0
                ],
                "title": "Efficient Multivariate Robust Mean Estimation Under Mean-Shift\n  Contamination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Multivariate Robust Mean Estimation Under Mean-Shift\n  Contamination"
                },
                "summary": "We study the algorithmic problem of robust mean estimation of an identity\ncovariance Gaussian in the presence of mean-shift contamination. In this\ncontamination model, we are given a set of points in $\\mathbb{R}^d$ generated\ni.i.d. via the following process. For a parameter $\\alpha<1/2$, the $i$-th\nsample $x_i$ is obtained as follows: with probability $1-\\alpha$, $x_i$ is\ndrawn from $\\mathcal{N}(\\mu, I)$, where $\\mu \\in \\mathbb{R}^d$ is the target\nmean; and with probability $\\alpha$, $x_i$ is drawn from $\\mathcal{N}(z_i, I)$,\nwhere $z_i$ is unknown and potentially arbitrary. Prior work characterized the\ninformation-theoretic limits of this task. Specifically, it was shown that, in\ncontrast to Huber contamination, in the presence of mean-shift contamination\nconsistent estimation is possible. On the other hand, all known robust\nestimators in the mean-shift model have running times exponential in the\ndimension. Here we give the first computationally efficient algorithm for\nhigh-dimensional robust mean estimation with mean-shift contamination that can\ntolerate a constant fraction of outliers. In particular, our algorithm has\nnear-optimal sample complexity, runs in sample-polynomial time, and\napproximates the target mean to any desired accuracy. Conceptually, our result\ncontributes to a growing body of work that studies inference with respect to\nnatural noise models lying in between fully adversarial and random settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the algorithmic problem of robust mean estimation of an identity\ncovariance Gaussian in the presence of mean-shift contamination. In this\ncontamination model, we are given a set of points in $\\mathbb{R}^d$ generated\ni.i.d. via the following process. For a parameter $\\alpha<1/2$, the $i$-th\nsample $x_i$ is obtained as follows: with probability $1-\\alpha$, $x_i$ is\ndrawn from $\\mathcal{N}(\\mu, I)$, where $\\mu \\in \\mathbb{R}^d$ is the target\nmean; and with probability $\\alpha$, $x_i$ is drawn from $\\mathcal{N}(z_i, I)$,\nwhere $z_i$ is unknown and potentially arbitrary. Prior work characterized the\ninformation-theoretic limits of this task. Specifically, it was shown that, in\ncontrast to Huber contamination, in the presence of mean-shift contamination\nconsistent estimation is possible. On the other hand, all known robust\nestimators in the mean-shift model have running times exponential in the\ndimension. Here we give the first computationally efficient algorithm for\nhigh-dimensional robust mean estimation with mean-shift contamination that can\ntolerate a constant fraction of outliers. In particular, our algorithm has\nnear-optimal sample complexity, runs in sample-polynomial time, and\napproximates the target mean to any desired accuracy. Conceptually, our result\ncontributes to a growing body of work that studies inference with respect to\nnatural noise models lying in between fully adversarial and random settings."
                },
                "authors": [
                    {
                        "name": "Ilias Diakonikolas"
                    },
                    {
                        "name": "Giannis Iakovidis"
                    },
                    {
                        "name": "Daniel M. Kane"
                    },
                    {
                        "name": "Thanasis Pittas"
                    }
                ],
                "author_detail": {
                    "name": "Thanasis Pittas"
                },
                "author": "Thanasis Pittas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14770v1",
                "updated": "2025-02-20T17:51:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    51,
                    10,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:51:10Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    51,
                    10,
                    3,
                    51,
                    0
                ],
                "title": "Determining Layer-wise Sparsity for Large Language Models Through a\n  Theoretical Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining Layer-wise Sparsity for Large Language Models Through a\n  Theoretical Perspective"
                },
                "summary": "In this paper, we address the challenge of determining the layer-wise\nsparsity rates of large language models (LLMs) through a theoretical\nperspective. Specifically, we identify a critical issue of\n''$\\textbf{reconstruction error explosion}$'' in existing LLMs sparsification\nmethods. This refers to the cumulative effect of reconstruction errors\nthroughout the sparsification process, where errors from earlier layers\npropagate and amplify in subsequent layers. As a result, the overall\nreconstruction error increases significantly, leading to a substantial\ndegradation in model performance. Through theoretical analysis, we derive a\nsimple yet effective approach to layer-wise sparsity allocation that mitigates\nthis issue. Our method uses a monotonically increasing arithmetic progression,\nreducing the process of determining sparsity rates for multiple layers to the\ndetermination of a single common difference hyperparameter. Remarkably, this\nallows for the optimal layer-wise sparsity rates to be identified with just a\nfew trials. Both our theoretical analysis and experimental results demonstrate\nthat this sparsity allocation scheme is near optimal. Extensive experiments\nshow that our method significantly improves the performance of sparse LLMs\nacross various architectures, outperforming existing layer-wise sparsity\nmethods. Furthermore, it enhances the performance of various compression\ntechniques and is applicable to vision and multimodal models. Notably, our\nmethod achieves a reduction of 52.10 in perplexity for the 70$\\%$ sparse\nLLaMA2-7B model obtained via Wanda, improves average zero-shot accuracy by\n10.50$\\%$, and delivers speedups of 2.63$\\times$ and 2.23$\\times$ on CPU and\nGPU, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we address the challenge of determining the layer-wise\nsparsity rates of large language models (LLMs) through a theoretical\nperspective. Specifically, we identify a critical issue of\n''$\\textbf{reconstruction error explosion}$'' in existing LLMs sparsification\nmethods. This refers to the cumulative effect of reconstruction errors\nthroughout the sparsification process, where errors from earlier layers\npropagate and amplify in subsequent layers. As a result, the overall\nreconstruction error increases significantly, leading to a substantial\ndegradation in model performance. Through theoretical analysis, we derive a\nsimple yet effective approach to layer-wise sparsity allocation that mitigates\nthis issue. Our method uses a monotonically increasing arithmetic progression,\nreducing the process of determining sparsity rates for multiple layers to the\ndetermination of a single common difference hyperparameter. Remarkably, this\nallows for the optimal layer-wise sparsity rates to be identified with just a\nfew trials. Both our theoretical analysis and experimental results demonstrate\nthat this sparsity allocation scheme is near optimal. Extensive experiments\nshow that our method significantly improves the performance of sparse LLMs\nacross various architectures, outperforming existing layer-wise sparsity\nmethods. Furthermore, it enhances the performance of various compression\ntechniques and is applicable to vision and multimodal models. Notably, our\nmethod achieves a reduction of 52.10 in perplexity for the 70$\\%$ sparse\nLLaMA2-7B model obtained via Wanda, improves average zero-shot accuracy by\n10.50$\\%$, and delivers speedups of 2.63$\\times$ and 2.23$\\times$ on CPU and\nGPU, respectively."
                },
                "authors": [
                    {
                        "name": "Weizhong Huang"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14768v1",
                "updated": "2025-02-20T17:49:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    49,
                    26,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:49:26Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    49,
                    26,
                    3,
                    51,
                    0
                ],
                "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement\n  Learning"
                },
                "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of\nrule-based reinforcement learning (RL) in large reasoning models. To analyze\nreasoning dynamics, we use synthetic logic puzzles as training data due to\ntheir controllable complexity and straightforward answer verification. We make\nsome key technical contributions that lead to effective and stable RL training:\na system prompt that emphasizes the thinking and answering process, a stringent\nformat reward function that penalizes outputs for taking shortcuts, and a\nstraightforward training recipe that achieves stable convergence. Our 7B model\ndevelops advanced reasoning skills-such as reflection, verification, and\nsummarization-that are absent from the logic corpus. Remarkably, after training\non just 5K logic problems, it demonstrates generalization abilities to the\nchallenging math benchmarks AIME and AMC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by the success of DeepSeek-R1, we explore the potential of\nrule-based reinforcement learning (RL) in large reasoning models. To analyze\nreasoning dynamics, we use synthetic logic puzzles as training data due to\ntheir controllable complexity and straightforward answer verification. We make\nsome key technical contributions that lead to effective and stable RL training:\na system prompt that emphasizes the thinking and answering process, a stringent\nformat reward function that penalizes outputs for taking shortcuts, and a\nstraightforward training recipe that achieves stable convergence. Our 7B model\ndevelops advanced reasoning skills-such as reflection, verification, and\nsummarization-that are absent from the logic corpus. Remarkably, after training\non just 5K logic problems, it demonstrates generalization abilities to the\nchallenging math benchmarks AIME and AMC."
                },
                "authors": [
                    {
                        "name": "Tian Xie"
                    },
                    {
                        "name": "Zitian Gao"
                    },
                    {
                        "name": "Qingnan Ren"
                    },
                    {
                        "name": "Haoming Luo"
                    },
                    {
                        "name": "Yuqian Hong"
                    },
                    {
                        "name": "Bryan Dai"
                    },
                    {
                        "name": "Joey Zhou"
                    },
                    {
                        "name": "Kai Qiu"
                    },
                    {
                        "name": "Zhirong Wu"
                    },
                    {
                        "name": "Chong Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chong Luo"
                },
                "author": "Chong Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14767v1",
                "updated": "2025-02-20T17:43:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    43,
                    40,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:43:40Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    43,
                    40,
                    3,
                    51,
                    0
                ],
                "title": "Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for\n  Scientific Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for\n  Scientific Comparative Analysis"
                },
                "summary": "With the exponential growth of research facilitated by modern technology and\nimproved accessibility, scientific discoveries have become increasingly\nfragmented within and across fields. This makes it challenging to assess the\nsignificance, novelty, incremental findings, and equivalent ideas between\nrelated works, particularly those from different research communities. Large\nlanguage models (LLMs) have recently demonstrated strong quantitative and\nqualitative reasoning abilities, and multi-agent LLM debates have shown promise\nin handling complex reasoning tasks by exploring diverse perspectives and\nreasoning paths. Inspired by this, we introduce Tree-of-Debate (ToD), a\nframework which converts scientific papers into LLM personas that debate their\nrespective novelties. To emphasize structured, critical reasoning rather than\nfocusing solely on outcomes, ToD dynamically constructs a debate tree, enabling\nfine-grained analysis of independent novelty arguments within scholarly\narticles. Through experiments on scientific literature across various domains,\nevaluated by expert researchers, we demonstrate that ToD generates informative\narguments, effectively contrasts papers, and supports researchers in their\nliterature review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential growth of research facilitated by modern technology and\nimproved accessibility, scientific discoveries have become increasingly\nfragmented within and across fields. This makes it challenging to assess the\nsignificance, novelty, incremental findings, and equivalent ideas between\nrelated works, particularly those from different research communities. Large\nlanguage models (LLMs) have recently demonstrated strong quantitative and\nqualitative reasoning abilities, and multi-agent LLM debates have shown promise\nin handling complex reasoning tasks by exploring diverse perspectives and\nreasoning paths. Inspired by this, we introduce Tree-of-Debate (ToD), a\nframework which converts scientific papers into LLM personas that debate their\nrespective novelties. To emphasize structured, critical reasoning rather than\nfocusing solely on outcomes, ToD dynamically constructs a debate tree, enabling\nfine-grained analysis of independent novelty arguments within scholarly\narticles. Through experiments on scientific literature across various domains,\nevaluated by expert researchers, we demonstrate that ToD generates informative\narguments, effectively contrasts papers, and supports researchers in their\nliterature review."
                },
                "authors": [
                    {
                        "name": "Priyanka Kargupta"
                    },
                    {
                        "name": "Ishika Agarwal"
                    },
                    {
                        "name": "Tal August"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "arxiv_comment": "Code available at: https://github.com/pkargupta/tree-of-debate",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14932v2",
                "updated": "2025-02-20T17:41:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    41,
                    47,
                    3,
                    51,
                    0
                ],
                "published": "2024-05-23T18:00:00Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    18,
                    0,
                    0,
                    3,
                    144,
                    0
                ],
                "title": "Fast Bayesian Inference for Neutrino Non-Standard Interactions at Dark\n  Matter Direct Detection Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Bayesian Inference for Neutrino Non-Standard Interactions at Dark\n  Matter Direct Detection Experiments"
                },
                "summary": "Multi-dimensional parameter spaces are commonly encountered in physics\ntheories that go beyond the Standard Model. However, they often possess\ncomplicated posterior geometries that are expensive to traverse using\ntechniques traditional to astroparticle physics. Several recent innovations,\nwhich are only beginning to make their way into this field, have made\nnavigating such complex posteriors possible. These include GPU acceleration,\nautomatic differentiation, and neural-network-guided reparameterization. We\napply these advancements to dark matter direct detection experiments in the\ncontext of non-standard neutrino interactions and benchmark their performances\nagainst traditional nested sampling techniques when conducting Bayesian\ninference. Compared to nested sampling alone, we find that these techniques\nincrease performance for both nested sampling and Hamiltonian Monte Carlo,\naccelerating inference by factors of $\\sim 100$ and $\\sim 60$, respectively. As\nnested sampling also evaluates the Bayesian evidence, these advancements can be\nexploited to improve model comparison performance while retaining compatibility\nwith existing implementations that are widely used in the natural sciences.\nUsing these techniques, we perform the first scan in the neutrino non-standard\ninteractions parameter space for direct detection experiments whereby all\nparameters are allowed to vary simultaneously. We expect that these\nadvancements are broadly applicable to other areas of astroparticle physics\nfeaturing multi-dimensional parameter spaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-dimensional parameter spaces are commonly encountered in physics\ntheories that go beyond the Standard Model. However, they often possess\ncomplicated posterior geometries that are expensive to traverse using\ntechniques traditional to astroparticle physics. Several recent innovations,\nwhich are only beginning to make their way into this field, have made\nnavigating such complex posteriors possible. These include GPU acceleration,\nautomatic differentiation, and neural-network-guided reparameterization. We\napply these advancements to dark matter direct detection experiments in the\ncontext of non-standard neutrino interactions and benchmark their performances\nagainst traditional nested sampling techniques when conducting Bayesian\ninference. Compared to nested sampling alone, we find that these techniques\nincrease performance for both nested sampling and Hamiltonian Monte Carlo,\naccelerating inference by factors of $\\sim 100$ and $\\sim 60$, respectively. As\nnested sampling also evaluates the Bayesian evidence, these advancements can be\nexploited to improve model comparison performance while retaining compatibility\nwith existing implementations that are widely used in the natural sciences.\nUsing these techniques, we perform the first scan in the neutrino non-standard\ninteractions parameter space for direct detection experiments whereby all\nparameters are allowed to vary simultaneously. We expect that these\nadvancements are broadly applicable to other areas of astroparticle physics\nfeaturing multi-dimensional parameter spaces."
                },
                "authors": [
                    {
                        "name": "Dorian W. P. Amaral"
                    },
                    {
                        "name": "Shixiao Liang"
                    },
                    {
                        "name": "Juehang Qin"
                    },
                    {
                        "name": "Christopher Tunnell"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Tunnell"
                },
                "author": "Christopher Tunnell",
                "arxiv_comment": "26 pages, 6 figures, 5 tables, 5 appendices. Compared to v1: Added\n  Bayesian to title, included more physical background, and added a table with\n  1D marginalised credible intervals for NSI parameters. Matches journal\n  version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14765v1",
                "updated": "2025-02-20T17:40:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    40,
                    21,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:40:21Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    40,
                    21,
                    3,
                    51,
                    0
                ],
                "title": "Step-by-Step Fact Verification System for Medical Claims with\n  Explainable Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-by-Step Fact Verification System for Medical Claims with\n  Explainable Reasoning"
                },
                "summary": "Fact verification (FV) aims to assess the veracity of a claim based on\nrelevant evidence. The traditional approach for automated FV includes a\nthree-part pipeline relying on short evidence snippets and encoder-only\ninference models. More recent approaches leverage the multi-turn nature of LLMs\nto address FV as a step-by-step problem where questions inquiring additional\ncontext are generated and answered until there is enough information to make a\ndecision. This iterative method makes the verification process rational and\nexplainable. While these methods have been tested for encyclopedic claims,\nexploration on domain-specific and realistic claims is missing. In this work,\nwe apply an iterative FV system on three medical fact-checking datasets and\nevaluate it with multiple settings, including different LLMs, external web\nsearch, and structured reasoning using logic predicates. We demonstrate\nimprovements in the final performance over traditional approaches and the high\npotential of step-by-step FV systems for domain-specific claims.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact verification (FV) aims to assess the veracity of a claim based on\nrelevant evidence. The traditional approach for automated FV includes a\nthree-part pipeline relying on short evidence snippets and encoder-only\ninference models. More recent approaches leverage the multi-turn nature of LLMs\nto address FV as a step-by-step problem where questions inquiring additional\ncontext are generated and answered until there is enough information to make a\ndecision. This iterative method makes the verification process rational and\nexplainable. While these methods have been tested for encyclopedic claims,\nexploration on domain-specific and realistic claims is missing. In this work,\nwe apply an iterative FV system on three medical fact-checking datasets and\nevaluate it with multiple settings, including different LLMs, external web\nsearch, and structured reasoning using logic predicates. We demonstrate\nimprovements in the final performance over traditional approaches and the high\npotential of step-by-step FV systems for domain-specific claims."
                },
                "authors": [
                    {
                        "name": "Juraj Vladika"
                    },
                    {
                        "name": "Ivana Hacajov"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "arxiv_comment": "Accepted to NAACL 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14762v1",
                "updated": "2025-02-20T17:37:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    37,
                    8,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:37:08Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    37,
                    8,
                    3,
                    51,
                    0
                ],
                "title": "Sculpting [CLS] Features for Pre-Trained Model-Based Class-Incremental\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sculpting [CLS] Features for Pre-Trained Model-Based Class-Incremental\n  Learning"
                },
                "summary": "Class-incremental learning requires models to continually acquire knowledge\nof new classes without forgetting old ones. Although pre-trained models have\ndemonstrated strong performance in class-incremental learning, they remain\nsusceptible to catastrophic forgetting when learning new concepts. Excessive\nplasticity in the models breaks generalizability and causes forgetting, while\nstrong stability results in insufficient adaptation to new classes. This\nnecessitates effective adaptation with minimal modifications to preserve the\ngeneral knowledge of pre-trained models. To address this challenge, we first\nintroduce a new parameter-efficient fine-tuning module 'Learn and Calibrate',\nor LuCA, designed to acquire knowledge through an adapter-calibrator couple,\nenabling effective adaptation with well-refined feature representations.\nSecond, for each learning session, we deploy a sparse LuCA module on top of the\nlast token just before the classifier, which we refer to as 'Token-level Sparse\nCalibration and Adaptation', or TOSCA. This strategic design improves the\northogonality between the modules and significantly reduces both training and\ninference complexity. By leaving the generalization capabilities of the\npre-trained models intact and adapting exclusively via the last token, our\napproach achieves a harmonious balance between stability and plasticity.\nExtensive experiments demonstrate TOSCA's state-of-the-art performance while\nintroducing ~8 times fewer parameters compared to prior methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Class-incremental learning requires models to continually acquire knowledge\nof new classes without forgetting old ones. Although pre-trained models have\ndemonstrated strong performance in class-incremental learning, they remain\nsusceptible to catastrophic forgetting when learning new concepts. Excessive\nplasticity in the models breaks generalizability and causes forgetting, while\nstrong stability results in insufficient adaptation to new classes. This\nnecessitates effective adaptation with minimal modifications to preserve the\ngeneral knowledge of pre-trained models. To address this challenge, we first\nintroduce a new parameter-efficient fine-tuning module 'Learn and Calibrate',\nor LuCA, designed to acquire knowledge through an adapter-calibrator couple,\nenabling effective adaptation with well-refined feature representations.\nSecond, for each learning session, we deploy a sparse LuCA module on top of the\nlast token just before the classifier, which we refer to as 'Token-level Sparse\nCalibration and Adaptation', or TOSCA. This strategic design improves the\northogonality between the modules and significantly reduces both training and\ninference complexity. By leaving the generalization capabilities of the\npre-trained models intact and adapting exclusively via the last token, our\napproach achieves a harmonious balance between stability and plasticity.\nExtensive experiments demonstrate TOSCA's state-of-the-art performance while\nintroducing ~8 times fewer parameters compared to prior methods."
                },
                "authors": [
                    {
                        "name": "Murat Onur Yildirim"
                    },
                    {
                        "name": "Elif Ceren Gok Yildirim"
                    },
                    {
                        "name": "Joaquin Vanschoren"
                    }
                ],
                "author_detail": {
                    "name": "Joaquin Vanschoren"
                },
                "author": "Joaquin Vanschoren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14760v1",
                "updated": "2025-02-20T17:35:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    35,
                    32,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:35:32Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    35,
                    32,
                    3,
                    51,
                    0
                ],
                "title": "EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of\n  Optimization Formulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of\n  Optimization Formulations"
                },
                "summary": "A fundamental problem in combinatorial optimization is identifying equivalent\nformulations, which can lead to more efficient solution strategies and deeper\ninsights into a problem's computational complexity. The need to automatically\nidentify equivalence between problem formulations has grown as optimization\ncopilots--systems that generate problem formulations from natural language\ndescriptions--have proliferated. However, existing approaches to checking\nformulation equivalence lack grounding, relying on simple heuristics which are\ninsufficient for rigorous validation. Inspired by Karp reductions, in this work\nwe introduce quasi-Karp equivalence, a formal criterion for determining when\ntwo optimization formulations are equivalent based on the existence of a\nmapping between their decision variables. We propose EquivaMap, a framework\nthat leverages large language models to automatically discover such mappings,\nenabling scalable and reliable equivalence verification. To evaluate our\napproach, we construct the first open-source dataset of equivalent optimization\nformulations, generated by applying transformations such as adding slack\nvariables or valid inequalities to existing formulations. Empirically,\nEquivaMap significantly outperforms existing methods, achieving substantial\nimprovements in correctly identifying formulation equivalence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental problem in combinatorial optimization is identifying equivalent\nformulations, which can lead to more efficient solution strategies and deeper\ninsights into a problem's computational complexity. The need to automatically\nidentify equivalence between problem formulations has grown as optimization\ncopilots--systems that generate problem formulations from natural language\ndescriptions--have proliferated. However, existing approaches to checking\nformulation equivalence lack grounding, relying on simple heuristics which are\ninsufficient for rigorous validation. Inspired by Karp reductions, in this work\nwe introduce quasi-Karp equivalence, a formal criterion for determining when\ntwo optimization formulations are equivalent based on the existence of a\nmapping between their decision variables. We propose EquivaMap, a framework\nthat leverages large language models to automatically discover such mappings,\nenabling scalable and reliable equivalence verification. To evaluate our\napproach, we construct the first open-source dataset of equivalent optimization\nformulations, generated by applying transformations such as adding slack\nvariables or valid inequalities to existing formulations. Empirically,\nEquivaMap significantly outperforms existing methods, achieving substantial\nimprovements in correctly identifying formulation equivalence."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Connor Lawless"
                    },
                    {
                        "name": "Ellen Vitercik"
                    },
                    {
                        "name": "Liu Leqi"
                    }
                ],
                "author_detail": {
                    "name": "Liu Leqi"
                },
                "author": "Liu Leqi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14759v1",
                "updated": "2025-02-20T17:34:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    34,
                    34,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:34:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    34,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "On the Influence of Context Size and Model Choice in Retrieval-Augmented\n  Generation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Influence of Context Size and Model Choice in Retrieval-Augmented\n  Generation Systems"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as an approach to augment\nlarge language models (LLMs) by reducing their reliance on static knowledge and\nimproving answer factuality. RAG retrieves relevant context snippets and\ngenerates an answer based on them. Despite its increasing industrial adoption,\nsystematic exploration of RAG components is lacking, particularly regarding the\nideal size of provided context, and the choice of base LLM and retrieval\nmethod. To help guide development of robust RAG systems, we evaluate various\ncontext sizes, BM25 and semantic search as retrievers, and eight base LLMs.\nMoving away from the usual RAG evaluation with short answers, we explore the\nmore challenging long-form question answering in two domains, where a good\nanswer has to utilize the entire context. Our findings indicate that final QA\nperformance improves steadily with up to 15 snippets but stagnates or declines\nbeyond that. Finally, we show that different general-purpose LLMs excel in the\nbiomedical domain than the encyclopedic one, and that open-domain evidence\nretrieval in large corpora is challenging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as an approach to augment\nlarge language models (LLMs) by reducing their reliance on static knowledge and\nimproving answer factuality. RAG retrieves relevant context snippets and\ngenerates an answer based on them. Despite its increasing industrial adoption,\nsystematic exploration of RAG components is lacking, particularly regarding the\nideal size of provided context, and the choice of base LLM and retrieval\nmethod. To help guide development of robust RAG systems, we evaluate various\ncontext sizes, BM25 and semantic search as retrievers, and eight base LLMs.\nMoving away from the usual RAG evaluation with short answers, we explore the\nmore challenging long-form question answering in two domains, where a good\nanswer has to utilize the entire context. Our findings indicate that final QA\nperformance improves steadily with up to 15 snippets but stagnates or declines\nbeyond that. Finally, we show that different general-purpose LLMs excel in the\nbiomedical domain than the encyclopedic one, and that open-domain evidence\nretrieval in large corpora is challenging."
                },
                "authors": [
                    {
                        "name": "Juraj Vladika"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "arxiv_comment": "Accepted to Findings of NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07263v2",
                "updated": "2025-02-20T17:30:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    30,
                    56,
                    3,
                    51,
                    0
                ],
                "published": "2024-04-10T18:00:05Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    18,
                    0,
                    5,
                    2,
                    101,
                    0
                ],
                "title": "The planted directed polymer: inferring a random walk from noisy images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The planted directed polymer: inferring a random walk from noisy images"
                },
                "summary": "We introduce and study the planted directed polymer, in which the path of a\nrandom walker is inferred from noisy 'images' accumulated at each timestep.\nFormulated as a nonlinear problem of Bayesian inference for a hidden Markov\nmodel, this problem is a generalization of the directed polymer problem of\nstatistical physics, coinciding with it in the limit of zero signal to noise.\nFor a 1D walker we present numerical investigations and analytical arguments\nthat no phase transition is present. When formulated on a Cayley tree, methods\ndeveloped for the directed polymer are used to show that there is a transition\nwith decreasing signal to noise where effective inference becomes impossible,\nmeaning that the average fractional overlap between the inferred and true paths\nfalls from one to zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce and study the planted directed polymer, in which the path of a\nrandom walker is inferred from noisy 'images' accumulated at each timestep.\nFormulated as a nonlinear problem of Bayesian inference for a hidden Markov\nmodel, this problem is a generalization of the directed polymer problem of\nstatistical physics, coinciding with it in the limit of zero signal to noise.\nFor a 1D walker we present numerical investigations and analytical arguments\nthat no phase transition is present. When formulated on a Cayley tree, methods\ndeveloped for the directed polymer are used to show that there is a transition\nwith decreasing signal to noise where effective inference becomes impossible,\nmeaning that the average fractional overlap between the inferred and true paths\nfalls from one to zero."
                },
                "authors": [
                    {
                        "name": "Sun Woo P. Kim"
                    },
                    {
                        "name": "Austen Lamacraft"
                    }
                ],
                "author_detail": {
                    "name": "Austen Lamacraft"
                },
                "author": "Austen Lamacraft",
                "arxiv_doi": "10.1103/PhysRevE.111.024135",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevE.111.024135",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.07263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14752v1",
                "updated": "2025-02-20T17:21:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    21,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:21:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    21,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "TritonBench: Benchmarking Large Language Model Capabilities for\n  Generating Triton Operators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TritonBench: Benchmarking Large Language Model Capabilities for\n  Generating Triton Operators"
                },
                "summary": "Triton, a high-level Python-like language designed for building efficient GPU\nkernels, is widely adopted in deep learning frameworks due to its portability,\nflexibility, and accessibility. However, programming and parallel optimization\nstill require considerable trial and error from Triton developers. Despite\nadvances in large language models (LLMs) for conventional code generation,\nthese models struggle to generate accurate, performance-optimized Triton code,\nas they lack awareness of its specifications and the complexities of GPU\nprogramming. More critically, there is an urgent need for systematic\nevaluations tailored to Triton. In this work, we introduce TritonBench, the\nfirst comprehensive benchmark for Triton operator generation. TritonBench\nfeatures two evaluation channels: a curated set of 184 real-world operators\nfrom GitHub and a collection of operators aligned with PyTorch interfaces.\nUnlike conventional code benchmarks prioritizing functional correctness,\nTritonBench also profiles efficiency performance on widely deployed GPUs\naligned with industry applications. Our study reveals that current\nstate-of-the-art code LLMs struggle to generate efficient Triton operators,\nhighlighting a significant gap in high-performance code generation. TritonBench\nwill be available at https://github.com/thunlp/TritonBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triton, a high-level Python-like language designed for building efficient GPU\nkernels, is widely adopted in deep learning frameworks due to its portability,\nflexibility, and accessibility. However, programming and parallel optimization\nstill require considerable trial and error from Triton developers. Despite\nadvances in large language models (LLMs) for conventional code generation,\nthese models struggle to generate accurate, performance-optimized Triton code,\nas they lack awareness of its specifications and the complexities of GPU\nprogramming. More critically, there is an urgent need for systematic\nevaluations tailored to Triton. In this work, we introduce TritonBench, the\nfirst comprehensive benchmark for Triton operator generation. TritonBench\nfeatures two evaluation channels: a curated set of 184 real-world operators\nfrom GitHub and a collection of operators aligned with PyTorch interfaces.\nUnlike conventional code benchmarks prioritizing functional correctness,\nTritonBench also profiles efficiency performance on widely deployed GPUs\naligned with industry applications. Our study reveals that current\nstate-of-the-art code LLMs struggle to generate efficient Triton operators,\nhighlighting a significant gap in high-performance code generation. TritonBench\nwill be available at https://github.com/thunlp/TritonBench."
                },
                "authors": [
                    {
                        "name": "Jianling Li"
                    },
                    {
                        "name": "Shangzhan Li"
                    },
                    {
                        "name": "Zhenye Gao"
                    },
                    {
                        "name": "Qi Shi"
                    },
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Zefan Wang"
                    },
                    {
                        "name": "Jiacheng Huang"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Jianrong Wang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14748v1",
                "updated": "2025-02-20T17:19:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    19,
                    41,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:19:41Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    19,
                    41,
                    3,
                    51,
                    0
                ],
                "title": "Large Language Models Struggle to Describe the Haystack without Human\n  Help: Human-in-the-loop Evaluation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Struggle to Describe the Haystack without Human\n  Help: Human-in-the-loop Evaluation of LLMs"
                },
                "summary": "A common use of NLP is to facilitate the understanding of large document\ncollections, with a shift from using traditional topic models to Large Language\nModels. Yet the effectiveness of using LLM for large corpus understanding in\nreal-world applications remains under-explored. This study measures the\nknowledge users acquire with unsupervised, supervised LLM-based exploratory\napproaches or traditional topic models on two datasets. While LLM-based methods\ngenerate more human-readable topics and show higher average win probabilities\nthan traditional models for data exploration, they produce overly generic\ntopics for domain-specific datasets that do not easily allow users to learn\nmuch about the documents. Adding human supervision to the LLM generation\nprocess improves data exploration by mitigating hallucination and\nover-genericity but requires greater human effort. In contrast, traditional.\nmodels like Latent Dirichlet Allocation (LDA) remain effective for exploration\nbut are less user-friendly. We show that LLMs struggle to describe the haystack\nof large corpora without human help, particularly domain-specific data, and\nface scaling and hallucination limitations due to context length constraints.\nDataset available at https://huggingface. co/datasets/zli12321/Bills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common use of NLP is to facilitate the understanding of large document\ncollections, with a shift from using traditional topic models to Large Language\nModels. Yet the effectiveness of using LLM for large corpus understanding in\nreal-world applications remains under-explored. This study measures the\nknowledge users acquire with unsupervised, supervised LLM-based exploratory\napproaches or traditional topic models on two datasets. While LLM-based methods\ngenerate more human-readable topics and show higher average win probabilities\nthan traditional models for data exploration, they produce overly generic\ntopics for domain-specific datasets that do not easily allow users to learn\nmuch about the documents. Adding human supervision to the LLM generation\nprocess improves data exploration by mitigating hallucination and\nover-genericity but requires greater human effort. In contrast, traditional.\nmodels like Latent Dirichlet Allocation (LDA) remain effective for exploration\nbut are less user-friendly. We show that LLMs struggle to describe the haystack\nof large corpora without human help, particularly domain-specific data, and\nface scaling and hallucination limitations due to context length constraints.\nDataset available at https://huggingface. co/datasets/zli12321/Bills."
                },
                "authors": [
                    {
                        "name": "Zongxia Li"
                    },
                    {
                        "name": "Lorena Calvo-Bartolom"
                    },
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Paiheng Xu"
                    },
                    {
                        "name": "Alden Dima"
                    },
                    {
                        "name": "Juan Francisco Fung"
                    },
                    {
                        "name": "Jordan Boyd-Graber"
                    }
                ],
                "author_detail": {
                    "name": "Jordan Boyd-Graber"
                },
                "author": "Jordan Boyd-Graber",
                "arxiv_comment": "21 Pages. LLM for Data Exploration and content analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14744v1",
                "updated": "2025-02-20T17:14:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    14,
                    34,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:14:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    14,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language\n  Models via Monitoring Hidden States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language\n  Models via Monitoring Hidden States"
                },
                "summary": "The integration of additional modalities increases the susceptibility of\nlarge vision-language models (LVLMs) to safety risks, such as jailbreak\nattacks, compared to their language-only counterparts. While existing research\nprimarily focuses on post-hoc alignment techniques, the underlying safety\nmechanisms within LVLMs remain largely unexplored. In this work , we\ninvestigate whether LVLMs inherently encode safety-relevant signals within\ntheir internal activations during inference. Our findings reveal that LVLMs\nexhibit distinct activation patterns when processing unsafe prompts, which can\nbe leveraged to detect and mitigate adversarial inputs without requiring\nextensive fine-tuning. Building on this insight, we introduce HiddenDetect, a\nnovel tuning-free framework that harnesses internal model activations to\nenhance safety. Experimental results show that {HiddenDetect} surpasses\nstate-of-the-art methods in detecting jailbreak attacks against LVLMs. By\nutilizing intrinsic safety-aware patterns, our method provides an efficient and\nscalable solution for strengthening LVLM robustness against multimodal threats.\nOur code will be released publicly at\nhttps://github.com/leigest519/HiddenDetect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of additional modalities increases the susceptibility of\nlarge vision-language models (LVLMs) to safety risks, such as jailbreak\nattacks, compared to their language-only counterparts. While existing research\nprimarily focuses on post-hoc alignment techniques, the underlying safety\nmechanisms within LVLMs remain largely unexplored. In this work , we\ninvestigate whether LVLMs inherently encode safety-relevant signals within\ntheir internal activations during inference. Our findings reveal that LVLMs\nexhibit distinct activation patterns when processing unsafe prompts, which can\nbe leveraged to detect and mitigate adversarial inputs without requiring\nextensive fine-tuning. Building on this insight, we introduce HiddenDetect, a\nnovel tuning-free framework that harnesses internal model activations to\nenhance safety. Experimental results show that {HiddenDetect} surpasses\nstate-of-the-art methods in detecting jailbreak attacks against LVLMs. By\nutilizing intrinsic safety-aware patterns, our method provides an efficient and\nscalable solution for strengthening LVLM robustness against multimodal threats.\nOur code will be released publicly at\nhttps://github.com/leigest519/HiddenDetect."
                },
                "authors": [
                    {
                        "name": "Yilei Jiang"
                    },
                    {
                        "name": "Xinyan Gao"
                    },
                    {
                        "name": "Tianshuo Peng"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Xiaoyong Zhu"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14743v1",
                "updated": "2025-02-20T17:12:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    12,
                    45,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:12:45Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    12,
                    45,
                    3,
                    51,
                    0
                ],
                "title": "Multi-Agent Coordination across Diverse Applications: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Coordination across Diverse Applications: A Survey"
                },
                "summary": "Multi-agent coordination studies the underlying mechanism enabling the\ntrending spread of diverse multi-agent systems (MAS) and has received\nincreasing attention, driven by the expansion of emerging applications and\nrapid AI advances. This survey outlines the current state of coordination\nresearch across applications through a unified understanding that answers four\nfundamental coordination questions: (1) what is coordination; (2) why\ncoordination; (3) who to coordinate with; and (4) how to coordinate. Our\npurpose is to explore existing ideas and expertise in coordination and their\nconnections across diverse applications, while identifying and highlighting\nemerging and promising research directions. First, general coordination\nproblems that are essential to varied applications are identified and analyzed.\nSecond, a number of MAS applications are surveyed, ranging from widely studied\ndomains, e.g., search and rescue, warehouse automation and logistics, and\ntransportation systems, to emerging fields including humanoid and\nanthropomorphic robots, satellite systems, and large language models (LLMs).\nFinally, open challenges about the scalability, heterogeneity, and learning\nmechanisms of MAS are analyzed and discussed. In particular, we identify the\nhybridization of hierarchical and decentralized coordination, human-MAS\ncoordination, and LLM-based MAS as promising future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent coordination studies the underlying mechanism enabling the\ntrending spread of diverse multi-agent systems (MAS) and has received\nincreasing attention, driven by the expansion of emerging applications and\nrapid AI advances. This survey outlines the current state of coordination\nresearch across applications through a unified understanding that answers four\nfundamental coordination questions: (1) what is coordination; (2) why\ncoordination; (3) who to coordinate with; and (4) how to coordinate. Our\npurpose is to explore existing ideas and expertise in coordination and their\nconnections across diverse applications, while identifying and highlighting\nemerging and promising research directions. First, general coordination\nproblems that are essential to varied applications are identified and analyzed.\nSecond, a number of MAS applications are surveyed, ranging from widely studied\ndomains, e.g., search and rescue, warehouse automation and logistics, and\ntransportation systems, to emerging fields including humanoid and\nanthropomorphic robots, satellite systems, and large language models (LLMs).\nFinally, open challenges about the scalability, heterogeneity, and learning\nmechanisms of MAS are analyzed and discussed. In particular, we identify the\nhybridization of hierarchical and decentralized coordination, human-MAS\ncoordination, and LLM-based MAS as promising future directions."
                },
                "authors": [
                    {
                        "name": "Lijun Sun"
                    },
                    {
                        "name": "Yijun Yang"
                    },
                    {
                        "name": "Qiqi Duan"
                    },
                    {
                        "name": "Yuhui Shi"
                    },
                    {
                        "name": "Chao Lyu"
                    },
                    {
                        "name": "Yu-Cheng Chang"
                    },
                    {
                        "name": "Chin-Teng Lin"
                    },
                    {
                        "name": "Yang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yang Shen"
                },
                "author": "Yang Shen",
                "arxiv_comment": "23 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14740v1",
                "updated": "2025-02-20T17:08:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    8,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:08:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    8,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "YOLOv12: A Breakdown of the Key Architectural Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YOLOv12: A Breakdown of the Key Architectural Features"
                },
                "summary": "This paper presents an architectural analysis of YOLOv12, a significant\nadvancement in single-stage, real-time object detection building upon the\nstrengths of its predecessors while introducing key improvements. The model\nincorporates an optimised backbone (R-ELAN), 7x7 separable convolutions, and\nFlashAttention-driven area-based attention, improving feature extraction,\nenhanced efficiency, and robust detections. With multiple model variants,\nsimilar to its predecessors, YOLOv12 offers scalable solutions for both\nlatency-sensitive and high-accuracy applications. Experimental results manifest\nconsistent gains in mean average precision (mAP) and inference speed, making\nYOLOv12 a compelling choice for applications in autonomous systems, security,\nand real-time analytics. By achieving an optimal balance between computational\nefficiency and performance, YOLOv12 sets a new benchmark for real-time computer\nvision, facilitating deployment across diverse hardware platforms, from edge\ndevices to high-performance clusters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an architectural analysis of YOLOv12, a significant\nadvancement in single-stage, real-time object detection building upon the\nstrengths of its predecessors while introducing key improvements. The model\nincorporates an optimised backbone (R-ELAN), 7x7 separable convolutions, and\nFlashAttention-driven area-based attention, improving feature extraction,\nenhanced efficiency, and robust detections. With multiple model variants,\nsimilar to its predecessors, YOLOv12 offers scalable solutions for both\nlatency-sensitive and high-accuracy applications. Experimental results manifest\nconsistent gains in mean average precision (mAP) and inference speed, making\nYOLOv12 a compelling choice for applications in autonomous systems, security,\nand real-time analytics. By achieving an optimal balance between computational\nefficiency and performance, YOLOv12 sets a new benchmark for real-time computer\nvision, facilitating deployment across diverse hardware platforms, from edge\ndevices to high-performance clusters."
                },
                "authors": [
                    {
                        "name": "Mujadded Al Rabbani Alif"
                    },
                    {
                        "name": "Muhammad Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Hussain"
                },
                "author": "Muhammad Hussain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14739v1",
                "updated": "2025-02-20T17:05:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    5,
                    58,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:05:58Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    5,
                    58,
                    3,
                    51,
                    0
                ],
                "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope."
                },
                "authors": [
                    {
                        "name": "M-A-P Team"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Yifan Yao"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Bingli Wang"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Kang Zhu"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Xiaolong Jin"
                    },
                    {
                        "name": "Zhenlin Wei"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Kaixing Deng"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Shian Jia"
                    },
                    {
                        "name": "Sichao Jiang"
                    },
                    {
                        "name": "Yiyan Liao"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Qinrui Li"
                    },
                    {
                        "name": "Sirun Li"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Yunwen Li"
                    },
                    {
                        "name": "Dehua Ma"
                    },
                    {
                        "name": "Yuansheng Ni"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Qiyao Wang"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Siwei Wu"
                    },
                    {
                        "name": "Tianshun Xing"
                    },
                    {
                        "name": "Ming Xu"
                    },
                    {
                        "name": "Zhenzhu Yang"
                    },
                    {
                        "name": "Zekun Moore Wang"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Yuelin Bai"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Chengtuo Cheng"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Keyi Ding"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Yun Huang"
                    },
                    {
                        "name": "Yaoru Li"
                    },
                    {
                        "name": "Yizhe Li"
                    },
                    {
                        "name": "Zhaoqun Li"
                    },
                    {
                        "name": "Tianhao Liang"
                    },
                    {
                        "name": "Chengdong Lin"
                    },
                    {
                        "name": "Hongquan Lin"
                    },
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Zhongyuan Peng"
                    },
                    {
                        "name": "Zifan Peng"
                    },
                    {
                        "name": "Qige Qi"
                    },
                    {
                        "name": "Shi Qiu"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Yizhou Tan"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Chenqing Wang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yiya Wang"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Jiajun Xu"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yuanhao Yue"
                    },
                    {
                        "name": "Tianyang Zhan"
                    },
                    {
                        "name": "Chun Zhang"
                    },
                    {
                        "name": "Jingyang Zhang"
                    },
                    {
                        "name": "Xiyue Zhang"
                    },
                    {
                        "name": "Xingjian Zhang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Xiangyu Zheng"
                    },
                    {
                        "name": "Chenghua Zhong"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Junran Peng"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Shi Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Qunshu Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14735v1",
                "updated": "2025-02-20T17:01:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    1,
                    57,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:01:57Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    1,
                    57,
                    3,
                    51,
                    0
                ],
                "title": "EAGER-LLM: Enhancing Large Language Models as Recommenders through\n  Exogenous Behavior-Semantic Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAGER-LLM: Enhancing Large Language Models as Recommenders through\n  Exogenous Behavior-Semantic Integration"
                },
                "summary": "Large language models (LLMs) are increasingly leveraged as foundational\nbackbones in the development of advanced recommender systems, offering enhanced\ncapabilities through their extensive knowledge and reasoning. Existing\nllm-based recommender systems (RSs) often face challenges due to the\nsignificant differences between the linguistic semantics of pre-trained LLMs\nand the collaborative semantics essential for RSs. These systems use\npre-trained linguistic semantics but learn collaborative semantics from scratch\nvia the llm-Backbone. However, LLMs are not designed for recommendations,\nleading to inefficient collaborative learning, weak result correlations, and\npoor integration of traditional RS features. To address these challenges, we\npropose EAGER-LLM, a decoder-only llm-based generative recommendation framework\nthat integrates endogenous and exogenous behavioral and semantic information in\na non-intrusive manner. Specifically, we propose 1)dual-source knowledge-rich\nitem indices that integrates indexing sequences for exogenous signals, enabling\nefficient link-wide processing; 2)non-invasive multiscale alignment\nreconstruction tasks guide the model toward a deeper understanding of both\ncollaborative and semantic signals; 3)an annealing adapter designed to finely\nbalance the model's recommendation performance with its comprehension\ncapabilities. We demonstrate EAGER-LLM's effectiveness through rigorous testing\non three public benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly leveraged as foundational\nbackbones in the development of advanced recommender systems, offering enhanced\ncapabilities through their extensive knowledge and reasoning. Existing\nllm-based recommender systems (RSs) often face challenges due to the\nsignificant differences between the linguistic semantics of pre-trained LLMs\nand the collaborative semantics essential for RSs. These systems use\npre-trained linguistic semantics but learn collaborative semantics from scratch\nvia the llm-Backbone. However, LLMs are not designed for recommendations,\nleading to inefficient collaborative learning, weak result correlations, and\npoor integration of traditional RS features. To address these challenges, we\npropose EAGER-LLM, a decoder-only llm-based generative recommendation framework\nthat integrates endogenous and exogenous behavioral and semantic information in\na non-intrusive manner. Specifically, we propose 1)dual-source knowledge-rich\nitem indices that integrates indexing sequences for exogenous signals, enabling\nefficient link-wide processing; 2)non-invasive multiscale alignment\nreconstruction tasks guide the model toward a deeper understanding of both\ncollaborative and semantic signals; 3)an annealing adapter designed to finely\nbalance the model's recommendation performance with its comprehension\ncapabilities. We demonstrate EAGER-LLM's effectiveness through rigorous testing\non three public benchmarks."
                },
                "authors": [
                    {
                        "name": "Minjie Hong"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Zehan Wang"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Sihang Cai"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Quanyu Dai"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Zhimeng Zhang"
                    },
                    {
                        "name": "Zhou Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Zhao"
                },
                "author": "Zhou Zhao",
                "arxiv_doi": "10.1145/3696410.3714933",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714933",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 6 figures, accpeted by WWW 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14727v1",
                "updated": "2025-02-20T16:54:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    54,
                    7,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:54:07Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    54,
                    7,
                    3,
                    51,
                    0
                ],
                "title": "WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken\n  Dialogue Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken\n  Dialogue Models"
                },
                "summary": "Retrieval Augmented Generation (RAG) has gained widespread adoption owing to\nits capacity to empower large language models (LLMs) to integrate external\nknowledge. However, existing RAG frameworks are primarily designed for\ntext-based LLMs and rely on Automatic Speech Recognition to process speech\ninput, which discards crucial audio information, risks transcription errors,\nand increases computational overhead. Therefore, we introduce WavRAG, the first\nretrieval augmented generation framework with native, end-to-end audio support.\nWavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw\naudio for both embedding and retrieval. 2) WavRAG integrates audio and text\ninto a unified knowledge representation. Specifically, we propose the\nWavRetriever to facilitate the retrieval from a text-audio hybrid knowledge\nbase, and further enhance the in-context capabilities of spoken dialogue models\nthrough the integration of chain-of-thought reasoning. In comparison to\nstate-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval\nperformance while delivering a 10x acceleration. Furthermore, WavRAG's unique\ntext-audio hybrid retrieval capability extends the boundaries of RAG to the\naudio modality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has gained widespread adoption owing to\nits capacity to empower large language models (LLMs) to integrate external\nknowledge. However, existing RAG frameworks are primarily designed for\ntext-based LLMs and rely on Automatic Speech Recognition to process speech\ninput, which discards crucial audio information, risks transcription errors,\nand increases computational overhead. Therefore, we introduce WavRAG, the first\nretrieval augmented generation framework with native, end-to-end audio support.\nWavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw\naudio for both embedding and retrieval. 2) WavRAG integrates audio and text\ninto a unified knowledge representation. Specifically, we propose the\nWavRetriever to facilitate the retrieval from a text-audio hybrid knowledge\nbase, and further enhance the in-context capabilities of spoken dialogue models\nthrough the integration of chain-of-thought reasoning. In comparison to\nstate-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval\nperformance while delivering a 10x acceleration. Furthermore, WavRAG's unique\ntext-audio hybrid retrieval capability extends the boundaries of RAG to the\naudio modality."
                },
                "authors": [
                    {
                        "name": "Yifu Chen"
                    },
                    {
                        "name": "Shengpeng Ji"
                    },
                    {
                        "name": "Haoxiao Wang"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Jinzheng He"
                    },
                    {
                        "name": "Jin Xu"
                    },
                    {
                        "name": "Zhou Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Zhao"
                },
                "author": "Zhou Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14721v1",
                "updated": "2025-02-20T16:48:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    48,
                    14,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    48,
                    14,
                    3,
                    51,
                    0
                ],
                "title": "Multi-dataset synergistic in supervised learning to pre-label structural\n  components in point clouds from shell construction scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-dataset synergistic in supervised learning to pre-label structural\n  components in point clouds from shell construction scenes"
                },
                "summary": "The significant effort required to annotate data for new training datasets\nhinders computer vision research and machine learning in the construction\nindustry. This work explores adapting standard datasets and the latest\ntransformer model architectures for point cloud semantic segmentation in the\ncontext of shell construction sites. Unlike common approaches focused on object\nsegmentation of building interiors and furniture, this study addressed the\nchallenges of segmenting complex structural components in Architecture,\nEngineering, and Construction (AEC). We establish a baseline through supervised\ntraining and a custom validation dataset, evaluate the cross-domain inference\nwith large-scale indoor datasets, and utilize transfer learning to maximize\nsegmentation performance with minimal new data. The findings indicate that with\nminimal fine-tuning, pre-trained transformer architectures offer an effective\nstrategy for building component segmentation. Our results are promising for\nautomating the annotation of new, previously unseen data when creating larger\ntraining resources and for the segmentation of frequently recurring objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant effort required to annotate data for new training datasets\nhinders computer vision research and machine learning in the construction\nindustry. This work explores adapting standard datasets and the latest\ntransformer model architectures for point cloud semantic segmentation in the\ncontext of shell construction sites. Unlike common approaches focused on object\nsegmentation of building interiors and furniture, this study addressed the\nchallenges of segmenting complex structural components in Architecture,\nEngineering, and Construction (AEC). We establish a baseline through supervised\ntraining and a custom validation dataset, evaluate the cross-domain inference\nwith large-scale indoor datasets, and utilize transfer learning to maximize\nsegmentation performance with minimal new data. The findings indicate that with\nminimal fine-tuning, pre-trained transformer architectures offer an effective\nstrategy for building component segmentation. Our results are promising for\nautomating the annotation of new, previously unseen data when creating larger\ntraining resources and for the segmentation of frequently recurring objects."
                },
                "authors": [
                    {
                        "name": "Lukas Rauch"
                    },
                    {
                        "name": "Thomas Braml"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Braml"
                },
                "author": "Thomas Braml",
                "arxiv_comment": "18 pages, 8 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.08469v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.08469v6",
                "updated": "2025-02-20T16:48:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    48,
                    8,
                    3,
                    51,
                    0
                ],
                "published": "2023-08-16T16:19:50Z",
                "published_parsed": [
                    2023,
                    8,
                    16,
                    16,
                    19,
                    50,
                    2,
                    228,
                    0
                ],
                "title": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series\n  Forecasters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series\n  Forecasters"
                },
                "summary": "Multivariate time-series forecasting is vital in various domains, e.g.,\neconomic planning and weather prediction. Deep train-from-scratch models have\nexhibited effective performance yet require large amounts of data, which limits\nreal-world applicability. Recently, researchers have leveraged the\nrepresentation learning transferability of pre-trained Large Language Models\n(LLMs) to handle limited non-linguistic datasets effectively. However,\nincorporating LLMs with time-series data presents challenges of limited\nadaptation due to different compositions between time-series and linguistic\ndata, and the inability to process multi-scale temporal information. To tackle\nthese challenges, we propose LLM4TS, a framework for time-series forecasting\nwith pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the\ntime-series alignment stage to align LLMs with the nuances of time-series data,\nand the forecasting fine-tuning stage for downstream time-series forecasting\ntasks. Furthermore, our framework features a novel two-level aggregation method\nthat integrates multi-scale temporal data within pre-trained LLMs, enhancing\ntheir ability to interpret time-specific information. In experiments across 7\ntime-series forecasting datasets, LLM4TS is superior to existing\nstate-of-the-art methods compared with trained-from-scratch models in full-shot\nscenarios, and also achieves the highest rank in few-shot scenarios. In\naddition, evaluations compared with different unsupervised representation\nlearning approaches highlight LLM4TS's effectiveness with representation\nlearning in forecasting tasks. Ablation studies further validate each\ncomponent's contribution to LLM4TS and underscore the essential role of\nutilizing LLM's pre-trained weights for optimal performance. The code is\navailable at https://github.com/blacksnail789521/LLM4TS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate time-series forecasting is vital in various domains, e.g.,\neconomic planning and weather prediction. Deep train-from-scratch models have\nexhibited effective performance yet require large amounts of data, which limits\nreal-world applicability. Recently, researchers have leveraged the\nrepresentation learning transferability of pre-trained Large Language Models\n(LLMs) to handle limited non-linguistic datasets effectively. However,\nincorporating LLMs with time-series data presents challenges of limited\nadaptation due to different compositions between time-series and linguistic\ndata, and the inability to process multi-scale temporal information. To tackle\nthese challenges, we propose LLM4TS, a framework for time-series forecasting\nwith pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the\ntime-series alignment stage to align LLMs with the nuances of time-series data,\nand the forecasting fine-tuning stage for downstream time-series forecasting\ntasks. Furthermore, our framework features a novel two-level aggregation method\nthat integrates multi-scale temporal data within pre-trained LLMs, enhancing\ntheir ability to interpret time-specific information. In experiments across 7\ntime-series forecasting datasets, LLM4TS is superior to existing\nstate-of-the-art methods compared with trained-from-scratch models in full-shot\nscenarios, and also achieves the highest rank in few-shot scenarios. In\naddition, evaluations compared with different unsupervised representation\nlearning approaches highlight LLM4TS's effectiveness with representation\nlearning in forecasting tasks. Ablation studies further validate each\ncomponent's contribution to LLM4TS and underscore the essential role of\nutilizing LLM's pre-trained weights for optimal performance. The code is\navailable at https://github.com/blacksnail789521/LLM4TS."
                },
                "authors": [
                    {
                        "name": "Ching Chang"
                    },
                    {
                        "name": "Wei-Yao Wang"
                    },
                    {
                        "name": "Wen-Chih Peng"
                    },
                    {
                        "name": "Tien-Fu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tien-Fu Chen"
                },
                "author": "Tien-Fu Chen",
                "arxiv_doi": "10.1145/3719207 10.1145/3719207",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3719207",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3719207",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.08469v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.08469v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in ACM Transactions on Intelligent Systems\n  and Technology (TIST) 2025. The final published version will be available at\n  https://doi.org/10.1145/3719207",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13019v2",
                "updated": "2025-02-20T16:47:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    47,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-18T16:38:39Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    38,
                    39,
                    1,
                    49,
                    0
                ],
                "title": "Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented\n  Generation"
                },
                "summary": "Despite the remarkable capabilities of Large Language Models (LLMs) in\nvarious NLP tasks, they remain vulnerable to hallucinations due to their\nlimited parametric knowledge and lack of domain-specific expertise.\nRetrieval-Augmented Generation (RAG) addresses this challenge by incorporating\nexternal document retrieval to augment the knowledge base of LLMs. In this\napproach, RAG retrieves document chunks from an external corpus in response to\na query, which are then used as context for the downstream language model to\ngenerate an answer. However, these retrieved knowledge sources often include\nirrelevant or erroneous information, undermining the effectiveness of RAG in\ndownstream tasks. To overcome this limitation, we introduce a compact,\nefficient, and pluggable module designed to refine external knowledge sources\nbefore feeding them to the generator. The module reconstructs retrieved content\nby extracting the most relevant and supportive information and reorganising it\ninto a concise, query-specific format. Through a three-stage training paradigm\n- comprising supervised fine-tuning, contrastive multi-task learning, and\nreinforcement learning-based alignment - it prioritises critical knowledge and\naligns it with the generator's preferences. This method enables LLMs to produce\noutputs that are more accurate, reliable, and contextually appropriate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable capabilities of Large Language Models (LLMs) in\nvarious NLP tasks, they remain vulnerable to hallucinations due to their\nlimited parametric knowledge and lack of domain-specific expertise.\nRetrieval-Augmented Generation (RAG) addresses this challenge by incorporating\nexternal document retrieval to augment the knowledge base of LLMs. In this\napproach, RAG retrieves document chunks from an external corpus in response to\na query, which are then used as context for the downstream language model to\ngenerate an answer. However, these retrieved knowledge sources often include\nirrelevant or erroneous information, undermining the effectiveness of RAG in\ndownstream tasks. To overcome this limitation, we introduce a compact,\nefficient, and pluggable module designed to refine external knowledge sources\nbefore feeding them to the generator. The module reconstructs retrieved content\nby extracting the most relevant and supportive information and reorganising it\ninto a concise, query-specific format. Through a three-stage training paradigm\n- comprising supervised fine-tuning, contrastive multi-task learning, and\nreinforcement learning-based alignment - it prioritises critical knowledge and\naligns it with the generator's preferences. This method enables LLMs to produce\noutputs that are more accurate, reliable, and contextually appropriate."
                },
                "authors": [
                    {
                        "name": "Sha Li"
                    },
                    {
                        "name": "Naren Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Naren Ramakrishnan"
                },
                "author": "Naren Ramakrishnan",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14719v1",
                "updated": "2025-02-20T16:44:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    44,
                    54,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:44:54Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    44,
                    54,
                    3,
                    51,
                    0
                ],
                "title": "Internal Incoherency Scores for Constraint-based Causal Discovery\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal Incoherency Scores for Constraint-based Causal Discovery\n  Algorithms"
                },
                "summary": "Causal discovery aims to infer causal graphs from observational or\nexperimental data. Methods such as the popular PC algorithm are based on\nconditional independence testing and utilize enabling assumptions, such as the\nfaithfulness assumption, for their inferences. In practice, these assumptions,\nas well as the functional assumptions inherited from the chosen conditional\nindependence test, are typically taken as a given and not further tested for\ntheir validity on the data. In this work, we propose internal coherency scores\nthat allow testing for assumption violations and finite sample errors, whenever\ndetectable without requiring ground truth or further statistical tests. We\nprovide a complete classification of erroneous results, including a distinction\nbetween detectable and undetectable errors, and prove that the detectable\nerroneous results can be measured by our scores. We illustrate our coherency\nscores on the PC algorithm with simulated and real-world datasets, and envision\nthat testing for internal coherency can become a standard tool in applying\nconstraint-based methods, much like a suite of tests is used to validate the\nassumptions of classical regression analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal discovery aims to infer causal graphs from observational or\nexperimental data. Methods such as the popular PC algorithm are based on\nconditional independence testing and utilize enabling assumptions, such as the\nfaithfulness assumption, for their inferences. In practice, these assumptions,\nas well as the functional assumptions inherited from the chosen conditional\nindependence test, are typically taken as a given and not further tested for\ntheir validity on the data. In this work, we propose internal coherency scores\nthat allow testing for assumption violations and finite sample errors, whenever\ndetectable without requiring ground truth or further statistical tests. We\nprovide a complete classification of erroneous results, including a distinction\nbetween detectable and undetectable errors, and prove that the detectable\nerroneous results can be measured by our scores. We illustrate our coherency\nscores on the PC algorithm with simulated and real-world datasets, and envision\nthat testing for internal coherency can become a standard tool in applying\nconstraint-based methods, much like a suite of tests is used to validate the\nassumptions of classical regression analysis."
                },
                "authors": [
                    {
                        "name": "Sofia Faltenbacher"
                    },
                    {
                        "name": "Jonas Wahl"
                    },
                    {
                        "name": "Rebecca Herman"
                    },
                    {
                        "name": "Jakob Runge"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Runge"
                },
                "author": "Jakob Runge",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14718v1",
                "updated": "2025-02-20T16:44:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    44,
                    46,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:44:46Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    44,
                    46,
                    3,
                    51,
                    0
                ],
                "title": "Entity Framing and Role Portrayal in the News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity Framing and Role Portrayal in the News"
                },
                "summary": "We introduce a novel multilingual hierarchical corpus annotated for entity\nframing and role portrayal in news articles. The dataset uses a unique taxonomy\ninspired by storytelling elements, comprising 22 fine-grained roles, or\narchetypes, nested within three main categories: protagonist, antagonist, and\ninnocent. Each archetype is carefully defined, capturing nuanced portrayals of\nentities such as guardian, martyr, and underdog for protagonists; tyrant,\ndeceiver, and bigot for antagonists; and victim, scapegoat, and exploited for\ninnocents. The dataset includes 1,378 recent news articles in five languages\n(Bulgarian, English, Hindi, European Portuguese, and Russian) focusing on two\ncritical domains of global significance: the Ukraine-Russia War and Climate\nChange. Over 5,800 entity mentions have been annotated with role labels. This\ndataset serves as a valuable resource for research into role portrayal and has\nbroader implications for news analysis. We describe the characteristics of the\ndataset and the annotation process, and we report evaluation results on\nfine-tuned state-of-the-art multilingual transformers and hierarchical\nzero-shot learning using LLMs at the level of a document, a paragraph, and a\nsentence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel multilingual hierarchical corpus annotated for entity\nframing and role portrayal in news articles. The dataset uses a unique taxonomy\ninspired by storytelling elements, comprising 22 fine-grained roles, or\narchetypes, nested within three main categories: protagonist, antagonist, and\ninnocent. Each archetype is carefully defined, capturing nuanced portrayals of\nentities such as guardian, martyr, and underdog for protagonists; tyrant,\ndeceiver, and bigot for antagonists; and victim, scapegoat, and exploited for\ninnocents. The dataset includes 1,378 recent news articles in five languages\n(Bulgarian, English, Hindi, European Portuguese, and Russian) focusing on two\ncritical domains of global significance: the Ukraine-Russia War and Climate\nChange. Over 5,800 entity mentions have been annotated with role labels. This\ndataset serves as a valuable resource for research into role portrayal and has\nbroader implications for news analysis. We describe the characteristics of the\ndataset and the annotation process, and we report evaluation results on\nfine-tuned state-of-the-art multilingual transformers and hierarchical\nzero-shot learning using LLMs at the level of a document, a paragraph, and a\nsentence."
                },
                "authors": [
                    {
                        "name": "Tarek Mahmoud"
                    },
                    {
                        "name": "Zhuohan Xie"
                    },
                    {
                        "name": "Dimitar Dimitrov"
                    },
                    {
                        "name": "Nikolaos Nikolaidis"
                    },
                    {
                        "name": "Purificao Silvano"
                    },
                    {
                        "name": "Roman Yangarber"
                    },
                    {
                        "name": "Shivam Sharma"
                    },
                    {
                        "name": "Elisa Sartori"
                    },
                    {
                        "name": "Nicolas Stefanovitch"
                    },
                    {
                        "name": "Giovanni Da San Martino"
                    },
                    {
                        "name": "Jakub Piskorski"
                    },
                    {
                        "name": "Preslav Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Preslav Nakov"
                },
                "author": "Preslav Nakov",
                "arxiv_comment": "23 pages, 12 figures. Submitted to ACL Rolling Review (ARR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14716v1",
                "updated": "2025-02-20T16:42:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    42,
                    9,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:42:09Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    42,
                    9,
                    3,
                    51,
                    0
                ],
                "title": "Outlier Detection in Mendelian Randomisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outlier Detection in Mendelian Randomisation"
                },
                "summary": "Mendelian Randomisation (MR) uses genetic variants as instrumental variables\nto infer causal effects of exposures on an outcome. One key assumption of MR is\nthat the genetic variants used as instrumental variables are independent of the\noutcome conditional on the risk factor and unobserved confounders. Violations\nof this assumption, i.e. the effect of the instrumental variables on the\noutcome through a path other than the risk factor included in the model (which\ncan be caused by pleiotropy), are common phenomena in human genetics. Genetic\nvariants, which deviate from this assumption, appear as outliers to the MR\nmodel fit and can be detected by the general heterogeneity statistics proposed\nin the literature, which are known to suffer from overdispersion, i.e. too many\ngenetic variants are declared as false outliers. We propose a method that\ncorrects for overdispersion of the heterogeneity statistics in uni- and\nmultivariable MR analysis by making use of the estimated inflation factor to\ncorrectly remove outlying instruments and therefore account for pleiotropic\neffects. Our method is applicable to summary-level data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mendelian Randomisation (MR) uses genetic variants as instrumental variables\nto infer causal effects of exposures on an outcome. One key assumption of MR is\nthat the genetic variants used as instrumental variables are independent of the\noutcome conditional on the risk factor and unobserved confounders. Violations\nof this assumption, i.e. the effect of the instrumental variables on the\noutcome through a path other than the risk factor included in the model (which\ncan be caused by pleiotropy), are common phenomena in human genetics. Genetic\nvariants, which deviate from this assumption, appear as outliers to the MR\nmodel fit and can be detected by the general heterogeneity statistics proposed\nin the literature, which are known to suffer from overdispersion, i.e. too many\ngenetic variants are declared as false outliers. We propose a method that\ncorrects for overdispersion of the heterogeneity statistics in uni- and\nmultivariable MR analysis by making use of the estimated inflation factor to\ncorrectly remove outlying instruments and therefore account for pleiotropic\neffects. Our method is applicable to summary-level data."
                },
                "authors": [
                    {
                        "name": "Maximilian M Mandl"
                    },
                    {
                        "name": "Anne-Laure Boulesteix"
                    },
                    {
                        "name": "Stephen Burgess"
                    },
                    {
                        "name": "Verena Zuber"
                    }
                ],
                "author_detail": {
                    "name": "Verena Zuber"
                },
                "author": "Verena Zuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09636v2",
                "updated": "2025-02-20T16:40:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    40,
                    48,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-09T04:40:35Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    4,
                    40,
                    35,
                    6,
                    40,
                    0
                ],
                "title": "Reading between the Lines: Can LLMs Identify Cross-Cultural\n  Communication Gaps?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reading between the Lines: Can LLMs Identify Cross-Cultural\n  Communication Gaps?"
                },
                "summary": "In a rapidly globalizing and digital world, content such as book and product\nreviews created by people from diverse cultures are read and consumed by others\nfrom different corners of the world. In this paper, we investigate the extent\nand patterns of gaps in understandability of book reviews due to the presence\nof culturally-specific items and elements that might be alien to users from\nanother culture. Our user-study on 57 book reviews from Goodreads reveal that\n83\\% of the reviews had at least one culture-specific difficult-to-understand\nelement. We also evaluate the efficacy of GPT-4o in identifying such items,\ngiven the cultural background of the reader; the results are mixed, implying a\nsignificant scope for improvement. Our datasets are available here:\nhttps://github.com/sougata-ub/reading_between_lines",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a rapidly globalizing and digital world, content such as book and product\nreviews created by people from diverse cultures are read and consumed by others\nfrom different corners of the world. In this paper, we investigate the extent\nand patterns of gaps in understandability of book reviews due to the presence\nof culturally-specific items and elements that might be alien to users from\nanother culture. Our user-study on 57 book reviews from Goodreads reveal that\n83\\% of the reviews had at least one culture-specific difficult-to-understand\nelement. We also evaluate the efficacy of GPT-4o in identifying such items,\ngiven the cultural background of the reader; the results are mixed, implying a\nsignificant scope for improvement. Our datasets are available here:\nhttps://github.com/sougata-ub/reading_between_lines"
                },
                "authors": [
                    {
                        "name": "Sougata Saha"
                    },
                    {
                        "name": "Saurabh Kumar Pandey"
                    },
                    {
                        "name": "Harshit Gupta"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14714v1",
                "updated": "2025-02-20T16:39:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    39,
                    57,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:39:57Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    39,
                    57,
                    3,
                    51,
                    0
                ],
                "title": "From Knowledge Generation to Knowledge Verification: Examining the\n  BioMedical Generative Capabilities of ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Knowledge Generation to Knowledge Verification: Examining the\n  BioMedical Generative Capabilities of ChatGPT"
                },
                "summary": "The generative capabilities of LLM models present opportunities in\naccelerating tasks and concerns with the authenticity of the knowledge it\nproduces. To address the concerns, we present a computational approach that\nsystematically evaluates the factual accuracy of biomedical knowledge that an\nLLM model has been prompted to generate. Our approach encompasses two\nprocesses: the generation of disease-centric associations and the verification\nof them using the semantic knowledge of the biomedical ontologies. Using\nChatGPT as the select LLM model, we designed a set of prompt-engineering\nprocesses to generate linkages between diseases, drugs, symptoms, and genes to\nestablish grounds for assessments. Experimental results demonstrate high\naccuracy in identifying disease terms (88%-97%), drug names (90%-91%), and\ngenetic information (88%-98%). The symptom term identification accuracy was\nnotably lower (49%-61%), as verified against the DOID, ChEBI, SYMPTOM, and GO\nontologies accordingly. The verification of associations reveals literature\ncoverage rates of (89%-91%) among disease-drug and disease-gene associations.\nThe low identification accuracy for symptom terms also contributed to the\nverification of symptom-related associations (49%-62%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generative capabilities of LLM models present opportunities in\naccelerating tasks and concerns with the authenticity of the knowledge it\nproduces. To address the concerns, we present a computational approach that\nsystematically evaluates the factual accuracy of biomedical knowledge that an\nLLM model has been prompted to generate. Our approach encompasses two\nprocesses: the generation of disease-centric associations and the verification\nof them using the semantic knowledge of the biomedical ontologies. Using\nChatGPT as the select LLM model, we designed a set of prompt-engineering\nprocesses to generate linkages between diseases, drugs, symptoms, and genes to\nestablish grounds for assessments. Experimental results demonstrate high\naccuracy in identifying disease terms (88%-97%), drug names (90%-91%), and\ngenetic information (88%-98%). The symptom term identification accuracy was\nnotably lower (49%-61%), as verified against the DOID, ChEBI, SYMPTOM, and GO\nontologies accordingly. The verification of associations reveals literature\ncoverage rates of (89%-91%) among disease-drug and disease-gene associations.\nThe low identification accuracy for symptom terms also contributed to the\nverification of symptom-related associations (49%-62%)."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdeen Hamed"
                    },
                    {
                        "name": "Byung Suk Lee"
                    }
                ],
                "author_detail": {
                    "name": "Byung Suk Lee"
                },
                "author": "Byung Suk Lee",
                "arxiv_comment": "26 pages, 6 figures, In Review with a Cell Press Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.4; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14709v1",
                "updated": "2025-02-20T16:34:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    34,
                    46,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:34:46Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    34,
                    46,
                    3,
                    51,
                    0
                ],
                "title": "Data-Efficient Pretraining with Group-Level Data Influence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Efficient Pretraining with Group-Level Data Influence Modeling"
                },
                "summary": "Data-efficient pretraining has shown tremendous potential to elevate scaling\nlaws. This paper argues that effective pretraining data should be curated at\nthe group level, treating a set of data points as a whole rather than as\nindependent contributors. To achieve that, we propose Group-Level Data\nInfluence Modeling (Group-MATES), a novel data-efficient pretraining method\nthat captures and optimizes group-level data utility. Specifically, Group-MATES\ncollects oracle group-level influences by locally probing the pretraining model\nwith data sets. It then fine-tunes a relational data influence model to\napproximate oracles as relationship-weighted aggregations of individual\ninfluences. The fine-tuned model selects the data subset by maximizing its\ngroup-level influence prediction, with influence-aware clustering to enable\nefficient inference. Experiments on the DCLM benchmark demonstrate that\nGroup-MATES achieves a 10% relative core score improvement on 22 downstream\ntasks over DCLM-Baseline and 5% over individual-influence-based methods,\nestablishing a new state-of-the-art. Further analyses highlight the\neffectiveness of relational data influence models in capturing intricate\ninteractions between data points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-efficient pretraining has shown tremendous potential to elevate scaling\nlaws. This paper argues that effective pretraining data should be curated at\nthe group level, treating a set of data points as a whole rather than as\nindependent contributors. To achieve that, we propose Group-Level Data\nInfluence Modeling (Group-MATES), a novel data-efficient pretraining method\nthat captures and optimizes group-level data utility. Specifically, Group-MATES\ncollects oracle group-level influences by locally probing the pretraining model\nwith data sets. It then fine-tunes a relational data influence model to\napproximate oracles as relationship-weighted aggregations of individual\ninfluences. The fine-tuned model selects the data subset by maximizing its\ngroup-level influence prediction, with influence-aware clustering to enable\nefficient inference. Experiments on the DCLM benchmark demonstrate that\nGroup-MATES achieves a 10% relative core score improvement on 22 downstream\ntasks over DCLM-Baseline and 5% over individual-influence-based methods,\nestablishing a new state-of-the-art. Further analyses highlight the\neffectiveness of relational data influence models in capturing intricate\ninteractions between data points."
                },
                "authors": [
                    {
                        "name": "Zichun Yu"
                    },
                    {
                        "name": "Fei Peng"
                    },
                    {
                        "name": "Jie Lei"
                    },
                    {
                        "name": "Arnold Overwijk"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Chenyan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Chenyan Xiong"
                },
                "author": "Chenyan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17711v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17711v2",
                "updated": "2025-02-20T16:30:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    30,
                    46,
                    3,
                    51,
                    0
                ],
                "published": "2025-01-29T15:28:06Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    15,
                    28,
                    6,
                    2,
                    29,
                    0
                ],
                "title": "STGCN-LSTM for Olympic Medal Prediction: Dynamic Power Modeling and\n  Causal Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STGCN-LSTM for Olympic Medal Prediction: Dynamic Power Modeling and\n  Causal Policy Optimization"
                },
                "summary": "This paper proposes a novel hybrid model, STGCN-LSTM, to forecast Olympic\nmedal distributions by integrating the spatio-temporal relationships among\ncountries and the long-term dependencies of national performance. The\nSpatial-Temporal Graph Convolution Network (STGCN) captures geographic and\ninteractive factors-such as coaching exchange and socio-economic links-while\nthe Long Short-Term Memory (LSTM) module models historical trends in medal\ncounts, economic data, and demographics. To address zero-inflated outputs\n(i.e., the disparity between countries that consistently yield wins and those\nnever having won medals), a Zero-Inflated Compound Poisson (ZICP) framework is\nincorporated to separate random zeros from structural zeros, providing a\nclearer view of potential breakthrough performances. Validation includes\nhistorical backtracking, policy shock simulations, and causal inference checks,\nconfirming the robustness of the proposed method. Results shed light on the\ninfluence of coaching mobility, event specialization, and strategic investment\non medal forecasts, offering a data-driven foundation for optimizing sports\npolicies and resource allocation in diverse Olympic contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel hybrid model, STGCN-LSTM, to forecast Olympic\nmedal distributions by integrating the spatio-temporal relationships among\ncountries and the long-term dependencies of national performance. The\nSpatial-Temporal Graph Convolution Network (STGCN) captures geographic and\ninteractive factors-such as coaching exchange and socio-economic links-while\nthe Long Short-Term Memory (LSTM) module models historical trends in medal\ncounts, economic data, and demographics. To address zero-inflated outputs\n(i.e., the disparity between countries that consistently yield wins and those\nnever having won medals), a Zero-Inflated Compound Poisson (ZICP) framework is\nincorporated to separate random zeros from structural zeros, providing a\nclearer view of potential breakthrough performances. Validation includes\nhistorical backtracking, policy shock simulations, and causal inference checks,\nconfirming the robustness of the proposed method. Results shed light on the\ninfluence of coaching mobility, event specialization, and strategic investment\non medal forecasts, offering a data-driven foundation for optimizing sports\npolicies and resource allocation in diverse Olympic contexts."
                },
                "authors": [
                    {
                        "name": "Yiquan Wang"
                    },
                    {
                        "name": "Jiaying Wang"
                    },
                    {
                        "name": "Tin-Yeh Huang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Guanjie Yang"
                    },
                    {
                        "name": "Zihao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zihao Xu"
                },
                "author": "Zihao Xu",
                "arxiv_comment": "18pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17711v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17711v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14701v1",
                "updated": "2025-02-20T16:25:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    25,
                    40,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:25:40Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    25,
                    40,
                    3,
                    51,
                    0
                ],
                "title": "Catching the wisps: Stellar mass-loss limits from low-frequency radio\n  observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catching the wisps: Stellar mass-loss limits from low-frequency radio\n  observations"
                },
                "summary": "The winds of low-mass stars carry away angular momentum and impact the\natmospheres of surrounding planets. Determining the properties of these winds\nis necessary to understand the mass-loss history of the star and the evolution\nof exoplanetary atmospheres. Due to their tenuous nature, the winds of low-mass\nmain-sequence stars are difficult to detect. The few existing techniques for\nmeasuring these winds are indirect, with the most common inference method for\nwinds of low-mass stars being astrospheric Lyman-$\\alpha$ absorption combined\nwith complex hydrodynamical modelling of the interaction between the stellar\nwind and the interstellar medium. Here, we employ a more direct method to place\nupper limits on the mass-loss rates of low-mass stars by combining observations\nof low-frequency coherent radio emission, the lack of free-free absorption, and\na simple stellar wind model. We determine upper limits on the mass-loss rate\nfor a sample of 19 M dwarf stars detected with the LOFAR telescope at 120--168\nMHz, reaching a sensitivity within an order of magnitude of the solar mass-loss\nrate for cold stars with a surface magnetic field strength of $\\sim$100 G. The\nsensitivity of our method does not depend on distance or spectral type,\nallowing us to find mass-loss rate constraints for stars up to spectral type M6\nand out to a distance of 50 pc, later and farther than previous measurements.\nWith upcoming low-frequency surveys with both LOFAR and the Square Kilometre\nArray, the number of stars with mass-loss rate upper limits determined with\nthis method could reach $\\sim$1000.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The winds of low-mass stars carry away angular momentum and impact the\natmospheres of surrounding planets. Determining the properties of these winds\nis necessary to understand the mass-loss history of the star and the evolution\nof exoplanetary atmospheres. Due to their tenuous nature, the winds of low-mass\nmain-sequence stars are difficult to detect. The few existing techniques for\nmeasuring these winds are indirect, with the most common inference method for\nwinds of low-mass stars being astrospheric Lyman-$\\alpha$ absorption combined\nwith complex hydrodynamical modelling of the interaction between the stellar\nwind and the interstellar medium. Here, we employ a more direct method to place\nupper limits on the mass-loss rates of low-mass stars by combining observations\nof low-frequency coherent radio emission, the lack of free-free absorption, and\na simple stellar wind model. We determine upper limits on the mass-loss rate\nfor a sample of 19 M dwarf stars detected with the LOFAR telescope at 120--168\nMHz, reaching a sensitivity within an order of magnitude of the solar mass-loss\nrate for cold stars with a surface magnetic field strength of $\\sim$100 G. The\nsensitivity of our method does not depend on distance or spectral type,\nallowing us to find mass-loss rate constraints for stars up to spectral type M6\nand out to a distance of 50 pc, later and farther than previous measurements.\nWith upcoming low-frequency surveys with both LOFAR and the Square Kilometre\nArray, the number of stars with mass-loss rate upper limits determined with\nthis method could reach $\\sim$1000."
                },
                "authors": [
                    {
                        "name": "Sanne Bloot"
                    },
                    {
                        "name": "Harish K. Vedantham"
                    },
                    {
                        "name": "Robert D. Kavanagh"
                    },
                    {
                        "name": "Joseph R. Callingham"
                    },
                    {
                        "name": "Benjamin J. S. Pope"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin J. S. Pope"
                },
                "author": "Benjamin J. S. Pope",
                "arxiv_comment": "Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12844v2",
                "updated": "2025-02-20T16:20:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    20,
                    11,
                    3,
                    51,
                    0
                ],
                "published": "2024-07-04T17:57:38Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    17,
                    57,
                    38,
                    3,
                    186,
                    0
                ],
                "title": "metabench -- A Sparse Benchmark of Reasoning and Knowledge in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "metabench -- A Sparse Benchmark of Reasoning and Knowledge in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) vary in their abilities on a range of tasks.\nInitiatives such as the Open LLM Leaderboard aim to quantify these differences\nwith several large benchmarks (sets of test items to which an LLM can respond\neither correctly or incorrectly). However, high correlations within and between\nbenchmark scores suggest that (1) there exists a small set of common underlying\nabilities that these benchmarks measure, and (2) items tap into redundant\ninformation and the benchmarks may thus be considerably compressed. We use data\nfrom n > 5000 LLMs to identify the most informative items of six benchmarks,\nARC, GSM8K, HellaSwag, MMLU, TruthfulQA and WinoGrande (with d = 28,632 items\nin total). From them we distill a sparse benchmark, metabench, that has less\nthan 3% of the original size of all six benchmarks combined. This new sparse\nbenchmark goes beyond point scores by yielding estimators of the underlying\nbenchmark-specific abilities. We show that these estimators (1) can be used to\nreconstruct each original individual benchmark score with, on average, 1.24%\nroot mean square error (RMSE), (2) reconstruct the original total score with\n0.58% RMSE, and (3) have a single underlying common factor whose Spearman\ncorrelation with the total score is r = 0.94.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) vary in their abilities on a range of tasks.\nInitiatives such as the Open LLM Leaderboard aim to quantify these differences\nwith several large benchmarks (sets of test items to which an LLM can respond\neither correctly or incorrectly). However, high correlations within and between\nbenchmark scores suggest that (1) there exists a small set of common underlying\nabilities that these benchmarks measure, and (2) items tap into redundant\ninformation and the benchmarks may thus be considerably compressed. We use data\nfrom n > 5000 LLMs to identify the most informative items of six benchmarks,\nARC, GSM8K, HellaSwag, MMLU, TruthfulQA and WinoGrande (with d = 28,632 items\nin total). From them we distill a sparse benchmark, metabench, that has less\nthan 3% of the original size of all six benchmarks combined. This new sparse\nbenchmark goes beyond point scores by yielding estimators of the underlying\nbenchmark-specific abilities. We show that these estimators (1) can be used to\nreconstruct each original individual benchmark score with, on average, 1.24%\nroot mean square error (RMSE), (2) reconstruct the original total score with\n0.58% RMSE, and (3) have a single underlying common factor whose Spearman\ncorrelation with the total score is r = 0.94."
                },
                "authors": [
                    {
                        "name": "Alex Kipnis"
                    },
                    {
                        "name": "Konstantinos Voudouris"
                    },
                    {
                        "name": "Luca M. Schulze Buschoff"
                    },
                    {
                        "name": "Eric Schulz"
                    }
                ],
                "author_detail": {
                    "name": "Eric Schulz"
                },
                "author": "Eric Schulz",
                "arxiv_comment": "accepted for publication at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14693v1",
                "updated": "2025-02-20T16:19:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    19,
                    9,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:19:09Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    19,
                    9,
                    3,
                    51,
                    0
                ],
                "title": "I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree\n  Search"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown remarkable\npotential in automating machine learning tasks. However, existing LLM-based\nagents often struggle with low-diversity and suboptimal code generation. While\nrecent work has introduced Monte Carlo Tree Search (MCTS) to address these\nissues, limitations persist in the quality and diversity of thoughts generated,\nas well as in the scalar value feedback mechanisms used for node selection. In\nthis study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a\nnovel approach that iteratively expands tree nodes through an introspective\nprocess that meticulously analyzes solutions and results from parent and\nsibling nodes. This facilitates a continuous refinement of the node in the\nsearch tree, thereby enhancing the overall decision-making process.Furthermore,\nwe integrate a Large Language Model (LLM)-based value model to facilitate\ndirect evaluation of each node's solution prior to conducting comprehensive\ncomputational rollouts. A hybrid rewarding mechanism is implemented to\nseamlessly transition the Q-value from LLM-estimated scores to actual\nperformance scores. This allows higher-quality nodes to be traversed\nearlier.Applied to the various ML tasks, our approach demonstrates a6\\%\nabsolute improvement in performance compared to the strong open-source AutoML\nagents, showcasing its effectiveness in enhancing agentic AutoML systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown remarkable\npotential in automating machine learning tasks. However, existing LLM-based\nagents often struggle with low-diversity and suboptimal code generation. While\nrecent work has introduced Monte Carlo Tree Search (MCTS) to address these\nissues, limitations persist in the quality and diversity of thoughts generated,\nas well as in the scalar value feedback mechanisms used for node selection. In\nthis study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a\nnovel approach that iteratively expands tree nodes through an introspective\nprocess that meticulously analyzes solutions and results from parent and\nsibling nodes. This facilitates a continuous refinement of the node in the\nsearch tree, thereby enhancing the overall decision-making process.Furthermore,\nwe integrate a Large Language Model (LLM)-based value model to facilitate\ndirect evaluation of each node's solution prior to conducting comprehensive\ncomputational rollouts. A hybrid rewarding mechanism is implemented to\nseamlessly transition the Q-value from LLM-estimated scores to actual\nperformance scores. This allows higher-quality nodes to be traversed\nearlier.Applied to the various ML tasks, our approach demonstrates a6\\%\nabsolute improvement in performance compared to the strong open-source AutoML\nagents, showcasing its effectiveness in enhancing agentic AutoML systems."
                },
                "authors": [
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Feng Wei"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Yuxi Qian"
                    },
                    {
                        "name": "Xinhui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xinhui Wu"
                },
                "author": "Xinhui Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14689v1",
                "updated": "2025-02-20T16:16:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    16,
                    34,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:16:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    16,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "Confidence Estimation via Sequential Likelihood Mixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Estimation via Sequential Likelihood Mixing"
                },
                "summary": "We present a universal framework for constructing confidence sets based on\nsequential likelihood mixing. Building upon classical results from sequential\nanalysis, we provide a unifying perspective on several recent lines of work,\nand establish fundamental connections between sequential mixing, Bayesian\ninference and regret inequalities from online estimation. The framework applies\nto any realizable family of likelihood functions and allows for non-i.i.d. data\nand anytime validity. Moreover, the framework seamlessly integrates standard\napproximate inference techniques, such as variational inference and\nsampling-based methods, and extends to misspecified model classes, while\npreserving provable coverage guarantees. We illustrate the power of the\nframework by deriving tighter confidence sequences for classical settings,\nincluding sequential linear regression and sparse estimation, with simplified\nproofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a universal framework for constructing confidence sets based on\nsequential likelihood mixing. Building upon classical results from sequential\nanalysis, we provide a unifying perspective on several recent lines of work,\nand establish fundamental connections between sequential mixing, Bayesian\ninference and regret inequalities from online estimation. The framework applies\nto any realizable family of likelihood functions and allows for non-i.i.d. data\nand anytime validity. Moreover, the framework seamlessly integrates standard\napproximate inference techniques, such as variational inference and\nsampling-based methods, and extends to misspecified model classes, while\npreserving provable coverage guarantees. We illustrate the power of the\nframework by deriving tighter confidence sequences for classical settings,\nincluding sequential linear regression and sparse estimation, with simplified\nproofs."
                },
                "authors": [
                    {
                        "name": "Johannes Kirschner"
                    },
                    {
                        "name": "Andreas Krause"
                    },
                    {
                        "name": "Michele Meziu"
                    },
                    {
                        "name": "Mojmir Mutny"
                    }
                ],
                "author_detail": {
                    "name": "Mojmir Mutny"
                },
                "author": "Mojmir Mutny",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14682v1",
                "updated": "2025-02-20T16:11:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    11,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:11:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    11,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Bridging the Gap: Transforming Natural Language Questions into SQL\n  Queries via Abstract Query Pattern and Contextual Schema Markup",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Gap: Transforming Natural Language Questions into SQL\n  Queries via Abstract Query Pattern and Contextual Schema Markup"
                },
                "summary": "Large language models have demonstrated excellent performance in many tasks,\nincluding Text-to-SQL, due to their powerful in-context learning capabilities.\nThey are becoming the mainstream approach for Text-to-SQL. However, these\nmethods still have a significant gap compared to human performance, especially\non complex questions. As the complexity of questions increases, the gap between\nquestions and SQLs increases. We identify two important gaps: the structural\nmapping gap and the lexical mapping gap. To tackle these two gaps, we propose\nPAS-SQL, an efficient SQL generation pipeline based on LLMs, which alleviates\ngaps through Abstract Query Pattern (AQP) and Contextual Schema Markup (CSM).\nAQP aims to obtain the structural pattern of the question by removing\ndatabase-related information, which enables us to find structurally similar\ndemonstrations. CSM aims to associate database-related text span in the\nquestion with specific tables or columns in the database, which alleviates the\nlexical mapping gap. Experimental results on the Spider and BIRD datasets\ndemonstrate the effectiveness of our proposed method. Specifically, PAS-SQL +\nGPT-4o sets a new state-of-the-art on the Spider benchmark with an execution\naccuracy of 87.9\\%, and achieves leading results on the BIRD dataset with an\nexecution accuracy of 64.67\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated excellent performance in many tasks,\nincluding Text-to-SQL, due to their powerful in-context learning capabilities.\nThey are becoming the mainstream approach for Text-to-SQL. However, these\nmethods still have a significant gap compared to human performance, especially\non complex questions. As the complexity of questions increases, the gap between\nquestions and SQLs increases. We identify two important gaps: the structural\nmapping gap and the lexical mapping gap. To tackle these two gaps, we propose\nPAS-SQL, an efficient SQL generation pipeline based on LLMs, which alleviates\ngaps through Abstract Query Pattern (AQP) and Contextual Schema Markup (CSM).\nAQP aims to obtain the structural pattern of the question by removing\ndatabase-related information, which enables us to find structurally similar\ndemonstrations. CSM aims to associate database-related text span in the\nquestion with specific tables or columns in the database, which alleviates the\nlexical mapping gap. Experimental results on the Spider and BIRD datasets\ndemonstrate the effectiveness of our proposed method. Specifically, PAS-SQL +\nGPT-4o sets a new state-of-the-art on the Spider benchmark with an execution\naccuracy of 87.9\\%, and achieves leading results on the BIRD dataset with an\nexecution accuracy of 64.67\\%."
                },
                "authors": [
                    {
                        "name": "Yonghui Kong"
                    },
                    {
                        "name": "Hongbing Hu"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Siyuan Chai"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14678v1",
                "updated": "2025-02-20T16:09:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    9,
                    55,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:09:55Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    9,
                    55,
                    3,
                    51,
                    0
                ],
                "title": "How to Get Your LLM to Generate Challenging Problems for Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Get Your LLM to Generate Challenging Problems for Evaluation"
                },
                "summary": "The pace of evolution of Large Language Models (LLMs) necessitates new\napproaches for rigorous and comprehensive evaluation. Traditional human\nannotation is increasingly impracticable due to the complexities and costs\ninvolved in generating high-quality, challenging problems. In this work, we\nintroduce CHASE, a unified framework to synthetically generate challenging\nproblems using LLMs without human involvement. For a given task, our approach\nbuilds a hard problem in a bottom-up manner from simpler components. Moreover,\nour framework decomposes the generation process into independently verifiable\nsub-tasks, thereby ensuring a high level of quality and correctness. We\nimplement CHASE to create evaluation benchmarks across three diverse domains:\n(1) document-based question answering, (2) repository-level code completion,\nand (3) math reasoning. The performance of state-of-the-art LLMs on these\nsynthetic benchmarks lies in the range of 40-60% accuracy, thereby\ndemonstrating the effectiveness of our framework at generating challenging\nproblems. We publicly release our benchmarks and code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pace of evolution of Large Language Models (LLMs) necessitates new\napproaches for rigorous and comprehensive evaluation. Traditional human\nannotation is increasingly impracticable due to the complexities and costs\ninvolved in generating high-quality, challenging problems. In this work, we\nintroduce CHASE, a unified framework to synthetically generate challenging\nproblems using LLMs without human involvement. For a given task, our approach\nbuilds a hard problem in a bottom-up manner from simpler components. Moreover,\nour framework decomposes the generation process into independently verifiable\nsub-tasks, thereby ensuring a high level of quality and correctness. We\nimplement CHASE to create evaluation benchmarks across three diverse domains:\n(1) document-based question answering, (2) repository-level code completion,\nand (3) math reasoning. The performance of state-of-the-art LLMs on these\nsynthetic benchmarks lies in the range of 40-60% accuracy, thereby\ndemonstrating the effectiveness of our framework at generating challenging\nproblems. We publicly release our benchmarks and code."
                },
                "authors": [
                    {
                        "name": "Arkil Patel"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Dzmitry Bahdanau"
                    }
                ],
                "author_detail": {
                    "name": "Dzmitry Bahdanau"
                },
                "author": "Dzmitry Bahdanau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14677v1",
                "updated": "2025-02-20T16:09:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    9,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:09:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    9,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Data-Constrained Synthesis of Training Data for De-Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Constrained Synthesis of Training Data for De-Identification"
                },
                "summary": "Many sensitive domains -- such as the clinical domain -- lack widely\navailable datasets due to privacy risks. The increasing generative capabilities\nof large language models (LLMs) have made synthetic datasets a viable path\nforward. In this study, we domain-adapt LLMs to the clinical domain and\ngenerate synthetic clinical texts that are machine-annotated with tags for\npersonally identifiable information using capable encoder-based NER models. The\nsynthetic corpora are then used to train synthetic NER models. The results show\nthat training NER models using synthetic corpora incurs only a small drop in\npredictive performance. The limits of this process are investigated in a\nsystematic ablation study -- using both Swedish and Spanish data. Our analysis\nshows that smaller datasets can be sufficient for domain-adapting LLMs for data\nsynthesis. Instead, the effectiveness of this process is almost entirely\ncontingent on the performance of the machine-annotating NER models trained\nusing the original data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many sensitive domains -- such as the clinical domain -- lack widely\navailable datasets due to privacy risks. The increasing generative capabilities\nof large language models (LLMs) have made synthetic datasets a viable path\nforward. In this study, we domain-adapt LLMs to the clinical domain and\ngenerate synthetic clinical texts that are machine-annotated with tags for\npersonally identifiable information using capable encoder-based NER models. The\nsynthetic corpora are then used to train synthetic NER models. The results show\nthat training NER models using synthetic corpora incurs only a small drop in\npredictive performance. The limits of this process are investigated in a\nsystematic ablation study -- using both Swedish and Spanish data. Our analysis\nshows that smaller datasets can be sufficient for domain-adapting LLMs for data\nsynthesis. Instead, the effectiveness of this process is almost entirely\ncontingent on the performance of the machine-annotating NER models trained\nusing the original data."
                },
                "authors": [
                    {
                        "name": "Thomas Vakili"
                    },
                    {
                        "name": "Aron Henriksson"
                    },
                    {
                        "name": "Hercules Dalianis"
                    }
                ],
                "author_detail": {
                    "name": "Hercules Dalianis"
                },
                "author": "Hercules Dalianis",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14671v1",
                "updated": "2025-02-20T16:05:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    5,
                    45,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:05:45Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    5,
                    45,
                    3,
                    51,
                    0
                ],
                "title": "Explanations of Deep Language Models Explain Language Representations in\n  the Brain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanations of Deep Language Models Explain Language Representations in\n  the Brain"
                },
                "summary": "Recent advances in artificial intelligence have given rise to large language\nmodels (LLMs) that not only achieve human-like performance but also share\ncomputational principles with the brain's language processing mechanisms. While\nprevious research has primarily focused on aligning LLMs' internal\nrepresentations with neural activity, we introduce a novel approach that\nleverages explainable AI (XAI) methods to forge deeper connections between the\ntwo domains. Using attribution methods, we quantified how preceding words\ncontribute to an LLM's next-word predictions and employed these explanations to\npredict fMRI recordings from participants listening to the same narratives. Our\nfindings demonstrate that attribution methods robustly predict brain activity\nacross the language network, surpassing traditional internal representations in\nearly language areas. This alignment is hierarchical: early-layer explanations\ncorrespond to the initial stages of language processing in the brain, while\nlater layers align with more advanced stages. Moreover, the layers more\ninfluential on LLM next-word prediction$\\unicode{x2014}$those with higher\nattribution scores$\\unicode{x2014}$exhibited stronger alignment with neural\nactivity. This work establishes a bidirectional bridge between AI and\nneuroscience. First, we demonstrate that attribution methods offer a powerful\nlens for investigating the neural mechanisms of language comprehension,\nrevealing how meaning emerges from preceding context. Second, we propose using\nbrain alignment as a metric to evaluate the validity of attribution methods,\nproviding a framework for assessing their biological plausibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in artificial intelligence have given rise to large language\nmodels (LLMs) that not only achieve human-like performance but also share\ncomputational principles with the brain's language processing mechanisms. While\nprevious research has primarily focused on aligning LLMs' internal\nrepresentations with neural activity, we introduce a novel approach that\nleverages explainable AI (XAI) methods to forge deeper connections between the\ntwo domains. Using attribution methods, we quantified how preceding words\ncontribute to an LLM's next-word predictions and employed these explanations to\npredict fMRI recordings from participants listening to the same narratives. Our\nfindings demonstrate that attribution methods robustly predict brain activity\nacross the language network, surpassing traditional internal representations in\nearly language areas. This alignment is hierarchical: early-layer explanations\ncorrespond to the initial stages of language processing in the brain, while\nlater layers align with more advanced stages. Moreover, the layers more\ninfluential on LLM next-word prediction$\\unicode{x2014}$those with higher\nattribution scores$\\unicode{x2014}$exhibited stronger alignment with neural\nactivity. This work establishes a bidirectional bridge between AI and\nneuroscience. First, we demonstrate that attribution methods offer a powerful\nlens for investigating the neural mechanisms of language comprehension,\nrevealing how meaning emerges from preceding context. Second, we propose using\nbrain alignment as a metric to evaluate the validity of attribution methods,\nproviding a framework for assessing their biological plausibility."
                },
                "authors": [
                    {
                        "name": "Maryam Rahimi"
                    },
                    {
                        "name": "Yadollah Yaghoobzadeh"
                    },
                    {
                        "name": "Mohammad Reza Daliri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Reza Daliri"
                },
                "arxiv_affiliation": "School of Cognitive Sciences, Institute for Research in Fundamental Sciences, Tehran, Iran",
                "author": "Mohammad Reza Daliri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14669v1",
                "updated": "2025-02-20T16:05:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    5,
                    18,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:05:18Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    5,
                    18,
                    3,
                    51,
                    0
                ],
                "title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via\n  GRPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via\n  GRPO"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nlanguage processing, yet they often struggle with tasks requiring genuine\nvisual spatial reasoning. In this paper, we introduce a novel two-stage\ntraining framework designed to equip standard LLMs with visual reasoning\nabilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)\non a curated dataset of tokenized maze representations to teach the model to\npredict step-by-step movement commands. Next, we apply Group Relative Policy\nOptimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted\nreward function to refine the model's sequential decision-making and encourage\nemergent chain-of-thought behaviors. Experimental results on synthetically\ngenerated mazes show that while a baseline model fails to navigate the maze,\nthe SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning\nboosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more\nrobust and self-corrective reasoning, highlighting the potential of our\napproach to bridge the gap between language models and visual spatial tasks.\nThese findings offer promising implications for applications in robotics,\nautonomous navigation, and other domains that require integrated visual and\nsequential reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nlanguage processing, yet they often struggle with tasks requiring genuine\nvisual spatial reasoning. In this paper, we introduce a novel two-stage\ntraining framework designed to equip standard LLMs with visual reasoning\nabilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)\non a curated dataset of tokenized maze representations to teach the model to\npredict step-by-step movement commands. Next, we apply Group Relative Policy\nOptimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted\nreward function to refine the model's sequential decision-making and encourage\nemergent chain-of-thought behaviors. Experimental results on synthetically\ngenerated mazes show that while a baseline model fails to navigate the maze,\nthe SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning\nboosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more\nrobust and self-corrective reasoning, highlighting the potential of our\napproach to bridge the gap between language models and visual spatial tasks.\nThese findings offer promising implications for applications in robotics,\nautonomous navigation, and other domains that require integrated visual and\nsequential reasoning."
                },
                "authors": [
                    {
                        "name": "Alan Dao"
                    },
                    {
                        "name": "Dinh Bach Vu"
                    }
                ],
                "author_detail": {
                    "name": "Dinh Bach Vu"
                },
                "arxiv_affiliation": "Gia Tuan Dao",
                "author": "Dinh Bach Vu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v3",
                "updated": "2025-02-20T16:01:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    1,
                    34,
                    3,
                    51,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14662v1",
                "updated": "2025-02-20T15:58:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    58,
                    25,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:58:25Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    58,
                    25,
                    3,
                    51,
                    0
                ],
                "title": "InstructAgent: Building User Controllable Recommender via LLM Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructAgent: Building User Controllable Recommender via LLM Agent"
                },
                "summary": "Traditional recommender systems usually take the user-platform paradigm,\nwhere users are directly exposed under the control of the platform's\nrecommendation algorithms. However, the defect of recommendation algorithms may\nput users in very vulnerable positions under this paradigm. First, many\nsophisticated models are often designed with commercial objectives in mind,\nfocusing on the platform's benefits, which may hinder their ability to protect\nand capture users' true interests. Second, these models are typically optimized\nusing data from all users, which may overlook individual user's preferences.\nDue to these shortcomings, users may experience several disadvantages under the\ntraditional user-platform direct exposure paradigm, such as lack of control\nover the recommender system, potential manipulation by the platform, echo\nchamber effects, or lack of personalization for less active users due to the\ndominance of active users during collaborative learning. Therefore, there is an\nurgent need to develop a new paradigm to protect user interests and alleviate\nthese issues. Recently, some researchers have introduced LLM agents to simulate\nuser behaviors, these approaches primarily aim to optimize platform-side\nperformance, leaving core issues in recommender systems unresolved. To address\nthese limitations, we propose a new user-agent-platform paradigm, where agent\nserves as the protective shield between user and recommender system that\nenables indirect exposure. To this end, we first construct four recommendation\ndatasets, denoted as $\\dataset$, along with user instructions for each record.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional recommender systems usually take the user-platform paradigm,\nwhere users are directly exposed under the control of the platform's\nrecommendation algorithms. However, the defect of recommendation algorithms may\nput users in very vulnerable positions under this paradigm. First, many\nsophisticated models are often designed with commercial objectives in mind,\nfocusing on the platform's benefits, which may hinder their ability to protect\nand capture users' true interests. Second, these models are typically optimized\nusing data from all users, which may overlook individual user's preferences.\nDue to these shortcomings, users may experience several disadvantages under the\ntraditional user-platform direct exposure paradigm, such as lack of control\nover the recommender system, potential manipulation by the platform, echo\nchamber effects, or lack of personalization for less active users due to the\ndominance of active users during collaborative learning. Therefore, there is an\nurgent need to develop a new paradigm to protect user interests and alleviate\nthese issues. Recently, some researchers have introduced LLM agents to simulate\nuser behaviors, these approaches primarily aim to optimize platform-side\nperformance, leaving core issues in recommender systems unresolved. To address\nthese limitations, we propose a new user-agent-platform paradigm, where agent\nserves as the protective shield between user and recommender system that\nenables indirect exposure. To this end, we first construct four recommendation\ndatasets, denoted as $\\dataset$, along with user instructions for each record."
                },
                "authors": [
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Xuying Ning"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Min Xu"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "WWW2025@HCRS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14660v1",
                "updated": "2025-02-20T15:55:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    55,
                    52,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:55:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    55,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "Beyond the Surface: Uncovering Implicit Locations with LLMs for\n  Personalized Local News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Surface: Uncovering Implicit Locations with LLMs for\n  Personalized Local News"
                },
                "summary": "News recommendation systems personalize homepage content to boost engagement,\nbut factors like content type, editorial stance, and geographic focus impact\nrecommendations. Local newspapers balance coverage across regions, yet\nidentifying local articles is challenging due to implicit location cues like\nslang or landmarks.\n  Traditional methods, such as Named Entity Recognition (NER) and Knowledge\nGraphs, infer locations, but Large Language Models (LLMs) offer new\npossibilities while raising concerns about accuracy and explainability.\n  This paper explores LLMs for local article classification in Taboola's\n\"Homepage For You\" system, comparing them to traditional techniques. Key\nfindings: (1) Knowledge Graphs enhance NER models' ability to detect implicit\nlocations, (2) LLMs outperform traditional methods, and (3) LLMs can\neffectively identify local content without requiring Knowledge Graph\nintegration.\n  Offline evaluations showed LLMs excel at implicit location classification,\nwhile online A/B tests showed a significant increased in local views. A\nscalable pipeline integrating LLM-based location classification boosted local\narticle distribution by 27%, preserving newspapers' brand identity and\nenhancing homepage personalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News recommendation systems personalize homepage content to boost engagement,\nbut factors like content type, editorial stance, and geographic focus impact\nrecommendations. Local newspapers balance coverage across regions, yet\nidentifying local articles is challenging due to implicit location cues like\nslang or landmarks.\n  Traditional methods, such as Named Entity Recognition (NER) and Knowledge\nGraphs, infer locations, but Large Language Models (LLMs) offer new\npossibilities while raising concerns about accuracy and explainability.\n  This paper explores LLMs for local article classification in Taboola's\n\"Homepage For You\" system, comparing them to traditional techniques. Key\nfindings: (1) Knowledge Graphs enhance NER models' ability to detect implicit\nlocations, (2) LLMs outperform traditional methods, and (3) LLMs can\neffectively identify local content without requiring Knowledge Graph\nintegration.\n  Offline evaluations showed LLMs excel at implicit location classification,\nwhile online A/B tests showed a significant increased in local views. A\nscalable pipeline integrating LLM-based location classification boosted local\narticle distribution by 27%, preserving newspapers' brand identity and\nenhancing homepage personalization."
                },
                "authors": [
                    {
                        "name": "Gali Katz"
                    },
                    {
                        "name": "Hai Sitton"
                    },
                    {
                        "name": "Guy Gonen"
                    },
                    {
                        "name": "Yohay Kaplan"
                    }
                ],
                "author_detail": {
                    "name": "Yohay Kaplan"
                },
                "author": "Yohay Kaplan",
                "arxiv_comment": "10 pages, 2 figures, submitted to kdd",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18935v2",
                "updated": "2025-02-20T15:54:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    54,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-01-31T07:40:34Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    7,
                    40,
                    34,
                    4,
                    31,
                    0
                ],
                "title": "TabFSBench: Tabular Benchmark for Feature Shifts in Open Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabFSBench: Tabular Benchmark for Feature Shifts in Open Environment"
                },
                "summary": "Tabular data is widely utilized in various machine learning tasks. Current\ntabular learning research predominantly focuses on closed environments, while\nin real-world applications, open environments are often encountered, where\ndistribution and feature shifts occur, leading to significant degradation in\nmodel performance. Previous research has primarily concentrated on mitigating\ndistribution shifts, whereas feature shifts, a distinctive and unexplored\nchallenge of tabular data, have garnered limited attention. To this end, this\npaper conducts the first comprehensive study on feature shifts in tabular data\nand introduces the first tabular feature-shift benchmark (TabFSBench).\nTabFSBench evaluates impacts of four distinct feature-shift scenarios on four\ntabular model categories across various datasets and assesses the performance\nof large language models (LLMs) and tabular LLMs in the tabular benchmark for\nthe first time. Our study demonstrates three main observations: (1) most\ntabular models have the limited applicability in feature-shift scenarios; (2)\nthe shifted feature set importance has a linear relationship with model\nperformance degradation; (3) model performance in closed environments\ncorrelates with feature-shift performance. Future research direction is also\nexplored for each observation. TabFSBench is released for public access by\nusing a few lines of Python codes at https://github.com/LAMDASZ-ML/TabFSBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data is widely utilized in various machine learning tasks. Current\ntabular learning research predominantly focuses on closed environments, while\nin real-world applications, open environments are often encountered, where\ndistribution and feature shifts occur, leading to significant degradation in\nmodel performance. Previous research has primarily concentrated on mitigating\ndistribution shifts, whereas feature shifts, a distinctive and unexplored\nchallenge of tabular data, have garnered limited attention. To this end, this\npaper conducts the first comprehensive study on feature shifts in tabular data\nand introduces the first tabular feature-shift benchmark (TabFSBench).\nTabFSBench evaluates impacts of four distinct feature-shift scenarios on four\ntabular model categories across various datasets and assesses the performance\nof large language models (LLMs) and tabular LLMs in the tabular benchmark for\nthe first time. Our study demonstrates three main observations: (1) most\ntabular models have the limited applicability in feature-shift scenarios; (2)\nthe shifted feature set importance has a linear relationship with model\nperformance degradation; (3) model performance in closed environments\ncorrelates with feature-shift performance. Future research direction is also\nexplored for each observation. TabFSBench is released for public access by\nusing a few lines of Python codes at https://github.com/LAMDASZ-ML/TabFSBench."
                },
                "authors": [
                    {
                        "name": "Zi-Jian Cheng"
                    },
                    {
                        "name": "Zi-Yi Jia"
                    },
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Yu-Feng Li"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    }
                ],
                "author_detail": {
                    "name": "Lan-Zhe Guo"
                },
                "author": "Lan-Zhe Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14645v1",
                "updated": "2025-02-20T15:32:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    32,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:32:31Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    32,
                    31,
                    3,
                    51,
                    0
                ],
                "title": "Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual\n  Knowledge Synchronization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual\n  Knowledge Synchronization in LLMs"
                },
                "summary": "Knowledge editing allows for efficient adaptation of large language models\n(LLMs) to new information or corrections without requiring full retraining.\nHowever, prior methods typically focus on either single-language editing or\nbasic multilingual editing, failing to achieve true cross-linguistic knowledge\nsynchronization. To address this, we present a simple and practical\nstate-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE),\ndesigned to propagate knowledge from a dominant language to other languages\neffectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition\nInstruction Tuning (XE-IT), which fine-tunes the model on a curated parallel\ndataset to modify in-scope knowledge while preserving unrelated information,\nand (ii) Target-language Preference Optimization (TL-PO), which applies\nadvanced optimization techniques to ensure consistency across languages,\nfostering the transfer of updates. Additionally, we contribute a high-quality,\ncross-lingual dataset, specifically designed to enhance knowledge transfer\nacross languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks\nshow that X-KDE significantly enhances cross-lingual performance, achieving an\naverage improvement of +8.19%, while maintaining high accuracy in monolingual\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing allows for efficient adaptation of large language models\n(LLMs) to new information or corrections without requiring full retraining.\nHowever, prior methods typically focus on either single-language editing or\nbasic multilingual editing, failing to achieve true cross-linguistic knowledge\nsynchronization. To address this, we present a simple and practical\nstate-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE),\ndesigned to propagate knowledge from a dominant language to other languages\neffectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition\nInstruction Tuning (XE-IT), which fine-tunes the model on a curated parallel\ndataset to modify in-scope knowledge while preserving unrelated information,\nand (ii) Target-language Preference Optimization (TL-PO), which applies\nadvanced optimization techniques to ensure consistency across languages,\nfostering the transfer of updates. Additionally, we contribute a high-quality,\ncross-lingual dataset, specifically designed to enhance knowledge transfer\nacross languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks\nshow that X-KDE significantly enhances cross-lingual performance, achieving an\naverage improvement of +8.19%, while maintaining high accuracy in monolingual\nsettings."
                },
                "authors": [
                    {
                        "name": "Yuchen Wu"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14644v1",
                "updated": "2025-02-20T15:32:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    32,
                    24,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:32:24Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    32,
                    24,
                    3,
                    51,
                    0
                ],
                "title": "LIFT: Improving Long Context Understanding of Large Language Models\n  through Long Input Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIFT: Improving Long Context Understanding of Large Language Models\n  through Long Input Fine-Tuning"
                },
                "summary": "Long context understanding remains challenging for large language models due\nto their limited context windows. This paper presents Long Input Fine-Tuning\n(LIFT), a novel framework for long-context modeling that can improve the\nlong-context performance of arbitrary (short-context) LLMs by dynamically\nadapting model parameters based on the long input. Importantly, LIFT, rather\nthan endlessly extending the context window size to accommodate increasingly\nlonger inputs in context, chooses to store and absorb the long input in\nparameter. By fine-tuning the long input into model parameters, LIFT allows\nshort-context LLMs to answer questions even when the required information is\nnot provided in the context during inference. Furthermore, to enhance LIFT\nperformance while maintaining the original in-context learning (ICL)\ncapabilities, we introduce Gated Memory, a specialized attention adapter that\nautomatically balances long input memorization and ICL. We provide a\ncomprehensive analysis of the strengths and limitations of LIFT on long context\nunderstanding, offering valuable directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context understanding remains challenging for large language models due\nto their limited context windows. This paper presents Long Input Fine-Tuning\n(LIFT), a novel framework for long-context modeling that can improve the\nlong-context performance of arbitrary (short-context) LLMs by dynamically\nadapting model parameters based on the long input. Importantly, LIFT, rather\nthan endlessly extending the context window size to accommodate increasingly\nlonger inputs in context, chooses to store and absorb the long input in\nparameter. By fine-tuning the long input into model parameters, LIFT allows\nshort-context LLMs to answer questions even when the required information is\nnot provided in the context during inference. Furthermore, to enhance LIFT\nperformance while maintaining the original in-context learning (ICL)\ncapabilities, we introduce Gated Memory, a specialized attention adapter that\nautomatically balances long input memorization and ICL. We provide a\ncomprehensive analysis of the strengths and limitations of LIFT on long context\nunderstanding, offering valuable directions for future research."
                },
                "authors": [
                    {
                        "name": "Yansheng Mao"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Haotong Yang"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Xiyuan Wang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2412.13626",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14643v1",
                "updated": "2025-02-20T15:30:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    30,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:30:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    30,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Length-Controlled Margin-Based Preference Optimization without Reference\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Length-Controlled Margin-Based Preference Optimization without Reference\n  Model"
                },
                "summary": "Direct Preference Optimization (DPO) is a widely adopted offline algorithm\nfor preference-based reinforcement learning from human feedback (RLHF),\ndesigned to improve training simplicity and stability by redefining reward\nfunctions. However, DPO is hindered by several limitations, including length\nbias, memory inefficiency, and probability degradation. To address these\nchallenges, we propose Length-Controlled Margin-Based Preference Optimization\n(LMPO), a more efficient and robust alternative. LMPO introduces a uniform\nreference model as an upper bound for the DPO loss, enabling a more accurate\napproximation of the original optimization objective. Additionally, an average\nlog-probability optimization strategy is employed to minimize discrepancies\nbetween training and inference phases. A key innovation of LMPO lies in its\nLength-Controlled Margin-Based loss function, integrated within the\nBradley-Terry framework. This loss function regulates response length while\nsimultaneously widening the margin between preferred and rejected outputs. By\ndoing so, it mitigates probability degradation for both accepted and discarded\nresponses, addressing a significant limitation of existing methods. We evaluate\nLMPO against state-of-the-art preference optimization techniques on two\nopen-ended large language models, Mistral and LLaMA3, across six conditional\nbenchmarks. Our experimental results demonstrate that LMPO effectively controls\nresponse length, reduces probability degradation, and outperforms existing\napproaches. The code is available at \\url{https://github.com/gengxuli/LMPO}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) is a widely adopted offline algorithm\nfor preference-based reinforcement learning from human feedback (RLHF),\ndesigned to improve training simplicity and stability by redefining reward\nfunctions. However, DPO is hindered by several limitations, including length\nbias, memory inefficiency, and probability degradation. To address these\nchallenges, we propose Length-Controlled Margin-Based Preference Optimization\n(LMPO), a more efficient and robust alternative. LMPO introduces a uniform\nreference model as an upper bound for the DPO loss, enabling a more accurate\napproximation of the original optimization objective. Additionally, an average\nlog-probability optimization strategy is employed to minimize discrepancies\nbetween training and inference phases. A key innovation of LMPO lies in its\nLength-Controlled Margin-Based loss function, integrated within the\nBradley-Terry framework. This loss function regulates response length while\nsimultaneously widening the margin between preferred and rejected outputs. By\ndoing so, it mitigates probability degradation for both accepted and discarded\nresponses, addressing a significant limitation of existing methods. We evaluate\nLMPO against state-of-the-art preference optimization techniques on two\nopen-ended large language models, Mistral and LLaMA3, across six conditional\nbenchmarks. Our experimental results demonstrate that LMPO effectively controls\nresponse length, reduces probability degradation, and outperforms existing\napproaches. The code is available at \\url{https://github.com/gengxuli/LMPO}."
                },
                "authors": [
                    {
                        "name": "Gengxu Li"
                    },
                    {
                        "name": "Tingyu Xia"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14642v1",
                "updated": "2025-02-20T15:29:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    29,
                    32,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:29:32Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    29,
                    32,
                    3,
                    51,
                    0
                ],
                "title": "How Far are LLMs from Being Our Digital Twins? A Benchmark for\n  Persona-Based Behavior Chain Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far are LLMs from Being Our Digital Twins? A Benchmark for\n  Persona-Based Behavior Chain Simulation"
                },
                "summary": "Recently, LLMs have garnered increasing attention across academic disciplines\nfor their potential as human digital twins, virtual proxies designed to\nreplicate individuals and autonomously perform tasks such as decision-making,\nproblem-solving, and reasoning on their behalf. However, current evaluations of\nLLMs primarily emphasize dialogue simulation while overlooking human behavior\nsimulation, which is crucial for digital twins. To address this gap, we\nintroduce BehaviorChain, the first benchmark for evaluating LLMs' ability to\nsimulate continuous human behavior. BehaviorChain comprises diverse,\nhigh-quality, persona-based behavior chains, totaling 15,846 distinct behaviors\nacross 1,001 unique personas, each with detailed history and profile metadata.\nFor evaluation, we integrate persona metadata into LLMs and employ them to\niteratively infer contextually appropriate behaviors within dynamic scenarios\nprovided by BehaviorChain. Comprehensive evaluation results demonstrated that\neven state-of-the-art models struggle with accurately simulating continuous\nhuman behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, LLMs have garnered increasing attention across academic disciplines\nfor their potential as human digital twins, virtual proxies designed to\nreplicate individuals and autonomously perform tasks such as decision-making,\nproblem-solving, and reasoning on their behalf. However, current evaluations of\nLLMs primarily emphasize dialogue simulation while overlooking human behavior\nsimulation, which is crucial for digital twins. To address this gap, we\nintroduce BehaviorChain, the first benchmark for evaluating LLMs' ability to\nsimulate continuous human behavior. BehaviorChain comprises diverse,\nhigh-quality, persona-based behavior chains, totaling 15,846 distinct behaviors\nacross 1,001 unique personas, each with detailed history and profile metadata.\nFor evaluation, we integrate persona metadata into LLMs and employ them to\niteratively infer contextually appropriate behaviors within dynamic scenarios\nprovided by BehaviorChain. Comprehensive evaluation results demonstrated that\neven state-of-the-art models struggle with accurately simulating continuous\nhuman behavior."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Heming Xia"
                    },
                    {
                        "name": "Xinfeng Yuan"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00883v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00883v4",
                "updated": "2025-02-20T15:26:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    26,
                    44,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-02T19:25:41Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    19,
                    25,
                    41,
                    6,
                    33,
                    0
                ],
                "title": "SimPER: A Minimalist Approach to Preference Alignment without\n  Hyperparameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimPER: A Minimalist Approach to Preference Alignment without\n  Hyperparameters"
                },
                "summary": "Existing preference optimization objectives for language model alignment\nrequire additional hyperparameters that must be extensively tuned to achieve\noptimal performance, increasing both the complexity and time required for\nfine-tuning large language models. In this paper, we propose a simple yet\neffective hyperparameter-free preference optimization algorithm for alignment.\nWe observe that promising performance can be achieved simply by optimizing\ninverse perplexity, which is calculated as the inverse of the exponentiated\naverage log-likelihood of the chosen and rejected responses in the preference\ndataset. The resulting simple learning objective, SimPER, is easy to implement\nand eliminates the need for expensive hyperparameter tuning and a reference\nmodel, making it both computationally and memory efficient. Extensive\nexperiments on widely used real-world benchmarks, including MT-Bench,\nAlpacaEval 2, and 10 key benchmarks of the Open LLM Leaderboard with 5 base\nmodels, demonstrate that SimPER consistently and significantly outperforms\nexisting approaches-even without any hyperparameters or a reference model . For\nexample, despite its simplicity, SimPER outperforms state-of-the-art methods by\nup to 5.7 points on AlpacaEval 2 and achieves the highest average ranking\nacross 10 benchmarks on the Open LLM Leaderboard. The source code for SimPER is\npublicly available at: https://github.com/tengxiao1/SimPER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing preference optimization objectives for language model alignment\nrequire additional hyperparameters that must be extensively tuned to achieve\noptimal performance, increasing both the complexity and time required for\nfine-tuning large language models. In this paper, we propose a simple yet\neffective hyperparameter-free preference optimization algorithm for alignment.\nWe observe that promising performance can be achieved simply by optimizing\ninverse perplexity, which is calculated as the inverse of the exponentiated\naverage log-likelihood of the chosen and rejected responses in the preference\ndataset. The resulting simple learning objective, SimPER, is easy to implement\nand eliminates the need for expensive hyperparameter tuning and a reference\nmodel, making it both computationally and memory efficient. Extensive\nexperiments on widely used real-world benchmarks, including MT-Bench,\nAlpacaEval 2, and 10 key benchmarks of the Open LLM Leaderboard with 5 base\nmodels, demonstrate that SimPER consistently and significantly outperforms\nexisting approaches-even without any hyperparameters or a reference model . For\nexample, despite its simplicity, SimPER outperforms state-of-the-art methods by\nup to 5.7 points on AlpacaEval 2 and achieves the highest average ranking\nacross 10 benchmarks on the Open LLM Leaderboard. The source code for SimPER is\npublicly available at: https://github.com/tengxiao1/SimPER."
                },
                "authors": [
                    {
                        "name": "Teng Xiao"
                    },
                    {
                        "name": "Yige Yuan"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Mingxiao Li"
                    },
                    {
                        "name": "Shangsong Liang"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Vasant G Honavar"
                    }
                ],
                "author_detail": {
                    "name": "Vasant G Honavar"
                },
                "author": "Vasant G Honavar",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00883v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00883v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14637v1",
                "updated": "2025-02-20T15:20:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    20,
                    37,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:20:37Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    20,
                    37,
                    3,
                    51,
                    0
                ],
                "title": "ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality\n  Protein Backbone Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality\n  Protein Backbone Generation"
                },
                "summary": "Protein backbone generation plays a central role in de novo protein design\nand is significant for many biological and medical applications. Although\ndiffusion and flow-based generative models provide potential solutions to this\nchallenging task, they often generate proteins with undesired designability and\nsuffer computational inefficiency. In this study, we propose a novel rectified\nquaternion flow (ReQFlow) matching method for fast and high-quality protein\nbackbone generation. In particular, our method generates a local translation\nand a 3D rotation from random noise for each residue in a protein chain, which\nrepresents each 3D rotation as a unit quaternion and constructs its flow by\nspherical linear interpolation (SLERP) in an exponential format. We train the\nmodel by quaternion flow (QFlow) matching with guaranteed numerical stability\nand rectify the QFlow model to accelerate its inference and improve the\ndesignability of generated protein backbones, leading to the proposed ReQFlow\nmodel. Experiments show that ReQFlow achieves state-of-the-art performance in\nprotein backbone generation while requiring much fewer sampling steps and\nsignificantly less inference time (e.g., being 37x faster than RFDiffusion and\n62x faster than Genie2 when generating a backbone of length 300), demonstrating\nits effectiveness and efficiency. The code is available at\nhttps://github.com/AngxiaoYue/ReQFlow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein backbone generation plays a central role in de novo protein design\nand is significant for many biological and medical applications. Although\ndiffusion and flow-based generative models provide potential solutions to this\nchallenging task, they often generate proteins with undesired designability and\nsuffer computational inefficiency. In this study, we propose a novel rectified\nquaternion flow (ReQFlow) matching method for fast and high-quality protein\nbackbone generation. In particular, our method generates a local translation\nand a 3D rotation from random noise for each residue in a protein chain, which\nrepresents each 3D rotation as a unit quaternion and constructs its flow by\nspherical linear interpolation (SLERP) in an exponential format. We train the\nmodel by quaternion flow (QFlow) matching with guaranteed numerical stability\nand rectify the QFlow model to accelerate its inference and improve the\ndesignability of generated protein backbones, leading to the proposed ReQFlow\nmodel. Experiments show that ReQFlow achieves state-of-the-art performance in\nprotein backbone generation while requiring much fewer sampling steps and\nsignificantly less inference time (e.g., being 37x faster than RFDiffusion and\n62x faster than Genie2 when generating a backbone of length 300), demonstrating\nits effectiveness and efficiency. The code is available at\nhttps://github.com/AngxiaoYue/ReQFlow."
                },
                "authors": [
                    {
                        "name": "Angxiao Yue"
                    },
                    {
                        "name": "Zichong Wang"
                    },
                    {
                        "name": "Hongteng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hongteng Xu"
                },
                "author": "Hongteng Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14634v1",
                "updated": "2025-02-20T15:16:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    16,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:16:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    16,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "CER: Confidence Enhanced Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CER: Confidence Enhanced Reasoning in LLMs"
                },
                "summary": "Ensuring the reliability of Large Language Models (LLMs) in complex reasoning\ntasks remains a formidable challenge, particularly in scenarios that demand\nprecise mathematical calculations and knowledge-intensive open-domain\ngeneration. In this work, we introduce an uncertainty-aware framework designed\nto enhance the accuracy of LLM responses by systematically incorporating model\nconfidence at critical decision points. We propose an approach that encourages\nmulti-step reasoning in LLMs and quantify the confidence of intermediate\nanswers such as numerical results in mathematical reasoning and proper nouns in\nopen-domain generation. Then, the overall confidence of each reasoning chain is\nevaluated based on confidence of these critical intermediate steps. Finally, we\naggregate the answer of generated response paths in a way that reflects the\nreliability of each generated content (as opposed to self-consistency in which\neach generated chain contributes equally to majority voting). We conducted\nextensive experiments in five datasets, three mathematical datasets and two\nopen-domain datasets, using four LLMs. The results consistently validate the\neffectiveness of our novel confidence aggregation method, leading to an\naccuracy improvement of up to 7.4% and 5.8% over baseline approaches in math\nand open-domain generation tasks, respectively. Code is publicly available at\nhttps://github.com/ Aquasar11/CER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the reliability of Large Language Models (LLMs) in complex reasoning\ntasks remains a formidable challenge, particularly in scenarios that demand\nprecise mathematical calculations and knowledge-intensive open-domain\ngeneration. In this work, we introduce an uncertainty-aware framework designed\nto enhance the accuracy of LLM responses by systematically incorporating model\nconfidence at critical decision points. We propose an approach that encourages\nmulti-step reasoning in LLMs and quantify the confidence of intermediate\nanswers such as numerical results in mathematical reasoning and proper nouns in\nopen-domain generation. Then, the overall confidence of each reasoning chain is\nevaluated based on confidence of these critical intermediate steps. Finally, we\naggregate the answer of generated response paths in a way that reflects the\nreliability of each generated content (as opposed to self-consistency in which\neach generated chain contributes equally to majority voting). We conducted\nextensive experiments in five datasets, three mathematical datasets and two\nopen-domain datasets, using four LLMs. The results consistently validate the\neffectiveness of our novel confidence aggregation method, leading to an\naccuracy improvement of up to 7.4% and 5.8% over baseline approaches in math\nand open-domain generation tasks, respectively. Code is publicly available at\nhttps://github.com/ Aquasar11/CER."
                },
                "authors": [
                    {
                        "name": "Ali Razghandi"
                    },
                    {
                        "name": "Seyed Mohammad Hadi Hosseini"
                    },
                    {
                        "name": "Mahdieh Soleymani Baghshah"
                    }
                ],
                "author_detail": {
                    "name": "Mahdieh Soleymani Baghshah"
                },
                "author": "Mahdieh Soleymani Baghshah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14632v1",
                "updated": "2025-02-20T15:10:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    10,
                    5,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:10:05Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    10,
                    5,
                    3,
                    51,
                    0
                ],
                "title": "Augmenting Coaching with GenAI: Insights into Use, Effectiveness, and\n  Future Potential",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting Coaching with GenAI: Insights into Use, Effectiveness, and\n  Future Potential"
                },
                "summary": "The integration of generative AI (GenAI) tools, particularly large language\nmodels (LLMs), is transforming professional coaching workflows. This study\nexplores how coaches use GenAI, the perceived benefits and limitations of these\ntools, and broader attitudes toward AI-assisted coaching. A survey of 205\ncoaching professionals reveals widespread adoption of GenAI for research,\ncontent creation, and administrative support, while its role in relational and\ninterpretative coaching remains limited. Findings indicate that AI literacy and\nperceived AI impact strongly predict GenAI adoption, with positive attitudes\nfostering greater use. Ethical considerations, particularly transparency and\ndata privacy, are a key concern, with frequent AI users demonstrating greater\nethical awareness. Regression analyses show that while perceived effectiveness\ndrives GenAI adoption, concerns about AI replacing human coaches do not\nsignificantly influence usage. Coaches express interest in future AI\ncapabilities that enhance personalization, real-time feedback, and\nadministrative automation while maintaining human oversight. The study\nhighlights that GenAI functions best as an augmentation tool rather than a\nreplacement, emphasizing the need for AI literacy training, ethical guidelines,\nand human-centered AI integration. These findings contribute to the ongoing\ndiscourse on human-AI collaboration, advocating for responsible and effective\nAI adoption in professional coaching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of generative AI (GenAI) tools, particularly large language\nmodels (LLMs), is transforming professional coaching workflows. This study\nexplores how coaches use GenAI, the perceived benefits and limitations of these\ntools, and broader attitudes toward AI-assisted coaching. A survey of 205\ncoaching professionals reveals widespread adoption of GenAI for research,\ncontent creation, and administrative support, while its role in relational and\ninterpretative coaching remains limited. Findings indicate that AI literacy and\nperceived AI impact strongly predict GenAI adoption, with positive attitudes\nfostering greater use. Ethical considerations, particularly transparency and\ndata privacy, are a key concern, with frequent AI users demonstrating greater\nethical awareness. Regression analyses show that while perceived effectiveness\ndrives GenAI adoption, concerns about AI replacing human coaches do not\nsignificantly influence usage. Coaches express interest in future AI\ncapabilities that enhance personalization, real-time feedback, and\nadministrative automation while maintaining human oversight. The study\nhighlights that GenAI functions best as an augmentation tool rather than a\nreplacement, emphasizing the need for AI literacy training, ethical guidelines,\nand human-centered AI integration. These findings contribute to the ongoing\ndiscourse on human-AI collaboration, advocating for responsible and effective\nAI adoption in professional coaching."
                },
                "authors": [
                    {
                        "name": "Jennifer Haase"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer Haase"
                },
                "author": "Jennifer Haase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14631v1",
                "updated": "2025-02-20T15:10:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    10,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:10:00Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    10,
                    0,
                    3,
                    51,
                    0
                ],
                "title": "Synergistic Fusion of Multi-Source Knowledge via Evidence Theory for\n  High-Entropy Alloy Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergistic Fusion of Multi-Source Knowledge via Evidence Theory for\n  High-Entropy Alloy Discovery"
                },
                "summary": "Discovering novel high-entropy alloys (HEAs) with desirable properties is\nchallenging due to the vast compositional space and complex phase formation\nmechanisms. Efficient exploration of this space requires a strategic approach\nthat integrates heterogeneous knowledge sources. Here, we propose a framework\nthat systematically combines knowledge extracted from computational material\ndatasets with domain knowledge distilled from scientific literature using large\nlanguage models (LLMs). A central feature of this approach is the explicit\nconsideration of element substitutability, identifying chemically similar\nelements that can be interchanged to potentially stabilize desired HEAs.\nDempster-Shafer theory, a mathematical framework for reasoning under\nuncertainty, is employed to model and combine substitutabilities based on\naggregated evidence from multiple sources. The framework predicts the phase\nstability of candidate HEA compositions and is systematically evaluated on both\nquaternary alloy systems, demonstrating superior performance compared to\nbaseline machine learning models and methods reliant on single-source evidence\nin cross-validation experiments. By leveraging multi-source knowledge, the\nframework retains robust predictive power even when key elements are absent\nfrom the training data, underscoring its potential for knowledge transfer and\nextrapolation. Furthermore, the enhanced interpretability of the methodology\noffers insights into the fundamental factors governing HEA formation. Overall,\nthis work provides a promising strategy for accelerating HEA discovery by\nintegrating computational and textual knowledge sources, enabling efficient\nexploration of vast compositional spaces with improved generalization and\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering novel high-entropy alloys (HEAs) with desirable properties is\nchallenging due to the vast compositional space and complex phase formation\nmechanisms. Efficient exploration of this space requires a strategic approach\nthat integrates heterogeneous knowledge sources. Here, we propose a framework\nthat systematically combines knowledge extracted from computational material\ndatasets with domain knowledge distilled from scientific literature using large\nlanguage models (LLMs). A central feature of this approach is the explicit\nconsideration of element substitutability, identifying chemically similar\nelements that can be interchanged to potentially stabilize desired HEAs.\nDempster-Shafer theory, a mathematical framework for reasoning under\nuncertainty, is employed to model and combine substitutabilities based on\naggregated evidence from multiple sources. The framework predicts the phase\nstability of candidate HEA compositions and is systematically evaluated on both\nquaternary alloy systems, demonstrating superior performance compared to\nbaseline machine learning models and methods reliant on single-source evidence\nin cross-validation experiments. By leveraging multi-source knowledge, the\nframework retains robust predictive power even when key elements are absent\nfrom the training data, underscoring its potential for knowledge transfer and\nextrapolation. Furthermore, the enhanced interpretability of the methodology\noffers insights into the fundamental factors governing HEA formation. Overall,\nthis work provides a promising strategy for accelerating HEA discovery by\nintegrating computational and textual knowledge sources, enabling efficient\nexploration of vast compositional spaces with improved generalization and\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Minh-Quyet Ha"
                    },
                    {
                        "name": "Dinh-Khiet Le"
                    },
                    {
                        "name": "Duc-Anh Dao"
                    },
                    {
                        "name": "Tien-Sinh Vu"
                    },
                    {
                        "name": "Duong-Nguyen Nguyen"
                    },
                    {
                        "name": "Viet-Cuong Nguyen"
                    },
                    {
                        "name": "Hiori Kino"
                    },
                    {
                        "name": "Van-Nam Huynh"
                    },
                    {
                        "name": "Hieu-Chi Dam"
                    }
                ],
                "author_detail": {
                    "name": "Hieu-Chi Dam"
                },
                "author": "Hieu-Chi Dam",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14628v1",
                "updated": "2025-02-20T15:07:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    7,
                    2,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:07:02Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    7,
                    2,
                    3,
                    51,
                    0
                ],
                "title": "PEARL: Towards Permutation-Resilient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEARL: Towards Permutation-Resilient LLMs"
                },
                "summary": "The in-context learning (ICL) capability of large language models (LLMs)\nenables them to perform challenging tasks using provided demonstrations.\nHowever, ICL is highly sensitive to the ordering of demonstrations, leading to\ninstability in predictions. This paper shows that this vulnerability can be\nexploited to design a natural attack - difficult for model providers to detect\n- that achieves nearly 80% success rate on LLaMA-3 by simply permuting the\ndemonstrations. Existing mitigation methods primarily rely on post-processing\nand fail to enhance the model's inherent robustness to input permutations,\nraising concerns about safety and reliability of LLMs. To address this issue,\nwe propose Permutation-resilient learning (PEARL), a novel framework based on\ndistributionally robust optimization (DRO), which optimizes model performance\nagainst the worst-case input permutation. Specifically, PEARL consists of a\npermutation-proposal network (P-Net) and the LLM. The P-Net generates the most\nchallenging permutations by treating it as an optimal transport problem, which\nis solved using an entropy-constrained Sinkhorn algorithm. Through minimax\noptimization, the P-Net and the LLM iteratively optimize against each other,\nprogressively improving the LLM's robustness. Experiments on synthetic\npre-training and real-world instruction tuning tasks demonstrate that PEARL\neffectively mitigates permutation attacks and enhances performance. Notably,\ndespite being trained on fewer shots and shorter contexts, PEARL achieves\nperformance gains of up to 40% when scaled to many-shot and long-context\nscenarios, highlighting its efficiency and generalization capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The in-context learning (ICL) capability of large language models (LLMs)\nenables them to perform challenging tasks using provided demonstrations.\nHowever, ICL is highly sensitive to the ordering of demonstrations, leading to\ninstability in predictions. This paper shows that this vulnerability can be\nexploited to design a natural attack - difficult for model providers to detect\n- that achieves nearly 80% success rate on LLaMA-3 by simply permuting the\ndemonstrations. Existing mitigation methods primarily rely on post-processing\nand fail to enhance the model's inherent robustness to input permutations,\nraising concerns about safety and reliability of LLMs. To address this issue,\nwe propose Permutation-resilient learning (PEARL), a novel framework based on\ndistributionally robust optimization (DRO), which optimizes model performance\nagainst the worst-case input permutation. Specifically, PEARL consists of a\npermutation-proposal network (P-Net) and the LLM. The P-Net generates the most\nchallenging permutations by treating it as an optimal transport problem, which\nis solved using an entropy-constrained Sinkhorn algorithm. Through minimax\noptimization, the P-Net and the LLM iteratively optimize against each other,\nprogressively improving the LLM's robustness. Experiments on synthetic\npre-training and real-world instruction tuning tasks demonstrate that PEARL\neffectively mitigates permutation attacks and enhances performance. Notably,\ndespite being trained on fewer shots and shorter contexts, PEARL achieves\nperformance gains of up to 40% when scaled to many-shot and long-context\nscenarios, highlighting its efficiency and generalization capabilities."
                },
                "authors": [
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Xiaoyan Zhao"
                    },
                    {
                        "name": "Bin Liang"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14621v1",
                "updated": "2025-02-20T15:00:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    0,
                    34,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:00:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    0,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "Asymptotic Analysis and Practical Evaluation of Jump Rate Estimators in\n  Piecewise-Deterministic Markov Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotic Analysis and Practical Evaluation of Jump Rate Estimators in\n  Piecewise-Deterministic Markov Processes"
                },
                "summary": "Piecewise-deterministic Markov processes (PDMPs) offer a powerful stochastic\nmodeling framework that combines deterministic trajectories with random\nperturbations at random times. Estimating their local characteristics\n(particularly the jump rate) is an important yet challenging task. In recent\nyears, non-parametric methods for jump rate inference have been developed, but\nthese approaches often rely on distinct theoretical frameworks, complicating\ndirect comparisons. In this paper, we propose a unified framework to\nstandardize and consolidate state-of-the-art approaches. We establish new\nresults on consistency and asymptotic normality within this framework, enabling\nrigorous theoretical comparisons of convergence rates and asymptotic variances.\nNotably, we demonstrate that no single method uniformly outperforms the others,\neven within the same model. These theoretical insights are validated through\nnumerical simulations using a representative PDMP application: the TCP model.\nFurthermore, we extend the comparison to real-world data, focusing on cell\ngrowth and division dynamics in Escherichia coli. This work enhances the\ntheoretical understanding of PDMP inference while offering practical insights\ninto the relative strengths and limitations of existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piecewise-deterministic Markov processes (PDMPs) offer a powerful stochastic\nmodeling framework that combines deterministic trajectories with random\nperturbations at random times. Estimating their local characteristics\n(particularly the jump rate) is an important yet challenging task. In recent\nyears, non-parametric methods for jump rate inference have been developed, but\nthese approaches often rely on distinct theoretical frameworks, complicating\ndirect comparisons. In this paper, we propose a unified framework to\nstandardize and consolidate state-of-the-art approaches. We establish new\nresults on consistency and asymptotic normality within this framework, enabling\nrigorous theoretical comparisons of convergence rates and asymptotic variances.\nNotably, we demonstrate that no single method uniformly outperforms the others,\neven within the same model. These theoretical insights are validated through\nnumerical simulations using a representative PDMP application: the TCP model.\nFurthermore, we extend the comparison to real-world data, focusing on cell\ngrowth and division dynamics in Escherichia coli. This work enhances the\ntheoretical understanding of PDMP inference while offering practical insights\ninto the relative strengths and limitations of existing methods."
                },
                "authors": [
                    {
                        "name": "Romain Azas"
                    },
                    {
                        "name": "Solune Denis"
                    }
                ],
                "author_detail": {
                    "name": "Solune Denis"
                },
                "author": "Solune Denis",
                "arxiv_comment": "33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62M05, 62G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05806v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05806v2",
                "updated": "2025-02-20T14:58:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    58,
                    39,
                    3,
                    51,
                    0
                ],
                "published": "2024-09-09T17:11:51Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    11,
                    51,
                    0,
                    253,
                    0
                ],
                "title": "CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics,\n  Facts, and Logic Error Correction in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics,\n  Facts, and Logic Error Correction in LLMs"
                },
                "summary": "Chinese, as a linguistic system rich in depth and complexity, is\ncharacterized by distinctive elements such as ancient poetry, proverbs, idioms,\nand other cultural constructs. However, current Large Language Models (LLMs)\nface limitations in these specialized domains, highlighting the need for the\ndevelopment of comprehensive datasets that can assess, continuously update, and\nprogressively improve these culturally-grounded linguistic competencies through\ntargeted training optimizations. To address this gap, we introduce CKnowEdit,\nthe first-ever Chinese knowledge editing dataset designed to correct\nlinguistic, factual, and logical errors in LLMs. We collect seven types of\nknowledge from a wide range of sources, including classical texts, idioms, and\ncontent from Baidu Tieba Ruozhiba, taking into account the unique polyphony,\nantithesis, and logical structures inherent in the Chinese language. By\nanalyzing this dataset, we highlight the challenges current LLMs face in\nmastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques reveals opportunities to advance the correction of Chinese\nknowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese, as a linguistic system rich in depth and complexity, is\ncharacterized by distinctive elements such as ancient poetry, proverbs, idioms,\nand other cultural constructs. However, current Large Language Models (LLMs)\nface limitations in these specialized domains, highlighting the need for the\ndevelopment of comprehensive datasets that can assess, continuously update, and\nprogressively improve these culturally-grounded linguistic competencies through\ntargeted training optimizations. To address this gap, we introduce CKnowEdit,\nthe first-ever Chinese knowledge editing dataset designed to correct\nlinguistic, factual, and logical errors in LLMs. We collect seven types of\nknowledge from a wide range of sources, including classical texts, idioms, and\ncontent from Baidu Tieba Ruozhiba, taking into account the unique polyphony,\nantithesis, and logical structures inherent in the Chinese language. By\nanalyzing this dataset, we highlight the challenges current LLMs face in\nmastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques reveals opportunities to advance the correction of Chinese\nknowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit."
                },
                "authors": [
                    {
                        "name": "Jizhan Fang"
                    },
                    {
                        "name": "Tianhe Lu"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Ziyan Jiang"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Ongoing work; project website is available at\n  https://zjunlp.github.io/project/CKnowEdit code and dataset are available at\n  https://github.com/zjunlp/EasyEdit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05806v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05806v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14620v1",
                "updated": "2025-02-20T14:58:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    58,
                    37,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:58:37Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    58,
                    37,
                    3,
                    51,
                    0
                ],
                "title": "Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline\n  Comparison for Semantic Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline\n  Comparison for Semantic Similarity"
                },
                "summary": "This paper investigates the efficacy of RWKV, a novel language model\narchitecture known for its linear attention mechanism, for generating sentence\nembeddings in a zero-shot setting. I conduct a layer-wise analysis to evaluate\nthe semantic similarity captured by embeddings from different hidden layers of\na pre-trained RWKV model. The performance is assessed on the Microsoft Research\nParaphrase Corpus (MRPC) dataset using Spearman correlation and compared\nagainst a GloVe-based baseline. My results indicate that while RWKV embeddings\ncapture some semantic relatedness, they underperform compared to the GloVe\nbaseline in terms of Spearman correlation. I also analyze the inference time\nand GPU memory usage, highlighting the computational trade-offs associated with\nRWKV embeddings. The findings suggest that while RWKV offers potential\nadvantages in terms of linear scaling, its zero-shot sentence embedding quality\nfor semantic similarity tasks requires further investigation and potential\ntask-specific fine-tuning to match or exceed simpler baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the efficacy of RWKV, a novel language model\narchitecture known for its linear attention mechanism, for generating sentence\nembeddings in a zero-shot setting. I conduct a layer-wise analysis to evaluate\nthe semantic similarity captured by embeddings from different hidden layers of\na pre-trained RWKV model. The performance is assessed on the Microsoft Research\nParaphrase Corpus (MRPC) dataset using Spearman correlation and compared\nagainst a GloVe-based baseline. My results indicate that while RWKV embeddings\ncapture some semantic relatedness, they underperform compared to the GloVe\nbaseline in terms of Spearman correlation. I also analyze the inference time\nand GPU memory usage, highlighting the computational trade-offs associated with\nRWKV embeddings. The findings suggest that while RWKV offers potential\nadvantages in terms of linear scaling, its zero-shot sentence embedding quality\nfor semantic similarity tasks requires further investigation and potential\ntask-specific fine-tuning to match or exceed simpler baselines."
                },
                "authors": [
                    {
                        "name": "Xinghan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Xinghan Pan"
                },
                "author": "Xinghan Pan",
                "arxiv_comment": "17 pages, 3 tables, preprint on ArXiV, includes detailed analysis of\n  RWKV for semantic similarity tasks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.7.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14619v1",
                "updated": "2025-02-20T14:57:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    57,
                    14,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:57:14Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    57,
                    14,
                    3,
                    51,
                    0
                ],
                "title": "Reward Models Identify Consistency, Not Causality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Models Identify Consistency, Not Causality"
                },
                "summary": "Reward models (RMs) play a crucial role in aligning large language models\n(LLMs) with human preferences and enhancing reasoning quality. Traditionally,\nRMs are trained to rank candidate outputs based on their correctness and\ncoherence. However, in this work, we present several surprising findings that\nchallenge common assumptions about RM behavior. Our analysis reveals that\nstate-of-the-art reward models prioritize structural consistency over causal\ncorrectness. Specifically, removing the problem statement has minimal impact on\nreward scores, whereas altering numerical values or disrupting the reasoning\nflow significantly affects RM outputs. Furthermore, RMs exhibit a strong\ndependence on complete reasoning trajectories truncated or incomplete steps\nlead to significant variations in reward assignments, indicating that RMs\nprimarily rely on learned reasoning patterns rather than explicit problem\ncomprehension. These findings hold across multiple architectures, datasets, and\ntasks, leading to three key insights: (1) RMs primarily assess coherence rather\nthan true reasoning quality; (2) The role of explicit problem comprehension in\nreward assignment is overstated; (3) Current RMs may be more effective at\nranking responses than verifying logical validity. Our results suggest a\nfundamental limitation in existing reward modeling approaches, emphasizing the\nneed for a shift toward causality-aware reward models that go beyond\nconsistency-driven evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) play a crucial role in aligning large language models\n(LLMs) with human preferences and enhancing reasoning quality. Traditionally,\nRMs are trained to rank candidate outputs based on their correctness and\ncoherence. However, in this work, we present several surprising findings that\nchallenge common assumptions about RM behavior. Our analysis reveals that\nstate-of-the-art reward models prioritize structural consistency over causal\ncorrectness. Specifically, removing the problem statement has minimal impact on\nreward scores, whereas altering numerical values or disrupting the reasoning\nflow significantly affects RM outputs. Furthermore, RMs exhibit a strong\ndependence on complete reasoning trajectories truncated or incomplete steps\nlead to significant variations in reward assignments, indicating that RMs\nprimarily rely on learned reasoning patterns rather than explicit problem\ncomprehension. These findings hold across multiple architectures, datasets, and\ntasks, leading to three key insights: (1) RMs primarily assess coherence rather\nthan true reasoning quality; (2) The role of explicit problem comprehension in\nreward assignment is overstated; (3) Current RMs may be more effective at\nranking responses than verifying logical validity. Our results suggest a\nfundamental limitation in existing reward modeling approaches, emphasizing the\nneed for a shift toward causality-aware reward models that go beyond\nconsistency-driven evaluation."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Junnan Li"
                    }
                ],
                "author_detail": {
                    "name": "Junnan Li"
                },
                "author": "Junnan Li",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14617v1",
                "updated": "2025-02-20T14:57:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    57,
                    8,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:57:08Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    57,
                    8,
                    3,
                    51,
                    0
                ],
                "title": "Serving Models, Fast and Slow:Optimizing Heterogeneous LLM Inferencing\n  Workloads at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Models, Fast and Slow:Optimizing Heterogeneous LLM Inferencing\n  Workloads at Scale"
                },
                "summary": "Large Language Model (LLM) inference workloads handled by global cloud\nproviders can include both latency-sensitive and insensitive tasks, creating a\ndiverse range of Service Level Agreement (SLA) requirements. Managing these\nmixed workloads is challenging due to the complexity of the inference stack,\nwhich includes multiple LLMs, hardware configurations, and geographic\ndistributions. Current optimization strategies often silo these tasks to ensure\nthat SLAs are met for latency-sensitive tasks, but this leads to significant\nunder-utilization of expensive GPU resources despite the availability of spot\nand on-demand Virtual Machine (VM) provisioning. We propose SAGESERVE, a\ncomprehensive LLM serving framework that employs adaptive control knobs at\nvarying time scales, ensuring SLA compliance while maximizing the utilization\nof valuable GPU resources. Short-term optimizations include efficient request\nrouting to data center regions, while long-term strategies involve scaling GPU\nVMs out/in and redeploying models to existing VMs to align with traffic\npatterns. These strategies are formulated as an optimization problem for\nresource allocation and solved using Integer Linear Programming (ILP). We\nperform empirical and simulation studies based on production workload traces\nwith over 8M requests using four open-source models deployed across three\nregions. SAGESERVE achieves up to 25% savings in GPU-hours while maintaining\ntail latency and satisfying all SLOs, and it reduces the scaling overhead\ncompared to baselines by up to 80%, confirming the effectiveness of our\nproposal. In terms of dollar cost, this can save cloud providers up to $2M over\nthe course of a month.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference workloads handled by global cloud\nproviders can include both latency-sensitive and insensitive tasks, creating a\ndiverse range of Service Level Agreement (SLA) requirements. Managing these\nmixed workloads is challenging due to the complexity of the inference stack,\nwhich includes multiple LLMs, hardware configurations, and geographic\ndistributions. Current optimization strategies often silo these tasks to ensure\nthat SLAs are met for latency-sensitive tasks, but this leads to significant\nunder-utilization of expensive GPU resources despite the availability of spot\nand on-demand Virtual Machine (VM) provisioning. We propose SAGESERVE, a\ncomprehensive LLM serving framework that employs adaptive control knobs at\nvarying time scales, ensuring SLA compliance while maximizing the utilization\nof valuable GPU resources. Short-term optimizations include efficient request\nrouting to data center regions, while long-term strategies involve scaling GPU\nVMs out/in and redeploying models to existing VMs to align with traffic\npatterns. These strategies are formulated as an optimization problem for\nresource allocation and solved using Integer Linear Programming (ILP). We\nperform empirical and simulation studies based on production workload traces\nwith over 8M requests using four open-source models deployed across three\nregions. SAGESERVE achieves up to 25% savings in GPU-hours while maintaining\ntail latency and satisfying all SLOs, and it reduces the scaling overhead\ncompared to baselines by up to 80%, confirming the effectiveness of our\nproposal. In terms of dollar cost, this can save cloud providers up to $2M over\nthe course of a month."
                },
                "authors": [
                    {
                        "name": "Shashwat Jaiswal"
                    },
                    {
                        "name": "Kunal Jain"
                    },
                    {
                        "name": "Yogesh Simmhan"
                    },
                    {
                        "name": "Anjaly Parayil"
                    },
                    {
                        "name": "Ankur Mallick"
                    },
                    {
                        "name": "Rujia Wang"
                    },
                    {
                        "name": "Renee St. Amant"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Victor Rhle"
                    },
                    {
                        "name": "Anoop Kulkarni"
                    },
                    {
                        "name": "Steve Kofsky"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "15 pages, 17 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14614v1",
                "updated": "2025-02-20T14:52:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    52,
                    36,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:52:36Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    52,
                    36,
                    3,
                    51,
                    0
                ],
                "title": "FIND: Fine-grained Information Density Guided Adaptive\n  Retrieval-Augmented Generation for Disease Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIND: Fine-grained Information Density Guided Adaptive\n  Retrieval-Augmented Generation for Disease Diagnosis"
                },
                "summary": "Retrieval-Augmented Large Language Models (LLMs), which integrate external\nknowledge into LLMs, have shown remarkable performance in various medical\ndomains, including clinical diagnosis. However, existing RAG methods struggle\nto effectively assess task difficulty to make retrieval decisions, thereby\nfailing to meet the clinical requirements for balancing efficiency and\naccuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained\n\\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework\nthat improves the reliability of RAG in disease diagnosis scenarios. FIND\nincorporates a fine-grained adaptive control module to determine whether\nretrieval is necessary based on the information density of the input. By\noptimizing the retrieval process and implementing a knowledge filtering module,\nFIND ensures that the retrieval is better suited to clinical scenarios.\nExperiments on three Chinese electronic medical record datasets demonstrate\nthat FIND significantly outperforms various baseline methods, highlighting its\neffectiveness in clinical diagnosis tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Large Language Models (LLMs), which integrate external\nknowledge into LLMs, have shown remarkable performance in various medical\ndomains, including clinical diagnosis. However, existing RAG methods struggle\nto effectively assess task difficulty to make retrieval decisions, thereby\nfailing to meet the clinical requirements for balancing efficiency and\naccuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained\n\\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework\nthat improves the reliability of RAG in disease diagnosis scenarios. FIND\nincorporates a fine-grained adaptive control module to determine whether\nretrieval is necessary based on the information density of the input. By\noptimizing the retrieval process and implementing a knowledge filtering module,\nFIND ensures that the retrieval is better suited to clinical scenarios.\nExperiments on three Chinese electronic medical record datasets demonstrate\nthat FIND significantly outperforms various baseline methods, highlighting its\neffectiveness in clinical diagnosis tasks."
                },
                "authors": [
                    {
                        "name": "Mingyi Jia"
                    },
                    {
                        "name": "Junwen Duan"
                    },
                    {
                        "name": "Yan Song"
                    },
                    {
                        "name": "Jianxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Wang"
                },
                "author": "Jianxin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11844v2",
                "updated": "2025-02-20T14:52:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    52,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-17T14:37:47Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    37,
                    47,
                    0,
                    48,
                    0
                ],
                "title": "BaxBench: Can LLMs Generate Correct and Secure Backends?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaxBench: Can LLMs Generate Correct and Secure Backends?"
                },
                "summary": "The automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits,\nand solve algorithmic coding tasks. However, to achieve full automation, LLMs\nshould be able to generate production-quality, self-contained application\nmodules. To evaluate the capabilities of LLMs in solving this challenge, we\nintroduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for\nthe generation of backend applications. We focus on backends for three critical\nreasons: (i) they are practically relevant, building the core components of\nmost modern web and cloud software, (ii) they are difficult to get right,\nrequiring multiple functions and files to achieve the desired functionality,\nand (iii) they are security-critical, as they are exposed to untrusted\nthird-parties, making secure solutions that prevent deployment-time attacks an\nimperative. BaxBench validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by\nexecuting end-to-end exploits. Our experiments reveal key limitations of\ncurrent LLMs in both functionality and security: (i) even the best model,\nOpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could\nsuccessfully execute security exploits on more than half of the correct\nprograms generated by each LLM; and (iii) in less popular backend frameworks,\nmodels further struggle to generate correct and secure applications. Progress\non BaxBench signifies important steps towards autonomous and secure software\ndevelopment with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits,\nand solve algorithmic coding tasks. However, to achieve full automation, LLMs\nshould be able to generate production-quality, self-contained application\nmodules. To evaluate the capabilities of LLMs in solving this challenge, we\nintroduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for\nthe generation of backend applications. We focus on backends for three critical\nreasons: (i) they are practically relevant, building the core components of\nmost modern web and cloud software, (ii) they are difficult to get right,\nrequiring multiple functions and files to achieve the desired functionality,\nand (iii) they are security-critical, as they are exposed to untrusted\nthird-parties, making secure solutions that prevent deployment-time attacks an\nimperative. BaxBench validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by\nexecuting end-to-end exploits. Our experiments reveal key limitations of\ncurrent LLMs in both functionality and security: (i) even the best model,\nOpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could\nsuccessfully execute security exploits on more than half of the correct\nprograms generated by each LLM; and (iii) in less popular backend frameworks,\nmodels further struggle to generate correct and secure applications. Progress\non BaxBench signifies important steps towards autonomous and secure software\ndevelopment with LLMs."
                },
                "authors": [
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Niels Mndler"
                    },
                    {
                        "name": "Victor Chibotaru"
                    },
                    {
                        "name": "Veselin Raychev"
                    },
                    {
                        "name": "Maximilian Baader"
                    },
                    {
                        "name": "Nikola Jovanovi"
                    },
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14613v1",
                "updated": "2025-02-20T14:52:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    52,
                    23,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:52:23Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    52,
                    23,
                    3,
                    51,
                    0
                ],
                "title": "Behavioral Analysis of Information Salience in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavioral Analysis of Information Salience in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) excel at text summarization, a task that\nrequires models to select content based on its importance. However, the exact\nnotion of salience that LLMs have internalized remains unclear. To bridge this\ngap, we introduce an explainable framework to systematically derive and\ninvestigate information salience in LLMs through their summarization behavior.\nUsing length-controlled summarization as a behavioral probe into the content\nselection process, and tracing the answerability of Questions Under Discussion\nthroughout, we derive a proxy for how models prioritize information. Our\nexperiments on 13 models across four datasets reveal that LLMs have a nuanced,\nhierarchical notion of salience, generally consistent across model families and\nsizes. While models show highly consistent behavior and hence salience\npatterns, this notion of salience cannot be accessed through introspection, and\nonly weakly correlates with human perceptions of information salience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at text summarization, a task that\nrequires models to select content based on its importance. However, the exact\nnotion of salience that LLMs have internalized remains unclear. To bridge this\ngap, we introduce an explainable framework to systematically derive and\ninvestigate information salience in LLMs through their summarization behavior.\nUsing length-controlled summarization as a behavioral probe into the content\nselection process, and tracing the answerability of Questions Under Discussion\nthroughout, we derive a proxy for how models prioritize information. Our\nexperiments on 13 models across four datasets reveal that LLMs have a nuanced,\nhierarchical notion of salience, generally consistent across model families and\nsizes. While models show highly consistent behavior and hence salience\npatterns, this notion of salience cannot be accessed through introspection, and\nonly weakly correlates with human perceptions of information salience."
                },
                "authors": [
                    {
                        "name": "Jan Trienes"
                    },
                    {
                        "name": "Jrg Schltterer"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    },
                    {
                        "name": "Christin Seifert"
                    }
                ],
                "author_detail": {
                    "name": "Christin Seifert"
                },
                "author": "Christin Seifert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.11738v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.11738v4",
                "updated": "2025-02-20T14:43:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    43,
                    20,
                    3,
                    51,
                    0
                ],
                "published": "2023-08-22T18:58:21Z",
                "published_parsed": [
                    2023,
                    8,
                    22,
                    18,
                    58,
                    21,
                    1,
                    234,
                    0
                ],
                "title": "Lifted Inference beyond First-Order Logic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifted Inference beyond First-Order Logic"
                },
                "summary": "Weighted First Order Model Counting (WFOMC) is fundamental to probabilistic\ninference in statistical relational learning models. As WFOMC is known to be\nintractable in general ($\\#$P-complete), logical fragments that admit\npolynomial time WFOMC are of significant interest. Such fragments are called\ndomain liftable. Recent works have shown that the two-variable fragment of\nfirst order logic extended with counting quantifiers ($\\mathrm{C^2}$) is\ndomain-liftable. However, many properties of real-world data, like acyclicity\nin citation networks and connectivity in social networks, cannot be modeled in\n$\\mathrm{C^2}$, or first order logic in general. In this work, we expand the\ndomain liftability of $\\mathrm{C^2}$ with multiple such properties. We show\nthat any $\\mathrm{C^2}$ sentence remains domain liftable when one of its\nrelations is restricted to represent a directed acyclic graph, a connected\ngraph, a tree (resp. a directed tree) or a forest (resp. a directed forest).\nAll our results rely on a novel and general methodology of \"counting by\nsplitting\". Besides their application to probabilistic inference, our results\nprovide a general framework for counting combinatorial structures. We expand a\nvast array of previous results in discrete mathematics literature on directed\nacyclic graphs, phylogenetic networks, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weighted First Order Model Counting (WFOMC) is fundamental to probabilistic\ninference in statistical relational learning models. As WFOMC is known to be\nintractable in general ($\\#$P-complete), logical fragments that admit\npolynomial time WFOMC are of significant interest. Such fragments are called\ndomain liftable. Recent works have shown that the two-variable fragment of\nfirst order logic extended with counting quantifiers ($\\mathrm{C^2}$) is\ndomain-liftable. However, many properties of real-world data, like acyclicity\nin citation networks and connectivity in social networks, cannot be modeled in\n$\\mathrm{C^2}$, or first order logic in general. In this work, we expand the\ndomain liftability of $\\mathrm{C^2}$ with multiple such properties. We show\nthat any $\\mathrm{C^2}$ sentence remains domain liftable when one of its\nrelations is restricted to represent a directed acyclic graph, a connected\ngraph, a tree (resp. a directed tree) or a forest (resp. a directed forest).\nAll our results rely on a novel and general methodology of \"counting by\nsplitting\". Besides their application to probabilistic inference, our results\nprovide a general framework for counting combinatorial structures. We expand a\nvast array of previous results in discrete mathematics literature on directed\nacyclic graphs, phylogenetic networks, etc."
                },
                "authors": [
                    {
                        "name": "Sagar Malhotra"
                    },
                    {
                        "name": "Davide Bizzaro"
                    },
                    {
                        "name": "Luciano Serafini"
                    }
                ],
                "author_detail": {
                    "name": "Luciano Serafini"
                },
                "author": "Luciano Serafini",
                "arxiv_comment": "Conditionally accepted (Minor revisions) Artificial Intelligence\n  Journal. Explanation for practical implementation of cardinality constraints\n  added in Appendix .arXiv admin note: text overlap with arXiv:2302.09830",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.11738v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.11738v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14600v1",
                "updated": "2025-02-20T14:33:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    33,
                    40,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:33:40Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    33,
                    40,
                    3,
                    51,
                    0
                ],
                "title": "Spectral decomposition-assisted multi-study factor analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral decomposition-assisted multi-study factor analysis"
                },
                "summary": "This article focuses on covariance estimation for multi-study data. Popular\napproaches employ factor-analytic terms with shared and study-specific loadings\nthat decompose the variance into (i) a shared low-rank component, (ii)\nstudy-specific low-rank components, and (iii) a diagonal term capturing\nidiosyncratic variability. Our proposed methodology estimates the latent\nfactors via spectral decompositions and infers the factor loadings via\nsurrogate regression tasks, avoiding identifiability and computational issues\nof existing alternatives. Reliably inferring shared vs study-specific\ncomponents requires novel developments that are of independent interest. The\napproximation error decreases as the sample size and the data dimension\ndiverge, formalizing a blessing of dimensionality. Conditionally on the\nfactors, loadings and residual error variances are inferred via conjugate\nnormal-inverse gamma priors. The conditional posterior distribution of factor\nloadings has a simple product form across outcomes, facilitating\nparallelization. We show favorable asymptotic properties, including central\nlimit theorems for point estimators and posterior contraction, and excellent\nempirical performance in simulations. The methods are applied to integrate\nthree studies on gene associations among immune cells.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article focuses on covariance estimation for multi-study data. Popular\napproaches employ factor-analytic terms with shared and study-specific loadings\nthat decompose the variance into (i) a shared low-rank component, (ii)\nstudy-specific low-rank components, and (iii) a diagonal term capturing\nidiosyncratic variability. Our proposed methodology estimates the latent\nfactors via spectral decompositions and infers the factor loadings via\nsurrogate regression tasks, avoiding identifiability and computational issues\nof existing alternatives. Reliably inferring shared vs study-specific\ncomponents requires novel developments that are of independent interest. The\napproximation error decreases as the sample size and the data dimension\ndiverge, formalizing a blessing of dimensionality. Conditionally on the\nfactors, loadings and residual error variances are inferred via conjugate\nnormal-inverse gamma priors. The conditional posterior distribution of factor\nloadings has a simple product form across outcomes, facilitating\nparallelization. We show favorable asymptotic properties, including central\nlimit theorems for point estimators and posterior contraction, and excellent\nempirical performance in simulations. The methods are applied to integrate\nthree studies on gene associations among immune cells."
                },
                "authors": [
                    {
                        "name": "Lorenzo Mauri"
                    },
                    {
                        "name": "Niccol Anceschi"
                    },
                    {
                        "name": "David B. Dunson"
                    }
                ],
                "author_detail": {
                    "name": "David B. Dunson"
                },
                "author": "David B. Dunson",
                "arxiv_comment": "Main 36 pages (including bibliography). With Appendix, 80 Pages and 3\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14595v1",
                "updated": "2025-02-20T14:29:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    29,
                    8,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:29:08Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    29,
                    8,
                    3,
                    51,
                    0
                ],
                "title": "Neutron versus proton scattering on exotic nuclei: the $^9$He example",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron versus proton scattering on exotic nuclei: the $^9$He example"
                },
                "summary": "Neutron scattering on exotic nuclides is a class of processes which can not\nbe studied directly now and in any observable future. Resonance proton\nscattering of exotic nuclide on a thick target in inverse kinematics can be\nused to infer the properties of the low-energy neutron scattering of this\nnuclide assuming the isobaric symmetry. However, the results of such resonance\nproton scattering reactions are so far analyzed in theoretical approaches\n(optical, R-matrix models), which are missing important aspects of isospin\ndynamics, isospin violation in continuum and threshold dynamics. The isospin\nconserving coupled-channel model (ICM) is proposed, which provides a more\nreliable basis for understanding of such experimental studies. Qualitatively\ndifferent phase shifts for the $^{8}$He+$p$ $T=5/2$ and $T=3/2$ resonances are\npredicted by ICM with quite unusual profile for the $T=5/2$ states. Alternative\ninterpretation of the existing $^{8}$He+$p$ data is proposed. The observable\nproperties of the $T=5/2$ resonances may be strongly affected by the\nisobaric-partner $T=3/2$ states. Crucial importance of studies of the\nneutron-emission channel for disentangling this possible influence is\ndemonstrated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron scattering on exotic nuclides is a class of processes which can not\nbe studied directly now and in any observable future. Resonance proton\nscattering of exotic nuclide on a thick target in inverse kinematics can be\nused to infer the properties of the low-energy neutron scattering of this\nnuclide assuming the isobaric symmetry. However, the results of such resonance\nproton scattering reactions are so far analyzed in theoretical approaches\n(optical, R-matrix models), which are missing important aspects of isospin\ndynamics, isospin violation in continuum and threshold dynamics. The isospin\nconserving coupled-channel model (ICM) is proposed, which provides a more\nreliable basis for understanding of such experimental studies. Qualitatively\ndifferent phase shifts for the $^{8}$He+$p$ $T=5/2$ and $T=3/2$ resonances are\npredicted by ICM with quite unusual profile for the $T=5/2$ states. Alternative\ninterpretation of the existing $^{8}$He+$p$ data is proposed. The observable\nproperties of the $T=5/2$ resonances may be strongly affected by the\nisobaric-partner $T=3/2$ states. Crucial importance of studies of the\nneutron-emission channel for disentangling this possible influence is\ndemonstrated."
                },
                "authors": [
                    {
                        "name": "M. S. Khirk"
                    },
                    {
                        "name": "L. V. Grigorenko"
                    },
                    {
                        "name": "D. E. Lanskoy"
                    },
                    {
                        "name": "P. G. Sharov"
                    }
                ],
                "author_detail": {
                    "name": "P. G. Sharov"
                },
                "arxiv_affiliation": "Institute of Physics in Opava, Silesian University in Opava, Opava, Czech Republic",
                "author": "P. G. Sharov",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14592v1",
                "updated": "2025-02-20T14:27:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    27,
                    24,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:27:24Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    27,
                    24,
                    3,
                    51,
                    0
                ],
                "title": "\"Don't Forget the Teachers\": Towards an Educator-Centered Understanding\n  of Harms from Large Language Models in Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Don't Forget the Teachers\": Towards an Educator-Centered Understanding\n  of Harms from Large Language Models in Education"
                },
                "summary": "Education technologies (edtech) are increasingly incorporating new features\nbuilt on large language models (LLMs), with the goals of enriching the\nprocesses of teaching and learning and ultimately improving learning outcomes.\nHowever, the potential downstream impacts of LLM-based edtech remain\nunderstudied. Prior attempts to map the risks of LLMs have not been tailored to\neducation specifically, even though it is a unique domain in many respects:\nfrom its population (students are often children, who can be especially\nimpacted by technology) to its goals (providing the correct answer may be less\nimportant for learners than understanding how to arrive at an answer) to its\nimplications for higher-order skills that generalize across contexts (e.g.,\ncritical thinking and collaboration). We conducted semi-structured interviews\nwith six edtech providers representing leaders in the K-12 space, as well as a\ndiverse group of 23 educators with varying levels of experience with LLM-based\nedtech. Through a thematic analysis, we explored how each group is\nanticipating, observing, and accounting for potential harms from LLMs in\neducation. We find that, while edtech providers focus primarily on mitigating\ntechnical harms, i.e., those that can be measured based solely on LLM outputs\nthemselves, educators are more concerned about harms that result from the\nbroader impacts of LLMs, i.e., those that require observation of interactions\nbetween students, educators, school systems, and edtech to measure. Overall, we\n(1) develop an education-specific overview of potential harms from LLMs, (2)\nhighlight gaps between conceptions of harm by edtech providers and those by\neducators, and (3) make recommendations to facilitate the centering of\neducators in the design and development of edtech tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Education technologies (edtech) are increasingly incorporating new features\nbuilt on large language models (LLMs), with the goals of enriching the\nprocesses of teaching and learning and ultimately improving learning outcomes.\nHowever, the potential downstream impacts of LLM-based edtech remain\nunderstudied. Prior attempts to map the risks of LLMs have not been tailored to\neducation specifically, even though it is a unique domain in many respects:\nfrom its population (students are often children, who can be especially\nimpacted by technology) to its goals (providing the correct answer may be less\nimportant for learners than understanding how to arrive at an answer) to its\nimplications for higher-order skills that generalize across contexts (e.g.,\ncritical thinking and collaboration). We conducted semi-structured interviews\nwith six edtech providers representing leaders in the K-12 space, as well as a\ndiverse group of 23 educators with varying levels of experience with LLM-based\nedtech. Through a thematic analysis, we explored how each group is\nanticipating, observing, and accounting for potential harms from LLMs in\neducation. We find that, while edtech providers focus primarily on mitigating\ntechnical harms, i.e., those that can be measured based solely on LLM outputs\nthemselves, educators are more concerned about harms that result from the\nbroader impacts of LLMs, i.e., those that require observation of interactions\nbetween students, educators, school systems, and edtech to measure. Overall, we\n(1) develop an education-specific overview of potential harms from LLMs, (2)\nhighlight gaps between conceptions of harm by edtech providers and those by\neducators, and (3) make recommendations to facilitate the centering of\neducators in the design and development of edtech tools."
                },
                "authors": [
                    {
                        "name": "Emma Harvey"
                    },
                    {
                        "name": "Allison Koenecke"
                    },
                    {
                        "name": "Rene F. Kizilcec"
                    }
                ],
                "author_detail": {
                    "name": "Rene F. Kizilcec"
                },
                "author": "Rene F. Kizilcec",
                "arxiv_comment": "To appear in the 2025 ACM CHI Conference on Human Factors in\n  Computing Systems (CHI '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01387v3",
                "updated": "2025-02-20T14:09:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    9,
                    1,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-03T14:22:03Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    14,
                    22,
                    3,
                    0,
                    34,
                    0
                ],
                "title": "TeLL-Drive: Enhancing Autonomous Driving with Teacher LLM-Guided Deep\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeLL-Drive: Enhancing Autonomous Driving with Teacher LLM-Guided Deep\n  Reinforcement Learning"
                },
                "summary": "Although Deep Reinforcement Learning (DRL) and Large Language Models (LLMs)\neach show promise in addressing decision-making challenges in autonomous\ndriving, DRL often suffers from high sample complexity, while LLMs have\ndifficulty ensuring real-time decision making. To address these limitations, we\npropose TeLL-Drive, a hybrid framework that integrates a Teacher LLM to guide\nan attention-based Student DRL policy. By incorporating risk metrics,\nhistorical scenario retrieval, and domain heuristics into context-rich prompts,\nthe LLM produces high-level driving strategies through chain-of-thought\nreasoning. A self-attention mechanism then fuses these strategies with the DRL\nagent's exploration, accelerating policy convergence and boosting robustness\nacross diverse driving conditions. The experimental results, evaluated across\nmultiple traffic scenarios, show that TeLL-Drive outperforms existing baseline\nmethods, including other LLM-based approaches, in terms of success rates,\naverage returns, and real-time feasibility. Ablation studies underscore the\nimportance of each model component, especially the synergy between the\nattention mechanism and LLM-driven guidance. Finally, we build a virtual-real\nfusion experimental platform to verify the real-time performance, robustness,\nand reliability of the algorithm running on real vehicles through\nvehicle-in-loop experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Deep Reinforcement Learning (DRL) and Large Language Models (LLMs)\neach show promise in addressing decision-making challenges in autonomous\ndriving, DRL often suffers from high sample complexity, while LLMs have\ndifficulty ensuring real-time decision making. To address these limitations, we\npropose TeLL-Drive, a hybrid framework that integrates a Teacher LLM to guide\nan attention-based Student DRL policy. By incorporating risk metrics,\nhistorical scenario retrieval, and domain heuristics into context-rich prompts,\nthe LLM produces high-level driving strategies through chain-of-thought\nreasoning. A self-attention mechanism then fuses these strategies with the DRL\nagent's exploration, accelerating policy convergence and boosting robustness\nacross diverse driving conditions. The experimental results, evaluated across\nmultiple traffic scenarios, show that TeLL-Drive outperforms existing baseline\nmethods, including other LLM-based approaches, in terms of success rates,\naverage returns, and real-time feasibility. Ablation studies underscore the\nimportance of each model component, especially the synergy between the\nattention mechanism and LLM-driven guidance. Finally, we build a virtual-real\nfusion experimental platform to verify the real-time performance, robustness,\nand reliability of the algorithm running on real vehicles through\nvehicle-in-loop experiments."
                },
                "authors": [
                    {
                        "name": "Chengkai Xu"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Shiyu Fang"
                    },
                    {
                        "name": "Yiming Cui"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Peng Hang"
                    },
                    {
                        "name": "Jian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jian Sun"
                },
                "author": "Jian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14572v1",
                "updated": "2025-02-20T13:56:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    56,
                    21,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T13:56:21Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    56,
                    21,
                    3,
                    51,
                    0
                ],
                "title": "Factor Graph-based Interpretable Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factor Graph-based Interpretable Neural Networks"
                },
                "summary": "Comprehensible neural network explanations are foundations for a better\nunderstanding of decisions, especially when the input data are infused with\nmalicious perturbations. Existing solutions generally mitigate the impact of\nperturbations through adversarial training, yet they fail to generate\ncomprehensible explanations under unknown perturbations. To address this\nchallenge, we propose AGAIN, a fActor GrAph-based Interpretable neural Network,\nwhich is capable of generating comprehensible explanations under unknown\nperturbations. Instead of retraining like previous solutions, the proposed\nAGAIN directly integrates logical rules by which logical errors in explanations\nare identified and rectified during inference. Specifically, we construct the\nfactor graph to express logical rules between explanations and categories. By\ntreating logical rules as exogenous knowledge, AGAIN can identify\nincomprehensible explanations that violate real-world logic. Furthermore, we\npropose an interactive intervention switch strategy rectifying explanations\nbased on the logical guidance from the factor graph without learning\nperturbations, which overcomes the inherent limitation of adversarial\ntraining-based methods in defending only against known perturbations.\nAdditionally, we theoretically demonstrate the effectiveness of employing\nfactor graph by proving that the comprehensibility of explanations is strongly\ncorrelated with factor graph. Extensive experiments are conducted on three\ndatasets and experimental results illustrate the superior performance of AGAIN\ncompared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensible neural network explanations are foundations for a better\nunderstanding of decisions, especially when the input data are infused with\nmalicious perturbations. Existing solutions generally mitigate the impact of\nperturbations through adversarial training, yet they fail to generate\ncomprehensible explanations under unknown perturbations. To address this\nchallenge, we propose AGAIN, a fActor GrAph-based Interpretable neural Network,\nwhich is capable of generating comprehensible explanations under unknown\nperturbations. Instead of retraining like previous solutions, the proposed\nAGAIN directly integrates logical rules by which logical errors in explanations\nare identified and rectified during inference. Specifically, we construct the\nfactor graph to express logical rules between explanations and categories. By\ntreating logical rules as exogenous knowledge, AGAIN can identify\nincomprehensible explanations that violate real-world logic. Furthermore, we\npropose an interactive intervention switch strategy rectifying explanations\nbased on the logical guidance from the factor graph without learning\nperturbations, which overcomes the inherent limitation of adversarial\ntraining-based methods in defending only against known perturbations.\nAdditionally, we theoretically demonstrate the effectiveness of employing\nfactor graph by proving that the comprehensibility of explanations is strongly\ncorrelated with factor graph. Extensive experiments are conducted on three\ndatasets and experimental results illustrate the superior performance of AGAIN\ncompared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Yicong Li"
                    },
                    {
                        "name": "Kuanjiu Zhou"
                    },
                    {
                        "name": "Shuo Yu"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Renqiang Luo"
                    },
                    {
                        "name": "Xiaodong Li"
                    },
                    {
                        "name": "Feng Xia"
                    }
                ],
                "author_detail": {
                    "name": "Feng Xia"
                },
                "author": "Feng Xia",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14565v1",
                "updated": "2025-02-20T13:50:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    50,
                    2,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T13:50:02Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    50,
                    2,
                    3,
                    51,
                    0
                ],
                "title": "ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification"
                },
                "summary": "Self-awareness, i.e., the ability to assess and correct one's own generation,\nis a fundamental aspect of human intelligence, making its replication in large\nlanguage models (LLMs) an important yet challenging task. Previous works tackle\nthis by employing extensive reinforcement learning or rather relying on large\nexternal verifiers. In this work, we propose Refine via Intrinsic\nSelf-Verification (ReVISE), an efficient and effective framework that enables\nLLMs to self-correct their outputs through self-verification. The core idea of\nReVISE is to enable LLMs to verify their reasoning processes and continually\nrethink reasoning trajectories based on its verification. We introduce a\nstructured curriculum based upon online preference learning to implement this\nefficiently. Specifically, as ReVISE involves two challenging tasks (i.e.,\nself-verification and reasoning correction), we tackle each task sequentially\nusing curriculum learning, collecting both failed and successful reasoning\npaths to construct preference pairs for efficient training. During inference,\nour approach enjoys natural test-time scaling by integrating self-verification\nand correction capabilities, further enhanced by our proposed confidence-aware\ndecoding mechanism. Our experiments on various reasoning tasks demonstrate that\nReVISE achieves efficient self-correction and significantly improves reasoning\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-awareness, i.e., the ability to assess and correct one's own generation,\nis a fundamental aspect of human intelligence, making its replication in large\nlanguage models (LLMs) an important yet challenging task. Previous works tackle\nthis by employing extensive reinforcement learning or rather relying on large\nexternal verifiers. In this work, we propose Refine via Intrinsic\nSelf-Verification (ReVISE), an efficient and effective framework that enables\nLLMs to self-correct their outputs through self-verification. The core idea of\nReVISE is to enable LLMs to verify their reasoning processes and continually\nrethink reasoning trajectories based on its verification. We introduce a\nstructured curriculum based upon online preference learning to implement this\nefficiently. Specifically, as ReVISE involves two challenging tasks (i.e.,\nself-verification and reasoning correction), we tackle each task sequentially\nusing curriculum learning, collecting both failed and successful reasoning\npaths to construct preference pairs for efficient training. During inference,\nour approach enjoys natural test-time scaling by integrating self-verification\nand correction capabilities, further enhanced by our proposed confidence-aware\ndecoding mechanism. Our experiments on various reasoning tasks demonstrate that\nReVISE achieves efficient self-correction and significantly improves reasoning\nperformance."
                },
                "authors": [
                    {
                        "name": "Hyunseok Lee"
                    },
                    {
                        "name": "Seunghyuk Oh"
                    },
                    {
                        "name": "Jaehyung Kim"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "Jihoon Tack"
                    }
                ],
                "author_detail": {
                    "name": "Jihoon Tack"
                },
                "author": "Jihoon Tack",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14563v1",
                "updated": "2025-02-20T13:47:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    47,
                    51,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T13:47:51Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    47,
                    51,
                    3,
                    51,
                    0
                ],
                "title": "Plan-over-Graph: Towards Parallelable LLM Agent Schedule",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plan-over-Graph: Towards Parallelable LLM Agent Schedule"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional abilities in\nreasoning for task planning. However, challenges remain under-explored for\nparallel schedules. This paper introduces a novel paradigm, plan-over-graph, in\nwhich the model first decomposes a real-life textual task into executable\nsubtasks and constructs an abstract task graph. The model then understands this\ntask graph as input and generates a plan for parallel execution. To enhance the\nplanning capability of complex, scalable graphs, we design an automated and\ncontrollable pipeline to generate synthetic graphs and propose a two-stage\ntraining scheme. Experimental results show that our plan-over-graph method\nsignificantly improves task performance on both API-based LLMs and trainable\nopen-sourced LLMs. By normalizing complex tasks as graphs, our method naturally\nsupports parallel execution, demonstrating global efficiency. The code and data\nare available at https://github.com/zsq259/Plan-over-Graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional abilities in\nreasoning for task planning. However, challenges remain under-explored for\nparallel schedules. This paper introduces a novel paradigm, plan-over-graph, in\nwhich the model first decomposes a real-life textual task into executable\nsubtasks and constructs an abstract task graph. The model then understands this\ntask graph as input and generates a plan for parallel execution. To enhance the\nplanning capability of complex, scalable graphs, we design an automated and\ncontrollable pipeline to generate synthetic graphs and propose a two-stage\ntraining scheme. Experimental results show that our plan-over-graph method\nsignificantly improves task performance on both API-based LLMs and trainable\nopen-sourced LLMs. By normalizing complex tasks as graphs, our method naturally\nsupports parallel execution, demonstrating global efficiency. The code and data\nare available at https://github.com/zsq259/Plan-over-Graph."
                },
                "authors": [
                    {
                        "name": "Shiqi Zhang"
                    },
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14561v1",
                "updated": "2025-02-20T13:45:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    45,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T13:45:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    45,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Can LLMs Predict Citation Intent? An Experimental Analysis of In-context\n  Learning and Fine-tuning on Open LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Predict Citation Intent? An Experimental Analysis of In-context\n  Learning and Fine-tuning on Open LLMs"
                },
                "summary": "This work investigates the ability of open Large Language Models (LLMs) to\npredict citation intent through in-context learning and fine-tuning. Unlike\ntraditional approaches that rely on pre-trained models like SciBERT, which\nrequire extensive domain-specific pretraining and specialized architectures, we\ndemonstrate that general-purpose LLMs can be adapted to this task with minimal\ntask-specific data. We evaluate twelve model variations across five prominent\nopen LLM families using zero, one, few, and many-shot prompting to assess\nperformance across scenarios. Our experimental study identifies the\ntop-performing model through extensive experimentation of in-context\nlearning-related parameters, which we fine-tune to further enhance task\nperformance. The results highlight the strengths and limitations of LLMs in\nrecognizing citation intents, providing valuable insights for model selection\nand prompt engineering. Additionally, we make our end-to-end evaluation\nframework and models openly available for future use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work investigates the ability of open Large Language Models (LLMs) to\npredict citation intent through in-context learning and fine-tuning. Unlike\ntraditional approaches that rely on pre-trained models like SciBERT, which\nrequire extensive domain-specific pretraining and specialized architectures, we\ndemonstrate that general-purpose LLMs can be adapted to this task with minimal\ntask-specific data. We evaluate twelve model variations across five prominent\nopen LLM families using zero, one, few, and many-shot prompting to assess\nperformance across scenarios. Our experimental study identifies the\ntop-performing model through extensive experimentation of in-context\nlearning-related parameters, which we fine-tune to further enhance task\nperformance. The results highlight the strengths and limitations of LLMs in\nrecognizing citation intents, providing valuable insights for model selection\nand prompt engineering. Additionally, we make our end-to-end evaluation\nframework and models openly available for future use."
                },
                "authors": [
                    {
                        "name": "Paris Koloveas"
                    },
                    {
                        "name": "Serafeim Chatzopoulos"
                    },
                    {
                        "name": "Thanasis Vergoulis"
                    },
                    {
                        "name": "Christos Tryfonopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Christos Tryfonopoulos"
                },
                "author": "Christos Tryfonopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14560v1",
                "updated": "2025-02-20T13:45:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    45,
                    17,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T13:45:17Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    45,
                    17,
                    3,
                    51,
                    0
                ],
                "title": "Less is More: Improving LLM Alignment via Preference Data Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Improving LLM Alignment via Preference Data Selection"
                },
                "summary": "Direct Preference Optimization (DPO) has emerged as a promising approach for\naligning large language models with human preferences. While prior work mainly\nextends DPO from the aspect of the objective function, we instead improve DPO\nfrom the largely overlooked but critical aspect of data selection.\nSpecifically, we address the issue of parameter shrinkage caused by noisy data\nby proposing a novel margin-maximization principle for dataset curation in DPO\ntraining. To accurately estimate margins for data selection, we propose a\ndual-margin guided approach that considers both external reward margins and\nimplicit DPO reward margins. Extensive experiments demonstrate that our method\nreduces computational cost dramatically while improving performance.\nRemarkably, by using just 10\\% of the Ultrafeedback dataset, our approach\nachieves 3\\% to 8\\% improvements across various Llama and Mistral series models\non the AlpacaEval 2.0 benchmark. Furthermore, our approach seamlessly extends\nto iterative DPO, yielding a roughly 3\\% improvement with 25\\% online data,\nwhile further reducing training time. These results highlight the potential of\ndata selection strategies for advancing preference optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has emerged as a promising approach for\naligning large language models with human preferences. While prior work mainly\nextends DPO from the aspect of the objective function, we instead improve DPO\nfrom the largely overlooked but critical aspect of data selection.\nSpecifically, we address the issue of parameter shrinkage caused by noisy data\nby proposing a novel margin-maximization principle for dataset curation in DPO\ntraining. To accurately estimate margins for data selection, we propose a\ndual-margin guided approach that considers both external reward margins and\nimplicit DPO reward margins. Extensive experiments demonstrate that our method\nreduces computational cost dramatically while improving performance.\nRemarkably, by using just 10\\% of the Ultrafeedback dataset, our approach\nachieves 3\\% to 8\\% improvements across various Llama and Mistral series models\non the AlpacaEval 2.0 benchmark. Furthermore, our approach seamlessly extends\nto iterative DPO, yielding a roughly 3\\% improvement with 25\\% online data,\nwhile further reducing training time. These results highlight the potential of\ndata selection strategies for advancing preference optimization."
                },
                "authors": [
                    {
                        "name": "Xun Deng"
                    },
                    {
                        "name": "Han Zhong"
                    },
                    {
                        "name": "Rui Ai"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14541v1",
                "updated": "2025-02-20T13:20:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    20,
                    19,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T13:20:19Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    20,
                    19,
                    3,
                    51,
                    0
                ],
                "title": "LLM-based User Profile Management for Recommender System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based User Profile Management for Recommender System"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has opened new\nopportunities in recommender systems by enabling zero-shot recommendation\nwithout conventional training. Despite their potential, most existing works\nrely solely on users' purchase histories, leaving significant room for\nimprovement by incorporating user-generated textual data, such as reviews and\nproduct descriptions. Addressing this gap, we propose PURE, a novel LLM-based\nrecommendation framework that builds and maintains evolving user profiles by\nsystematically extracting and summarizing key information from user reviews.\nPURE consists of three core components: a Review Extractor for identifying user\npreferences and key product features, a Profile Updater for refining and\nupdating user profiles, and a Recommender for generating personalized\nrecommendations using the most current profile. To evaluate PURE, we introduce\na continuous sequential recommendation task that reflects real-world scenarios\nby adding reviews over time and updating predictions incrementally. Our\nexperimental results on Amazon datasets demonstrate that PURE outperforms\nexisting LLM-based methods, effectively leveraging long-term user information\nwhile managing token limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has opened new\nopportunities in recommender systems by enabling zero-shot recommendation\nwithout conventional training. Despite their potential, most existing works\nrely solely on users' purchase histories, leaving significant room for\nimprovement by incorporating user-generated textual data, such as reviews and\nproduct descriptions. Addressing this gap, we propose PURE, a novel LLM-based\nrecommendation framework that builds and maintains evolving user profiles by\nsystematically extracting and summarizing key information from user reviews.\nPURE consists of three core components: a Review Extractor for identifying user\npreferences and key product features, a Profile Updater for refining and\nupdating user profiles, and a Recommender for generating personalized\nrecommendations using the most current profile. To evaluate PURE, we introduce\na continuous sequential recommendation task that reflects real-world scenarios\nby adding reviews over time and updating predictions incrementally. Our\nexperimental results on Amazon datasets demonstrate that PURE outperforms\nexisting LLM-based methods, effectively leveraging long-term user information\nwhile managing token limitations."
                },
                "authors": [
                    {
                        "name": "Seunghwan Bang"
                    },
                    {
                        "name": "Hwanjun Song"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjun Song"
                },
                "author": "Hwanjun Song",
                "arxiv_comment": "Submitted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03293v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03293v4",
                "updated": "2025-02-20T13:17:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    17,
                    1,
                    3,
                    51,
                    0
                ],
                "published": "2024-06-05T14:02:31Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    14,
                    2,
                    31,
                    2,
                    157,
                    0
                ],
                "title": "Text-to-Image Rectified Flow as Plug-and-Play Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Image Rectified Flow as Plug-and-Play Priors"
                },
                "summary": "Large-scale diffusion models have achieved remarkable performance in\ngenerative tasks. Beyond their initial training applications, these models have\nproven their ability to function as versatile plug-and-play priors. For\ninstance, 2D diffusion models can serve as loss functions to optimize 3D\nimplicit models. Rectified flow, a novel class of generative models, enforces a\nlinear progression from the source to the target distribution and has\ndemonstrated superior performance across various domains. Compared to\ndiffusion-based methods, rectified flow approaches surpass in terms of\ngeneration quality and efficiency, requiring fewer inference steps. In this\nwork, we present theoretical and experimental evidence demonstrating that\nrectified flow based methods offer similar functionalities to diffusion models\n- they can also serve as effective priors. Besides the generative capabilities\nof diffusion priors, motivated by the unique time-symmetry properties of\nrectified flow models, a variant of our method can additionally perform image\ninversion. Experimentally, our rectified flow-based priors outperform their\ndiffusion counterparts - the SDS and VSD losses - in text-to-3D generation. Our\nmethod also displays competitive performance in image inversion and editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale diffusion models have achieved remarkable performance in\ngenerative tasks. Beyond their initial training applications, these models have\nproven their ability to function as versatile plug-and-play priors. For\ninstance, 2D diffusion models can serve as loss functions to optimize 3D\nimplicit models. Rectified flow, a novel class of generative models, enforces a\nlinear progression from the source to the target distribution and has\ndemonstrated superior performance across various domains. Compared to\ndiffusion-based methods, rectified flow approaches surpass in terms of\ngeneration quality and efficiency, requiring fewer inference steps. In this\nwork, we present theoretical and experimental evidence demonstrating that\nrectified flow based methods offer similar functionalities to diffusion models\n- they can also serve as effective priors. Besides the generative capabilities\nof diffusion priors, motivated by the unique time-symmetry properties of\nrectified flow models, a variant of our method can additionally perform image\ninversion. Experimentally, our rectified flow-based priors outperform their\ndiffusion counterparts - the SDS and VSD losses - in text-to-3D generation. Our\nmethod also displays competitive performance in image inversion and editing."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Xulei Yang"
                    },
                    {
                        "name": "Fayao Liu"
                    },
                    {
                        "name": "Guosheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Guosheng Lin"
                },
                "author": "Guosheng Lin",
                "arxiv_comment": "ICLR 2025 Camera Ready. Code:\n  https://github.com/yangxiaofeng/rectified_flow_prior",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03293v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03293v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v1",
                "updated": "2025-02-20T18:59:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14860v1",
                "updated": "2025-02-20T18:59:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:59:31Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    31,
                    3,
                    51,
                    0
                ],
                "title": "Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning"
                },
                "summary": "Large language models (LLMs) often fail to ask effective questions under\nuncertainty, making them unreliable in domains where proactive\ninformation-gathering is essential for decisionmaking. We present ALFA, a\nframework that improves LLM question-asking by (i) decomposing the notion of a\n\"good\" question into a set of theory-grounded attributes (e.g., clarity,\nrelevance), (ii) controllably synthesizing attribute-specific question\nvariations, and (iii) aligning models via preference-based optimization to\nexplicitly learn to ask better questions along these fine-grained attributes.\nFocusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs\ndataset, composed of 17k real-world clinical interactions augmented with 80k\nattribute-specific preference pairs of follow-up questions, as well as a novel\nexpert-annotated interactive healthcare QA task to evaluate question-asking\nabilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on\nMediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level\nwin-rate of 64.4% and strong generalizability. Our findings suggest that\nexplicitly guiding question-asking with structured, fine-grained attributes\noffers a scalable path to improve LLMs, especially in expert application\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often fail to ask effective questions under\nuncertainty, making them unreliable in domains where proactive\ninformation-gathering is essential for decisionmaking. We present ALFA, a\nframework that improves LLM question-asking by (i) decomposing the notion of a\n\"good\" question into a set of theory-grounded attributes (e.g., clarity,\nrelevance), (ii) controllably synthesizing attribute-specific question\nvariations, and (iii) aligning models via preference-based optimization to\nexplicitly learn to ask better questions along these fine-grained attributes.\nFocusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs\ndataset, composed of 17k real-world clinical interactions augmented with 80k\nattribute-specific preference pairs of follow-up questions, as well as a novel\nexpert-annotated interactive healthcare QA task to evaluate question-asking\nabilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on\nMediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level\nwin-rate of 64.4% and strong generalizability. Our findings suggest that\nexplicitly guiding question-asking with structured, fine-grained attributes\noffers a scalable path to improve LLMs, especially in expert application\ndomains."
                },
                "authors": [
                    {
                        "name": "Shuyue Stella Li"
                    },
                    {
                        "name": "Jimin Mun"
                    },
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Jonathan S. Ilgen"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "22 pages, 8 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14856v1",
                "updated": "2025-02-20T18:58:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    58,
                    10,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:58:10Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    58,
                    10,
                    3,
                    51,
                    0
                ],
                "title": "FR-Spec: Accelerating Large-Vocabulary Language Models via\n  Frequency-Ranked Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FR-Spec: Accelerating Large-Vocabulary Language Models via\n  Frequency-Ranked Speculative Sampling"
                },
                "summary": "Speculative sampling has emerged as an important technique for accelerating\nthe auto-regressive generation process of large language models (LLMs) by\nutilizing a draft-then-verify mechanism to produce multiple tokens per forward\npass. While state-of-the-art speculative sampling methods use only a single\nlayer and a language modeling (LM) head as the draft model to achieve\nimpressive layer compression, their efficiency gains are substantially reduced\nfor large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.\nTo address this, we present FR-Spec, a frequency-ranked speculative sampling\nframework that optimizes draft candidate selection through vocabulary space\ncompression. By constraining the draft search to a frequency-prioritized token\nsubset, our method reduces LM Head computation overhead by 75% while ensuring\nthe equivalence of the final output distribution. Experiments across multiple\ndatasets demonstrate an average of 1.12$\\times$ speedup over the\nstate-of-the-art speculative sampling method EAGLE-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling has emerged as an important technique for accelerating\nthe auto-regressive generation process of large language models (LLMs) by\nutilizing a draft-then-verify mechanism to produce multiple tokens per forward\npass. While state-of-the-art speculative sampling methods use only a single\nlayer and a language modeling (LM) head as the draft model to achieve\nimpressive layer compression, their efficiency gains are substantially reduced\nfor large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.\nTo address this, we present FR-Spec, a frequency-ranked speculative sampling\nframework that optimizes draft candidate selection through vocabulary space\ncompression. By constraining the draft search to a frequency-prioritized token\nsubset, our method reduces LM Head computation overhead by 75% while ensuring\nthe equivalence of the final output distribution. Experiments across multiple\ndatasets demonstrate an average of 1.12$\\times$ speedup over the\nstate-of-the-art speculative sampling method EAGLE-2."
                },
                "authors": [
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Tengyu Pan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yudi Zhang"
                    },
                    {
                        "name": "Ao Sun"
                    },
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Kaihuo Zhang"
                    },
                    {
                        "name": "Weilun Zhao"
                    },
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14855v1",
                "updated": "2025-02-20T18:58:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    58,
                    7,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:58:07Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    58,
                    7,
                    3,
                    51,
                    0
                ],
                "title": "Prompt-to-Leaderboard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-to-Leaderboard"
                },
                "summary": "Large language model (LLM) evaluations typically rely on aggregated metrics\nlike accuracy or human preference, averaging across users and prompts. This\naveraging obscures user- and prompt-specific variations in model performance.\nTo address this, we propose Prompt-to-Leaderboard (P2L), a method that produces\nleaderboards specific to a prompt. The core idea is to train an LLM taking\nnatural language prompts as input to output a vector of Bradley-Terry\ncoefficients which are then used to predict the human preference vote. The\nresulting prompt-dependent leaderboards allow for unsupervised task-specific\nevaluation, optimal routing of queries to models, personalization, and\nautomated evaluation of model strengths and weaknesses. Data from Chatbot Arena\nsuggest that P2L better captures the nuanced landscape of language model\nperformance than the averaged leaderboard. Furthermore, our findings suggest\nthat P2L's ability to produce prompt-specific evaluations follows a power law\nscaling similar to that observed in LLMs themselves. In January 2025, the\nrouter we trained based on this methodology achieved the \\#1 spot in the\nChatbot Arena leaderboard. Our code is available at this GitHub link:\nhttps://github.com/lmarena/p2l.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) evaluations typically rely on aggregated metrics\nlike accuracy or human preference, averaging across users and prompts. This\naveraging obscures user- and prompt-specific variations in model performance.\nTo address this, we propose Prompt-to-Leaderboard (P2L), a method that produces\nleaderboards specific to a prompt. The core idea is to train an LLM taking\nnatural language prompts as input to output a vector of Bradley-Terry\ncoefficients which are then used to predict the human preference vote. The\nresulting prompt-dependent leaderboards allow for unsupervised task-specific\nevaluation, optimal routing of queries to models, personalization, and\nautomated evaluation of model strengths and weaknesses. Data from Chatbot Arena\nsuggest that P2L better captures the nuanced landscape of language model\nperformance than the averaged leaderboard. Furthermore, our findings suggest\nthat P2L's ability to produce prompt-specific evaluations follows a power law\nscaling similar to that observed in LLMs themselves. In January 2025, the\nrouter we trained based on this methodology achieved the \\#1 spot in the\nChatbot Arena leaderboard. Our code is available at this GitHub link:\nhttps://github.com/lmarena/p2l."
                },
                "authors": [
                    {
                        "name": "Evan Frick"
                    },
                    {
                        "name": "Connor Chen"
                    },
                    {
                        "name": "Joseph Tennyson"
                    },
                    {
                        "name": "Tianle Li"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Anastasios N. Angelopoulos"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14854v1",
                "updated": "2025-02-20T18:58:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    58,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:58:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    58,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "CLIPPER: Compression enables long-context synthetic data generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIPPER: Compression enables long-context synthetic data generation"
                },
                "summary": "LLM developers are increasingly reliant on synthetic data, but generating\nhigh-quality data for complex long-context reasoning tasks remains challenging.\nWe introduce CLIPPER, a compression-based approach for generating synthetic\ndata tailored to narrative claim verification - a task that requires reasoning\nover a book to verify a given claim. Instead of generating claims directly from\nthe raw text of the book, which results in artifact-riddled claims, CLIPPER\nfirst compresses the book into chapter outlines and book summaries and then\nuses these intermediate representations to generate complex claims and\ncorresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces\nclaims that are more valid, grounded, and complex. Using CLIPPER, we construct\na dataset of 19K synthetic book claims paired with their source texts and\nchain-of-thought reasoning, and use it to fine-tune three open-weight models.\nOur best model achieves breakthrough results on narrative claim verification\n(from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for\nsub-10B models on the NoCha leaderboard. Further analysis shows that our models\ngenerate more detailed and grounded chain-of-thought reasoning while also\nimproving performance on other narrative understanding tasks (e.g.,\nNarrativeQA).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM developers are increasingly reliant on synthetic data, but generating\nhigh-quality data for complex long-context reasoning tasks remains challenging.\nWe introduce CLIPPER, a compression-based approach for generating synthetic\ndata tailored to narrative claim verification - a task that requires reasoning\nover a book to verify a given claim. Instead of generating claims directly from\nthe raw text of the book, which results in artifact-riddled claims, CLIPPER\nfirst compresses the book into chapter outlines and book summaries and then\nuses these intermediate representations to generate complex claims and\ncorresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces\nclaims that are more valid, grounded, and complex. Using CLIPPER, we construct\na dataset of 19K synthetic book claims paired with their source texts and\nchain-of-thought reasoning, and use it to fine-tune three open-weight models.\nOur best model achieves breakthrough results on narrative claim verification\n(from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for\nsub-10B models on the NoCha leaderboard. Further analysis shows that our models\ngenerate more detailed and grounded chain-of-thought reasoning while also\nimproving performance on other narrative understanding tasks (e.g.,\nNarrativeQA)."
                },
                "authors": [
                    {
                        "name": "Chau Minh Pham"
                    },
                    {
                        "name": "Yapei Chang"
                    },
                    {
                        "name": "Mohit Iyyer"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Iyyer"
                },
                "author": "Mohit Iyyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14848v1",
                "updated": "2025-02-20T18:56:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    56,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:56:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    56,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "GATE: Graph-based Adaptive Tool Evolution Across Diverse Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Graph-based Adaptive Tool Evolution Across Diverse Tasks"
                },
                "summary": "Large Language Models (LLMs) have shown great promise in tool-making, yet\nexisting frameworks often struggle to efficiently construct reliable toolsets\nand are limited to single-task settings. To address these challenges, we\npropose GATE (Graph-based Adaptive Tool Evolution), an adaptive framework that\ndynamically constructs and evolves a hierarchical graph of reusable tools\nacross multiple scenarios. We evaluate GATE on open-ended tasks (Minecraft),\nagent-based tasks (TextCraft, DABench), and code generation tasks (MATH, Date,\nTabMWP). Our results show that GATE achieves up to 4.3x faster milestone\ncompletion in Minecraft compared to the previous SOTA, and provides an average\nimprovement of 9.23% over existing tool-making methods in code generation tasks\nand 10.03% in agent tasks. GATE demonstrates the power of adaptive evolution,\nbalancing tool quantity, complexity, and functionality while maintaining high\nefficiency. Code and data are available at\n\\url{https://github.com/ayanami2003/GATE}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown great promise in tool-making, yet\nexisting frameworks often struggle to efficiently construct reliable toolsets\nand are limited to single-task settings. To address these challenges, we\npropose GATE (Graph-based Adaptive Tool Evolution), an adaptive framework that\ndynamically constructs and evolves a hierarchical graph of reusable tools\nacross multiple scenarios. We evaluate GATE on open-ended tasks (Minecraft),\nagent-based tasks (TextCraft, DABench), and code generation tasks (MATH, Date,\nTabMWP). Our results show that GATE achieves up to 4.3x faster milestone\ncompletion in Minecraft compared to the previous SOTA, and provides an average\nimprovement of 9.23% over existing tool-making methods in code generation tasks\nand 10.03% in agent tasks. GATE demonstrates the power of adaptive evolution,\nbalancing tool quantity, complexity, and functionality while maintaining high\nefficiency. Code and data are available at\n\\url{https://github.com/ayanami2003/GATE}."
                },
                "authors": [
                    {
                        "name": "Jianwen Luo"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Jinxiang Meng"
                    },
                    {
                        "name": "Fangyu Lei"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Shanshan Jiang"
                    },
                    {
                        "name": "Bin Dong"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "arxiv_comment": "8 pages of main text, 38 pages of appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14847v1",
                "updated": "2025-02-20T18:55:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    55,
                    39,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:55:39Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    55,
                    39,
                    3,
                    51,
                    0
                ],
                "title": "Red-Teaming LLM Multi-Agent Systems via Communication Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red-Teaming LLM Multi-Agent Systems via Communication Attacks"
                },
                "summary": "Large Language Model-based Multi-Agent Systems (LLM-MAS) have revolutionized\ncomplex problem-solving capability by enabling sophisticated agent\ncollaboration through message-based communications. While the communication\nframework is crucial for agent coordination, it also introduces a critical yet\nunexplored security vulnerability. In this work, we introduce\nAgent-in-the-Middle (AiTM), a novel attack that exploits the fundamental\ncommunication mechanisms in LLM-MAS by intercepting and manipulating\ninter-agent messages. Unlike existing attacks that compromise individual\nagents, AiTM demonstrates how an adversary can compromise entire multi-agent\nsystems by only manipulating the messages passing between agents. To enable the\nattack under the challenges of limited control and role-restricted\ncommunication format, we develop an LLM-powered adversarial agent with a\nreflection mechanism that generates contextually-aware malicious instructions.\nOur comprehensive evaluation across various frameworks, communication\nstructures, and real-world applications demonstrates that LLM-MAS is vulnerable\nto communication-based attacks, highlighting the need for robust security\nmeasures in multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Multi-Agent Systems (LLM-MAS) have revolutionized\ncomplex problem-solving capability by enabling sophisticated agent\ncollaboration through message-based communications. While the communication\nframework is crucial for agent coordination, it also introduces a critical yet\nunexplored security vulnerability. In this work, we introduce\nAgent-in-the-Middle (AiTM), a novel attack that exploits the fundamental\ncommunication mechanisms in LLM-MAS by intercepting and manipulating\ninter-agent messages. Unlike existing attacks that compromise individual\nagents, AiTM demonstrates how an adversary can compromise entire multi-agent\nsystems by only manipulating the messages passing between agents. To enable the\nattack under the challenges of limited control and role-restricted\ncommunication format, we develop an LLM-powered adversarial agent with a\nreflection mechanism that generates contextually-aware malicious instructions.\nOur comprehensive evaluation across various frameworks, communication\nstructures, and real-world applications demonstrates that LLM-MAS is vulnerable\nto communication-based attacks, highlighting the need for robust security\nmeasures in multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Pengfei He"
                    },
                    {
                        "name": "Yupin Lin"
                    },
                    {
                        "name": "Shen Dong"
                    },
                    {
                        "name": "Han Xu"
                    },
                    {
                        "name": "Yue Xing"
                    },
                    {
                        "name": "Hui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Liu"
                },
                "author": "Hui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14846v1",
                "updated": "2025-02-20T18:55:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    55,
                    30,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:55:30Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    55,
                    30,
                    3,
                    51,
                    0
                ],
                "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data Generation"
                },
                "summary": "Reasoning about images with rich text, such as charts and documents, is a\ncritical application of vision-language models (VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically create synthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-quality instruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-language instruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developing multimodal agents capable of acting in real-world\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning about images with rich text, such as charts and documents, is a\ncritical application of vision-language models (VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically create synthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-quality instruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-language instruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developing multimodal agents capable of acting in real-world\nenvironments."
                },
                "authors": [
                    {
                        "name": "Yue Yang"
                    },
                    {
                        "name": "Ajay Patel"
                    },
                    {
                        "name": "Matt Deitke"
                    },
                    {
                        "name": "Tanmay Gupta"
                    },
                    {
                        "name": "Luca Weihs"
                    },
                    {
                        "name": "Andrew Head"
                    },
                    {
                        "name": "Mark Yatskar"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Aniruddha Kembhavi"
                    },
                    {
                        "name": "Christopher Clark"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Clark"
                },
                "author": "Christopher Clark",
                "arxiv_comment": "20 pages, 19 figures, 9 tables, website:\n  https://yueyang1996.github.io/cosyn/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14838v1",
                "updated": "2025-02-20T18:51:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    51,
                    12,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:51:12Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    51,
                    12,
                    3,
                    51,
                    0
                ],
                "title": "Revealing and Mitigating Over-Attention in Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing and Mitigating Over-Attention in Knowledge Editing"
                },
                "summary": "Large Language Models have demonstrated superior performance across a wide\nrange of tasks, but they still exhibit undesirable errors due to incorrect\nknowledge learned from the training data. To avoid this, knowledge editing\nmethods emerged to precisely edit the specific model knowledge via efficiently\nmodifying a very small percentage of parameters. % However, those methods can\nlead to the problem of Specificity Failure: when the content related to the\nedited knowledge occurs in the context, it can inadvertently corrupt other\npre-existing knowledge. However, those methods can lead to the problem of\nSpecificity Failure, where the existing knowledge and capabilities are severely\ndegraded due to editing. Our preliminary indicates that Specificity Failure\nprimarily stems from the model's attention heads assigning excessive attention\nscores to entities related to the edited knowledge, thereby unduly focusing on\nspecific snippets within the context, which we denote as the Attention Drift\nphenomenon. To mitigate such Attention Drift issue, we introduce a simple yet\neffective method Selective Attention Drift Restriction}(SADR), which introduces\nan additional regularization term during the knowledge editing process to\nrestrict changes in the attention weight distribution, thereby preventing undue\nfocus on the edited entity. Experiments on five frequently used strong LLMs\ndemonstrate the effectiveness of our method, where SADR can significantly\nmitigate Specificity Failure in the predominant knowledge editing tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have demonstrated superior performance across a wide\nrange of tasks, but they still exhibit undesirable errors due to incorrect\nknowledge learned from the training data. To avoid this, knowledge editing\nmethods emerged to precisely edit the specific model knowledge via efficiently\nmodifying a very small percentage of parameters. % However, those methods can\nlead to the problem of Specificity Failure: when the content related to the\nedited knowledge occurs in the context, it can inadvertently corrupt other\npre-existing knowledge. However, those methods can lead to the problem of\nSpecificity Failure, where the existing knowledge and capabilities are severely\ndegraded due to editing. Our preliminary indicates that Specificity Failure\nprimarily stems from the model's attention heads assigning excessive attention\nscores to entities related to the edited knowledge, thereby unduly focusing on\nspecific snippets within the context, which we denote as the Attention Drift\nphenomenon. To mitigate such Attention Drift issue, we introduce a simple yet\neffective method Selective Attention Drift Restriction}(SADR), which introduces\nan additional regularization term during the knowledge editing process to\nrestrict changes in the attention weight distribution, thereby preventing undue\nfocus on the edited entity. Experiments on five frequently used strong LLMs\ndemonstrate the effectiveness of our method, where SADR can significantly\nmitigate Specificity Failure in the predominant knowledge editing tasks."
                },
                "authors": [
                    {
                        "name": "Pinzheng Wang"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Keyan Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qiaoming Zhu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v1",
                "updated": "2025-02-20T18:50:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07752v2",
                "updated": "2025-02-20T18:48:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    48,
                    58,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-11T18:27:19Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    27,
                    19,
                    1,
                    42,
                    0
                ],
                "title": "Towards Efficient Optimizer Design for LLM via Structured Fisher\n  Approximation with a Low-Rank Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Optimizer Design for LLM via Structured Fisher\n  Approximation with a Low-Rank Extension"
                },
                "summary": "Designing efficient optimizers for large language models (LLMs) with\nlow-memory requirements and fast convergence is an important and challenging\nproblem. This paper makes a step towards the systematic design of such\noptimizers through the lens of structured Fisher information matrix (FIM)\napproximation. We show that many state-of-the-art efficient optimizers can be\nviewed as solutions to FIM approximation (under the Frobenius norm) with\nspecific structural assumptions. Building on these insights, we propose two\ndesign recommendations of practical efficient optimizers for LLMs, involving\nthe careful selection of structural assumptions to balance generality and\nefficiency, and enhancing memory efficiency of optimizers with general\nstructures through a novel low-rank extension framework. We demonstrate how to\nuse each design approach by deriving new memory-efficient optimizers: Row and\nColumn Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation\n(Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the\neffectiveness, showing faster and better convergence than existing\nmemory-efficient baselines and Adam with little memory overhead. Notably, Alice\nachieves better than 2x faster convergence over Adam, while RACS delivers\nstrong performance on the 1B model with SGD-like memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing efficient optimizers for large language models (LLMs) with\nlow-memory requirements and fast convergence is an important and challenging\nproblem. This paper makes a step towards the systematic design of such\noptimizers through the lens of structured Fisher information matrix (FIM)\napproximation. We show that many state-of-the-art efficient optimizers can be\nviewed as solutions to FIM approximation (under the Frobenius norm) with\nspecific structural assumptions. Building on these insights, we propose two\ndesign recommendations of practical efficient optimizers for LLMs, involving\nthe careful selection of structural assumptions to balance generality and\nefficiency, and enhancing memory efficiency of optimizers with general\nstructures through a novel low-rank extension framework. We demonstrate how to\nuse each design approach by deriving new memory-efficient optimizers: Row and\nColumn Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation\n(Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the\neffectiveness, showing faster and better convergence than existing\nmemory-efficient baselines and Adam with little memory overhead. Notably, Alice\nachieves better than 2x faster convergence over Adam, while RACS delivers\nstrong performance on the 1B model with SGD-like memory."
                },
                "authors": [
                    {
                        "name": "Wenbo Gong"
                    },
                    {
                        "name": "Meyer Scetbon"
                    },
                    {
                        "name": "Chao Ma"
                    },
                    {
                        "name": "Edward Meeds"
                    }
                ],
                "author_detail": {
                    "name": "Edward Meeds"
                },
                "author": "Edward Meeds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13433v2",
                "updated": "2025-02-20T18:47:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    47,
                    24,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-19T05:07:56Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    5,
                    7,
                    56,
                    2,
                    50,
                    0
                ],
                "title": "MATS: An Audio Language Model under Text-only Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATS: An Audio Language Model under Text-only Supervision"
                },
                "summary": "Large audio-language models (LALMs), built upon powerful Large Language\nModels (LLMs), have exhibited remarkable audio comprehension and reasoning\ncapabilities. However, the training of LALMs demands a large corpus of\naudio-language pairs, which requires substantial costs in both data collection\nand training resources. In this paper, we propose MATS, an audio-language\nmultimodal LLM designed to handle Multiple Audio task using solely Text-only\nSupervision. By leveraging pre-trained audio-language alignment models such as\nCLAP, we develop a text-only training strategy that projects the shared\naudio-language latent space into LLM latent space, endowing the LLM with audio\ncomprehension capabilities without relying on audio data during training. To\nfurther bridge the modality gap between audio and language embeddings within\nCLAP, we propose the Strongly-related noisy text with audio (Santa) mechanism.\nSanta maps audio embeddings into CLAP language embedding space while preserving\nessential information from the audio input. Extensive experiments demonstrate\nthat MATS, despite being trained exclusively on text data, achieves competitive\nperformance compared to recent LALMs trained on large-scale audio-language\npairs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large audio-language models (LALMs), built upon powerful Large Language\nModels (LLMs), have exhibited remarkable audio comprehension and reasoning\ncapabilities. However, the training of LALMs demands a large corpus of\naudio-language pairs, which requires substantial costs in both data collection\nand training resources. In this paper, we propose MATS, an audio-language\nmultimodal LLM designed to handle Multiple Audio task using solely Text-only\nSupervision. By leveraging pre-trained audio-language alignment models such as\nCLAP, we develop a text-only training strategy that projects the shared\naudio-language latent space into LLM latent space, endowing the LLM with audio\ncomprehension capabilities without relying on audio data during training. To\nfurther bridge the modality gap between audio and language embeddings within\nCLAP, we propose the Strongly-related noisy text with audio (Santa) mechanism.\nSanta maps audio embeddings into CLAP language embedding space while preserving\nessential information from the audio input. Extensive experiments demonstrate\nthat MATS, despite being trained exclusively on text data, achieves competitive\nperformance compared to recent LALMs trained on large-scale audio-language\npairs."
                },
                "authors": [
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Ruibing Hou"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Shiguang Shan"
                    },
                    {
                        "name": "Xilin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xilin Chen"
                },
                "author": "Xilin Chen",
                "arxiv_comment": "19 pages,11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14830v1",
                "updated": "2025-02-20T18:45:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    45,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:45:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    45,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in\n  Fine-Tuned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in\n  Fine-Tuned LLMs"
                },
                "summary": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align)."
                },
                "authors": [
                    {
                        "name": "Danni Liu"
                    },
                    {
                        "name": "Jan Niehues"
                    }
                ],
                "author_detail": {
                    "name": "Jan Niehues"
                },
                "author": "Jan Niehues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14828v1",
                "updated": "2025-02-20T18:45:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    45,
                    1,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:45:01Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    45,
                    1,
                    3,
                    51,
                    0
                ],
                "title": "Fundamental Limitations in Defending LLM Finetuning APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limitations in Defending LLM Finetuning APIs"
                },
                "summary": "LLM developers have imposed technical interventions to prevent fine-tuning\nmisuse attacks, attacks where adversaries evade safeguards by fine-tuning the\nmodel using a public API. Previous work has established several successful\nattacks against specific fine-tuning API defences. In this work, we show that\ndefences of fine-tuning APIs that seek to detect individual harmful training or\ninference samples ('pointwise' detection) are fundamentally limited in their\nability to prevent fine-tuning attacks. We construct 'pointwise-undetectable'\nattacks that repurpose entropy in benign model outputs (e.g. semantic or\nsyntactic variations) to covertly transmit dangerous knowledge. Our attacks are\ncomposed solely of unsuspicious benign samples that can be collected from the\nmodel before fine-tuning, meaning training and inference samples are all\nindividually benign and low-perplexity. We test our attacks against the OpenAI\nfine-tuning API, finding they succeed in eliciting answers to harmful\nmultiple-choice questions, and that they evade an enhanced monitoring system we\ndesign that successfully detects other fine-tuning attacks. We encourage the\ncommunity to develop defences that tackle the fundamental limitations we\nuncover in pointwise fine-tuning API defences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM developers have imposed technical interventions to prevent fine-tuning\nmisuse attacks, attacks where adversaries evade safeguards by fine-tuning the\nmodel using a public API. Previous work has established several successful\nattacks against specific fine-tuning API defences. In this work, we show that\ndefences of fine-tuning APIs that seek to detect individual harmful training or\ninference samples ('pointwise' detection) are fundamentally limited in their\nability to prevent fine-tuning attacks. We construct 'pointwise-undetectable'\nattacks that repurpose entropy in benign model outputs (e.g. semantic or\nsyntactic variations) to covertly transmit dangerous knowledge. Our attacks are\ncomposed solely of unsuspicious benign samples that can be collected from the\nmodel before fine-tuning, meaning training and inference samples are all\nindividually benign and low-perplexity. We test our attacks against the OpenAI\nfine-tuning API, finding they succeed in eliciting answers to harmful\nmultiple-choice questions, and that they evade an enhanced monitoring system we\ndesign that successfully detects other fine-tuning attacks. We encourage the\ncommunity to develop defences that tackle the fundamental limitations we\nuncover in pointwise fine-tuning API defences."
                },
                "authors": [
                    {
                        "name": "Xander Davies"
                    },
                    {
                        "name": "Eric Winsor"
                    },
                    {
                        "name": "Tomek Korbak"
                    },
                    {
                        "name": "Alexandra Souly"
                    },
                    {
                        "name": "Robert Kirk"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "Yarin Gal"
                    }
                ],
                "author_detail": {
                    "name": "Yarin Gal"
                },
                "author": "Yarin Gal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14822v1",
                "updated": "2025-02-20T18:42:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    42,
                    58,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:42:58Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    42,
                    58,
                    3,
                    51,
                    0
                ],
                "title": "A Survey of Model Architectures in Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Model Architectures in Information Retrieval"
                },
                "summary": "This survey examines the evolution of model architectures in information\nretrieval (IR), focusing on two key aspects: backbone models for feature\nextraction and end-to-end system architectures for relevance estimation. The\nreview intentionally separates architectural considerations from training\nmethodologies to provide a focused analysis of structural innovations in IR\nsystems.We trace the development from traditional term-based methods to modern\nneural approaches, particularly highlighting the impact of transformer-based\nmodels and subsequent large language models (LLMs). We conclude by discussing\nemerging challenges and future directions, including architectural\noptimizations for performance and scalability, handling of multimodal,\nmultilingual data, and adaptation to novel application domains beyond\ntraditional search paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey examines the evolution of model architectures in information\nretrieval (IR), focusing on two key aspects: backbone models for feature\nextraction and end-to-end system architectures for relevance estimation. The\nreview intentionally separates architectural considerations from training\nmethodologies to provide a focused analysis of structural innovations in IR\nsystems.We trace the development from traditional term-based methods to modern\nneural approaches, particularly highlighting the impact of transformer-based\nmodels and subsequent large language models (LLMs). We conclude by discussing\nemerging challenges and future directions, including architectural\noptimizations for performance and scalability, handling of multimodal,\nmultilingual data, and adaptation to novel application domains beyond\ntraditional search paradigms."
                },
                "authors": [
                    {
                        "name": "Zhichao Xu"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Crystina Zhang"
                    },
                    {
                        "name": "Puxuan Yu"
                    },
                    {
                        "name": "Bei Wang"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Vivek Srikumar"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Srikumar"
                },
                "author": "Vivek Srikumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04370v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04370v3",
                "updated": "2025-02-20T18:42:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    42,
                    41,
                    3,
                    51,
                    0
                ],
                "published": "2024-06-01T02:08:44Z",
                "published_parsed": [
                    2024,
                    6,
                    1,
                    2,
                    8,
                    44,
                    5,
                    153,
                    0
                ],
                "title": "Large Language Model Confidence Estimation via Black-Box Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Confidence Estimation via Black-Box Access"
                },
                "summary": "Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\\&A tasks as well\nas of Pegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\\&A tasks as well\nas of Pegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset."
                },
                "authors": [
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    },
                    {
                        "name": "Soumya Ghosh"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Prasanna Sattigeri"
                    }
                ],
                "author_detail": {
                    "name": "Prasanna Sattigeri"
                },
                "author": "Prasanna Sattigeri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04370v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04370v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14820v1",
                "updated": "2025-02-20T18:41:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    41,
                    48,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:41:48Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    41,
                    48,
                    3,
                    51,
                    0
                ],
                "title": "eC-Tab2Text: Aspect-Based Text Generation from e-Commerce Product Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eC-Tab2Text: Aspect-Based Text Generation from e-Commerce Product Tables"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional versatility across\ndiverse domains, yet their application in e-commerce remains underexplored due\nto a lack of domain-specific datasets. To address this gap, we introduce\neC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce,\nincluding detailed product attributes and user-specific queries. Leveraging\neC-Tab2Text, we focus on text generation from product tables, enabling LLMs to\nproduce high-quality, attribute-specific product reviews from structured\ntabular data. Fine-tuned models were rigorously evaluated using standard\nTable2Text metrics, alongside correctness, faithfulness, and fluency\nassessments. Our results demonstrate substantial improvements in generating\ncontextually accurate reviews, highlighting the transformative potential of\ntailored datasets and fine-tuning methodologies in optimizing e-commerce\nworkflows. This work highlights the potential of LLMs in e-commerce workflows\nand the essential role of domain-specific datasets in tailoring them to\nindustry-specific challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional versatility across\ndiverse domains, yet their application in e-commerce remains underexplored due\nto a lack of domain-specific datasets. To address this gap, we introduce\neC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce,\nincluding detailed product attributes and user-specific queries. Leveraging\neC-Tab2Text, we focus on text generation from product tables, enabling LLMs to\nproduce high-quality, attribute-specific product reviews from structured\ntabular data. Fine-tuned models were rigorously evaluated using standard\nTable2Text metrics, alongside correctness, faithfulness, and fluency\nassessments. Our results demonstrate substantial improvements in generating\ncontextually accurate reviews, highlighting the transformative potential of\ntailored datasets and fine-tuning methodologies in optimizing e-commerce\nworkflows. This work highlights the potential of LLMs in e-commerce workflows\nand the essential role of domain-specific datasets in tailoring them to\nindustry-specific challenges."
                },
                "authors": [
                    {
                        "name": "Luis Antonio Gutirrez Guanilo"
                    },
                    {
                        "name": "Mir Tafseer Nayeem"
                    },
                    {
                        "name": "Cristian Lpez"
                    },
                    {
                        "name": "Davood Rafiei"
                    }
                ],
                "author_detail": {
                    "name": "Davood Rafiei"
                },
                "author": "Davood Rafiei",
                "arxiv_comment": "NAACL 2025 (Industry Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14816v1",
                "updated": "2025-02-20T18:37:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    37,
                    32,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:37:32Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    37,
                    32,
                    3,
                    51,
                    0
                ],
                "title": "Dynamic Low-Rank Sparse Adaptation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Low-Rank Sparse Adaptation for Large Language Models"
                },
                "summary": "Despite the efficacy of network sparsity in alleviating the deployment strain\nof Large Language Models (LLMs), it endures significant performance\ndegradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs\noffers an intuitive approach to counter this predicament, while it holds\nshortcomings include: 1) The inability to integrate LoRA weights into sparse\nLLMs post-training, and 2) Insufficient performance recovery at high sparsity\nratios. In this paper, we introduce dynamic Low-rank Sparse Adaptation (LoSA),\na novel method that seamlessly integrates low-rank adaptation into LLM sparsity\nwithin a unified framework, thereby enhancing the performance of sparse LLMs\nwithout increasing the inference latency. In particular, LoSA dynamically\nsparsifies the LoRA outcomes based on the corresponding sparse weights during\nfine-tuning, thus guaranteeing that the LoRA module can be integrated into the\nsparse LLMs post-training. Besides, LoSA leverages Representation Mutual\nInformation (RMI) as an indicator to determine the importance of layers,\nthereby efficiently determining the layer-wise sparsity rates during\nfine-tuning. Predicated on this, LoSA adjusts the rank of the LoRA module based\non the variability in layer-wise reconstruction errors, allocating an\nappropriate fine-tuning for each layer to reduce the output discrepancies\nbetween dense and sparse LLMs. Extensive experiments tell that LoSA can\nefficiently boost the efficacy of sparse LLMs within a few hours, without\nintroducing any additional inferential burden. For example, LoSA reduced the\nperplexity of sparse LLaMA-2-7B by 68.73 and increased zero-shot accuracy by\n16.32$\\%$, achieving a 2.60$\\times$ speedup on CPU and 2.23$\\times$ speedup on\nGPU, requiring only 45 minutes of fine-tuning on a single NVIDIA A100 80GB GPU.\nCode is available at https://github.com/wzhuang-xmu/LoSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the efficacy of network sparsity in alleviating the deployment strain\nof Large Language Models (LLMs), it endures significant performance\ndegradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs\noffers an intuitive approach to counter this predicament, while it holds\nshortcomings include: 1) The inability to integrate LoRA weights into sparse\nLLMs post-training, and 2) Insufficient performance recovery at high sparsity\nratios. In this paper, we introduce dynamic Low-rank Sparse Adaptation (LoSA),\na novel method that seamlessly integrates low-rank adaptation into LLM sparsity\nwithin a unified framework, thereby enhancing the performance of sparse LLMs\nwithout increasing the inference latency. In particular, LoSA dynamically\nsparsifies the LoRA outcomes based on the corresponding sparse weights during\nfine-tuning, thus guaranteeing that the LoRA module can be integrated into the\nsparse LLMs post-training. Besides, LoSA leverages Representation Mutual\nInformation (RMI) as an indicator to determine the importance of layers,\nthereby efficiently determining the layer-wise sparsity rates during\nfine-tuning. Predicated on this, LoSA adjusts the rank of the LoRA module based\non the variability in layer-wise reconstruction errors, allocating an\nappropriate fine-tuning for each layer to reduce the output discrepancies\nbetween dense and sparse LLMs. Extensive experiments tell that LoSA can\nefficiently boost the efficacy of sparse LLMs within a few hours, without\nintroducing any additional inferential burden. For example, LoSA reduced the\nperplexity of sparse LLaMA-2-7B by 68.73 and increased zero-shot accuracy by\n16.32$\\%$, achieving a 2.60$\\times$ speedup on CPU and 2.23$\\times$ speedup on\nGPU, requiring only 45 minutes of fine-tuning on a single NVIDIA A100 80GB GPU.\nCode is available at https://github.com/wzhuang-xmu/LoSA."
                },
                "authors": [
                    {
                        "name": "Weizhong Huang"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14815v1",
                "updated": "2025-02-20T18:36:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    36,
                    25,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:36:25Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    36,
                    25,
                    3,
                    51,
                    0
                ],
                "title": "Optimizing Model Selection for Compound AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Model Selection for Compound AI Systems"
                },
                "summary": "Compound AI systems that combine multiple LLM calls, such as self-refine and\nmulti-agent-debate, achieve strong performance on many AI tasks. We address a\ncore question in optimizing compound systems: for each LLM call or module in\nthe system, how should one decide which LLM to use? We show that these LLM\nchoices have a large effect on quality, but the search space is exponential. We\npropose LLMSelector, an efficient framework for model selection in compound\nsystems, which leverages two key empirical insights: (i) end-to-end performance\nis often monotonic in how well each module performs, with all other modules\nheld fixed, and (ii) per-module performance can be estimated accurately by an\nLLM. Building upon these insights, LLMSelector iteratively selects one module\nand allocates to it the model with the highest module-wise performance, as\nestimated by an LLM, until no further gain is possible. LLMSelector is\napplicable to any compound system with a bounded number of modules, and its\nnumber of API calls scales linearly with the number of modules, achieving\nhigh-quality model allocation both empirically and theoretically. Experiments\nwith popular compound systems such as multi-agent debate and self-refine using\nLLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector\nconfers 5%-70% accuracy gains compared to using the same LLM for all modules.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compound AI systems that combine multiple LLM calls, such as self-refine and\nmulti-agent-debate, achieve strong performance on many AI tasks. We address a\ncore question in optimizing compound systems: for each LLM call or module in\nthe system, how should one decide which LLM to use? We show that these LLM\nchoices have a large effect on quality, but the search space is exponential. We\npropose LLMSelector, an efficient framework for model selection in compound\nsystems, which leverages two key empirical insights: (i) end-to-end performance\nis often monotonic in how well each module performs, with all other modules\nheld fixed, and (ii) per-module performance can be estimated accurately by an\nLLM. Building upon these insights, LLMSelector iteratively selects one module\nand allocates to it the model with the highest module-wise performance, as\nestimated by an LLM, until no further gain is possible. LLMSelector is\napplicable to any compound system with a bounded number of modules, and its\nnumber of API calls scales linearly with the number of modules, achieving\nhigh-quality model allocation both empirically and theoretically. Experiments\nwith popular compound systems such as multi-agent debate and self-refine using\nLLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector\nconfers 5%-70% accuracy gains compared to using the same LLM for all modules."
                },
                "authors": [
                    {
                        "name": "Lingjiao Chen"
                    },
                    {
                        "name": "Jared Quincy Davis"
                    },
                    {
                        "name": "Boris Hanin"
                    },
                    {
                        "name": "Peter Bailis"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14803v1",
                "updated": "2025-02-20T18:26:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    26,
                    39,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:26:39Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    26,
                    39,
                    3,
                    51,
                    0
                ],
                "title": "Planning, scheduling, and execution on the Moon: the CADRE technology\n  demonstration mission",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning, scheduling, and execution on the Moon: the CADRE technology\n  demonstration mission"
                },
                "summary": "NASA's Cooperative Autonomous Distributed Robotic Exploration (CADRE)\nmission, slated for flight to the Moon's Reiner Gamma region in 2025/2026, is\ndesigned to demonstrate multi-agent autonomous exploration of the Lunar surface\nand sub-surface. A team of three robots and a base station will autonomously\nexplore a region near the lander, collecting the data required for 3D\nreconstruction of the surface with no human input; and then autonomously\nperform distributed sensing with multi-static ground penetrating radars (GPR),\ndriving in formation while performing coordinated radar soundings to create a\nmap of the subsurface. At the core of CADRE's software architecture is a novel\nautonomous, distributed planning, scheduling, and execution (PS&E) system. The\nsystem coordinates the robots' activities, planning and executing tasks that\nrequire multiple robots' participation while ensuring that each individual\nrobot's thermal and power resources stay within prescribed bounds, and\nrespecting ground-prescribed sleep-wake cycles. The system uses a\ncentralized-planning, distributed-execution paradigm, and a leader election\nmechanism ensures robustness to failures of individual agents. In this paper,\nwe describe the architecture of CADRE's PS&E system; discuss its design\nrationale; and report on verification and validation (V&V) testing of the\nsystem on CADRE's hardware in preparation for deployment on the Moon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NASA's Cooperative Autonomous Distributed Robotic Exploration (CADRE)\nmission, slated for flight to the Moon's Reiner Gamma region in 2025/2026, is\ndesigned to demonstrate multi-agent autonomous exploration of the Lunar surface\nand sub-surface. A team of three robots and a base station will autonomously\nexplore a region near the lander, collecting the data required for 3D\nreconstruction of the surface with no human input; and then autonomously\nperform distributed sensing with multi-static ground penetrating radars (GPR),\ndriving in formation while performing coordinated radar soundings to create a\nmap of the subsurface. At the core of CADRE's software architecture is a novel\nautonomous, distributed planning, scheduling, and execution (PS&E) system. The\nsystem coordinates the robots' activities, planning and executing tasks that\nrequire multiple robots' participation while ensuring that each individual\nrobot's thermal and power resources stay within prescribed bounds, and\nrespecting ground-prescribed sleep-wake cycles. The system uses a\ncentralized-planning, distributed-execution paradigm, and a leader election\nmechanism ensures robustness to failures of individual agents. In this paper,\nwe describe the architecture of CADRE's PS&E system; discuss its design\nrationale; and report on verification and validation (V&V) testing of the\nsystem on CADRE's hardware in preparation for deployment on the Moon."
                },
                "authors": [
                    {
                        "name": "Gregg Rabideau"
                    },
                    {
                        "name": "Joseph Russino"
                    },
                    {
                        "name": "Andrew Branch"
                    },
                    {
                        "name": "Nihal Dhamani"
                    },
                    {
                        "name": "Tiago Stegun Vaquero"
                    },
                    {
                        "name": "Steve Chien"
                    },
                    {
                        "name": "Jean-Pierre de la Croix"
                    },
                    {
                        "name": "Federico Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Federico Rossi"
                },
                "author": "Federico Rossi",
                "arxiv_comment": "To be presented at AAMAS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14802v1",
                "updated": "2025-02-20T18:26:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    26,
                    2,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:26:02Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    26,
                    2,
                    3,
                    51,
                    0
                ],
                "title": "From RAG to Memory: Non-Parametric Continual Learning for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From RAG to Memory: Non-Parametric Continual Learning for Large Language\n  Models"
                },
                "summary": "Our ability to continuously acquire, organize, and leverage knowledge is a\nkey feature of human intelligence that AI systems must approximate to unlock\ntheir full potential. Given the challenges in continual learning with large\nlanguage models (LLMs), retrieval-augmented generation (RAG) has become the\ndominant way to introduce new information. However, its reliance on vector\nretrieval hinders its ability to mimic the dynamic and interconnected nature of\nhuman long-term memory. Recent RAG approaches augment vector embeddings with\nvarious structures like knowledge graphs to address some of these gaps, namely\nsense-making and associativity. However, their performance on more basic\nfactual memory tasks drops considerably below standard RAG. We address this\nunintended deterioration and propose HippoRAG 2, a framework that outperforms\nstandard RAG comprehensively on factual, sense-making, and associative memory\ntasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in\nHippoRAG and enhances it with deeper passage integration and more effective\nonline use of an LLM. This combination pushes this RAG system closer to the\neffectiveness of human long-term memory, achieving a 7% improvement in\nassociative memory tasks over the state-of-the-art embedding model while also\nexhibiting superior factual knowledge and sense-making memory capabilities.\nThis work paves the way for non-parametric continual learning for LLMs. Our\ncode and data will be released at https://github.com/OSU-NLP-Group/HippoRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our ability to continuously acquire, organize, and leverage knowledge is a\nkey feature of human intelligence that AI systems must approximate to unlock\ntheir full potential. Given the challenges in continual learning with large\nlanguage models (LLMs), retrieval-augmented generation (RAG) has become the\ndominant way to introduce new information. However, its reliance on vector\nretrieval hinders its ability to mimic the dynamic and interconnected nature of\nhuman long-term memory. Recent RAG approaches augment vector embeddings with\nvarious structures like knowledge graphs to address some of these gaps, namely\nsense-making and associativity. However, their performance on more basic\nfactual memory tasks drops considerably below standard RAG. We address this\nunintended deterioration and propose HippoRAG 2, a framework that outperforms\nstandard RAG comprehensively on factual, sense-making, and associative memory\ntasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in\nHippoRAG and enhances it with deeper passage integration and more effective\nonline use of an LLM. This combination pushes this RAG system closer to the\neffectiveness of human long-term memory, achieving a 7% improvement in\nassociative memory tasks over the state-of-the-art embedding model while also\nexhibiting superior factual knowledge and sense-making memory capabilities.\nThis work paves the way for non-parametric continual learning for LLMs. Our\ncode and data will be released at https://github.com/OSU-NLP-Group/HippoRAG."
                },
                "authors": [
                    {
                        "name": "Bernal Jimnez Gutirrez"
                    },
                    {
                        "name": "Yiheng Shu"
                    },
                    {
                        "name": "Weijian Qi"
                    },
                    {
                        "name": "Sizhe Zhou"
                    },
                    {
                        "name": "Yu Su"
                    }
                ],
                "author_detail": {
                    "name": "Yu Su"
                },
                "author": "Yu Su",
                "arxiv_comment": "Code and data to be released at:\n  https://github.com/OSU-NLP-Group/HippoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14796v1",
                "updated": "2025-02-20T18:17:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    17,
                    26,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:17:26Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    17,
                    26,
                    3,
                    51,
                    0
                ],
                "title": "A Multi-Agent Perspective on Modern Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Agent Perspective on Modern Information Retrieval"
                },
                "summary": "The rise of large language models (LLMs) has introduced a new era in\ninformation retrieval (IR), where queries and documents that were once assumed\nto be generated exclusively by humans can now also be created by automated\nagents. These agents can formulate queries, generate documents, and perform\nranking. This shift challenges some long-standing IR paradigms and calls for a\nreassessment of both theoretical frameworks and practical methodologies. We\nadvocate for a multi-agent perspective to better capture the complex\ninteractions between query agents, document agents, and ranker agents. Through\nempirical exploration of various multi-agent retrieval settings, we reveal the\nsignificant impact of these interactions on system performance. Our findings\nunderscore the need to revisit classical IR paradigms and develop new\nframeworks for more effective modeling and evaluation of modern retrieval\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has introduced a new era in\ninformation retrieval (IR), where queries and documents that were once assumed\nto be generated exclusively by humans can now also be created by automated\nagents. These agents can formulate queries, generate documents, and perform\nranking. This shift challenges some long-standing IR paradigms and calls for a\nreassessment of both theoretical frameworks and practical methodologies. We\nadvocate for a multi-agent perspective to better capture the complex\ninteractions between query agents, document agents, and ranker agents. Through\nempirical exploration of various multi-agent retrieval settings, we reveal the\nsignificant impact of these interactions on system performance. Our findings\nunderscore the need to revisit classical IR paradigms and develop new\nframeworks for more effective modeling and evaluation of modern retrieval\nsystems."
                },
                "authors": [
                    {
                        "name": "Haya Nachimovsky"
                    },
                    {
                        "name": "Moshe Tennenholtz"
                    },
                    {
                        "name": "Oren Kurland"
                    }
                ],
                "author_detail": {
                    "name": "Oren Kurland"
                },
                "author": "Oren Kurland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14791v1",
                "updated": "2025-02-20T18:11:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    11,
                    38,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:11:38Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    11,
                    38,
                    3,
                    51,
                    0
                ],
                "title": "Rapid Word Learning Through Meta In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid Word Learning Through Meta In-Context Learning"
                },
                "summary": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks."
                },
                "authors": [
                    {
                        "name": "Wentao Wang"
                    },
                    {
                        "name": "Guangyuan Jiang"
                    },
                    {
                        "name": "Tal Linzen"
                    },
                    {
                        "name": "Brenden M. Lake"
                    }
                ],
                "author_detail": {
                    "name": "Brenden M. Lake"
                },
                "author": "Brenden M. Lake",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13491v2",
                "updated": "2025-02-20T18:09:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    9,
                    53,
                    3,
                    51,
                    0
                ],
                "published": "2024-03-20T10:48:00Z",
                "published_parsed": [
                    2024,
                    3,
                    20,
                    10,
                    48,
                    0,
                    2,
                    80,
                    0
                ],
                "title": "Dimensionality-Reduction Techniques for Approximate Nearest Neighbor\n  Search: A Survey and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dimensionality-Reduction Techniques for Approximate Nearest Neighbor\n  Search: A Survey and Evaluation"
                },
                "summary": "Approximate Nearest Neighbor Search (ANNS) on high-dimensional vectors has\nbecome a fundamental and essential component in various machine learning tasks.\nRecently, with the rapid development of deep learning models and the\napplications of Large Language Models (LLMs), the dimensionality of the vectors\nkeeps growing in order to accommodate a richer semantic representation. This\nposes a major challenge to the ANNS solutions since distance calculation cost\nin ANNS grows linearly with the dimensionality of vectors. To overcome this\nchallenge, dimensionality-reduction techniques can be leveraged to accelerate\nthe distance calculation in the search process. In this paper, we investigate\nsix dimensionality-reduction techniques that have the potential to improve ANNS\nsolutions, including classical algorithms such as PCA and vector quantization,\nas well as algorithms based on deep learning approaches. We further describe\ntwo frameworks to apply these techniques in the ANNS workflow, and\ntheoretically analyze the time and space costs, as well as the beneficial\nthreshold for the pruning ratio of these techniques. The surveyed techniques\nare evaluated on six public datasets. The analysis of the results reveals the\ncharacteristics of the different families of techniques and provides insights\ninto the promising future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest Neighbor Search (ANNS) on high-dimensional vectors has\nbecome a fundamental and essential component in various machine learning tasks.\nRecently, with the rapid development of deep learning models and the\napplications of Large Language Models (LLMs), the dimensionality of the vectors\nkeeps growing in order to accommodate a richer semantic representation. This\nposes a major challenge to the ANNS solutions since distance calculation cost\nin ANNS grows linearly with the dimensionality of vectors. To overcome this\nchallenge, dimensionality-reduction techniques can be leveraged to accelerate\nthe distance calculation in the search process. In this paper, we investigate\nsix dimensionality-reduction techniques that have the potential to improve ANNS\nsolutions, including classical algorithms such as PCA and vector quantization,\nas well as algorithms based on deep learning approaches. We further describe\ntwo frameworks to apply these techniques in the ANNS workflow, and\ntheoretically analyze the time and space costs, as well as the beneficial\nthreshold for the pruning ratio of these techniques. The surveyed techniques\nare evaluated on six public datasets. The analysis of the results reveals the\ncharacteristics of the different families of techniques and provides insights\ninto the promising future research directions."
                },
                "authors": [
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Haoran Xiong"
                    },
                    {
                        "name": "Qitong Wang"
                    },
                    {
                        "name": "Zhenying He"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Themis Palpanas"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14776v1",
                "updated": "2025-02-20T17:59:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    59,
                    45,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:59:45Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    59,
                    45,
                    3,
                    51,
                    0
                ],
                "title": "SurveyX: Academic Survey Automation via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurveyX: Academic Survey Automation via Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional comprehension\ncapabilities and a vast knowledge base, suggesting that LLMs can serve as\nefficient tools for automated survey generation. However, recent research\nrelated to automated survey generation remains constrained by some critical\nlimitations like finite context window, lack of in-depth content discussion,\nand absence of systematic evaluation frameworks. Inspired by human writing\nprocesses, we propose SurveyX, an efficient and organized system for automated\nsurvey generation that decomposes the survey composing process into two phases:\nthe Preparation and Generation phases. By innovatively introducing online\nreference retrieval, a pre-processing method called AttributeTree, and a\nre-polishing process, SurveyX significantly enhances the efficacy of survey\ncomposition. Experimental evaluation results show that SurveyX outperforms\nexisting automated survey generation systems in content quality (0.259\nimprovement) and citation quality (1.76 enhancement), approaching human expert\nperformance across multiple evaluation dimensions. Examples of surveys\ngenerated by SurveyX are available on www.surveyx.cn",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional comprehension\ncapabilities and a vast knowledge base, suggesting that LLMs can serve as\nefficient tools for automated survey generation. However, recent research\nrelated to automated survey generation remains constrained by some critical\nlimitations like finite context window, lack of in-depth content discussion,\nand absence of systematic evaluation frameworks. Inspired by human writing\nprocesses, we propose SurveyX, an efficient and organized system for automated\nsurvey generation that decomposes the survey composing process into two phases:\nthe Preparation and Generation phases. By innovatively introducing online\nreference retrieval, a pre-processing method called AttributeTree, and a\nre-polishing process, SurveyX significantly enhances the efficacy of survey\ncomposition. Experimental evaluation results show that SurveyX outperforms\nexisting automated survey generation systems in content quality (0.259\nimprovement) and citation quality (1.76 enhancement), approaching human expert\nperformance across multiple evaluation dimensions. Examples of surveys\ngenerated by SurveyX are available on www.surveyx.cn"
                },
                "authors": [
                    {
                        "name": "Xun Liang"
                    },
                    {
                        "name": "Jiawei Yang"
                    },
                    {
                        "name": "Yezhaohui Wang"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Keming Mao"
                    },
                    {
                        "name": "Zhiyu li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu li"
                },
                "author": "Zhiyu li",
                "arxiv_comment": "15 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14770v1",
                "updated": "2025-02-20T17:51:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    51,
                    10,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:51:10Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    51,
                    10,
                    3,
                    51,
                    0
                ],
                "title": "Determining Layer-wise Sparsity for Large Language Models Through a\n  Theoretical Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining Layer-wise Sparsity for Large Language Models Through a\n  Theoretical Perspective"
                },
                "summary": "In this paper, we address the challenge of determining the layer-wise\nsparsity rates of large language models (LLMs) through a theoretical\nperspective. Specifically, we identify a critical issue of\n''$\\textbf{reconstruction error explosion}$'' in existing LLMs sparsification\nmethods. This refers to the cumulative effect of reconstruction errors\nthroughout the sparsification process, where errors from earlier layers\npropagate and amplify in subsequent layers. As a result, the overall\nreconstruction error increases significantly, leading to a substantial\ndegradation in model performance. Through theoretical analysis, we derive a\nsimple yet effective approach to layer-wise sparsity allocation that mitigates\nthis issue. Our method uses a monotonically increasing arithmetic progression,\nreducing the process of determining sparsity rates for multiple layers to the\ndetermination of a single common difference hyperparameter. Remarkably, this\nallows for the optimal layer-wise sparsity rates to be identified with just a\nfew trials. Both our theoretical analysis and experimental results demonstrate\nthat this sparsity allocation scheme is near optimal. Extensive experiments\nshow that our method significantly improves the performance of sparse LLMs\nacross various architectures, outperforming existing layer-wise sparsity\nmethods. Furthermore, it enhances the performance of various compression\ntechniques and is applicable to vision and multimodal models. Notably, our\nmethod achieves a reduction of 52.10 in perplexity for the 70$\\%$ sparse\nLLaMA2-7B model obtained via Wanda, improves average zero-shot accuracy by\n10.50$\\%$, and delivers speedups of 2.63$\\times$ and 2.23$\\times$ on CPU and\nGPU, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we address the challenge of determining the layer-wise\nsparsity rates of large language models (LLMs) through a theoretical\nperspective. Specifically, we identify a critical issue of\n''$\\textbf{reconstruction error explosion}$'' in existing LLMs sparsification\nmethods. This refers to the cumulative effect of reconstruction errors\nthroughout the sparsification process, where errors from earlier layers\npropagate and amplify in subsequent layers. As a result, the overall\nreconstruction error increases significantly, leading to a substantial\ndegradation in model performance. Through theoretical analysis, we derive a\nsimple yet effective approach to layer-wise sparsity allocation that mitigates\nthis issue. Our method uses a monotonically increasing arithmetic progression,\nreducing the process of determining sparsity rates for multiple layers to the\ndetermination of a single common difference hyperparameter. Remarkably, this\nallows for the optimal layer-wise sparsity rates to be identified with just a\nfew trials. Both our theoretical analysis and experimental results demonstrate\nthat this sparsity allocation scheme is near optimal. Extensive experiments\nshow that our method significantly improves the performance of sparse LLMs\nacross various architectures, outperforming existing layer-wise sparsity\nmethods. Furthermore, it enhances the performance of various compression\ntechniques and is applicable to vision and multimodal models. Notably, our\nmethod achieves a reduction of 52.10 in perplexity for the 70$\\%$ sparse\nLLaMA2-7B model obtained via Wanda, improves average zero-shot accuracy by\n10.50$\\%$, and delivers speedups of 2.63$\\times$ and 2.23$\\times$ on CPU and\nGPU, respectively."
                },
                "authors": [
                    {
                        "name": "Weizhong Huang"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14768v1",
                "updated": "2025-02-20T17:49:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    49,
                    26,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:49:26Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    49,
                    26,
                    3,
                    51,
                    0
                ],
                "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement\n  Learning"
                },
                "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of\nrule-based reinforcement learning (RL) in large reasoning models. To analyze\nreasoning dynamics, we use synthetic logic puzzles as training data due to\ntheir controllable complexity and straightforward answer verification. We make\nsome key technical contributions that lead to effective and stable RL training:\na system prompt that emphasizes the thinking and answering process, a stringent\nformat reward function that penalizes outputs for taking shortcuts, and a\nstraightforward training recipe that achieves stable convergence. Our 7B model\ndevelops advanced reasoning skills-such as reflection, verification, and\nsummarization-that are absent from the logic corpus. Remarkably, after training\non just 5K logic problems, it demonstrates generalization abilities to the\nchallenging math benchmarks AIME and AMC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by the success of DeepSeek-R1, we explore the potential of\nrule-based reinforcement learning (RL) in large reasoning models. To analyze\nreasoning dynamics, we use synthetic logic puzzles as training data due to\ntheir controllable complexity and straightforward answer verification. We make\nsome key technical contributions that lead to effective and stable RL training:\na system prompt that emphasizes the thinking and answering process, a stringent\nformat reward function that penalizes outputs for taking shortcuts, and a\nstraightforward training recipe that achieves stable convergence. Our 7B model\ndevelops advanced reasoning skills-such as reflection, verification, and\nsummarization-that are absent from the logic corpus. Remarkably, after training\non just 5K logic problems, it demonstrates generalization abilities to the\nchallenging math benchmarks AIME and AMC."
                },
                "authors": [
                    {
                        "name": "Tian Xie"
                    },
                    {
                        "name": "Zitian Gao"
                    },
                    {
                        "name": "Qingnan Ren"
                    },
                    {
                        "name": "Haoming Luo"
                    },
                    {
                        "name": "Yuqian Hong"
                    },
                    {
                        "name": "Bryan Dai"
                    },
                    {
                        "name": "Joey Zhou"
                    },
                    {
                        "name": "Kai Qiu"
                    },
                    {
                        "name": "Zhirong Wu"
                    },
                    {
                        "name": "Chong Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chong Luo"
                },
                "author": "Chong Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14767v1",
                "updated": "2025-02-20T17:43:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    43,
                    40,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:43:40Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    43,
                    40,
                    3,
                    51,
                    0
                ],
                "title": "Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for\n  Scientific Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for\n  Scientific Comparative Analysis"
                },
                "summary": "With the exponential growth of research facilitated by modern technology and\nimproved accessibility, scientific discoveries have become increasingly\nfragmented within and across fields. This makes it challenging to assess the\nsignificance, novelty, incremental findings, and equivalent ideas between\nrelated works, particularly those from different research communities. Large\nlanguage models (LLMs) have recently demonstrated strong quantitative and\nqualitative reasoning abilities, and multi-agent LLM debates have shown promise\nin handling complex reasoning tasks by exploring diverse perspectives and\nreasoning paths. Inspired by this, we introduce Tree-of-Debate (ToD), a\nframework which converts scientific papers into LLM personas that debate their\nrespective novelties. To emphasize structured, critical reasoning rather than\nfocusing solely on outcomes, ToD dynamically constructs a debate tree, enabling\nfine-grained analysis of independent novelty arguments within scholarly\narticles. Through experiments on scientific literature across various domains,\nevaluated by expert researchers, we demonstrate that ToD generates informative\narguments, effectively contrasts papers, and supports researchers in their\nliterature review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential growth of research facilitated by modern technology and\nimproved accessibility, scientific discoveries have become increasingly\nfragmented within and across fields. This makes it challenging to assess the\nsignificance, novelty, incremental findings, and equivalent ideas between\nrelated works, particularly those from different research communities. Large\nlanguage models (LLMs) have recently demonstrated strong quantitative and\nqualitative reasoning abilities, and multi-agent LLM debates have shown promise\nin handling complex reasoning tasks by exploring diverse perspectives and\nreasoning paths. Inspired by this, we introduce Tree-of-Debate (ToD), a\nframework which converts scientific papers into LLM personas that debate their\nrespective novelties. To emphasize structured, critical reasoning rather than\nfocusing solely on outcomes, ToD dynamically constructs a debate tree, enabling\nfine-grained analysis of independent novelty arguments within scholarly\narticles. Through experiments on scientific literature across various domains,\nevaluated by expert researchers, we demonstrate that ToD generates informative\narguments, effectively contrasts papers, and supports researchers in their\nliterature review."
                },
                "authors": [
                    {
                        "name": "Priyanka Kargupta"
                    },
                    {
                        "name": "Ishika Agarwal"
                    },
                    {
                        "name": "Tal August"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "arxiv_comment": "Code available at: https://github.com/pkargupta/tree-of-debate",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14765v1",
                "updated": "2025-02-20T17:40:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    40,
                    21,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:40:21Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    40,
                    21,
                    3,
                    51,
                    0
                ],
                "title": "Step-by-Step Fact Verification System for Medical Claims with\n  Explainable Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-by-Step Fact Verification System for Medical Claims with\n  Explainable Reasoning"
                },
                "summary": "Fact verification (FV) aims to assess the veracity of a claim based on\nrelevant evidence. The traditional approach for automated FV includes a\nthree-part pipeline relying on short evidence snippets and encoder-only\ninference models. More recent approaches leverage the multi-turn nature of LLMs\nto address FV as a step-by-step problem where questions inquiring additional\ncontext are generated and answered until there is enough information to make a\ndecision. This iterative method makes the verification process rational and\nexplainable. While these methods have been tested for encyclopedic claims,\nexploration on domain-specific and realistic claims is missing. In this work,\nwe apply an iterative FV system on three medical fact-checking datasets and\nevaluate it with multiple settings, including different LLMs, external web\nsearch, and structured reasoning using logic predicates. We demonstrate\nimprovements in the final performance over traditional approaches and the high\npotential of step-by-step FV systems for domain-specific claims.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact verification (FV) aims to assess the veracity of a claim based on\nrelevant evidence. The traditional approach for automated FV includes a\nthree-part pipeline relying on short evidence snippets and encoder-only\ninference models. More recent approaches leverage the multi-turn nature of LLMs\nto address FV as a step-by-step problem where questions inquiring additional\ncontext are generated and answered until there is enough information to make a\ndecision. This iterative method makes the verification process rational and\nexplainable. While these methods have been tested for encyclopedic claims,\nexploration on domain-specific and realistic claims is missing. In this work,\nwe apply an iterative FV system on three medical fact-checking datasets and\nevaluate it with multiple settings, including different LLMs, external web\nsearch, and structured reasoning using logic predicates. We demonstrate\nimprovements in the final performance over traditional approaches and the high\npotential of step-by-step FV systems for domain-specific claims."
                },
                "authors": [
                    {
                        "name": "Juraj Vladika"
                    },
                    {
                        "name": "Ivana Hacajov"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "arxiv_comment": "Accepted to NAACL 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14760v1",
                "updated": "2025-02-20T17:35:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    35,
                    32,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:35:32Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    35,
                    32,
                    3,
                    51,
                    0
                ],
                "title": "EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of\n  Optimization Formulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of\n  Optimization Formulations"
                },
                "summary": "A fundamental problem in combinatorial optimization is identifying equivalent\nformulations, which can lead to more efficient solution strategies and deeper\ninsights into a problem's computational complexity. The need to automatically\nidentify equivalence between problem formulations has grown as optimization\ncopilots--systems that generate problem formulations from natural language\ndescriptions--have proliferated. However, existing approaches to checking\nformulation equivalence lack grounding, relying on simple heuristics which are\ninsufficient for rigorous validation. Inspired by Karp reductions, in this work\nwe introduce quasi-Karp equivalence, a formal criterion for determining when\ntwo optimization formulations are equivalent based on the existence of a\nmapping between their decision variables. We propose EquivaMap, a framework\nthat leverages large language models to automatically discover such mappings,\nenabling scalable and reliable equivalence verification. To evaluate our\napproach, we construct the first open-source dataset of equivalent optimization\nformulations, generated by applying transformations such as adding slack\nvariables or valid inequalities to existing formulations. Empirically,\nEquivaMap significantly outperforms existing methods, achieving substantial\nimprovements in correctly identifying formulation equivalence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental problem in combinatorial optimization is identifying equivalent\nformulations, which can lead to more efficient solution strategies and deeper\ninsights into a problem's computational complexity. The need to automatically\nidentify equivalence between problem formulations has grown as optimization\ncopilots--systems that generate problem formulations from natural language\ndescriptions--have proliferated. However, existing approaches to checking\nformulation equivalence lack grounding, relying on simple heuristics which are\ninsufficient for rigorous validation. Inspired by Karp reductions, in this work\nwe introduce quasi-Karp equivalence, a formal criterion for determining when\ntwo optimization formulations are equivalent based on the existence of a\nmapping between their decision variables. We propose EquivaMap, a framework\nthat leverages large language models to automatically discover such mappings,\nenabling scalable and reliable equivalence verification. To evaluate our\napproach, we construct the first open-source dataset of equivalent optimization\nformulations, generated by applying transformations such as adding slack\nvariables or valid inequalities to existing formulations. Empirically,\nEquivaMap significantly outperforms existing methods, achieving substantial\nimprovements in correctly identifying formulation equivalence."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Connor Lawless"
                    },
                    {
                        "name": "Ellen Vitercik"
                    },
                    {
                        "name": "Liu Leqi"
                    }
                ],
                "author_detail": {
                    "name": "Liu Leqi"
                },
                "author": "Liu Leqi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14759v1",
                "updated": "2025-02-20T17:34:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    34,
                    34,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:34:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    34,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "On the Influence of Context Size and Model Choice in Retrieval-Augmented\n  Generation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Influence of Context Size and Model Choice in Retrieval-Augmented\n  Generation Systems"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as an approach to augment\nlarge language models (LLMs) by reducing their reliance on static knowledge and\nimproving answer factuality. RAG retrieves relevant context snippets and\ngenerates an answer based on them. Despite its increasing industrial adoption,\nsystematic exploration of RAG components is lacking, particularly regarding the\nideal size of provided context, and the choice of base LLM and retrieval\nmethod. To help guide development of robust RAG systems, we evaluate various\ncontext sizes, BM25 and semantic search as retrievers, and eight base LLMs.\nMoving away from the usual RAG evaluation with short answers, we explore the\nmore challenging long-form question answering in two domains, where a good\nanswer has to utilize the entire context. Our findings indicate that final QA\nperformance improves steadily with up to 15 snippets but stagnates or declines\nbeyond that. Finally, we show that different general-purpose LLMs excel in the\nbiomedical domain than the encyclopedic one, and that open-domain evidence\nretrieval in large corpora is challenging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as an approach to augment\nlarge language models (LLMs) by reducing their reliance on static knowledge and\nimproving answer factuality. RAG retrieves relevant context snippets and\ngenerates an answer based on them. Despite its increasing industrial adoption,\nsystematic exploration of RAG components is lacking, particularly regarding the\nideal size of provided context, and the choice of base LLM and retrieval\nmethod. To help guide development of robust RAG systems, we evaluate various\ncontext sizes, BM25 and semantic search as retrievers, and eight base LLMs.\nMoving away from the usual RAG evaluation with short answers, we explore the\nmore challenging long-form question answering in two domains, where a good\nanswer has to utilize the entire context. Our findings indicate that final QA\nperformance improves steadily with up to 15 snippets but stagnates or declines\nbeyond that. Finally, we show that different general-purpose LLMs excel in the\nbiomedical domain than the encyclopedic one, and that open-domain evidence\nretrieval in large corpora is challenging."
                },
                "authors": [
                    {
                        "name": "Juraj Vladika"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "arxiv_comment": "Accepted to Findings of NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14752v1",
                "updated": "2025-02-20T17:21:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    21,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:21:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    21,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "TritonBench: Benchmarking Large Language Model Capabilities for\n  Generating Triton Operators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TritonBench: Benchmarking Large Language Model Capabilities for\n  Generating Triton Operators"
                },
                "summary": "Triton, a high-level Python-like language designed for building efficient GPU\nkernels, is widely adopted in deep learning frameworks due to its portability,\nflexibility, and accessibility. However, programming and parallel optimization\nstill require considerable trial and error from Triton developers. Despite\nadvances in large language models (LLMs) for conventional code generation,\nthese models struggle to generate accurate, performance-optimized Triton code,\nas they lack awareness of its specifications and the complexities of GPU\nprogramming. More critically, there is an urgent need for systematic\nevaluations tailored to Triton. In this work, we introduce TritonBench, the\nfirst comprehensive benchmark for Triton operator generation. TritonBench\nfeatures two evaluation channels: a curated set of 184 real-world operators\nfrom GitHub and a collection of operators aligned with PyTorch interfaces.\nUnlike conventional code benchmarks prioritizing functional correctness,\nTritonBench also profiles efficiency performance on widely deployed GPUs\naligned with industry applications. Our study reveals that current\nstate-of-the-art code LLMs struggle to generate efficient Triton operators,\nhighlighting a significant gap in high-performance code generation. TritonBench\nwill be available at https://github.com/thunlp/TritonBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triton, a high-level Python-like language designed for building efficient GPU\nkernels, is widely adopted in deep learning frameworks due to its portability,\nflexibility, and accessibility. However, programming and parallel optimization\nstill require considerable trial and error from Triton developers. Despite\nadvances in large language models (LLMs) for conventional code generation,\nthese models struggle to generate accurate, performance-optimized Triton code,\nas they lack awareness of its specifications and the complexities of GPU\nprogramming. More critically, there is an urgent need for systematic\nevaluations tailored to Triton. In this work, we introduce TritonBench, the\nfirst comprehensive benchmark for Triton operator generation. TritonBench\nfeatures two evaluation channels: a curated set of 184 real-world operators\nfrom GitHub and a collection of operators aligned with PyTorch interfaces.\nUnlike conventional code benchmarks prioritizing functional correctness,\nTritonBench also profiles efficiency performance on widely deployed GPUs\naligned with industry applications. Our study reveals that current\nstate-of-the-art code LLMs struggle to generate efficient Triton operators,\nhighlighting a significant gap in high-performance code generation. TritonBench\nwill be available at https://github.com/thunlp/TritonBench."
                },
                "authors": [
                    {
                        "name": "Jianling Li"
                    },
                    {
                        "name": "Shangzhan Li"
                    },
                    {
                        "name": "Zhenye Gao"
                    },
                    {
                        "name": "Qi Shi"
                    },
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Zefan Wang"
                    },
                    {
                        "name": "Jiacheng Huang"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Jianrong Wang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14748v1",
                "updated": "2025-02-20T17:19:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    19,
                    41,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:19:41Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    19,
                    41,
                    3,
                    51,
                    0
                ],
                "title": "Large Language Models Struggle to Describe the Haystack without Human\n  Help: Human-in-the-loop Evaluation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Struggle to Describe the Haystack without Human\n  Help: Human-in-the-loop Evaluation of LLMs"
                },
                "summary": "A common use of NLP is to facilitate the understanding of large document\ncollections, with a shift from using traditional topic models to Large Language\nModels. Yet the effectiveness of using LLM for large corpus understanding in\nreal-world applications remains under-explored. This study measures the\nknowledge users acquire with unsupervised, supervised LLM-based exploratory\napproaches or traditional topic models on two datasets. While LLM-based methods\ngenerate more human-readable topics and show higher average win probabilities\nthan traditional models for data exploration, they produce overly generic\ntopics for domain-specific datasets that do not easily allow users to learn\nmuch about the documents. Adding human supervision to the LLM generation\nprocess improves data exploration by mitigating hallucination and\nover-genericity but requires greater human effort. In contrast, traditional.\nmodels like Latent Dirichlet Allocation (LDA) remain effective for exploration\nbut are less user-friendly. We show that LLMs struggle to describe the haystack\nof large corpora without human help, particularly domain-specific data, and\nface scaling and hallucination limitations due to context length constraints.\nDataset available at https://huggingface. co/datasets/zli12321/Bills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common use of NLP is to facilitate the understanding of large document\ncollections, with a shift from using traditional topic models to Large Language\nModels. Yet the effectiveness of using LLM for large corpus understanding in\nreal-world applications remains under-explored. This study measures the\nknowledge users acquire with unsupervised, supervised LLM-based exploratory\napproaches or traditional topic models on two datasets. While LLM-based methods\ngenerate more human-readable topics and show higher average win probabilities\nthan traditional models for data exploration, they produce overly generic\ntopics for domain-specific datasets that do not easily allow users to learn\nmuch about the documents. Adding human supervision to the LLM generation\nprocess improves data exploration by mitigating hallucination and\nover-genericity but requires greater human effort. In contrast, traditional.\nmodels like Latent Dirichlet Allocation (LDA) remain effective for exploration\nbut are less user-friendly. We show that LLMs struggle to describe the haystack\nof large corpora without human help, particularly domain-specific data, and\nface scaling and hallucination limitations due to context length constraints.\nDataset available at https://huggingface. co/datasets/zli12321/Bills."
                },
                "authors": [
                    {
                        "name": "Zongxia Li"
                    },
                    {
                        "name": "Lorena Calvo-Bartolom"
                    },
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Paiheng Xu"
                    },
                    {
                        "name": "Alden Dima"
                    },
                    {
                        "name": "Juan Francisco Fung"
                    },
                    {
                        "name": "Jordan Boyd-Graber"
                    }
                ],
                "author_detail": {
                    "name": "Jordan Boyd-Graber"
                },
                "author": "Jordan Boyd-Graber",
                "arxiv_comment": "21 Pages. LLM for Data Exploration and content analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14743v1",
                "updated": "2025-02-20T17:12:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    12,
                    45,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:12:45Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    12,
                    45,
                    3,
                    51,
                    0
                ],
                "title": "Multi-Agent Coordination across Diverse Applications: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Coordination across Diverse Applications: A Survey"
                },
                "summary": "Multi-agent coordination studies the underlying mechanism enabling the\ntrending spread of diverse multi-agent systems (MAS) and has received\nincreasing attention, driven by the expansion of emerging applications and\nrapid AI advances. This survey outlines the current state of coordination\nresearch across applications through a unified understanding that answers four\nfundamental coordination questions: (1) what is coordination; (2) why\ncoordination; (3) who to coordinate with; and (4) how to coordinate. Our\npurpose is to explore existing ideas and expertise in coordination and their\nconnections across diverse applications, while identifying and highlighting\nemerging and promising research directions. First, general coordination\nproblems that are essential to varied applications are identified and analyzed.\nSecond, a number of MAS applications are surveyed, ranging from widely studied\ndomains, e.g., search and rescue, warehouse automation and logistics, and\ntransportation systems, to emerging fields including humanoid and\nanthropomorphic robots, satellite systems, and large language models (LLMs).\nFinally, open challenges about the scalability, heterogeneity, and learning\nmechanisms of MAS are analyzed and discussed. In particular, we identify the\nhybridization of hierarchical and decentralized coordination, human-MAS\ncoordination, and LLM-based MAS as promising future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent coordination studies the underlying mechanism enabling the\ntrending spread of diverse multi-agent systems (MAS) and has received\nincreasing attention, driven by the expansion of emerging applications and\nrapid AI advances. This survey outlines the current state of coordination\nresearch across applications through a unified understanding that answers four\nfundamental coordination questions: (1) what is coordination; (2) why\ncoordination; (3) who to coordinate with; and (4) how to coordinate. Our\npurpose is to explore existing ideas and expertise in coordination and their\nconnections across diverse applications, while identifying and highlighting\nemerging and promising research directions. First, general coordination\nproblems that are essential to varied applications are identified and analyzed.\nSecond, a number of MAS applications are surveyed, ranging from widely studied\ndomains, e.g., search and rescue, warehouse automation and logistics, and\ntransportation systems, to emerging fields including humanoid and\nanthropomorphic robots, satellite systems, and large language models (LLMs).\nFinally, open challenges about the scalability, heterogeneity, and learning\nmechanisms of MAS are analyzed and discussed. In particular, we identify the\nhybridization of hierarchical and decentralized coordination, human-MAS\ncoordination, and LLM-based MAS as promising future directions."
                },
                "authors": [
                    {
                        "name": "Lijun Sun"
                    },
                    {
                        "name": "Yijun Yang"
                    },
                    {
                        "name": "Qiqi Duan"
                    },
                    {
                        "name": "Yuhui Shi"
                    },
                    {
                        "name": "Chao Lyu"
                    },
                    {
                        "name": "Yu-Cheng Chang"
                    },
                    {
                        "name": "Chin-Teng Lin"
                    },
                    {
                        "name": "Yang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yang Shen"
                },
                "author": "Yang Shen",
                "arxiv_comment": "23 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14740v1",
                "updated": "2025-02-20T17:08:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    8,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:08:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    8,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "YOLOv12: A Breakdown of the Key Architectural Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YOLOv12: A Breakdown of the Key Architectural Features"
                },
                "summary": "This paper presents an architectural analysis of YOLOv12, a significant\nadvancement in single-stage, real-time object detection building upon the\nstrengths of its predecessors while introducing key improvements. The model\nincorporates an optimised backbone (R-ELAN), 7x7 separable convolutions, and\nFlashAttention-driven area-based attention, improving feature extraction,\nenhanced efficiency, and robust detections. With multiple model variants,\nsimilar to its predecessors, YOLOv12 offers scalable solutions for both\nlatency-sensitive and high-accuracy applications. Experimental results manifest\nconsistent gains in mean average precision (mAP) and inference speed, making\nYOLOv12 a compelling choice for applications in autonomous systems, security,\nand real-time analytics. By achieving an optimal balance between computational\nefficiency and performance, YOLOv12 sets a new benchmark for real-time computer\nvision, facilitating deployment across diverse hardware platforms, from edge\ndevices to high-performance clusters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an architectural analysis of YOLOv12, a significant\nadvancement in single-stage, real-time object detection building upon the\nstrengths of its predecessors while introducing key improvements. The model\nincorporates an optimised backbone (R-ELAN), 7x7 separable convolutions, and\nFlashAttention-driven area-based attention, improving feature extraction,\nenhanced efficiency, and robust detections. With multiple model variants,\nsimilar to its predecessors, YOLOv12 offers scalable solutions for both\nlatency-sensitive and high-accuracy applications. Experimental results manifest\nconsistent gains in mean average precision (mAP) and inference speed, making\nYOLOv12 a compelling choice for applications in autonomous systems, security,\nand real-time analytics. By achieving an optimal balance between computational\nefficiency and performance, YOLOv12 sets a new benchmark for real-time computer\nvision, facilitating deployment across diverse hardware platforms, from edge\ndevices to high-performance clusters."
                },
                "authors": [
                    {
                        "name": "Mujadded Al Rabbani Alif"
                    },
                    {
                        "name": "Muhammad Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Hussain"
                },
                "author": "Muhammad Hussain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14739v1",
                "updated": "2025-02-20T17:05:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    5,
                    58,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:05:58Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    5,
                    58,
                    3,
                    51,
                    0
                ],
                "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope."
                },
                "authors": [
                    {
                        "name": "M-A-P Team"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Yifan Yao"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Bingli Wang"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Kang Zhu"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Xiaolong Jin"
                    },
                    {
                        "name": "Zhenlin Wei"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Kaixing Deng"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Shian Jia"
                    },
                    {
                        "name": "Sichao Jiang"
                    },
                    {
                        "name": "Yiyan Liao"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Qinrui Li"
                    },
                    {
                        "name": "Sirun Li"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Yunwen Li"
                    },
                    {
                        "name": "Dehua Ma"
                    },
                    {
                        "name": "Yuansheng Ni"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Qiyao Wang"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Siwei Wu"
                    },
                    {
                        "name": "Tianshun Xing"
                    },
                    {
                        "name": "Ming Xu"
                    },
                    {
                        "name": "Zhenzhu Yang"
                    },
                    {
                        "name": "Zekun Moore Wang"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Yuelin Bai"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Chengtuo Cheng"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Keyi Ding"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Yun Huang"
                    },
                    {
                        "name": "Yaoru Li"
                    },
                    {
                        "name": "Yizhe Li"
                    },
                    {
                        "name": "Zhaoqun Li"
                    },
                    {
                        "name": "Tianhao Liang"
                    },
                    {
                        "name": "Chengdong Lin"
                    },
                    {
                        "name": "Hongquan Lin"
                    },
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Zhongyuan Peng"
                    },
                    {
                        "name": "Zifan Peng"
                    },
                    {
                        "name": "Qige Qi"
                    },
                    {
                        "name": "Shi Qiu"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Yizhou Tan"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Chenqing Wang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yiya Wang"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Jiajun Xu"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yuanhao Yue"
                    },
                    {
                        "name": "Tianyang Zhan"
                    },
                    {
                        "name": "Chun Zhang"
                    },
                    {
                        "name": "Jingyang Zhang"
                    },
                    {
                        "name": "Xiyue Zhang"
                    },
                    {
                        "name": "Xingjian Zhang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Xiangyu Zheng"
                    },
                    {
                        "name": "Chenghua Zhong"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Junran Peng"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Shi Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Qunshu Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14735v1",
                "updated": "2025-02-20T17:01:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    1,
                    57,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T17:01:57Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    1,
                    57,
                    3,
                    51,
                    0
                ],
                "title": "EAGER-LLM: Enhancing Large Language Models as Recommenders through\n  Exogenous Behavior-Semantic Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAGER-LLM: Enhancing Large Language Models as Recommenders through\n  Exogenous Behavior-Semantic Integration"
                },
                "summary": "Large language models (LLMs) are increasingly leveraged as foundational\nbackbones in the development of advanced recommender systems, offering enhanced\ncapabilities through their extensive knowledge and reasoning. Existing\nllm-based recommender systems (RSs) often face challenges due to the\nsignificant differences between the linguistic semantics of pre-trained LLMs\nand the collaborative semantics essential for RSs. These systems use\npre-trained linguistic semantics but learn collaborative semantics from scratch\nvia the llm-Backbone. However, LLMs are not designed for recommendations,\nleading to inefficient collaborative learning, weak result correlations, and\npoor integration of traditional RS features. To address these challenges, we\npropose EAGER-LLM, a decoder-only llm-based generative recommendation framework\nthat integrates endogenous and exogenous behavioral and semantic information in\na non-intrusive manner. Specifically, we propose 1)dual-source knowledge-rich\nitem indices that integrates indexing sequences for exogenous signals, enabling\nefficient link-wide processing; 2)non-invasive multiscale alignment\nreconstruction tasks guide the model toward a deeper understanding of both\ncollaborative and semantic signals; 3)an annealing adapter designed to finely\nbalance the model's recommendation performance with its comprehension\ncapabilities. We demonstrate EAGER-LLM's effectiveness through rigorous testing\non three public benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly leveraged as foundational\nbackbones in the development of advanced recommender systems, offering enhanced\ncapabilities through their extensive knowledge and reasoning. Existing\nllm-based recommender systems (RSs) often face challenges due to the\nsignificant differences between the linguistic semantics of pre-trained LLMs\nand the collaborative semantics essential for RSs. These systems use\npre-trained linguistic semantics but learn collaborative semantics from scratch\nvia the llm-Backbone. However, LLMs are not designed for recommendations,\nleading to inefficient collaborative learning, weak result correlations, and\npoor integration of traditional RS features. To address these challenges, we\npropose EAGER-LLM, a decoder-only llm-based generative recommendation framework\nthat integrates endogenous and exogenous behavioral and semantic information in\na non-intrusive manner. Specifically, we propose 1)dual-source knowledge-rich\nitem indices that integrates indexing sequences for exogenous signals, enabling\nefficient link-wide processing; 2)non-invasive multiscale alignment\nreconstruction tasks guide the model toward a deeper understanding of both\ncollaborative and semantic signals; 3)an annealing adapter designed to finely\nbalance the model's recommendation performance with its comprehension\ncapabilities. We demonstrate EAGER-LLM's effectiveness through rigorous testing\non three public benchmarks."
                },
                "authors": [
                    {
                        "name": "Minjie Hong"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Zehan Wang"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Sihang Cai"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Quanyu Dai"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Zhimeng Zhang"
                    },
                    {
                        "name": "Zhou Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Zhao"
                },
                "author": "Zhou Zhao",
                "arxiv_doi": "10.1145/3696410.3714933",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714933",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 6 figures, accpeted by WWW 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14727v1",
                "updated": "2025-02-20T16:54:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    54,
                    7,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:54:07Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    54,
                    7,
                    3,
                    51,
                    0
                ],
                "title": "WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken\n  Dialogue Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken\n  Dialogue Models"
                },
                "summary": "Retrieval Augmented Generation (RAG) has gained widespread adoption owing to\nits capacity to empower large language models (LLMs) to integrate external\nknowledge. However, existing RAG frameworks are primarily designed for\ntext-based LLMs and rely on Automatic Speech Recognition to process speech\ninput, which discards crucial audio information, risks transcription errors,\nand increases computational overhead. Therefore, we introduce WavRAG, the first\nretrieval augmented generation framework with native, end-to-end audio support.\nWavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw\naudio for both embedding and retrieval. 2) WavRAG integrates audio and text\ninto a unified knowledge representation. Specifically, we propose the\nWavRetriever to facilitate the retrieval from a text-audio hybrid knowledge\nbase, and further enhance the in-context capabilities of spoken dialogue models\nthrough the integration of chain-of-thought reasoning. In comparison to\nstate-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval\nperformance while delivering a 10x acceleration. Furthermore, WavRAG's unique\ntext-audio hybrid retrieval capability extends the boundaries of RAG to the\naudio modality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has gained widespread adoption owing to\nits capacity to empower large language models (LLMs) to integrate external\nknowledge. However, existing RAG frameworks are primarily designed for\ntext-based LLMs and rely on Automatic Speech Recognition to process speech\ninput, which discards crucial audio information, risks transcription errors,\nand increases computational overhead. Therefore, we introduce WavRAG, the first\nretrieval augmented generation framework with native, end-to-end audio support.\nWavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw\naudio for both embedding and retrieval. 2) WavRAG integrates audio and text\ninto a unified knowledge representation. Specifically, we propose the\nWavRetriever to facilitate the retrieval from a text-audio hybrid knowledge\nbase, and further enhance the in-context capabilities of spoken dialogue models\nthrough the integration of chain-of-thought reasoning. In comparison to\nstate-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval\nperformance while delivering a 10x acceleration. Furthermore, WavRAG's unique\ntext-audio hybrid retrieval capability extends the boundaries of RAG to the\naudio modality."
                },
                "authors": [
                    {
                        "name": "Yifu Chen"
                    },
                    {
                        "name": "Shengpeng Ji"
                    },
                    {
                        "name": "Haoxiao Wang"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Jinzheng He"
                    },
                    {
                        "name": "Jin Xu"
                    },
                    {
                        "name": "Zhou Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Zhao"
                },
                "author": "Zhou Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.08469v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.08469v6",
                "updated": "2025-02-20T16:48:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    48,
                    8,
                    3,
                    51,
                    0
                ],
                "published": "2023-08-16T16:19:50Z",
                "published_parsed": [
                    2023,
                    8,
                    16,
                    16,
                    19,
                    50,
                    2,
                    228,
                    0
                ],
                "title": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series\n  Forecasters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series\n  Forecasters"
                },
                "summary": "Multivariate time-series forecasting is vital in various domains, e.g.,\neconomic planning and weather prediction. Deep train-from-scratch models have\nexhibited effective performance yet require large amounts of data, which limits\nreal-world applicability. Recently, researchers have leveraged the\nrepresentation learning transferability of pre-trained Large Language Models\n(LLMs) to handle limited non-linguistic datasets effectively. However,\nincorporating LLMs with time-series data presents challenges of limited\nadaptation due to different compositions between time-series and linguistic\ndata, and the inability to process multi-scale temporal information. To tackle\nthese challenges, we propose LLM4TS, a framework for time-series forecasting\nwith pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the\ntime-series alignment stage to align LLMs with the nuances of time-series data,\nand the forecasting fine-tuning stage for downstream time-series forecasting\ntasks. Furthermore, our framework features a novel two-level aggregation method\nthat integrates multi-scale temporal data within pre-trained LLMs, enhancing\ntheir ability to interpret time-specific information. In experiments across 7\ntime-series forecasting datasets, LLM4TS is superior to existing\nstate-of-the-art methods compared with trained-from-scratch models in full-shot\nscenarios, and also achieves the highest rank in few-shot scenarios. In\naddition, evaluations compared with different unsupervised representation\nlearning approaches highlight LLM4TS's effectiveness with representation\nlearning in forecasting tasks. Ablation studies further validate each\ncomponent's contribution to LLM4TS and underscore the essential role of\nutilizing LLM's pre-trained weights for optimal performance. The code is\navailable at https://github.com/blacksnail789521/LLM4TS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate time-series forecasting is vital in various domains, e.g.,\neconomic planning and weather prediction. Deep train-from-scratch models have\nexhibited effective performance yet require large amounts of data, which limits\nreal-world applicability. Recently, researchers have leveraged the\nrepresentation learning transferability of pre-trained Large Language Models\n(LLMs) to handle limited non-linguistic datasets effectively. However,\nincorporating LLMs with time-series data presents challenges of limited\nadaptation due to different compositions between time-series and linguistic\ndata, and the inability to process multi-scale temporal information. To tackle\nthese challenges, we propose LLM4TS, a framework for time-series forecasting\nwith pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the\ntime-series alignment stage to align LLMs with the nuances of time-series data,\nand the forecasting fine-tuning stage for downstream time-series forecasting\ntasks. Furthermore, our framework features a novel two-level aggregation method\nthat integrates multi-scale temporal data within pre-trained LLMs, enhancing\ntheir ability to interpret time-specific information. In experiments across 7\ntime-series forecasting datasets, LLM4TS is superior to existing\nstate-of-the-art methods compared with trained-from-scratch models in full-shot\nscenarios, and also achieves the highest rank in few-shot scenarios. In\naddition, evaluations compared with different unsupervised representation\nlearning approaches highlight LLM4TS's effectiveness with representation\nlearning in forecasting tasks. Ablation studies further validate each\ncomponent's contribution to LLM4TS and underscore the essential role of\nutilizing LLM's pre-trained weights for optimal performance. The code is\navailable at https://github.com/blacksnail789521/LLM4TS."
                },
                "authors": [
                    {
                        "name": "Ching Chang"
                    },
                    {
                        "name": "Wei-Yao Wang"
                    },
                    {
                        "name": "Wen-Chih Peng"
                    },
                    {
                        "name": "Tien-Fu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tien-Fu Chen"
                },
                "author": "Tien-Fu Chen",
                "arxiv_doi": "10.1145/3719207 10.1145/3719207",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3719207",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3719207",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.08469v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.08469v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in ACM Transactions on Intelligent Systems\n  and Technology (TIST) 2025. The final published version will be available at\n  https://doi.org/10.1145/3719207",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13019v2",
                "updated": "2025-02-20T16:47:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    47,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-18T16:38:39Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    38,
                    39,
                    1,
                    49,
                    0
                ],
                "title": "Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented\n  Generation"
                },
                "summary": "Despite the remarkable capabilities of Large Language Models (LLMs) in\nvarious NLP tasks, they remain vulnerable to hallucinations due to their\nlimited parametric knowledge and lack of domain-specific expertise.\nRetrieval-Augmented Generation (RAG) addresses this challenge by incorporating\nexternal document retrieval to augment the knowledge base of LLMs. In this\napproach, RAG retrieves document chunks from an external corpus in response to\na query, which are then used as context for the downstream language model to\ngenerate an answer. However, these retrieved knowledge sources often include\nirrelevant or erroneous information, undermining the effectiveness of RAG in\ndownstream tasks. To overcome this limitation, we introduce a compact,\nefficient, and pluggable module designed to refine external knowledge sources\nbefore feeding them to the generator. The module reconstructs retrieved content\nby extracting the most relevant and supportive information and reorganising it\ninto a concise, query-specific format. Through a three-stage training paradigm\n- comprising supervised fine-tuning, contrastive multi-task learning, and\nreinforcement learning-based alignment - it prioritises critical knowledge and\naligns it with the generator's preferences. This method enables LLMs to produce\noutputs that are more accurate, reliable, and contextually appropriate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable capabilities of Large Language Models (LLMs) in\nvarious NLP tasks, they remain vulnerable to hallucinations due to their\nlimited parametric knowledge and lack of domain-specific expertise.\nRetrieval-Augmented Generation (RAG) addresses this challenge by incorporating\nexternal document retrieval to augment the knowledge base of LLMs. In this\napproach, RAG retrieves document chunks from an external corpus in response to\na query, which are then used as context for the downstream language model to\ngenerate an answer. However, these retrieved knowledge sources often include\nirrelevant or erroneous information, undermining the effectiveness of RAG in\ndownstream tasks. To overcome this limitation, we introduce a compact,\nefficient, and pluggable module designed to refine external knowledge sources\nbefore feeding them to the generator. The module reconstructs retrieved content\nby extracting the most relevant and supportive information and reorganising it\ninto a concise, query-specific format. Through a three-stage training paradigm\n- comprising supervised fine-tuning, contrastive multi-task learning, and\nreinforcement learning-based alignment - it prioritises critical knowledge and\naligns it with the generator's preferences. This method enables LLMs to produce\noutputs that are more accurate, reliable, and contextually appropriate."
                },
                "authors": [
                    {
                        "name": "Sha Li"
                    },
                    {
                        "name": "Naren Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Naren Ramakrishnan"
                },
                "author": "Naren Ramakrishnan",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14718v1",
                "updated": "2025-02-20T16:44:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    44,
                    46,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:44:46Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    44,
                    46,
                    3,
                    51,
                    0
                ],
                "title": "Entity Framing and Role Portrayal in the News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity Framing and Role Portrayal in the News"
                },
                "summary": "We introduce a novel multilingual hierarchical corpus annotated for entity\nframing and role portrayal in news articles. The dataset uses a unique taxonomy\ninspired by storytelling elements, comprising 22 fine-grained roles, or\narchetypes, nested within three main categories: protagonist, antagonist, and\ninnocent. Each archetype is carefully defined, capturing nuanced portrayals of\nentities such as guardian, martyr, and underdog for protagonists; tyrant,\ndeceiver, and bigot for antagonists; and victim, scapegoat, and exploited for\ninnocents. The dataset includes 1,378 recent news articles in five languages\n(Bulgarian, English, Hindi, European Portuguese, and Russian) focusing on two\ncritical domains of global significance: the Ukraine-Russia War and Climate\nChange. Over 5,800 entity mentions have been annotated with role labels. This\ndataset serves as a valuable resource for research into role portrayal and has\nbroader implications for news analysis. We describe the characteristics of the\ndataset and the annotation process, and we report evaluation results on\nfine-tuned state-of-the-art multilingual transformers and hierarchical\nzero-shot learning using LLMs at the level of a document, a paragraph, and a\nsentence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel multilingual hierarchical corpus annotated for entity\nframing and role portrayal in news articles. The dataset uses a unique taxonomy\ninspired by storytelling elements, comprising 22 fine-grained roles, or\narchetypes, nested within three main categories: protagonist, antagonist, and\ninnocent. Each archetype is carefully defined, capturing nuanced portrayals of\nentities such as guardian, martyr, and underdog for protagonists; tyrant,\ndeceiver, and bigot for antagonists; and victim, scapegoat, and exploited for\ninnocents. The dataset includes 1,378 recent news articles in five languages\n(Bulgarian, English, Hindi, European Portuguese, and Russian) focusing on two\ncritical domains of global significance: the Ukraine-Russia War and Climate\nChange. Over 5,800 entity mentions have been annotated with role labels. This\ndataset serves as a valuable resource for research into role portrayal and has\nbroader implications for news analysis. We describe the characteristics of the\ndataset and the annotation process, and we report evaluation results on\nfine-tuned state-of-the-art multilingual transformers and hierarchical\nzero-shot learning using LLMs at the level of a document, a paragraph, and a\nsentence."
                },
                "authors": [
                    {
                        "name": "Tarek Mahmoud"
                    },
                    {
                        "name": "Zhuohan Xie"
                    },
                    {
                        "name": "Dimitar Dimitrov"
                    },
                    {
                        "name": "Nikolaos Nikolaidis"
                    },
                    {
                        "name": "Purificao Silvano"
                    },
                    {
                        "name": "Roman Yangarber"
                    },
                    {
                        "name": "Shivam Sharma"
                    },
                    {
                        "name": "Elisa Sartori"
                    },
                    {
                        "name": "Nicolas Stefanovitch"
                    },
                    {
                        "name": "Giovanni Da San Martino"
                    },
                    {
                        "name": "Jakub Piskorski"
                    },
                    {
                        "name": "Preslav Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Preslav Nakov"
                },
                "author": "Preslav Nakov",
                "arxiv_comment": "23 pages, 12 figures. Submitted to ACL Rolling Review (ARR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09636v2",
                "updated": "2025-02-20T16:40:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    40,
                    48,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-09T04:40:35Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    4,
                    40,
                    35,
                    6,
                    40,
                    0
                ],
                "title": "Reading between the Lines: Can LLMs Identify Cross-Cultural\n  Communication Gaps?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reading between the Lines: Can LLMs Identify Cross-Cultural\n  Communication Gaps?"
                },
                "summary": "In a rapidly globalizing and digital world, content such as book and product\nreviews created by people from diverse cultures are read and consumed by others\nfrom different corners of the world. In this paper, we investigate the extent\nand patterns of gaps in understandability of book reviews due to the presence\nof culturally-specific items and elements that might be alien to users from\nanother culture. Our user-study on 57 book reviews from Goodreads reveal that\n83\\% of the reviews had at least one culture-specific difficult-to-understand\nelement. We also evaluate the efficacy of GPT-4o in identifying such items,\ngiven the cultural background of the reader; the results are mixed, implying a\nsignificant scope for improvement. Our datasets are available here:\nhttps://github.com/sougata-ub/reading_between_lines",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a rapidly globalizing and digital world, content such as book and product\nreviews created by people from diverse cultures are read and consumed by others\nfrom different corners of the world. In this paper, we investigate the extent\nand patterns of gaps in understandability of book reviews due to the presence\nof culturally-specific items and elements that might be alien to users from\nanother culture. Our user-study on 57 book reviews from Goodreads reveal that\n83\\% of the reviews had at least one culture-specific difficult-to-understand\nelement. We also evaluate the efficacy of GPT-4o in identifying such items,\ngiven the cultural background of the reader; the results are mixed, implying a\nsignificant scope for improvement. Our datasets are available here:\nhttps://github.com/sougata-ub/reading_between_lines"
                },
                "authors": [
                    {
                        "name": "Sougata Saha"
                    },
                    {
                        "name": "Saurabh Kumar Pandey"
                    },
                    {
                        "name": "Harshit Gupta"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14714v1",
                "updated": "2025-02-20T16:39:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    39,
                    57,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:39:57Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    39,
                    57,
                    3,
                    51,
                    0
                ],
                "title": "From Knowledge Generation to Knowledge Verification: Examining the\n  BioMedical Generative Capabilities of ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Knowledge Generation to Knowledge Verification: Examining the\n  BioMedical Generative Capabilities of ChatGPT"
                },
                "summary": "The generative capabilities of LLM models present opportunities in\naccelerating tasks and concerns with the authenticity of the knowledge it\nproduces. To address the concerns, we present a computational approach that\nsystematically evaluates the factual accuracy of biomedical knowledge that an\nLLM model has been prompted to generate. Our approach encompasses two\nprocesses: the generation of disease-centric associations and the verification\nof them using the semantic knowledge of the biomedical ontologies. Using\nChatGPT as the select LLM model, we designed a set of prompt-engineering\nprocesses to generate linkages between diseases, drugs, symptoms, and genes to\nestablish grounds for assessments. Experimental results demonstrate high\naccuracy in identifying disease terms (88%-97%), drug names (90%-91%), and\ngenetic information (88%-98%). The symptom term identification accuracy was\nnotably lower (49%-61%), as verified against the DOID, ChEBI, SYMPTOM, and GO\nontologies accordingly. The verification of associations reveals literature\ncoverage rates of (89%-91%) among disease-drug and disease-gene associations.\nThe low identification accuracy for symptom terms also contributed to the\nverification of symptom-related associations (49%-62%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generative capabilities of LLM models present opportunities in\naccelerating tasks and concerns with the authenticity of the knowledge it\nproduces. To address the concerns, we present a computational approach that\nsystematically evaluates the factual accuracy of biomedical knowledge that an\nLLM model has been prompted to generate. Our approach encompasses two\nprocesses: the generation of disease-centric associations and the verification\nof them using the semantic knowledge of the biomedical ontologies. Using\nChatGPT as the select LLM model, we designed a set of prompt-engineering\nprocesses to generate linkages between diseases, drugs, symptoms, and genes to\nestablish grounds for assessments. Experimental results demonstrate high\naccuracy in identifying disease terms (88%-97%), drug names (90%-91%), and\ngenetic information (88%-98%). The symptom term identification accuracy was\nnotably lower (49%-61%), as verified against the DOID, ChEBI, SYMPTOM, and GO\nontologies accordingly. The verification of associations reveals literature\ncoverage rates of (89%-91%) among disease-drug and disease-gene associations.\nThe low identification accuracy for symptom terms also contributed to the\nverification of symptom-related associations (49%-62%)."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdeen Hamed"
                    },
                    {
                        "name": "Byung Suk Lee"
                    }
                ],
                "author_detail": {
                    "name": "Byung Suk Lee"
                },
                "author": "Byung Suk Lee",
                "arxiv_comment": "26 pages, 6 figures, In Review with a Cell Press Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.4; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14707v1",
                "updated": "2025-02-20T16:31:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    31,
                    24,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:31:24Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    31,
                    24,
                    3,
                    51,
                    0
                ],
                "title": "TRUSWorthy: Toward Clinically Applicable Deep Learning for Confident\n  Detection of Prostate Cancer in Micro-Ultrasound",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRUSWorthy: Toward Clinically Applicable Deep Learning for Confident\n  Detection of Prostate Cancer in Micro-Ultrasound"
                },
                "summary": "While deep learning methods have shown great promise in improving the\neffectiveness of prostate cancer (PCa) diagnosis by detecting suspicious\nlesions from trans-rectal ultrasound (TRUS), they must overcome multiple\nsimultaneous challenges. There is high heterogeneity in tissue appearance,\nsignificant class imbalance in favor of benign examples, and scarcity in the\nnumber and quality of ground truth annotations available to train models.\nFailure to address even a single one of these problems can result in\nunacceptable clinical outcomes.We propose TRUSWorthy, a carefully designed,\ntuned, and integrated system for reliable PCa detection. Our pipeline\nintegrates self-supervised learning, multiple-instance learning aggregation\nusing transformers, random-undersampled boosting and ensembling: these address\nlabel scarcity, weak labels, class imbalance, and overconfidence, respectively.\nWe train and rigorously evaluate our method using a large, multi-center dataset\nof micro-ultrasound data. Our method outperforms previous state-of-the-art deep\nlearning methods in terms of accuracy and uncertainty calibration, with AUROC\nand balanced accuracy scores of 79.9% and 71.5%, respectively. On the top 20%\nof predictions with the highest confidence, we can achieve a balanced accuracy\nof up to 91%. The success of TRUSWorthy demonstrates the potential of\nintegrated deep learning solutions to meet clinical needs in a highly\nchallenging deployment setting, and is a significant step towards creating a\ntrustworthy system for computer-assisted PCa diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While deep learning methods have shown great promise in improving the\neffectiveness of prostate cancer (PCa) diagnosis by detecting suspicious\nlesions from trans-rectal ultrasound (TRUS), they must overcome multiple\nsimultaneous challenges. There is high heterogeneity in tissue appearance,\nsignificant class imbalance in favor of benign examples, and scarcity in the\nnumber and quality of ground truth annotations available to train models.\nFailure to address even a single one of these problems can result in\nunacceptable clinical outcomes.We propose TRUSWorthy, a carefully designed,\ntuned, and integrated system for reliable PCa detection. Our pipeline\nintegrates self-supervised learning, multiple-instance learning aggregation\nusing transformers, random-undersampled boosting and ensembling: these address\nlabel scarcity, weak labels, class imbalance, and overconfidence, respectively.\nWe train and rigorously evaluate our method using a large, multi-center dataset\nof micro-ultrasound data. Our method outperforms previous state-of-the-art deep\nlearning methods in terms of accuracy and uncertainty calibration, with AUROC\nand balanced accuracy scores of 79.9% and 71.5%, respectively. On the top 20%\nof predictions with the highest confidence, we can achieve a balanced accuracy\nof up to 91%. The success of TRUSWorthy demonstrates the potential of\nintegrated deep learning solutions to meet clinical needs in a highly\nchallenging deployment setting, and is a significant step towards creating a\ntrustworthy system for computer-assisted PCa diagnosis."
                },
                "authors": [
                    {
                        "name": "Mohamed Harmanani"
                    },
                    {
                        "name": "Paul F. R. Wilson"
                    },
                    {
                        "name": "Minh Nguyen Nhat To"
                    },
                    {
                        "name": "Mahdi Gilany"
                    },
                    {
                        "name": "Amoon Jamzad"
                    },
                    {
                        "name": "Fahimeh Fooladgar"
                    },
                    {
                        "name": "Brian Wodlinger"
                    },
                    {
                        "name": "Purang Abolmaesumi"
                    },
                    {
                        "name": "Parvin Mousavi"
                    }
                ],
                "author_detail": {
                    "name": "Parvin Mousavi"
                },
                "author": "Parvin Mousavi",
                "arxiv_comment": "accepted to IJCARS. This preprint has not undergone post-submission\n  improvements or corrections. To access the Version of Record of this article,\n  see the journal reference below",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.TO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12844v2",
                "updated": "2025-02-20T16:20:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    20,
                    11,
                    3,
                    51,
                    0
                ],
                "published": "2024-07-04T17:57:38Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    17,
                    57,
                    38,
                    3,
                    186,
                    0
                ],
                "title": "metabench -- A Sparse Benchmark of Reasoning and Knowledge in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "metabench -- A Sparse Benchmark of Reasoning and Knowledge in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) vary in their abilities on a range of tasks.\nInitiatives such as the Open LLM Leaderboard aim to quantify these differences\nwith several large benchmarks (sets of test items to which an LLM can respond\neither correctly or incorrectly). However, high correlations within and between\nbenchmark scores suggest that (1) there exists a small set of common underlying\nabilities that these benchmarks measure, and (2) items tap into redundant\ninformation and the benchmarks may thus be considerably compressed. We use data\nfrom n > 5000 LLMs to identify the most informative items of six benchmarks,\nARC, GSM8K, HellaSwag, MMLU, TruthfulQA and WinoGrande (with d = 28,632 items\nin total). From them we distill a sparse benchmark, metabench, that has less\nthan 3% of the original size of all six benchmarks combined. This new sparse\nbenchmark goes beyond point scores by yielding estimators of the underlying\nbenchmark-specific abilities. We show that these estimators (1) can be used to\nreconstruct each original individual benchmark score with, on average, 1.24%\nroot mean square error (RMSE), (2) reconstruct the original total score with\n0.58% RMSE, and (3) have a single underlying common factor whose Spearman\ncorrelation with the total score is r = 0.94.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) vary in their abilities on a range of tasks.\nInitiatives such as the Open LLM Leaderboard aim to quantify these differences\nwith several large benchmarks (sets of test items to which an LLM can respond\neither correctly or incorrectly). However, high correlations within and between\nbenchmark scores suggest that (1) there exists a small set of common underlying\nabilities that these benchmarks measure, and (2) items tap into redundant\ninformation and the benchmarks may thus be considerably compressed. We use data\nfrom n > 5000 LLMs to identify the most informative items of six benchmarks,\nARC, GSM8K, HellaSwag, MMLU, TruthfulQA and WinoGrande (with d = 28,632 items\nin total). From them we distill a sparse benchmark, metabench, that has less\nthan 3% of the original size of all six benchmarks combined. This new sparse\nbenchmark goes beyond point scores by yielding estimators of the underlying\nbenchmark-specific abilities. We show that these estimators (1) can be used to\nreconstruct each original individual benchmark score with, on average, 1.24%\nroot mean square error (RMSE), (2) reconstruct the original total score with\n0.58% RMSE, and (3) have a single underlying common factor whose Spearman\ncorrelation with the total score is r = 0.94."
                },
                "authors": [
                    {
                        "name": "Alex Kipnis"
                    },
                    {
                        "name": "Konstantinos Voudouris"
                    },
                    {
                        "name": "Luca M. Schulze Buschoff"
                    },
                    {
                        "name": "Eric Schulz"
                    }
                ],
                "author_detail": {
                    "name": "Eric Schulz"
                },
                "author": "Eric Schulz",
                "arxiv_comment": "accepted for publication at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14693v1",
                "updated": "2025-02-20T16:19:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    19,
                    9,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:19:09Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    19,
                    9,
                    3,
                    51,
                    0
                ],
                "title": "I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree\n  Search"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown remarkable\npotential in automating machine learning tasks. However, existing LLM-based\nagents often struggle with low-diversity and suboptimal code generation. While\nrecent work has introduced Monte Carlo Tree Search (MCTS) to address these\nissues, limitations persist in the quality and diversity of thoughts generated,\nas well as in the scalar value feedback mechanisms used for node selection. In\nthis study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a\nnovel approach that iteratively expands tree nodes through an introspective\nprocess that meticulously analyzes solutions and results from parent and\nsibling nodes. This facilitates a continuous refinement of the node in the\nsearch tree, thereby enhancing the overall decision-making process.Furthermore,\nwe integrate a Large Language Model (LLM)-based value model to facilitate\ndirect evaluation of each node's solution prior to conducting comprehensive\ncomputational rollouts. A hybrid rewarding mechanism is implemented to\nseamlessly transition the Q-value from LLM-estimated scores to actual\nperformance scores. This allows higher-quality nodes to be traversed\nearlier.Applied to the various ML tasks, our approach demonstrates a6\\%\nabsolute improvement in performance compared to the strong open-source AutoML\nagents, showcasing its effectiveness in enhancing agentic AutoML systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown remarkable\npotential in automating machine learning tasks. However, existing LLM-based\nagents often struggle with low-diversity and suboptimal code generation. While\nrecent work has introduced Monte Carlo Tree Search (MCTS) to address these\nissues, limitations persist in the quality and diversity of thoughts generated,\nas well as in the scalar value feedback mechanisms used for node selection. In\nthis study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a\nnovel approach that iteratively expands tree nodes through an introspective\nprocess that meticulously analyzes solutions and results from parent and\nsibling nodes. This facilitates a continuous refinement of the node in the\nsearch tree, thereby enhancing the overall decision-making process.Furthermore,\nwe integrate a Large Language Model (LLM)-based value model to facilitate\ndirect evaluation of each node's solution prior to conducting comprehensive\ncomputational rollouts. A hybrid rewarding mechanism is implemented to\nseamlessly transition the Q-value from LLM-estimated scores to actual\nperformance scores. This allows higher-quality nodes to be traversed\nearlier.Applied to the various ML tasks, our approach demonstrates a6\\%\nabsolute improvement in performance compared to the strong open-source AutoML\nagents, showcasing its effectiveness in enhancing agentic AutoML systems."
                },
                "authors": [
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Feng Wei"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Yuxi Qian"
                    },
                    {
                        "name": "Xinhui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xinhui Wu"
                },
                "author": "Xinhui Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14682v1",
                "updated": "2025-02-20T16:11:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    11,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:11:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    11,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Bridging the Gap: Transforming Natural Language Questions into SQL\n  Queries via Abstract Query Pattern and Contextual Schema Markup",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Gap: Transforming Natural Language Questions into SQL\n  Queries via Abstract Query Pattern and Contextual Schema Markup"
                },
                "summary": "Large language models have demonstrated excellent performance in many tasks,\nincluding Text-to-SQL, due to their powerful in-context learning capabilities.\nThey are becoming the mainstream approach for Text-to-SQL. However, these\nmethods still have a significant gap compared to human performance, especially\non complex questions. As the complexity of questions increases, the gap between\nquestions and SQLs increases. We identify two important gaps: the structural\nmapping gap and the lexical mapping gap. To tackle these two gaps, we propose\nPAS-SQL, an efficient SQL generation pipeline based on LLMs, which alleviates\ngaps through Abstract Query Pattern (AQP) and Contextual Schema Markup (CSM).\nAQP aims to obtain the structural pattern of the question by removing\ndatabase-related information, which enables us to find structurally similar\ndemonstrations. CSM aims to associate database-related text span in the\nquestion with specific tables or columns in the database, which alleviates the\nlexical mapping gap. Experimental results on the Spider and BIRD datasets\ndemonstrate the effectiveness of our proposed method. Specifically, PAS-SQL +\nGPT-4o sets a new state-of-the-art on the Spider benchmark with an execution\naccuracy of 87.9\\%, and achieves leading results on the BIRD dataset with an\nexecution accuracy of 64.67\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated excellent performance in many tasks,\nincluding Text-to-SQL, due to their powerful in-context learning capabilities.\nThey are becoming the mainstream approach for Text-to-SQL. However, these\nmethods still have a significant gap compared to human performance, especially\non complex questions. As the complexity of questions increases, the gap between\nquestions and SQLs increases. We identify two important gaps: the structural\nmapping gap and the lexical mapping gap. To tackle these two gaps, we propose\nPAS-SQL, an efficient SQL generation pipeline based on LLMs, which alleviates\ngaps through Abstract Query Pattern (AQP) and Contextual Schema Markup (CSM).\nAQP aims to obtain the structural pattern of the question by removing\ndatabase-related information, which enables us to find structurally similar\ndemonstrations. CSM aims to associate database-related text span in the\nquestion with specific tables or columns in the database, which alleviates the\nlexical mapping gap. Experimental results on the Spider and BIRD datasets\ndemonstrate the effectiveness of our proposed method. Specifically, PAS-SQL +\nGPT-4o sets a new state-of-the-art on the Spider benchmark with an execution\naccuracy of 87.9\\%, and achieves leading results on the BIRD dataset with an\nexecution accuracy of 64.67\\%."
                },
                "authors": [
                    {
                        "name": "Yonghui Kong"
                    },
                    {
                        "name": "Hongbing Hu"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Siyuan Chai"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14678v1",
                "updated": "2025-02-20T16:09:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    9,
                    55,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:09:55Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    9,
                    55,
                    3,
                    51,
                    0
                ],
                "title": "How to Get Your LLM to Generate Challenging Problems for Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Get Your LLM to Generate Challenging Problems for Evaluation"
                },
                "summary": "The pace of evolution of Large Language Models (LLMs) necessitates new\napproaches for rigorous and comprehensive evaluation. Traditional human\nannotation is increasingly impracticable due to the complexities and costs\ninvolved in generating high-quality, challenging problems. In this work, we\nintroduce CHASE, a unified framework to synthetically generate challenging\nproblems using LLMs without human involvement. For a given task, our approach\nbuilds a hard problem in a bottom-up manner from simpler components. Moreover,\nour framework decomposes the generation process into independently verifiable\nsub-tasks, thereby ensuring a high level of quality and correctness. We\nimplement CHASE to create evaluation benchmarks across three diverse domains:\n(1) document-based question answering, (2) repository-level code completion,\nand (3) math reasoning. The performance of state-of-the-art LLMs on these\nsynthetic benchmarks lies in the range of 40-60% accuracy, thereby\ndemonstrating the effectiveness of our framework at generating challenging\nproblems. We publicly release our benchmarks and code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pace of evolution of Large Language Models (LLMs) necessitates new\napproaches for rigorous and comprehensive evaluation. Traditional human\nannotation is increasingly impracticable due to the complexities and costs\ninvolved in generating high-quality, challenging problems. In this work, we\nintroduce CHASE, a unified framework to synthetically generate challenging\nproblems using LLMs without human involvement. For a given task, our approach\nbuilds a hard problem in a bottom-up manner from simpler components. Moreover,\nour framework decomposes the generation process into independently verifiable\nsub-tasks, thereby ensuring a high level of quality and correctness. We\nimplement CHASE to create evaluation benchmarks across three diverse domains:\n(1) document-based question answering, (2) repository-level code completion,\nand (3) math reasoning. The performance of state-of-the-art LLMs on these\nsynthetic benchmarks lies in the range of 40-60% accuracy, thereby\ndemonstrating the effectiveness of our framework at generating challenging\nproblems. We publicly release our benchmarks and code."
                },
                "authors": [
                    {
                        "name": "Arkil Patel"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Dzmitry Bahdanau"
                    }
                ],
                "author_detail": {
                    "name": "Dzmitry Bahdanau"
                },
                "author": "Dzmitry Bahdanau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14677v1",
                "updated": "2025-02-20T16:09:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    9,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:09:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    9,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Data-Constrained Synthesis of Training Data for De-Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Constrained Synthesis of Training Data for De-Identification"
                },
                "summary": "Many sensitive domains -- such as the clinical domain -- lack widely\navailable datasets due to privacy risks. The increasing generative capabilities\nof large language models (LLMs) have made synthetic datasets a viable path\nforward. In this study, we domain-adapt LLMs to the clinical domain and\ngenerate synthetic clinical texts that are machine-annotated with tags for\npersonally identifiable information using capable encoder-based NER models. The\nsynthetic corpora are then used to train synthetic NER models. The results show\nthat training NER models using synthetic corpora incurs only a small drop in\npredictive performance. The limits of this process are investigated in a\nsystematic ablation study -- using both Swedish and Spanish data. Our analysis\nshows that smaller datasets can be sufficient for domain-adapting LLMs for data\nsynthesis. Instead, the effectiveness of this process is almost entirely\ncontingent on the performance of the machine-annotating NER models trained\nusing the original data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many sensitive domains -- such as the clinical domain -- lack widely\navailable datasets due to privacy risks. The increasing generative capabilities\nof large language models (LLMs) have made synthetic datasets a viable path\nforward. In this study, we domain-adapt LLMs to the clinical domain and\ngenerate synthetic clinical texts that are machine-annotated with tags for\npersonally identifiable information using capable encoder-based NER models. The\nsynthetic corpora are then used to train synthetic NER models. The results show\nthat training NER models using synthetic corpora incurs only a small drop in\npredictive performance. The limits of this process are investigated in a\nsystematic ablation study -- using both Swedish and Spanish data. Our analysis\nshows that smaller datasets can be sufficient for domain-adapting LLMs for data\nsynthesis. Instead, the effectiveness of this process is almost entirely\ncontingent on the performance of the machine-annotating NER models trained\nusing the original data."
                },
                "authors": [
                    {
                        "name": "Thomas Vakili"
                    },
                    {
                        "name": "Aron Henriksson"
                    },
                    {
                        "name": "Hercules Dalianis"
                    }
                ],
                "author_detail": {
                    "name": "Hercules Dalianis"
                },
                "author": "Hercules Dalianis",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14671v1",
                "updated": "2025-02-20T16:05:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    5,
                    45,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:05:45Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    5,
                    45,
                    3,
                    51,
                    0
                ],
                "title": "Explanations of Deep Language Models Explain Language Representations in\n  the Brain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanations of Deep Language Models Explain Language Representations in\n  the Brain"
                },
                "summary": "Recent advances in artificial intelligence have given rise to large language\nmodels (LLMs) that not only achieve human-like performance but also share\ncomputational principles with the brain's language processing mechanisms. While\nprevious research has primarily focused on aligning LLMs' internal\nrepresentations with neural activity, we introduce a novel approach that\nleverages explainable AI (XAI) methods to forge deeper connections between the\ntwo domains. Using attribution methods, we quantified how preceding words\ncontribute to an LLM's next-word predictions and employed these explanations to\npredict fMRI recordings from participants listening to the same narratives. Our\nfindings demonstrate that attribution methods robustly predict brain activity\nacross the language network, surpassing traditional internal representations in\nearly language areas. This alignment is hierarchical: early-layer explanations\ncorrespond to the initial stages of language processing in the brain, while\nlater layers align with more advanced stages. Moreover, the layers more\ninfluential on LLM next-word prediction$\\unicode{x2014}$those with higher\nattribution scores$\\unicode{x2014}$exhibited stronger alignment with neural\nactivity. This work establishes a bidirectional bridge between AI and\nneuroscience. First, we demonstrate that attribution methods offer a powerful\nlens for investigating the neural mechanisms of language comprehension,\nrevealing how meaning emerges from preceding context. Second, we propose using\nbrain alignment as a metric to evaluate the validity of attribution methods,\nproviding a framework for assessing their biological plausibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in artificial intelligence have given rise to large language\nmodels (LLMs) that not only achieve human-like performance but also share\ncomputational principles with the brain's language processing mechanisms. While\nprevious research has primarily focused on aligning LLMs' internal\nrepresentations with neural activity, we introduce a novel approach that\nleverages explainable AI (XAI) methods to forge deeper connections between the\ntwo domains. Using attribution methods, we quantified how preceding words\ncontribute to an LLM's next-word predictions and employed these explanations to\npredict fMRI recordings from participants listening to the same narratives. Our\nfindings demonstrate that attribution methods robustly predict brain activity\nacross the language network, surpassing traditional internal representations in\nearly language areas. This alignment is hierarchical: early-layer explanations\ncorrespond to the initial stages of language processing in the brain, while\nlater layers align with more advanced stages. Moreover, the layers more\ninfluential on LLM next-word prediction$\\unicode{x2014}$those with higher\nattribution scores$\\unicode{x2014}$exhibited stronger alignment with neural\nactivity. This work establishes a bidirectional bridge between AI and\nneuroscience. First, we demonstrate that attribution methods offer a powerful\nlens for investigating the neural mechanisms of language comprehension,\nrevealing how meaning emerges from preceding context. Second, we propose using\nbrain alignment as a metric to evaluate the validity of attribution methods,\nproviding a framework for assessing their biological plausibility."
                },
                "authors": [
                    {
                        "name": "Maryam Rahimi"
                    },
                    {
                        "name": "Yadollah Yaghoobzadeh"
                    },
                    {
                        "name": "Mohammad Reza Daliri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Reza Daliri"
                },
                "arxiv_affiliation": "School of Cognitive Sciences, Institute for Research in Fundamental Sciences, Tehran, Iran",
                "author": "Mohammad Reza Daliri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14669v1",
                "updated": "2025-02-20T16:05:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    5,
                    18,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T16:05:18Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    5,
                    18,
                    3,
                    51,
                    0
                ],
                "title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via\n  GRPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via\n  GRPO"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nlanguage processing, yet they often struggle with tasks requiring genuine\nvisual spatial reasoning. In this paper, we introduce a novel two-stage\ntraining framework designed to equip standard LLMs with visual reasoning\nabilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)\non a curated dataset of tokenized maze representations to teach the model to\npredict step-by-step movement commands. Next, we apply Group Relative Policy\nOptimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted\nreward function to refine the model's sequential decision-making and encourage\nemergent chain-of-thought behaviors. Experimental results on synthetically\ngenerated mazes show that while a baseline model fails to navigate the maze,\nthe SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning\nboosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more\nrobust and self-corrective reasoning, highlighting the potential of our\napproach to bridge the gap between language models and visual spatial tasks.\nThese findings offer promising implications for applications in robotics,\nautonomous navigation, and other domains that require integrated visual and\nsequential reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nlanguage processing, yet they often struggle with tasks requiring genuine\nvisual spatial reasoning. In this paper, we introduce a novel two-stage\ntraining framework designed to equip standard LLMs with visual reasoning\nabilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)\non a curated dataset of tokenized maze representations to teach the model to\npredict step-by-step movement commands. Next, we apply Group Relative Policy\nOptimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted\nreward function to refine the model's sequential decision-making and encourage\nemergent chain-of-thought behaviors. Experimental results on synthetically\ngenerated mazes show that while a baseline model fails to navigate the maze,\nthe SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning\nboosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more\nrobust and self-corrective reasoning, highlighting the potential of our\napproach to bridge the gap between language models and visual spatial tasks.\nThese findings offer promising implications for applications in robotics,\nautonomous navigation, and other domains that require integrated visual and\nsequential reasoning."
                },
                "authors": [
                    {
                        "name": "Alan Dao"
                    },
                    {
                        "name": "Dinh Bach Vu"
                    }
                ],
                "author_detail": {
                    "name": "Dinh Bach Vu"
                },
                "arxiv_affiliation": "Gia Tuan Dao",
                "author": "Dinh Bach Vu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v3",
                "updated": "2025-02-20T16:01:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    1,
                    34,
                    3,
                    51,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14662v1",
                "updated": "2025-02-20T15:58:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    58,
                    25,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:58:25Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    58,
                    25,
                    3,
                    51,
                    0
                ],
                "title": "InstructAgent: Building User Controllable Recommender via LLM Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructAgent: Building User Controllable Recommender via LLM Agent"
                },
                "summary": "Traditional recommender systems usually take the user-platform paradigm,\nwhere users are directly exposed under the control of the platform's\nrecommendation algorithms. However, the defect of recommendation algorithms may\nput users in very vulnerable positions under this paradigm. First, many\nsophisticated models are often designed with commercial objectives in mind,\nfocusing on the platform's benefits, which may hinder their ability to protect\nand capture users' true interests. Second, these models are typically optimized\nusing data from all users, which may overlook individual user's preferences.\nDue to these shortcomings, users may experience several disadvantages under the\ntraditional user-platform direct exposure paradigm, such as lack of control\nover the recommender system, potential manipulation by the platform, echo\nchamber effects, or lack of personalization for less active users due to the\ndominance of active users during collaborative learning. Therefore, there is an\nurgent need to develop a new paradigm to protect user interests and alleviate\nthese issues. Recently, some researchers have introduced LLM agents to simulate\nuser behaviors, these approaches primarily aim to optimize platform-side\nperformance, leaving core issues in recommender systems unresolved. To address\nthese limitations, we propose a new user-agent-platform paradigm, where agent\nserves as the protective shield between user and recommender system that\nenables indirect exposure. To this end, we first construct four recommendation\ndatasets, denoted as $\\dataset$, along with user instructions for each record.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional recommender systems usually take the user-platform paradigm,\nwhere users are directly exposed under the control of the platform's\nrecommendation algorithms. However, the defect of recommendation algorithms may\nput users in very vulnerable positions under this paradigm. First, many\nsophisticated models are often designed with commercial objectives in mind,\nfocusing on the platform's benefits, which may hinder their ability to protect\nand capture users' true interests. Second, these models are typically optimized\nusing data from all users, which may overlook individual user's preferences.\nDue to these shortcomings, users may experience several disadvantages under the\ntraditional user-platform direct exposure paradigm, such as lack of control\nover the recommender system, potential manipulation by the platform, echo\nchamber effects, or lack of personalization for less active users due to the\ndominance of active users during collaborative learning. Therefore, there is an\nurgent need to develop a new paradigm to protect user interests and alleviate\nthese issues. Recently, some researchers have introduced LLM agents to simulate\nuser behaviors, these approaches primarily aim to optimize platform-side\nperformance, leaving core issues in recommender systems unresolved. To address\nthese limitations, we propose a new user-agent-platform paradigm, where agent\nserves as the protective shield between user and recommender system that\nenables indirect exposure. To this end, we first construct four recommendation\ndatasets, denoted as $\\dataset$, along with user instructions for each record."
                },
                "authors": [
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Xuying Ning"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Min Xu"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "WWW2025@HCRS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14660v1",
                "updated": "2025-02-20T15:55:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    55,
                    52,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:55:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    55,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "Beyond the Surface: Uncovering Implicit Locations with LLMs for\n  Personalized Local News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Surface: Uncovering Implicit Locations with LLMs for\n  Personalized Local News"
                },
                "summary": "News recommendation systems personalize homepage content to boost engagement,\nbut factors like content type, editorial stance, and geographic focus impact\nrecommendations. Local newspapers balance coverage across regions, yet\nidentifying local articles is challenging due to implicit location cues like\nslang or landmarks.\n  Traditional methods, such as Named Entity Recognition (NER) and Knowledge\nGraphs, infer locations, but Large Language Models (LLMs) offer new\npossibilities while raising concerns about accuracy and explainability.\n  This paper explores LLMs for local article classification in Taboola's\n\"Homepage For You\" system, comparing them to traditional techniques. Key\nfindings: (1) Knowledge Graphs enhance NER models' ability to detect implicit\nlocations, (2) LLMs outperform traditional methods, and (3) LLMs can\neffectively identify local content without requiring Knowledge Graph\nintegration.\n  Offline evaluations showed LLMs excel at implicit location classification,\nwhile online A/B tests showed a significant increased in local views. A\nscalable pipeline integrating LLM-based location classification boosted local\narticle distribution by 27%, preserving newspapers' brand identity and\nenhancing homepage personalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News recommendation systems personalize homepage content to boost engagement,\nbut factors like content type, editorial stance, and geographic focus impact\nrecommendations. Local newspapers balance coverage across regions, yet\nidentifying local articles is challenging due to implicit location cues like\nslang or landmarks.\n  Traditional methods, such as Named Entity Recognition (NER) and Knowledge\nGraphs, infer locations, but Large Language Models (LLMs) offer new\npossibilities while raising concerns about accuracy and explainability.\n  This paper explores LLMs for local article classification in Taboola's\n\"Homepage For You\" system, comparing them to traditional techniques. Key\nfindings: (1) Knowledge Graphs enhance NER models' ability to detect implicit\nlocations, (2) LLMs outperform traditional methods, and (3) LLMs can\neffectively identify local content without requiring Knowledge Graph\nintegration.\n  Offline evaluations showed LLMs excel at implicit location classification,\nwhile online A/B tests showed a significant increased in local views. A\nscalable pipeline integrating LLM-based location classification boosted local\narticle distribution by 27%, preserving newspapers' brand identity and\nenhancing homepage personalization."
                },
                "authors": [
                    {
                        "name": "Gali Katz"
                    },
                    {
                        "name": "Hai Sitton"
                    },
                    {
                        "name": "Guy Gonen"
                    },
                    {
                        "name": "Yohay Kaplan"
                    }
                ],
                "author_detail": {
                    "name": "Yohay Kaplan"
                },
                "author": "Yohay Kaplan",
                "arxiv_comment": "10 pages, 2 figures, submitted to kdd",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18935v2",
                "updated": "2025-02-20T15:54:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    54,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-01-31T07:40:34Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    7,
                    40,
                    34,
                    4,
                    31,
                    0
                ],
                "title": "TabFSBench: Tabular Benchmark for Feature Shifts in Open Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabFSBench: Tabular Benchmark for Feature Shifts in Open Environment"
                },
                "summary": "Tabular data is widely utilized in various machine learning tasks. Current\ntabular learning research predominantly focuses on closed environments, while\nin real-world applications, open environments are often encountered, where\ndistribution and feature shifts occur, leading to significant degradation in\nmodel performance. Previous research has primarily concentrated on mitigating\ndistribution shifts, whereas feature shifts, a distinctive and unexplored\nchallenge of tabular data, have garnered limited attention. To this end, this\npaper conducts the first comprehensive study on feature shifts in tabular data\nand introduces the first tabular feature-shift benchmark (TabFSBench).\nTabFSBench evaluates impacts of four distinct feature-shift scenarios on four\ntabular model categories across various datasets and assesses the performance\nof large language models (LLMs) and tabular LLMs in the tabular benchmark for\nthe first time. Our study demonstrates three main observations: (1) most\ntabular models have the limited applicability in feature-shift scenarios; (2)\nthe shifted feature set importance has a linear relationship with model\nperformance degradation; (3) model performance in closed environments\ncorrelates with feature-shift performance. Future research direction is also\nexplored for each observation. TabFSBench is released for public access by\nusing a few lines of Python codes at https://github.com/LAMDASZ-ML/TabFSBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data is widely utilized in various machine learning tasks. Current\ntabular learning research predominantly focuses on closed environments, while\nin real-world applications, open environments are often encountered, where\ndistribution and feature shifts occur, leading to significant degradation in\nmodel performance. Previous research has primarily concentrated on mitigating\ndistribution shifts, whereas feature shifts, a distinctive and unexplored\nchallenge of tabular data, have garnered limited attention. To this end, this\npaper conducts the first comprehensive study on feature shifts in tabular data\nand introduces the first tabular feature-shift benchmark (TabFSBench).\nTabFSBench evaluates impacts of four distinct feature-shift scenarios on four\ntabular model categories across various datasets and assesses the performance\nof large language models (LLMs) and tabular LLMs in the tabular benchmark for\nthe first time. Our study demonstrates three main observations: (1) most\ntabular models have the limited applicability in feature-shift scenarios; (2)\nthe shifted feature set importance has a linear relationship with model\nperformance degradation; (3) model performance in closed environments\ncorrelates with feature-shift performance. Future research direction is also\nexplored for each observation. TabFSBench is released for public access by\nusing a few lines of Python codes at https://github.com/LAMDASZ-ML/TabFSBench."
                },
                "authors": [
                    {
                        "name": "Zi-Jian Cheng"
                    },
                    {
                        "name": "Zi-Yi Jia"
                    },
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Yu-Feng Li"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    }
                ],
                "author_detail": {
                    "name": "Lan-Zhe Guo"
                },
                "author": "Lan-Zhe Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14645v1",
                "updated": "2025-02-20T15:32:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    32,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:32:31Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    32,
                    31,
                    3,
                    51,
                    0
                ],
                "title": "Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual\n  Knowledge Synchronization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual\n  Knowledge Synchronization in LLMs"
                },
                "summary": "Knowledge editing allows for efficient adaptation of large language models\n(LLMs) to new information or corrections without requiring full retraining.\nHowever, prior methods typically focus on either single-language editing or\nbasic multilingual editing, failing to achieve true cross-linguistic knowledge\nsynchronization. To address this, we present a simple and practical\nstate-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE),\ndesigned to propagate knowledge from a dominant language to other languages\neffectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition\nInstruction Tuning (XE-IT), which fine-tunes the model on a curated parallel\ndataset to modify in-scope knowledge while preserving unrelated information,\nand (ii) Target-language Preference Optimization (TL-PO), which applies\nadvanced optimization techniques to ensure consistency across languages,\nfostering the transfer of updates. Additionally, we contribute a high-quality,\ncross-lingual dataset, specifically designed to enhance knowledge transfer\nacross languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks\nshow that X-KDE significantly enhances cross-lingual performance, achieving an\naverage improvement of +8.19%, while maintaining high accuracy in monolingual\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing allows for efficient adaptation of large language models\n(LLMs) to new information or corrections without requiring full retraining.\nHowever, prior methods typically focus on either single-language editing or\nbasic multilingual editing, failing to achieve true cross-linguistic knowledge\nsynchronization. To address this, we present a simple and practical\nstate-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE),\ndesigned to propagate knowledge from a dominant language to other languages\neffectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition\nInstruction Tuning (XE-IT), which fine-tunes the model on a curated parallel\ndataset to modify in-scope knowledge while preserving unrelated information,\nand (ii) Target-language Preference Optimization (TL-PO), which applies\nadvanced optimization techniques to ensure consistency across languages,\nfostering the transfer of updates. Additionally, we contribute a high-quality,\ncross-lingual dataset, specifically designed to enhance knowledge transfer\nacross languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks\nshow that X-KDE significantly enhances cross-lingual performance, achieving an\naverage improvement of +8.19%, while maintaining high accuracy in monolingual\nsettings."
                },
                "authors": [
                    {
                        "name": "Yuchen Wu"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14644v1",
                "updated": "2025-02-20T15:32:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    32,
                    24,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:32:24Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    32,
                    24,
                    3,
                    51,
                    0
                ],
                "title": "LIFT: Improving Long Context Understanding of Large Language Models\n  through Long Input Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIFT: Improving Long Context Understanding of Large Language Models\n  through Long Input Fine-Tuning"
                },
                "summary": "Long context understanding remains challenging for large language models due\nto their limited context windows. This paper presents Long Input Fine-Tuning\n(LIFT), a novel framework for long-context modeling that can improve the\nlong-context performance of arbitrary (short-context) LLMs by dynamically\nadapting model parameters based on the long input. Importantly, LIFT, rather\nthan endlessly extending the context window size to accommodate increasingly\nlonger inputs in context, chooses to store and absorb the long input in\nparameter. By fine-tuning the long input into model parameters, LIFT allows\nshort-context LLMs to answer questions even when the required information is\nnot provided in the context during inference. Furthermore, to enhance LIFT\nperformance while maintaining the original in-context learning (ICL)\ncapabilities, we introduce Gated Memory, a specialized attention adapter that\nautomatically balances long input memorization and ICL. We provide a\ncomprehensive analysis of the strengths and limitations of LIFT on long context\nunderstanding, offering valuable directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context understanding remains challenging for large language models due\nto their limited context windows. This paper presents Long Input Fine-Tuning\n(LIFT), a novel framework for long-context modeling that can improve the\nlong-context performance of arbitrary (short-context) LLMs by dynamically\nadapting model parameters based on the long input. Importantly, LIFT, rather\nthan endlessly extending the context window size to accommodate increasingly\nlonger inputs in context, chooses to store and absorb the long input in\nparameter. By fine-tuning the long input into model parameters, LIFT allows\nshort-context LLMs to answer questions even when the required information is\nnot provided in the context during inference. Furthermore, to enhance LIFT\nperformance while maintaining the original in-context learning (ICL)\ncapabilities, we introduce Gated Memory, a specialized attention adapter that\nautomatically balances long input memorization and ICL. We provide a\ncomprehensive analysis of the strengths and limitations of LIFT on long context\nunderstanding, offering valuable directions for future research."
                },
                "authors": [
                    {
                        "name": "Yansheng Mao"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Haotong Yang"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Xiyuan Wang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2412.13626",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14642v1",
                "updated": "2025-02-20T15:29:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    29,
                    32,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:29:32Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    29,
                    32,
                    3,
                    51,
                    0
                ],
                "title": "How Far are LLMs from Being Our Digital Twins? A Benchmark for\n  Persona-Based Behavior Chain Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far are LLMs from Being Our Digital Twins? A Benchmark for\n  Persona-Based Behavior Chain Simulation"
                },
                "summary": "Recently, LLMs have garnered increasing attention across academic disciplines\nfor their potential as human digital twins, virtual proxies designed to\nreplicate individuals and autonomously perform tasks such as decision-making,\nproblem-solving, and reasoning on their behalf. However, current evaluations of\nLLMs primarily emphasize dialogue simulation while overlooking human behavior\nsimulation, which is crucial for digital twins. To address this gap, we\nintroduce BehaviorChain, the first benchmark for evaluating LLMs' ability to\nsimulate continuous human behavior. BehaviorChain comprises diverse,\nhigh-quality, persona-based behavior chains, totaling 15,846 distinct behaviors\nacross 1,001 unique personas, each with detailed history and profile metadata.\nFor evaluation, we integrate persona metadata into LLMs and employ them to\niteratively infer contextually appropriate behaviors within dynamic scenarios\nprovided by BehaviorChain. Comprehensive evaluation results demonstrated that\neven state-of-the-art models struggle with accurately simulating continuous\nhuman behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, LLMs have garnered increasing attention across academic disciplines\nfor their potential as human digital twins, virtual proxies designed to\nreplicate individuals and autonomously perform tasks such as decision-making,\nproblem-solving, and reasoning on their behalf. However, current evaluations of\nLLMs primarily emphasize dialogue simulation while overlooking human behavior\nsimulation, which is crucial for digital twins. To address this gap, we\nintroduce BehaviorChain, the first benchmark for evaluating LLMs' ability to\nsimulate continuous human behavior. BehaviorChain comprises diverse,\nhigh-quality, persona-based behavior chains, totaling 15,846 distinct behaviors\nacross 1,001 unique personas, each with detailed history and profile metadata.\nFor evaluation, we integrate persona metadata into LLMs and employ them to\niteratively infer contextually appropriate behaviors within dynamic scenarios\nprovided by BehaviorChain. Comprehensive evaluation results demonstrated that\neven state-of-the-art models struggle with accurately simulating continuous\nhuman behavior."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Heming Xia"
                    },
                    {
                        "name": "Xinfeng Yuan"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00883v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00883v4",
                "updated": "2025-02-20T15:26:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    26,
                    44,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-02T19:25:41Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    19,
                    25,
                    41,
                    6,
                    33,
                    0
                ],
                "title": "SimPER: A Minimalist Approach to Preference Alignment without\n  Hyperparameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimPER: A Minimalist Approach to Preference Alignment without\n  Hyperparameters"
                },
                "summary": "Existing preference optimization objectives for language model alignment\nrequire additional hyperparameters that must be extensively tuned to achieve\noptimal performance, increasing both the complexity and time required for\nfine-tuning large language models. In this paper, we propose a simple yet\neffective hyperparameter-free preference optimization algorithm for alignment.\nWe observe that promising performance can be achieved simply by optimizing\ninverse perplexity, which is calculated as the inverse of the exponentiated\naverage log-likelihood of the chosen and rejected responses in the preference\ndataset. The resulting simple learning objective, SimPER, is easy to implement\nand eliminates the need for expensive hyperparameter tuning and a reference\nmodel, making it both computationally and memory efficient. Extensive\nexperiments on widely used real-world benchmarks, including MT-Bench,\nAlpacaEval 2, and 10 key benchmarks of the Open LLM Leaderboard with 5 base\nmodels, demonstrate that SimPER consistently and significantly outperforms\nexisting approaches-even without any hyperparameters or a reference model . For\nexample, despite its simplicity, SimPER outperforms state-of-the-art methods by\nup to 5.7 points on AlpacaEval 2 and achieves the highest average ranking\nacross 10 benchmarks on the Open LLM Leaderboard. The source code for SimPER is\npublicly available at: https://github.com/tengxiao1/SimPER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing preference optimization objectives for language model alignment\nrequire additional hyperparameters that must be extensively tuned to achieve\noptimal performance, increasing both the complexity and time required for\nfine-tuning large language models. In this paper, we propose a simple yet\neffective hyperparameter-free preference optimization algorithm for alignment.\nWe observe that promising performance can be achieved simply by optimizing\ninverse perplexity, which is calculated as the inverse of the exponentiated\naverage log-likelihood of the chosen and rejected responses in the preference\ndataset. The resulting simple learning objective, SimPER, is easy to implement\nand eliminates the need for expensive hyperparameter tuning and a reference\nmodel, making it both computationally and memory efficient. Extensive\nexperiments on widely used real-world benchmarks, including MT-Bench,\nAlpacaEval 2, and 10 key benchmarks of the Open LLM Leaderboard with 5 base\nmodels, demonstrate that SimPER consistently and significantly outperforms\nexisting approaches-even without any hyperparameters or a reference model . For\nexample, despite its simplicity, SimPER outperforms state-of-the-art methods by\nup to 5.7 points on AlpacaEval 2 and achieves the highest average ranking\nacross 10 benchmarks on the Open LLM Leaderboard. The source code for SimPER is\npublicly available at: https://github.com/tengxiao1/SimPER."
                },
                "authors": [
                    {
                        "name": "Teng Xiao"
                    },
                    {
                        "name": "Yige Yuan"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Mingxiao Li"
                    },
                    {
                        "name": "Shangsong Liang"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Vasant G Honavar"
                    }
                ],
                "author_detail": {
                    "name": "Vasant G Honavar"
                },
                "author": "Vasant G Honavar",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00883v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00883v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14634v1",
                "updated": "2025-02-20T15:16:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    16,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:16:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    16,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "CER: Confidence Enhanced Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CER: Confidence Enhanced Reasoning in LLMs"
                },
                "summary": "Ensuring the reliability of Large Language Models (LLMs) in complex reasoning\ntasks remains a formidable challenge, particularly in scenarios that demand\nprecise mathematical calculations and knowledge-intensive open-domain\ngeneration. In this work, we introduce an uncertainty-aware framework designed\nto enhance the accuracy of LLM responses by systematically incorporating model\nconfidence at critical decision points. We propose an approach that encourages\nmulti-step reasoning in LLMs and quantify the confidence of intermediate\nanswers such as numerical results in mathematical reasoning and proper nouns in\nopen-domain generation. Then, the overall confidence of each reasoning chain is\nevaluated based on confidence of these critical intermediate steps. Finally, we\naggregate the answer of generated response paths in a way that reflects the\nreliability of each generated content (as opposed to self-consistency in which\neach generated chain contributes equally to majority voting). We conducted\nextensive experiments in five datasets, three mathematical datasets and two\nopen-domain datasets, using four LLMs. The results consistently validate the\neffectiveness of our novel confidence aggregation method, leading to an\naccuracy improvement of up to 7.4% and 5.8% over baseline approaches in math\nand open-domain generation tasks, respectively. Code is publicly available at\nhttps://github.com/ Aquasar11/CER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the reliability of Large Language Models (LLMs) in complex reasoning\ntasks remains a formidable challenge, particularly in scenarios that demand\nprecise mathematical calculations and knowledge-intensive open-domain\ngeneration. In this work, we introduce an uncertainty-aware framework designed\nto enhance the accuracy of LLM responses by systematically incorporating model\nconfidence at critical decision points. We propose an approach that encourages\nmulti-step reasoning in LLMs and quantify the confidence of intermediate\nanswers such as numerical results in mathematical reasoning and proper nouns in\nopen-domain generation. Then, the overall confidence of each reasoning chain is\nevaluated based on confidence of these critical intermediate steps. Finally, we\naggregate the answer of generated response paths in a way that reflects the\nreliability of each generated content (as opposed to self-consistency in which\neach generated chain contributes equally to majority voting). We conducted\nextensive experiments in five datasets, three mathematical datasets and two\nopen-domain datasets, using four LLMs. The results consistently validate the\neffectiveness of our novel confidence aggregation method, leading to an\naccuracy improvement of up to 7.4% and 5.8% over baseline approaches in math\nand open-domain generation tasks, respectively. Code is publicly available at\nhttps://github.com/ Aquasar11/CER."
                },
                "authors": [
                    {
                        "name": "Ali Razghandi"
                    },
                    {
                        "name": "Seyed Mohammad Hadi Hosseini"
                    },
                    {
                        "name": "Mahdieh Soleymani Baghshah"
                    }
                ],
                "author_detail": {
                    "name": "Mahdieh Soleymani Baghshah"
                },
                "author": "Mahdieh Soleymani Baghshah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14632v1",
                "updated": "2025-02-20T15:10:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    10,
                    5,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:10:05Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    10,
                    5,
                    3,
                    51,
                    0
                ],
                "title": "Augmenting Coaching with GenAI: Insights into Use, Effectiveness, and\n  Future Potential",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting Coaching with GenAI: Insights into Use, Effectiveness, and\n  Future Potential"
                },
                "summary": "The integration of generative AI (GenAI) tools, particularly large language\nmodels (LLMs), is transforming professional coaching workflows. This study\nexplores how coaches use GenAI, the perceived benefits and limitations of these\ntools, and broader attitudes toward AI-assisted coaching. A survey of 205\ncoaching professionals reveals widespread adoption of GenAI for research,\ncontent creation, and administrative support, while its role in relational and\ninterpretative coaching remains limited. Findings indicate that AI literacy and\nperceived AI impact strongly predict GenAI adoption, with positive attitudes\nfostering greater use. Ethical considerations, particularly transparency and\ndata privacy, are a key concern, with frequent AI users demonstrating greater\nethical awareness. Regression analyses show that while perceived effectiveness\ndrives GenAI adoption, concerns about AI replacing human coaches do not\nsignificantly influence usage. Coaches express interest in future AI\ncapabilities that enhance personalization, real-time feedback, and\nadministrative automation while maintaining human oversight. The study\nhighlights that GenAI functions best as an augmentation tool rather than a\nreplacement, emphasizing the need for AI literacy training, ethical guidelines,\nand human-centered AI integration. These findings contribute to the ongoing\ndiscourse on human-AI collaboration, advocating for responsible and effective\nAI adoption in professional coaching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of generative AI (GenAI) tools, particularly large language\nmodels (LLMs), is transforming professional coaching workflows. This study\nexplores how coaches use GenAI, the perceived benefits and limitations of these\ntools, and broader attitudes toward AI-assisted coaching. A survey of 205\ncoaching professionals reveals widespread adoption of GenAI for research,\ncontent creation, and administrative support, while its role in relational and\ninterpretative coaching remains limited. Findings indicate that AI literacy and\nperceived AI impact strongly predict GenAI adoption, with positive attitudes\nfostering greater use. Ethical considerations, particularly transparency and\ndata privacy, are a key concern, with frequent AI users demonstrating greater\nethical awareness. Regression analyses show that while perceived effectiveness\ndrives GenAI adoption, concerns about AI replacing human coaches do not\nsignificantly influence usage. Coaches express interest in future AI\ncapabilities that enhance personalization, real-time feedback, and\nadministrative automation while maintaining human oversight. The study\nhighlights that GenAI functions best as an augmentation tool rather than a\nreplacement, emphasizing the need for AI literacy training, ethical guidelines,\nand human-centered AI integration. These findings contribute to the ongoing\ndiscourse on human-AI collaboration, advocating for responsible and effective\nAI adoption in professional coaching."
                },
                "authors": [
                    {
                        "name": "Jennifer Haase"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer Haase"
                },
                "author": "Jennifer Haase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14631v1",
                "updated": "2025-02-20T15:10:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    10,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:10:00Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    10,
                    0,
                    3,
                    51,
                    0
                ],
                "title": "Synergistic Fusion of Multi-Source Knowledge via Evidence Theory for\n  High-Entropy Alloy Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergistic Fusion of Multi-Source Knowledge via Evidence Theory for\n  High-Entropy Alloy Discovery"
                },
                "summary": "Discovering novel high-entropy alloys (HEAs) with desirable properties is\nchallenging due to the vast compositional space and complex phase formation\nmechanisms. Efficient exploration of this space requires a strategic approach\nthat integrates heterogeneous knowledge sources. Here, we propose a framework\nthat systematically combines knowledge extracted from computational material\ndatasets with domain knowledge distilled from scientific literature using large\nlanguage models (LLMs). A central feature of this approach is the explicit\nconsideration of element substitutability, identifying chemically similar\nelements that can be interchanged to potentially stabilize desired HEAs.\nDempster-Shafer theory, a mathematical framework for reasoning under\nuncertainty, is employed to model and combine substitutabilities based on\naggregated evidence from multiple sources. The framework predicts the phase\nstability of candidate HEA compositions and is systematically evaluated on both\nquaternary alloy systems, demonstrating superior performance compared to\nbaseline machine learning models and methods reliant on single-source evidence\nin cross-validation experiments. By leveraging multi-source knowledge, the\nframework retains robust predictive power even when key elements are absent\nfrom the training data, underscoring its potential for knowledge transfer and\nextrapolation. Furthermore, the enhanced interpretability of the methodology\noffers insights into the fundamental factors governing HEA formation. Overall,\nthis work provides a promising strategy for accelerating HEA discovery by\nintegrating computational and textual knowledge sources, enabling efficient\nexploration of vast compositional spaces with improved generalization and\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering novel high-entropy alloys (HEAs) with desirable properties is\nchallenging due to the vast compositional space and complex phase formation\nmechanisms. Efficient exploration of this space requires a strategic approach\nthat integrates heterogeneous knowledge sources. Here, we propose a framework\nthat systematically combines knowledge extracted from computational material\ndatasets with domain knowledge distilled from scientific literature using large\nlanguage models (LLMs). A central feature of this approach is the explicit\nconsideration of element substitutability, identifying chemically similar\nelements that can be interchanged to potentially stabilize desired HEAs.\nDempster-Shafer theory, a mathematical framework for reasoning under\nuncertainty, is employed to model and combine substitutabilities based on\naggregated evidence from multiple sources. The framework predicts the phase\nstability of candidate HEA compositions and is systematically evaluated on both\nquaternary alloy systems, demonstrating superior performance compared to\nbaseline machine learning models and methods reliant on single-source evidence\nin cross-validation experiments. By leveraging multi-source knowledge, the\nframework retains robust predictive power even when key elements are absent\nfrom the training data, underscoring its potential for knowledge transfer and\nextrapolation. Furthermore, the enhanced interpretability of the methodology\noffers insights into the fundamental factors governing HEA formation. Overall,\nthis work provides a promising strategy for accelerating HEA discovery by\nintegrating computational and textual knowledge sources, enabling efficient\nexploration of vast compositional spaces with improved generalization and\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Minh-Quyet Ha"
                    },
                    {
                        "name": "Dinh-Khiet Le"
                    },
                    {
                        "name": "Duc-Anh Dao"
                    },
                    {
                        "name": "Tien-Sinh Vu"
                    },
                    {
                        "name": "Duong-Nguyen Nguyen"
                    },
                    {
                        "name": "Viet-Cuong Nguyen"
                    },
                    {
                        "name": "Hiori Kino"
                    },
                    {
                        "name": "Van-Nam Huynh"
                    },
                    {
                        "name": "Hieu-Chi Dam"
                    }
                ],
                "author_detail": {
                    "name": "Hieu-Chi Dam"
                },
                "author": "Hieu-Chi Dam",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14628v1",
                "updated": "2025-02-20T15:07:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    7,
                    2,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T15:07:02Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    15,
                    7,
                    2,
                    3,
                    51,
                    0
                ],
                "title": "PEARL: Towards Permutation-Resilient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEARL: Towards Permutation-Resilient LLMs"
                },
                "summary": "The in-context learning (ICL) capability of large language models (LLMs)\nenables them to perform challenging tasks using provided demonstrations.\nHowever, ICL is highly sensitive to the ordering of demonstrations, leading to\ninstability in predictions. This paper shows that this vulnerability can be\nexploited to design a natural attack - difficult for model providers to detect\n- that achieves nearly 80% success rate on LLaMA-3 by simply permuting the\ndemonstrations. Existing mitigation methods primarily rely on post-processing\nand fail to enhance the model's inherent robustness to input permutations,\nraising concerns about safety and reliability of LLMs. To address this issue,\nwe propose Permutation-resilient learning (PEARL), a novel framework based on\ndistributionally robust optimization (DRO), which optimizes model performance\nagainst the worst-case input permutation. Specifically, PEARL consists of a\npermutation-proposal network (P-Net) and the LLM. The P-Net generates the most\nchallenging permutations by treating it as an optimal transport problem, which\nis solved using an entropy-constrained Sinkhorn algorithm. Through minimax\noptimization, the P-Net and the LLM iteratively optimize against each other,\nprogressively improving the LLM's robustness. Experiments on synthetic\npre-training and real-world instruction tuning tasks demonstrate that PEARL\neffectively mitigates permutation attacks and enhances performance. Notably,\ndespite being trained on fewer shots and shorter contexts, PEARL achieves\nperformance gains of up to 40% when scaled to many-shot and long-context\nscenarios, highlighting its efficiency and generalization capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The in-context learning (ICL) capability of large language models (LLMs)\nenables them to perform challenging tasks using provided demonstrations.\nHowever, ICL is highly sensitive to the ordering of demonstrations, leading to\ninstability in predictions. This paper shows that this vulnerability can be\nexploited to design a natural attack - difficult for model providers to detect\n- that achieves nearly 80% success rate on LLaMA-3 by simply permuting the\ndemonstrations. Existing mitigation methods primarily rely on post-processing\nand fail to enhance the model's inherent robustness to input permutations,\nraising concerns about safety and reliability of LLMs. To address this issue,\nwe propose Permutation-resilient learning (PEARL), a novel framework based on\ndistributionally robust optimization (DRO), which optimizes model performance\nagainst the worst-case input permutation. Specifically, PEARL consists of a\npermutation-proposal network (P-Net) and the LLM. The P-Net generates the most\nchallenging permutations by treating it as an optimal transport problem, which\nis solved using an entropy-constrained Sinkhorn algorithm. Through minimax\noptimization, the P-Net and the LLM iteratively optimize against each other,\nprogressively improving the LLM's robustness. Experiments on synthetic\npre-training and real-world instruction tuning tasks demonstrate that PEARL\neffectively mitigates permutation attacks and enhances performance. Notably,\ndespite being trained on fewer shots and shorter contexts, PEARL achieves\nperformance gains of up to 40% when scaled to many-shot and long-context\nscenarios, highlighting its efficiency and generalization capabilities."
                },
                "authors": [
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Xiaoyan Zhao"
                    },
                    {
                        "name": "Bin Liang"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05806v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05806v2",
                "updated": "2025-02-20T14:58:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    58,
                    39,
                    3,
                    51,
                    0
                ],
                "published": "2024-09-09T17:11:51Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    11,
                    51,
                    0,
                    253,
                    0
                ],
                "title": "CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics,\n  Facts, and Logic Error Correction in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics,\n  Facts, and Logic Error Correction in LLMs"
                },
                "summary": "Chinese, as a linguistic system rich in depth and complexity, is\ncharacterized by distinctive elements such as ancient poetry, proverbs, idioms,\nand other cultural constructs. However, current Large Language Models (LLMs)\nface limitations in these specialized domains, highlighting the need for the\ndevelopment of comprehensive datasets that can assess, continuously update, and\nprogressively improve these culturally-grounded linguistic competencies through\ntargeted training optimizations. To address this gap, we introduce CKnowEdit,\nthe first-ever Chinese knowledge editing dataset designed to correct\nlinguistic, factual, and logical errors in LLMs. We collect seven types of\nknowledge from a wide range of sources, including classical texts, idioms, and\ncontent from Baidu Tieba Ruozhiba, taking into account the unique polyphony,\nantithesis, and logical structures inherent in the Chinese language. By\nanalyzing this dataset, we highlight the challenges current LLMs face in\nmastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques reveals opportunities to advance the correction of Chinese\nknowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese, as a linguistic system rich in depth and complexity, is\ncharacterized by distinctive elements such as ancient poetry, proverbs, idioms,\nand other cultural constructs. However, current Large Language Models (LLMs)\nface limitations in these specialized domains, highlighting the need for the\ndevelopment of comprehensive datasets that can assess, continuously update, and\nprogressively improve these culturally-grounded linguistic competencies through\ntargeted training optimizations. To address this gap, we introduce CKnowEdit,\nthe first-ever Chinese knowledge editing dataset designed to correct\nlinguistic, factual, and logical errors in LLMs. We collect seven types of\nknowledge from a wide range of sources, including classical texts, idioms, and\ncontent from Baidu Tieba Ruozhiba, taking into account the unique polyphony,\nantithesis, and logical structures inherent in the Chinese language. By\nanalyzing this dataset, we highlight the challenges current LLMs face in\nmastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques reveals opportunities to advance the correction of Chinese\nknowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit."
                },
                "authors": [
                    {
                        "name": "Jizhan Fang"
                    },
                    {
                        "name": "Tianhe Lu"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Ziyan Jiang"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Ongoing work; project website is available at\n  https://zjunlp.github.io/project/CKnowEdit code and dataset are available at\n  https://github.com/zjunlp/EasyEdit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05806v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05806v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14619v1",
                "updated": "2025-02-20T14:57:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    57,
                    14,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:57:14Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    57,
                    14,
                    3,
                    51,
                    0
                ],
                "title": "Reward Models Identify Consistency, Not Causality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Models Identify Consistency, Not Causality"
                },
                "summary": "Reward models (RMs) play a crucial role in aligning large language models\n(LLMs) with human preferences and enhancing reasoning quality. Traditionally,\nRMs are trained to rank candidate outputs based on their correctness and\ncoherence. However, in this work, we present several surprising findings that\nchallenge common assumptions about RM behavior. Our analysis reveals that\nstate-of-the-art reward models prioritize structural consistency over causal\ncorrectness. Specifically, removing the problem statement has minimal impact on\nreward scores, whereas altering numerical values or disrupting the reasoning\nflow significantly affects RM outputs. Furthermore, RMs exhibit a strong\ndependence on complete reasoning trajectories truncated or incomplete steps\nlead to significant variations in reward assignments, indicating that RMs\nprimarily rely on learned reasoning patterns rather than explicit problem\ncomprehension. These findings hold across multiple architectures, datasets, and\ntasks, leading to three key insights: (1) RMs primarily assess coherence rather\nthan true reasoning quality; (2) The role of explicit problem comprehension in\nreward assignment is overstated; (3) Current RMs may be more effective at\nranking responses than verifying logical validity. Our results suggest a\nfundamental limitation in existing reward modeling approaches, emphasizing the\nneed for a shift toward causality-aware reward models that go beyond\nconsistency-driven evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) play a crucial role in aligning large language models\n(LLMs) with human preferences and enhancing reasoning quality. Traditionally,\nRMs are trained to rank candidate outputs based on their correctness and\ncoherence. However, in this work, we present several surprising findings that\nchallenge common assumptions about RM behavior. Our analysis reveals that\nstate-of-the-art reward models prioritize structural consistency over causal\ncorrectness. Specifically, removing the problem statement has minimal impact on\nreward scores, whereas altering numerical values or disrupting the reasoning\nflow significantly affects RM outputs. Furthermore, RMs exhibit a strong\ndependence on complete reasoning trajectories truncated or incomplete steps\nlead to significant variations in reward assignments, indicating that RMs\nprimarily rely on learned reasoning patterns rather than explicit problem\ncomprehension. These findings hold across multiple architectures, datasets, and\ntasks, leading to three key insights: (1) RMs primarily assess coherence rather\nthan true reasoning quality; (2) The role of explicit problem comprehension in\nreward assignment is overstated; (3) Current RMs may be more effective at\nranking responses than verifying logical validity. Our results suggest a\nfundamental limitation in existing reward modeling approaches, emphasizing the\nneed for a shift toward causality-aware reward models that go beyond\nconsistency-driven evaluation."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Junnan Li"
                    }
                ],
                "author_detail": {
                    "name": "Junnan Li"
                },
                "author": "Junnan Li",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14617v1",
                "updated": "2025-02-20T14:57:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    57,
                    8,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:57:08Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    57,
                    8,
                    3,
                    51,
                    0
                ],
                "title": "Serving Models, Fast and Slow:Optimizing Heterogeneous LLM Inferencing\n  Workloads at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Models, Fast and Slow:Optimizing Heterogeneous LLM Inferencing\n  Workloads at Scale"
                },
                "summary": "Large Language Model (LLM) inference workloads handled by global cloud\nproviders can include both latency-sensitive and insensitive tasks, creating a\ndiverse range of Service Level Agreement (SLA) requirements. Managing these\nmixed workloads is challenging due to the complexity of the inference stack,\nwhich includes multiple LLMs, hardware configurations, and geographic\ndistributions. Current optimization strategies often silo these tasks to ensure\nthat SLAs are met for latency-sensitive tasks, but this leads to significant\nunder-utilization of expensive GPU resources despite the availability of spot\nand on-demand Virtual Machine (VM) provisioning. We propose SAGESERVE, a\ncomprehensive LLM serving framework that employs adaptive control knobs at\nvarying time scales, ensuring SLA compliance while maximizing the utilization\nof valuable GPU resources. Short-term optimizations include efficient request\nrouting to data center regions, while long-term strategies involve scaling GPU\nVMs out/in and redeploying models to existing VMs to align with traffic\npatterns. These strategies are formulated as an optimization problem for\nresource allocation and solved using Integer Linear Programming (ILP). We\nperform empirical and simulation studies based on production workload traces\nwith over 8M requests using four open-source models deployed across three\nregions. SAGESERVE achieves up to 25% savings in GPU-hours while maintaining\ntail latency and satisfying all SLOs, and it reduces the scaling overhead\ncompared to baselines by up to 80%, confirming the effectiveness of our\nproposal. In terms of dollar cost, this can save cloud providers up to $2M over\nthe course of a month.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference workloads handled by global cloud\nproviders can include both latency-sensitive and insensitive tasks, creating a\ndiverse range of Service Level Agreement (SLA) requirements. Managing these\nmixed workloads is challenging due to the complexity of the inference stack,\nwhich includes multiple LLMs, hardware configurations, and geographic\ndistributions. Current optimization strategies often silo these tasks to ensure\nthat SLAs are met for latency-sensitive tasks, but this leads to significant\nunder-utilization of expensive GPU resources despite the availability of spot\nand on-demand Virtual Machine (VM) provisioning. We propose SAGESERVE, a\ncomprehensive LLM serving framework that employs adaptive control knobs at\nvarying time scales, ensuring SLA compliance while maximizing the utilization\nof valuable GPU resources. Short-term optimizations include efficient request\nrouting to data center regions, while long-term strategies involve scaling GPU\nVMs out/in and redeploying models to existing VMs to align with traffic\npatterns. These strategies are formulated as an optimization problem for\nresource allocation and solved using Integer Linear Programming (ILP). We\nperform empirical and simulation studies based on production workload traces\nwith over 8M requests using four open-source models deployed across three\nregions. SAGESERVE achieves up to 25% savings in GPU-hours while maintaining\ntail latency and satisfying all SLOs, and it reduces the scaling overhead\ncompared to baselines by up to 80%, confirming the effectiveness of our\nproposal. In terms of dollar cost, this can save cloud providers up to $2M over\nthe course of a month."
                },
                "authors": [
                    {
                        "name": "Shashwat Jaiswal"
                    },
                    {
                        "name": "Kunal Jain"
                    },
                    {
                        "name": "Yogesh Simmhan"
                    },
                    {
                        "name": "Anjaly Parayil"
                    },
                    {
                        "name": "Ankur Mallick"
                    },
                    {
                        "name": "Rujia Wang"
                    },
                    {
                        "name": "Renee St. Amant"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Victor Rhle"
                    },
                    {
                        "name": "Anoop Kulkarni"
                    },
                    {
                        "name": "Steve Kofsky"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "15 pages, 17 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14614v1",
                "updated": "2025-02-20T14:52:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    52,
                    36,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:52:36Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    52,
                    36,
                    3,
                    51,
                    0
                ],
                "title": "FIND: Fine-grained Information Density Guided Adaptive\n  Retrieval-Augmented Generation for Disease Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIND: Fine-grained Information Density Guided Adaptive\n  Retrieval-Augmented Generation for Disease Diagnosis"
                },
                "summary": "Retrieval-Augmented Large Language Models (LLMs), which integrate external\nknowledge into LLMs, have shown remarkable performance in various medical\ndomains, including clinical diagnosis. However, existing RAG methods struggle\nto effectively assess task difficulty to make retrieval decisions, thereby\nfailing to meet the clinical requirements for balancing efficiency and\naccuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained\n\\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework\nthat improves the reliability of RAG in disease diagnosis scenarios. FIND\nincorporates a fine-grained adaptive control module to determine whether\nretrieval is necessary based on the information density of the input. By\noptimizing the retrieval process and implementing a knowledge filtering module,\nFIND ensures that the retrieval is better suited to clinical scenarios.\nExperiments on three Chinese electronic medical record datasets demonstrate\nthat FIND significantly outperforms various baseline methods, highlighting its\neffectiveness in clinical diagnosis tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Large Language Models (LLMs), which integrate external\nknowledge into LLMs, have shown remarkable performance in various medical\ndomains, including clinical diagnosis. However, existing RAG methods struggle\nto effectively assess task difficulty to make retrieval decisions, thereby\nfailing to meet the clinical requirements for balancing efficiency and\naccuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained\n\\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework\nthat improves the reliability of RAG in disease diagnosis scenarios. FIND\nincorporates a fine-grained adaptive control module to determine whether\nretrieval is necessary based on the information density of the input. By\noptimizing the retrieval process and implementing a knowledge filtering module,\nFIND ensures that the retrieval is better suited to clinical scenarios.\nExperiments on three Chinese electronic medical record datasets demonstrate\nthat FIND significantly outperforms various baseline methods, highlighting its\neffectiveness in clinical diagnosis tasks."
                },
                "authors": [
                    {
                        "name": "Mingyi Jia"
                    },
                    {
                        "name": "Junwen Duan"
                    },
                    {
                        "name": "Yan Song"
                    },
                    {
                        "name": "Jianxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Wang"
                },
                "author": "Jianxin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11844v2",
                "updated": "2025-02-20T14:52:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    52,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-17T14:37:47Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    37,
                    47,
                    0,
                    48,
                    0
                ],
                "title": "BaxBench: Can LLMs Generate Correct and Secure Backends?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaxBench: Can LLMs Generate Correct and Secure Backends?"
                },
                "summary": "The automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits,\nand solve algorithmic coding tasks. However, to achieve full automation, LLMs\nshould be able to generate production-quality, self-contained application\nmodules. To evaluate the capabilities of LLMs in solving this challenge, we\nintroduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for\nthe generation of backend applications. We focus on backends for three critical\nreasons: (i) they are practically relevant, building the core components of\nmost modern web and cloud software, (ii) they are difficult to get right,\nrequiring multiple functions and files to achieve the desired functionality,\nand (iii) they are security-critical, as they are exposed to untrusted\nthird-parties, making secure solutions that prevent deployment-time attacks an\nimperative. BaxBench validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by\nexecuting end-to-end exploits. Our experiments reveal key limitations of\ncurrent LLMs in both functionality and security: (i) even the best model,\nOpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could\nsuccessfully execute security exploits on more than half of the correct\nprograms generated by each LLM; and (iii) in less popular backend frameworks,\nmodels further struggle to generate correct and secure applications. Progress\non BaxBench signifies important steps towards autonomous and secure software\ndevelopment with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits,\nand solve algorithmic coding tasks. However, to achieve full automation, LLMs\nshould be able to generate production-quality, self-contained application\nmodules. To evaluate the capabilities of LLMs in solving this challenge, we\nintroduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for\nthe generation of backend applications. We focus on backends for three critical\nreasons: (i) they are practically relevant, building the core components of\nmost modern web and cloud software, (ii) they are difficult to get right,\nrequiring multiple functions and files to achieve the desired functionality,\nand (iii) they are security-critical, as they are exposed to untrusted\nthird-parties, making secure solutions that prevent deployment-time attacks an\nimperative. BaxBench validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by\nexecuting end-to-end exploits. Our experiments reveal key limitations of\ncurrent LLMs in both functionality and security: (i) even the best model,\nOpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could\nsuccessfully execute security exploits on more than half of the correct\nprograms generated by each LLM; and (iii) in less popular backend frameworks,\nmodels further struggle to generate correct and secure applications. Progress\non BaxBench signifies important steps towards autonomous and secure software\ndevelopment with LLMs."
                },
                "authors": [
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Niels Mndler"
                    },
                    {
                        "name": "Victor Chibotaru"
                    },
                    {
                        "name": "Veselin Raychev"
                    },
                    {
                        "name": "Maximilian Baader"
                    },
                    {
                        "name": "Nikola Jovanovi"
                    },
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14613v1",
                "updated": "2025-02-20T14:52:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    52,
                    23,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:52:23Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    52,
                    23,
                    3,
                    51,
                    0
                ],
                "title": "Behavioral Analysis of Information Salience in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavioral Analysis of Information Salience in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) excel at text summarization, a task that\nrequires models to select content based on its importance. However, the exact\nnotion of salience that LLMs have internalized remains unclear. To bridge this\ngap, we introduce an explainable framework to systematically derive and\ninvestigate information salience in LLMs through their summarization behavior.\nUsing length-controlled summarization as a behavioral probe into the content\nselection process, and tracing the answerability of Questions Under Discussion\nthroughout, we derive a proxy for how models prioritize information. Our\nexperiments on 13 models across four datasets reveal that LLMs have a nuanced,\nhierarchical notion of salience, generally consistent across model families and\nsizes. While models show highly consistent behavior and hence salience\npatterns, this notion of salience cannot be accessed through introspection, and\nonly weakly correlates with human perceptions of information salience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at text summarization, a task that\nrequires models to select content based on its importance. However, the exact\nnotion of salience that LLMs have internalized remains unclear. To bridge this\ngap, we introduce an explainable framework to systematically derive and\ninvestigate information salience in LLMs through their summarization behavior.\nUsing length-controlled summarization as a behavioral probe into the content\nselection process, and tracing the answerability of Questions Under Discussion\nthroughout, we derive a proxy for how models prioritize information. Our\nexperiments on 13 models across four datasets reveal that LLMs have a nuanced,\nhierarchical notion of salience, generally consistent across model families and\nsizes. While models show highly consistent behavior and hence salience\npatterns, this notion of salience cannot be accessed through introspection, and\nonly weakly correlates with human perceptions of information salience."
                },
                "authors": [
                    {
                        "name": "Jan Trienes"
                    },
                    {
                        "name": "Jrg Schltterer"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    },
                    {
                        "name": "Christin Seifert"
                    }
                ],
                "author_detail": {
                    "name": "Christin Seifert"
                },
                "author": "Christin Seifert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14592v1",
                "updated": "2025-02-20T14:27:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    27,
                    24,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:27:24Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    27,
                    24,
                    3,
                    51,
                    0
                ],
                "title": "\"Don't Forget the Teachers\": Towards an Educator-Centered Understanding\n  of Harms from Large Language Models in Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Don't Forget the Teachers\": Towards an Educator-Centered Understanding\n  of Harms from Large Language Models in Education"
                },
                "summary": "Education technologies (edtech) are increasingly incorporating new features\nbuilt on large language models (LLMs), with the goals of enriching the\nprocesses of teaching and learning and ultimately improving learning outcomes.\nHowever, the potential downstream impacts of LLM-based edtech remain\nunderstudied. Prior attempts to map the risks of LLMs have not been tailored to\neducation specifically, even though it is a unique domain in many respects:\nfrom its population (students are often children, who can be especially\nimpacted by technology) to its goals (providing the correct answer may be less\nimportant for learners than understanding how to arrive at an answer) to its\nimplications for higher-order skills that generalize across contexts (e.g.,\ncritical thinking and collaboration). We conducted semi-structured interviews\nwith six edtech providers representing leaders in the K-12 space, as well as a\ndiverse group of 23 educators with varying levels of experience with LLM-based\nedtech. Through a thematic analysis, we explored how each group is\nanticipating, observing, and accounting for potential harms from LLMs in\neducation. We find that, while edtech providers focus primarily on mitigating\ntechnical harms, i.e., those that can be measured based solely on LLM outputs\nthemselves, educators are more concerned about harms that result from the\nbroader impacts of LLMs, i.e., those that require observation of interactions\nbetween students, educators, school systems, and edtech to measure. Overall, we\n(1) develop an education-specific overview of potential harms from LLMs, (2)\nhighlight gaps between conceptions of harm by edtech providers and those by\neducators, and (3) make recommendations to facilitate the centering of\neducators in the design and development of edtech tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Education technologies (edtech) are increasingly incorporating new features\nbuilt on large language models (LLMs), with the goals of enriching the\nprocesses of teaching and learning and ultimately improving learning outcomes.\nHowever, the potential downstream impacts of LLM-based edtech remain\nunderstudied. Prior attempts to map the risks of LLMs have not been tailored to\neducation specifically, even though it is a unique domain in many respects:\nfrom its population (students are often children, who can be especially\nimpacted by technology) to its goals (providing the correct answer may be less\nimportant for learners than understanding how to arrive at an answer) to its\nimplications for higher-order skills that generalize across contexts (e.g.,\ncritical thinking and collaboration). We conducted semi-structured interviews\nwith six edtech providers representing leaders in the K-12 space, as well as a\ndiverse group of 23 educators with varying levels of experience with LLM-based\nedtech. Through a thematic analysis, we explored how each group is\nanticipating, observing, and accounting for potential harms from LLMs in\neducation. We find that, while edtech providers focus primarily on mitigating\ntechnical harms, i.e., those that can be measured based solely on LLM outputs\nthemselves, educators are more concerned about harms that result from the\nbroader impacts of LLMs, i.e., those that require observation of interactions\nbetween students, educators, school systems, and edtech to measure. Overall, we\n(1) develop an education-specific overview of potential harms from LLMs, (2)\nhighlight gaps between conceptions of harm by edtech providers and those by\neducators, and (3) make recommendations to facilitate the centering of\neducators in the design and development of edtech tools."
                },
                "authors": [
                    {
                        "name": "Emma Harvey"
                    },
                    {
                        "name": "Allison Koenecke"
                    },
                    {
                        "name": "Rene F. Kizilcec"
                    }
                ],
                "author_detail": {
                    "name": "Rene F. Kizilcec"
                },
                "author": "Rene F. Kizilcec",
                "arxiv_comment": "To appear in the 2025 ACM CHI Conference on Human Factors in\n  Computing Systems (CHI '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14586v1",
                "updated": "2025-02-20T14:16:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    16,
                    15,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:16:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    16,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "Moshi Moshi? A Model Selection Hijacking Adversarial Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moshi Moshi? A Model Selection Hijacking Adversarial Attack"
                },
                "summary": "Model selection is a fundamental task in Machine Learning~(ML), focusing on\nselecting the most suitable model from a pool of candidates by evaluating their\nperformance on specific metrics. This process ensures optimal performance,\ncomputational efficiency, and adaptability to diverse tasks and environments.\nDespite its critical role, its security from the perspective of adversarial ML\nremains unexplored. This risk is heightened in the\nMachine-Learning-as-a-Service model, where users delegate the training phase\nand the model selection process to third-party providers, supplying data and\ntraining strategies. Therefore, attacks on model selection could harm both the\nuser and the provider, undermining model performance and driving up operational\ncosts.\n  In this work, we present MOSHI (MOdel Selection HIjacking adversarial\nattack), the first adversarial attack specifically targeting model selection.\nOur novel approach manipulates model selection data to favor the adversary,\neven without prior knowledge of the system. Utilizing a framework based on\nVariational Auto Encoders, we provide evidence that an attacker can induce\ninefficiencies in ML deployment. We test our attack on diverse computer vision\nand speech recognition benchmark tasks and different settings, obtaining an\naverage attack success rate of 75.42%. In particular, our attack causes an\naverage 88.30% decrease in generalization capabilities, an 83.33% increase in\nlatency, and an increase of up to 105.85% in energy consumption. These results\nhighlight the significant vulnerabilities in model selection processes and\ntheir potential impact on real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model selection is a fundamental task in Machine Learning~(ML), focusing on\nselecting the most suitable model from a pool of candidates by evaluating their\nperformance on specific metrics. This process ensures optimal performance,\ncomputational efficiency, and adaptability to diverse tasks and environments.\nDespite its critical role, its security from the perspective of adversarial ML\nremains unexplored. This risk is heightened in the\nMachine-Learning-as-a-Service model, where users delegate the training phase\nand the model selection process to third-party providers, supplying data and\ntraining strategies. Therefore, attacks on model selection could harm both the\nuser and the provider, undermining model performance and driving up operational\ncosts.\n  In this work, we present MOSHI (MOdel Selection HIjacking adversarial\nattack), the first adversarial attack specifically targeting model selection.\nOur novel approach manipulates model selection data to favor the adversary,\neven without prior knowledge of the system. Utilizing a framework based on\nVariational Auto Encoders, we provide evidence that an attacker can induce\ninefficiencies in ML deployment. We test our attack on diverse computer vision\nand speech recognition benchmark tasks and different settings, obtaining an\naverage attack success rate of 75.42%. In particular, our attack causes an\naverage 88.30% decrease in generalization capabilities, an 83.33% increase in\nlatency, and an increase of up to 105.85% in energy consumption. These results\nhighlight the significant vulnerabilities in model selection processes and\ntheir potential impact on real-world applications."
                },
                "authors": [
                    {
                        "name": "Riccardo Petrucci"
                    },
                    {
                        "name": "Luca Pajola"
                    },
                    {
                        "name": "Francesco Marchiori"
                    },
                    {
                        "name": "Luca Pasa"
                    },
                    {
                        "name": "Mauro conti"
                    }
                ],
                "author_detail": {
                    "name": "Mauro conti"
                },
                "author": "Mauro conti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01387v3",
                "updated": "2025-02-20T14:09:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    9,
                    1,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-03T14:22:03Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    14,
                    22,
                    3,
                    0,
                    34,
                    0
                ],
                "title": "TeLL-Drive: Enhancing Autonomous Driving with Teacher LLM-Guided Deep\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeLL-Drive: Enhancing Autonomous Driving with Teacher LLM-Guided Deep\n  Reinforcement Learning"
                },
                "summary": "Although Deep Reinforcement Learning (DRL) and Large Language Models (LLMs)\neach show promise in addressing decision-making challenges in autonomous\ndriving, DRL often suffers from high sample complexity, while LLMs have\ndifficulty ensuring real-time decision making. To address these limitations, we\npropose TeLL-Drive, a hybrid framework that integrates a Teacher LLM to guide\nan attention-based Student DRL policy. By incorporating risk metrics,\nhistorical scenario retrieval, and domain heuristics into context-rich prompts,\nthe LLM produces high-level driving strategies through chain-of-thought\nreasoning. A self-attention mechanism then fuses these strategies with the DRL\nagent's exploration, accelerating policy convergence and boosting robustness\nacross diverse driving conditions. The experimental results, evaluated across\nmultiple traffic scenarios, show that TeLL-Drive outperforms existing baseline\nmethods, including other LLM-based approaches, in terms of success rates,\naverage returns, and real-time feasibility. Ablation studies underscore the\nimportance of each model component, especially the synergy between the\nattention mechanism and LLM-driven guidance. Finally, we build a virtual-real\nfusion experimental platform to verify the real-time performance, robustness,\nand reliability of the algorithm running on real vehicles through\nvehicle-in-loop experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Deep Reinforcement Learning (DRL) and Large Language Models (LLMs)\neach show promise in addressing decision-making challenges in autonomous\ndriving, DRL often suffers from high sample complexity, while LLMs have\ndifficulty ensuring real-time decision making. To address these limitations, we\npropose TeLL-Drive, a hybrid framework that integrates a Teacher LLM to guide\nan attention-based Student DRL policy. By incorporating risk metrics,\nhistorical scenario retrieval, and domain heuristics into context-rich prompts,\nthe LLM produces high-level driving strategies through chain-of-thought\nreasoning. A self-attention mechanism then fuses these strategies with the DRL\nagent's exploration, accelerating policy convergence and boosting robustness\nacross diverse driving conditions. The experimental results, evaluated across\nmultiple traffic scenarios, show that TeLL-Drive outperforms existing baseline\nmethods, including other LLM-based approaches, in terms of success rates,\naverage returns, and real-time feasibility. Ablation studies underscore the\nimportance of each model component, especially the synergy between the\nattention mechanism and LLM-driven guidance. Finally, we build a virtual-real\nfusion experimental platform to verify the real-time performance, robustness,\nand reliability of the algorithm running on real vehicles through\nvehicle-in-loop experiments."
                },
                "authors": [
                    {
                        "name": "Chengkai Xu"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Shiyu Fang"
                    },
                    {
                        "name": "Yiming Cui"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Peng Hang"
                    },
                    {
                        "name": "Jian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jian Sun"
                },
                "author": "Jian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14565v1",
                "updated": "2025-02-20T13:50:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    50,
                    2,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T13:50:02Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    50,
                    2,
                    3,
                    51,
                    0
                ],
                "title": "ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification"
                },
                "summary": "Self-awareness, i.e., the ability to assess and correct one's own generation,\nis a fundamental aspect of human intelligence, making its replication in large\nlanguage models (LLMs) an important yet challenging task. Previous works tackle\nthis by employing extensive reinforcement learning or rather relying on large\nexternal verifiers. In this work, we propose Refine via Intrinsic\nSelf-Verification (ReVISE), an efficient and effective framework that enables\nLLMs to self-correct their outputs through self-verification. The core idea of\nReVISE is to enable LLMs to verify their reasoning processes and continually\nrethink reasoning trajectories based on its verification. We introduce a\nstructured curriculum based upon online preference learning to implement this\nefficiently. Specifically, as ReVISE involves two challenging tasks (i.e.,\nself-verification and reasoning correction), we tackle each task sequentially\nusing curriculum learning, collecting both failed and successful reasoning\npaths to construct preference pairs for efficient training. During inference,\nour approach enjoys natural test-time scaling by integrating self-verification\nand correction capabilities, further enhanced by our proposed confidence-aware\ndecoding mechanism. Our experiments on various reasoning tasks demonstrate that\nReVISE achieves efficient self-correction and significantly improves reasoning\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-awareness, i.e., the ability to assess and correct one's own generation,\nis a fundamental aspect of human intelligence, making its replication in large\nlanguage models (LLMs) an important yet challenging task. Previous works tackle\nthis by employing extensive reinforcement learning or rather relying on large\nexternal verifiers. In this work, we propose Refine via Intrinsic\nSelf-Verification (ReVISE), an efficient and effective framework that enables\nLLMs to self-correct their outputs through self-verification. The core idea of\nReVISE is to enable LLMs to verify their reasoning processes and continually\nrethink reasoning trajectories based on its verification. We introduce a\nstructured curriculum based upon online preference learning to implement this\nefficiently. Specifically, as ReVISE involves two challenging tasks (i.e.,\nself-verification and reasoning correction), we tackle each task sequentially\nusing curriculum learning, collecting both failed and successful reasoning\npaths to construct preference pairs for efficient training. During inference,\nour approach enjoys natural test-time scaling by integrating self-verification\nand correction capabilities, further enhanced by our proposed confidence-aware\ndecoding mechanism. Our experiments on various reasoning tasks demonstrate that\nReVISE achieves efficient self-correction and significantly improves reasoning\nperformance."
                },
                "authors": [
                    {
                        "name": "Hyunseok Lee"
                    },
                    {
                        "name": "Seunghyuk Oh"
                    },
                    {
                        "name": "Jaehyung Kim"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "Jihoon Tack"
                    }
                ],
                "author_detail": {
                    "name": "Jihoon Tack"
                },
                "author": "Jihoon Tack",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14563v1",
                "updated": "2025-02-20T13:47:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    47,
                    51,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T13:47:51Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    47,
                    51,
                    3,
                    51,
                    0
                ],
                "title": "Plan-over-Graph: Towards Parallelable LLM Agent Schedule",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plan-over-Graph: Towards Parallelable LLM Agent Schedule"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional abilities in\nreasoning for task planning. However, challenges remain under-explored for\nparallel schedules. This paper introduces a novel paradigm, plan-over-graph, in\nwhich the model first decomposes a real-life textual task into executable\nsubtasks and constructs an abstract task graph. The model then understands this\ntask graph as input and generates a plan for parallel execution. To enhance the\nplanning capability of complex, scalable graphs, we design an automated and\ncontrollable pipeline to generate synthetic graphs and propose a two-stage\ntraining scheme. Experimental results show that our plan-over-graph method\nsignificantly improves task performance on both API-based LLMs and trainable\nopen-sourced LLMs. By normalizing complex tasks as graphs, our method naturally\nsupports parallel execution, demonstrating global efficiency. The code and data\nare available at https://github.com/zsq259/Plan-over-Graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional abilities in\nreasoning for task planning. However, challenges remain under-explored for\nparallel schedules. This paper introduces a novel paradigm, plan-over-graph, in\nwhich the model first decomposes a real-life textual task into executable\nsubtasks and constructs an abstract task graph. The model then understands this\ntask graph as input and generates a plan for parallel execution. To enhance the\nplanning capability of complex, scalable graphs, we design an automated and\ncontrollable pipeline to generate synthetic graphs and propose a two-stage\ntraining scheme. Experimental results show that our plan-over-graph method\nsignificantly improves task performance on both API-based LLMs and trainable\nopen-sourced LLMs. By normalizing complex tasks as graphs, our method naturally\nsupports parallel execution, demonstrating global efficiency. The code and data\nare available at https://github.com/zsq259/Plan-over-Graph."
                },
                "authors": [
                    {
                        "name": "Shiqi Zhang"
                    },
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14561v1",
                "updated": "2025-02-20T13:45:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    45,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T13:45:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    45,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Can LLMs Predict Citation Intent? An Experimental Analysis of In-context\n  Learning and Fine-tuning on Open LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Predict Citation Intent? An Experimental Analysis of In-context\n  Learning and Fine-tuning on Open LLMs"
                },
                "summary": "This work investigates the ability of open Large Language Models (LLMs) to\npredict citation intent through in-context learning and fine-tuning. Unlike\ntraditional approaches that rely on pre-trained models like SciBERT, which\nrequire extensive domain-specific pretraining and specialized architectures, we\ndemonstrate that general-purpose LLMs can be adapted to this task with minimal\ntask-specific data. We evaluate twelve model variations across five prominent\nopen LLM families using zero, one, few, and many-shot prompting to assess\nperformance across scenarios. Our experimental study identifies the\ntop-performing model through extensive experimentation of in-context\nlearning-related parameters, which we fine-tune to further enhance task\nperformance. The results highlight the strengths and limitations of LLMs in\nrecognizing citation intents, providing valuable insights for model selection\nand prompt engineering. Additionally, we make our end-to-end evaluation\nframework and models openly available for future use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work investigates the ability of open Large Language Models (LLMs) to\npredict citation intent through in-context learning and fine-tuning. Unlike\ntraditional approaches that rely on pre-trained models like SciBERT, which\nrequire extensive domain-specific pretraining and specialized architectures, we\ndemonstrate that general-purpose LLMs can be adapted to this task with minimal\ntask-specific data. We evaluate twelve model variations across five prominent\nopen LLM families using zero, one, few, and many-shot prompting to assess\nperformance across scenarios. Our experimental study identifies the\ntop-performing model through extensive experimentation of in-context\nlearning-related parameters, which we fine-tune to further enhance task\nperformance. The results highlight the strengths and limitations of LLMs in\nrecognizing citation intents, providing valuable insights for model selection\nand prompt engineering. Additionally, we make our end-to-end evaluation\nframework and models openly available for future use."
                },
                "authors": [
                    {
                        "name": "Paris Koloveas"
                    },
                    {
                        "name": "Serafeim Chatzopoulos"
                    },
                    {
                        "name": "Thanasis Vergoulis"
                    },
                    {
                        "name": "Christos Tryfonopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Christos Tryfonopoulos"
                },
                "author": "Christos Tryfonopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14560v1",
                "updated": "2025-02-20T13:45:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    45,
                    17,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T13:45:17Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    45,
                    17,
                    3,
                    51,
                    0
                ],
                "title": "Less is More: Improving LLM Alignment via Preference Data Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Improving LLM Alignment via Preference Data Selection"
                },
                "summary": "Direct Preference Optimization (DPO) has emerged as a promising approach for\naligning large language models with human preferences. While prior work mainly\nextends DPO from the aspect of the objective function, we instead improve DPO\nfrom the largely overlooked but critical aspect of data selection.\nSpecifically, we address the issue of parameter shrinkage caused by noisy data\nby proposing a novel margin-maximization principle for dataset curation in DPO\ntraining. To accurately estimate margins for data selection, we propose a\ndual-margin guided approach that considers both external reward margins and\nimplicit DPO reward margins. Extensive experiments demonstrate that our method\nreduces computational cost dramatically while improving performance.\nRemarkably, by using just 10\\% of the Ultrafeedback dataset, our approach\nachieves 3\\% to 8\\% improvements across various Llama and Mistral series models\non the AlpacaEval 2.0 benchmark. Furthermore, our approach seamlessly extends\nto iterative DPO, yielding a roughly 3\\% improvement with 25\\% online data,\nwhile further reducing training time. These results highlight the potential of\ndata selection strategies for advancing preference optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has emerged as a promising approach for\naligning large language models with human preferences. While prior work mainly\nextends DPO from the aspect of the objective function, we instead improve DPO\nfrom the largely overlooked but critical aspect of data selection.\nSpecifically, we address the issue of parameter shrinkage caused by noisy data\nby proposing a novel margin-maximization principle for dataset curation in DPO\ntraining. To accurately estimate margins for data selection, we propose a\ndual-margin guided approach that considers both external reward margins and\nimplicit DPO reward margins. Extensive experiments demonstrate that our method\nreduces computational cost dramatically while improving performance.\nRemarkably, by using just 10\\% of the Ultrafeedback dataset, our approach\nachieves 3\\% to 8\\% improvements across various Llama and Mistral series models\non the AlpacaEval 2.0 benchmark. Furthermore, our approach seamlessly extends\nto iterative DPO, yielding a roughly 3\\% improvement with 25\\% online data,\nwhile further reducing training time. These results highlight the potential of\ndata selection strategies for advancing preference optimization."
                },
                "authors": [
                    {
                        "name": "Xun Deng"
                    },
                    {
                        "name": "Han Zhong"
                    },
                    {
                        "name": "Rui Ai"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14541v1",
                "updated": "2025-02-20T13:20:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    20,
                    19,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T13:20:19Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    20,
                    19,
                    3,
                    51,
                    0
                ],
                "title": "LLM-based User Profile Management for Recommender System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based User Profile Management for Recommender System"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has opened new\nopportunities in recommender systems by enabling zero-shot recommendation\nwithout conventional training. Despite their potential, most existing works\nrely solely on users' purchase histories, leaving significant room for\nimprovement by incorporating user-generated textual data, such as reviews and\nproduct descriptions. Addressing this gap, we propose PURE, a novel LLM-based\nrecommendation framework that builds and maintains evolving user profiles by\nsystematically extracting and summarizing key information from user reviews.\nPURE consists of three core components: a Review Extractor for identifying user\npreferences and key product features, a Profile Updater for refining and\nupdating user profiles, and a Recommender for generating personalized\nrecommendations using the most current profile. To evaluate PURE, we introduce\na continuous sequential recommendation task that reflects real-world scenarios\nby adding reviews over time and updating predictions incrementally. Our\nexperimental results on Amazon datasets demonstrate that PURE outperforms\nexisting LLM-based methods, effectively leveraging long-term user information\nwhile managing token limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has opened new\nopportunities in recommender systems by enabling zero-shot recommendation\nwithout conventional training. Despite their potential, most existing works\nrely solely on users' purchase histories, leaving significant room for\nimprovement by incorporating user-generated textual data, such as reviews and\nproduct descriptions. Addressing this gap, we propose PURE, a novel LLM-based\nrecommendation framework that builds and maintains evolving user profiles by\nsystematically extracting and summarizing key information from user reviews.\nPURE consists of three core components: a Review Extractor for identifying user\npreferences and key product features, a Profile Updater for refining and\nupdating user profiles, and a Recommender for generating personalized\nrecommendations using the most current profile. To evaluate PURE, we introduce\na continuous sequential recommendation task that reflects real-world scenarios\nby adding reviews over time and updating predictions incrementally. Our\nexperimental results on Amazon datasets demonstrate that PURE outperforms\nexisting LLM-based methods, effectively leveraging long-term user information\nwhile managing token limitations."
                },
                "authors": [
                    {
                        "name": "Seunghwan Bang"
                    },
                    {
                        "name": "Hwanjun Song"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjun Song"
                },
                "author": "Hwanjun Song",
                "arxiv_comment": "Submitted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14538v1",
                "updated": "2025-02-20T13:14:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    14,
                    41,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T13:14:41Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    14,
                    41,
                    3,
                    51,
                    0
                ],
                "title": "LoRA-GGPO: Mitigating Double Descent in LoRA Fine-Tuning via\n  Gradient-Guided Perturbation Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-GGPO: Mitigating Double Descent in LoRA Fine-Tuning via\n  Gradient-Guided Perturbation Optimization"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing, but their full fine-tuning remains resource-intensive.\nParameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation\n(LoRA), have emerged as a practical solution by approximating parameter updates\nwith low-rank matrices. However, LoRA often exhibits a \"double descent\"\nphenomenon during fine-tuning, where model performance degrades due to\noverfitting and limited expressiveness caused by low-rank constraints. To\naddress this issue, we propose LoRA-GGPO (Gradient-Guided Perturbation\nOptimization), a novel method that leverages gradient and weight norms to\ngenerate targeted perturbations. By optimizing the sharpness of the loss\nlandscape, LoRA-GGPO guides the model toward flatter minima, mitigating the\ndouble descent problem and improving generalization. Extensive experiments on\nnatural language understanding (NLU) and generation (NLG) tasks demonstrate\nthat LoRA-GGPO outperforms LoRA and its state-of-the-art variants. Furthermore,\nextended experiments specifically designed to analyze the double descent\nphenomenon confirm that LoRA-GGPO effectively alleviates this issue, producing\nmore robust and generalizable models. Our work provides a robust and efficient\nsolution for fine-tuning LLMs, with broad applicability in real-world\nscenarios. The code is available at https://github.com/llm172/LoRA-GGPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing, but their full fine-tuning remains resource-intensive.\nParameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation\n(LoRA), have emerged as a practical solution by approximating parameter updates\nwith low-rank matrices. However, LoRA often exhibits a \"double descent\"\nphenomenon during fine-tuning, where model performance degrades due to\noverfitting and limited expressiveness caused by low-rank constraints. To\naddress this issue, we propose LoRA-GGPO (Gradient-Guided Perturbation\nOptimization), a novel method that leverages gradient and weight norms to\ngenerate targeted perturbations. By optimizing the sharpness of the loss\nlandscape, LoRA-GGPO guides the model toward flatter minima, mitigating the\ndouble descent problem and improving generalization. Extensive experiments on\nnatural language understanding (NLU) and generation (NLG) tasks demonstrate\nthat LoRA-GGPO outperforms LoRA and its state-of-the-art variants. Furthermore,\nextended experiments specifically designed to analyze the double descent\nphenomenon confirm that LoRA-GGPO effectively alleviates this issue, producing\nmore robust and generalizable models. Our work provides a robust and efficient\nsolution for fine-tuning LLMs, with broad applicability in real-world\nscenarios. The code is available at https://github.com/llm172/LoRA-GGPO."
                },
                "authors": [
                    {
                        "name": "Yupeng Chang"
                    },
                    {
                        "name": "Chenlu Guo"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15791v2",
                "updated": "2025-02-20T13:07:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    7,
                    14,
                    3,
                    51,
                    0
                ],
                "published": "2025-01-27T05:35:25Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    35,
                    25,
                    0,
                    27,
                    0
                ],
                "title": "Harnessing Diverse Perspectives: A Multi-Agent Framework for Enhanced\n  Error Detection in Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Diverse Perspectives: A Multi-Agent Framework for Enhanced\n  Error Detection in Knowledge Graphs"
                },
                "summary": "Knowledge graphs are widely used in industrial applications, making error\ndetection crucial for ensuring the reliability of downstream applications.\nExisting error detection methods often fail to effectively utilize fine-grained\nsubgraph information and rely solely on fixed graph structures, while also\nlacking transparency in their decision-making processes, which results in\nsuboptimal detection performance. In this paper, we propose a novel Multi-Agent\nframework for Knowledge Graph Error Detection (MAKGED) that utilizes multiple\nlarge language models (LLMs) in a collaborative setting. By concatenating\nfine-grained, bidirectional subgraph embeddings with LLM-based query embeddings\nduring training, our framework integrates these representations to produce four\nspecialized agents. These agents utilize subgraph information from different\ndimensions to engage in multi-round discussions, thereby improving error\ndetection accuracy and ensuring a transparent decision-making process.\nExtensive experiments on FB15K and WN18RR demonstrate that MAKGED outperforms\nstate-of-the-art methods, enhancing the accuracy and robustness of KG\nevaluation. For specific industrial scenarios, our framework can facilitate the\ntraining of specialized agents using domain-specific knowledge graphs for error\ndetection, which highlights the potential industrial application value of our\nframework. Our code and datasets are available at\nhttps://github.com/kse-ElEvEn/MAKGED.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs are widely used in industrial applications, making error\ndetection crucial for ensuring the reliability of downstream applications.\nExisting error detection methods often fail to effectively utilize fine-grained\nsubgraph information and rely solely on fixed graph structures, while also\nlacking transparency in their decision-making processes, which results in\nsuboptimal detection performance. In this paper, we propose a novel Multi-Agent\nframework for Knowledge Graph Error Detection (MAKGED) that utilizes multiple\nlarge language models (LLMs) in a collaborative setting. By concatenating\nfine-grained, bidirectional subgraph embeddings with LLM-based query embeddings\nduring training, our framework integrates these representations to produce four\nspecialized agents. These agents utilize subgraph information from different\ndimensions to engage in multi-round discussions, thereby improving error\ndetection accuracy and ensuring a transparent decision-making process.\nExtensive experiments on FB15K and WN18RR demonstrate that MAKGED outperforms\nstate-of-the-art methods, enhancing the accuracy and robustness of KG\nevaluation. For specific industrial scenarios, our framework can facilitate the\ntraining of specialized agents using domain-specific knowledge graphs for error\ndetection, which highlights the potential industrial application value of our\nframework. Our code and datasets are available at\nhttps://github.com/kse-ElEvEn/MAKGED."
                },
                "authors": [
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Yi Huang"
                    },
                    {
                        "name": "Guilin Qi"
                    },
                    {
                        "name": "Junlan Feng"
                    },
                    {
                        "name": "Nan Hu"
                    },
                    {
                        "name": "Songlin Zhai"
                    },
                    {
                        "name": "Haohan Xue"
                    },
                    {
                        "name": "Yongrui Chen"
                    },
                    {
                        "name": "Ruoyan Shen"
                    },
                    {
                        "name": "Tongtong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongtong Wu"
                },
                "author": "Tongtong Wu",
                "arxiv_comment": "This paper has been ACCEPTED as a FULL PAPER at DASFAA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14529v1",
                "updated": "2025-02-20T13:02:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    2,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T13:02:00Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    2,
                    0,
                    3,
                    51,
                    0
                ],
                "title": "CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems\n  Based on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems\n  Based on Large Language Models"
                },
                "summary": "Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated\nremarkable real-world capabilities, effectively collaborating to complete\ncomplex tasks. While these systems are designed with safety mechanisms, such as\nrejecting harmful instructions through alignment, their security remains\nlargely unexplored. This gap leaves LLM-MASs vulnerable to targeted\ndisruptions. In this paper, we introduce Contagious Recursive Blocking Attacks\n(Corba), a novel and simple yet highly effective attack that disrupts\ninteractions between agents within an LLM-MAS. Corba leverages two key\nproperties: its contagious nature allows it to propagate across arbitrary\nnetwork topologies, while its recursive property enables sustained depletion of\ncomputational resources. Notably, these blocking attacks often involve\nseemingly benign instructions, making them particularly challenging to mitigate\nusing conventional alignment methods. We evaluate Corba on two widely-used\nLLM-MASs, namely, AutoGen and Camel across various topologies and commercial\nmodels. Additionally, we conduct more extensive experiments in open-ended\ninteractive LLM-MASs, demonstrating the effectiveness of Corba in complex\ntopology structures and open-source models. Our code is available at:\nhttps://github.com/zhrli324/Corba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated\nremarkable real-world capabilities, effectively collaborating to complete\ncomplex tasks. While these systems are designed with safety mechanisms, such as\nrejecting harmful instructions through alignment, their security remains\nlargely unexplored. This gap leaves LLM-MASs vulnerable to targeted\ndisruptions. In this paper, we introduce Contagious Recursive Blocking Attacks\n(Corba), a novel and simple yet highly effective attack that disrupts\ninteractions between agents within an LLM-MAS. Corba leverages two key\nproperties: its contagious nature allows it to propagate across arbitrary\nnetwork topologies, while its recursive property enables sustained depletion of\ncomputational resources. Notably, these blocking attacks often involve\nseemingly benign instructions, making them particularly challenging to mitigate\nusing conventional alignment methods. We evaluate Corba on two widely-used\nLLM-MASs, namely, AutoGen and Camel across various topologies and commercial\nmodels. Additionally, we conduct more extensive experiments in open-ended\ninteractive LLM-MASs, demonstrating the effectiveness of Corba in complex\ntopology structures and open-source models. Our code is available at:\nhttps://github.com/zhrli324/Corba."
                },
                "authors": [
                    {
                        "name": "Zhenhong Zhou"
                    },
                    {
                        "name": "Zherui Li"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Yuanhe Zhang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Qing Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qing Guo"
                },
                "author": "Qing Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14523v1",
                "updated": "2025-02-20T12:56:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    56,
                    16,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:56:16Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    56,
                    16,
                    3,
                    51,
                    0
                ],
                "title": "Generative adversarial networks vs large language models: a comparative\n  study on synthetic tabular data generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative adversarial networks vs large language models: a comparative\n  study on synthetic tabular data generation"
                },
                "summary": "We propose a new framework for zero-shot generation of synthetic tabular\ndata. Using the large language model (LLM) GPT-4o and plain-language prompting,\nwe demonstrate the ability to generate high-fidelity tabular data without\ntask-specific fine-tuning or access to real-world data (RWD) for pre-training.\nTo benchmark GPT-4o, we compared the fidelity and privacy of LLM-generated\nsynthetic data against data generated with the conditional tabular generative\nadversarial network (CTGAN), across three open-access datasets: Iris, Fish\nMeasurements, and Real Estate Valuation. Despite the zero-shot approach, GPT-4o\noutperformed CTGAN in preserving means, 95% confidence intervals, bivariate\ncorrelations, and data privacy of RWD, even at amplified sample sizes. Notably,\ncorrelations between parameters were consistently preserved with appropriate\ndirection and strength. However, refinement is necessary to better retain\ndistributional characteristics. These findings highlight the potential of LLMs\nin tabular data synthesis, offering an accessible alternative to generative\nadversarial networks and variational autoencoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new framework for zero-shot generation of synthetic tabular\ndata. Using the large language model (LLM) GPT-4o and plain-language prompting,\nwe demonstrate the ability to generate high-fidelity tabular data without\ntask-specific fine-tuning or access to real-world data (RWD) for pre-training.\nTo benchmark GPT-4o, we compared the fidelity and privacy of LLM-generated\nsynthetic data against data generated with the conditional tabular generative\nadversarial network (CTGAN), across three open-access datasets: Iris, Fish\nMeasurements, and Real Estate Valuation. Despite the zero-shot approach, GPT-4o\noutperformed CTGAN in preserving means, 95% confidence intervals, bivariate\ncorrelations, and data privacy of RWD, even at amplified sample sizes. Notably,\ncorrelations between parameters were consistently preserved with appropriate\ndirection and strength. However, refinement is necessary to better retain\ndistributional characteristics. These findings highlight the potential of LLMs\nin tabular data synthesis, offering an accessible alternative to generative\nadversarial networks and variational autoencoders."
                },
                "authors": [
                    {
                        "name": "Austin A. Barr"
                    },
                    {
                        "name": "Robert Rozman"
                    },
                    {
                        "name": "Eddie Guo"
                    }
                ],
                "author_detail": {
                    "name": "Eddie Guo"
                },
                "author": "Eddie Guo",
                "arxiv_comment": "12 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20127v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20127v3",
                "updated": "2025-02-20T12:55:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    55,
                    22,
                    3,
                    51,
                    0
                ],
                "published": "2024-12-28T12:11:28Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    12,
                    11,
                    28,
                    5,
                    363,
                    0
                ],
                "title": "M-MAD: Multidimensional Multi-Agent Debate for Advanced Machine\n  Translation Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-MAD: Multidimensional Multi-Agent Debate for Advanced Machine\n  Translation Evaluation"
                },
                "summary": "Recent advancements in large language models (LLMs) have given rise to the\nLLM-as-a-judge paradigm, showcasing their potential to deliver human-like\njudgments. However, in the field of machine translation (MT) evaluation,\ncurrent LLM-as-a-judge methods fall short of learned automatic metrics. In this\npaper, we propose Multidimensional Multi-Agent Debate (M-MAD), a systematic\nLLM-based multi-agent framework for advanced LLM-as-a-judge MT evaluation. Our\nfindings demonstrate that M-MAD achieves significant advancements by (1)\ndecoupling heuristic MQM criteria into distinct evaluation dimensions for\nfine-grained assessments; (2) employing multi-agent debates to harness the\ncollaborative reasoning capabilities of LLMs; (3) synthesizing\ndimension-specific results into a final evaluation judgment to ensure robust\nand reliable outcomes. Comprehensive experiments show that M-MAD not only\noutperforms all existing LLM-as-a-judge methods but also competes with\nstate-of-the-art reference-based automatic metrics, even when powered by a\nsuboptimal model like GPT-4o mini. Detailed ablations and analysis highlight\nthe superiority of our framework design, offering a fresh perspective for\nLLM-as-a-judge paradigm. Our code and data are publicly available at\nhttps://github.com/SU-JIAYUAN/M-MAD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have given rise to the\nLLM-as-a-judge paradigm, showcasing their potential to deliver human-like\njudgments. However, in the field of machine translation (MT) evaluation,\ncurrent LLM-as-a-judge methods fall short of learned automatic metrics. In this\npaper, we propose Multidimensional Multi-Agent Debate (M-MAD), a systematic\nLLM-based multi-agent framework for advanced LLM-as-a-judge MT evaluation. Our\nfindings demonstrate that M-MAD achieves significant advancements by (1)\ndecoupling heuristic MQM criteria into distinct evaluation dimensions for\nfine-grained assessments; (2) employing multi-agent debates to harness the\ncollaborative reasoning capabilities of LLMs; (3) synthesizing\ndimension-specific results into a final evaluation judgment to ensure robust\nand reliable outcomes. Comprehensive experiments show that M-MAD not only\noutperforms all existing LLM-as-a-judge methods but also competes with\nstate-of-the-art reference-based automatic metrics, even when powered by a\nsuboptimal model like GPT-4o mini. Detailed ablations and analysis highlight\nthe superiority of our framework design, offering a fresh perspective for\nLLM-as-a-judge paradigm. Our code and data are publicly available at\nhttps://github.com/SU-JIAYUAN/M-MAD."
                },
                "authors": [
                    {
                        "name": "Zhaopeng Feng"
                    },
                    {
                        "name": "Jiayuan Su"
                    },
                    {
                        "name": "Jiamei Zheng"
                    },
                    {
                        "name": "Jiahan Ren"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Hongwei Wang"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "Code and data are available at https://github.com/SU-JIAYUAN/M-MAD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20127v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20127v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14507v1",
                "updated": "2025-02-20T12:34:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    34,
                    46,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:34:46Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    34,
                    46,
                    3,
                    51,
                    0
                ],
                "title": "Can LLMs Simulate L2-English Dialogue? An Information-Theoretic Analysis\n  of L1-Dependent Biases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Simulate L2-English Dialogue? An Information-Theoretic Analysis\n  of L1-Dependent Biases"
                },
                "summary": "This study evaluates Large Language Models' (LLMs) ability to simulate\nnon-native-like English use observed in human second language (L2) learners\ninterfered with by their native first language (L1). In dialogue-based\ninterviews, we prompt LLMs to mimic L2 English learners with specific L1s\n(e.g., Japanese, Thai, Urdu) across seven languages, comparing their outputs to\nreal L2 learner data. Our analysis examines L1-driven linguistic biases, such\nas reference word usage and avoidance behaviors, using information-theoretic\nand distributional density measures. Results show that modern LLMs (e.g.,\nQwen2.5, LLAMA3.3, DeepseekV3, GPT-4o) replicate L1-dependent patterns observed\nin human L2 data, with distinct influences from various languages (e.g.,\nJapanese, Korean, and Mandarin significantly affect tense agreement, and Urdu\ninfluences noun-verb collocations). Our results reveal the potential of LLMs\nfor L2 dialogue generation and evaluation for future educational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates Large Language Models' (LLMs) ability to simulate\nnon-native-like English use observed in human second language (L2) learners\ninterfered with by their native first language (L1). In dialogue-based\ninterviews, we prompt LLMs to mimic L2 English learners with specific L1s\n(e.g., Japanese, Thai, Urdu) across seven languages, comparing their outputs to\nreal L2 learner data. Our analysis examines L1-driven linguistic biases, such\nas reference word usage and avoidance behaviors, using information-theoretic\nand distributional density measures. Results show that modern LLMs (e.g.,\nQwen2.5, LLAMA3.3, DeepseekV3, GPT-4o) replicate L1-dependent patterns observed\nin human L2 data, with distinct influences from various languages (e.g.,\nJapanese, Korean, and Mandarin significantly affect tense agreement, and Urdu\ninfluences noun-verb collocations). Our results reveal the potential of LLMs\nfor L2 dialogue generation and evaluation for future educational applications."
                },
                "authors": [
                    {
                        "name": "Rena Gao"
                    },
                    {
                        "name": "Xuetong Wu"
                    },
                    {
                        "name": "Tatsuki Kuribayashi"
                    },
                    {
                        "name": "Mingrui Ye"
                    },
                    {
                        "name": "Siya Qi"
                    },
                    {
                        "name": "Carsten Roever"
                    },
                    {
                        "name": "Yuanxing Liu"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Jey Han Lau"
                    }
                ],
                "author_detail": {
                    "name": "Jey Han Lau"
                },
                "author": "Jey Han Lau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14502v1",
                "updated": "2025-02-20T12:31:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:31:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?"
                },
                "summary": "The performance of Large Language Models (LLMs) on many tasks is greatly\nlimited by the knowledge learned during pre-training and stored in the model's\nparameters. Low-rank adaptation (LoRA) is a popular and efficient training\ntechnique for updating or domain-specific adaptation of LLMs. In this study, we\ninvestigate how new facts can be incorporated into the LLM using LoRA without\ncompromising the previously learned knowledge. We fine-tuned\nLlama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our\nexperiments have shown that the best results are obtained when the training\ndata contains a mixture of known and new facts. However, this approach is still\npotentially harmful because the model's performance on external\nquestion-answering benchmarks declines after such fine-tuning. When the\ntraining data is biased towards certain entities, the model tends to regress to\nfew overrepresented answers. In addition, we found that the model becomes more\nconfident and refuses to provide an answer in only few cases. These findings\nhighlight the potential pitfalls of LoRA-based LLM updates and underscore the\nimportance of training data composition and tuning parameters to balance new\nknowledge integration and general model capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Large Language Models (LLMs) on many tasks is greatly\nlimited by the knowledge learned during pre-training and stored in the model's\nparameters. Low-rank adaptation (LoRA) is a popular and efficient training\ntechnique for updating or domain-specific adaptation of LLMs. In this study, we\ninvestigate how new facts can be incorporated into the LLM using LoRA without\ncompromising the previously learned knowledge. We fine-tuned\nLlama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our\nexperiments have shown that the best results are obtained when the training\ndata contains a mixture of known and new facts. However, this approach is still\npotentially harmful because the model's performance on external\nquestion-answering benchmarks declines after such fine-tuning. When the\ntraining data is biased towards certain entities, the model tends to regress to\nfew overrepresented answers. In addition, we found that the model becomes more\nconfident and refuses to provide an answer in only few cases. These findings\nhighlight the potential pitfalls of LoRA-based LLM updates and underscore the\nimportance of training data composition and tuning parameters to balance new\nknowledge integration and general model capabilities."
                },
                "authors": [
                    {
                        "name": "Sergey Pletenev"
                    },
                    {
                        "name": "Maria Marina"
                    },
                    {
                        "name": "Daniil Moskovskiy"
                    },
                    {
                        "name": "Vasily Konovalov"
                    },
                    {
                        "name": "Pavel Braslavski"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Mikhail Salnikov"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Salnikov"
                },
                "author": "Mikhail Salnikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14499v1",
                "updated": "2025-02-20T12:28:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    28,
                    23,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:28:23Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    28,
                    23,
                    3,
                    51,
                    0
                ],
                "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents"
                },
                "summary": "We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for\nevaluating and developing LLM agents on AI research tasks. This is the first\nGym environment for machine learning (ML) tasks, enabling research on\nreinforcement learning (RL) algorithms for training such agents. MLGym-bench\nconsists of 13 diverse and open-ended AI research tasks from diverse domains\nsuch as computer vision, natural language processing, reinforcement learning,\nand game theory. Solving these tasks requires real-world AI research skills\nsuch as generating new ideas and hypotheses, creating and processing data,\nimplementing ML methods, training models, running experiments, analyzing the\nresults, and iterating through this process to improve on a given task. We\nevaluate a number of frontier large language models (LLMs) on our benchmarks\nsuch as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5\nPro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate\nmodels or agents, generate synthetic data at scale, as well as develop new\nlearning algorithms for training agents on AI research tasks. We find that\ncurrent frontier models can improve on the given baselines, usually by finding\nbetter hyperparameters, but do not generate novel hypotheses, algorithms,\narchitectures, or substantial improvements. We open-source our framework and\nbenchmark to facilitate future research in advancing the AI research\ncapabilities of LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for\nevaluating and developing LLM agents on AI research tasks. This is the first\nGym environment for machine learning (ML) tasks, enabling research on\nreinforcement learning (RL) algorithms for training such agents. MLGym-bench\nconsists of 13 diverse and open-ended AI research tasks from diverse domains\nsuch as computer vision, natural language processing, reinforcement learning,\nand game theory. Solving these tasks requires real-world AI research skills\nsuch as generating new ideas and hypotheses, creating and processing data,\nimplementing ML methods, training models, running experiments, analyzing the\nresults, and iterating through this process to improve on a given task. We\nevaluate a number of frontier large language models (LLMs) on our benchmarks\nsuch as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5\nPro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate\nmodels or agents, generate synthetic data at scale, as well as develop new\nlearning algorithms for training agents on AI research tasks. We find that\ncurrent frontier models can improve on the given baselines, usually by finding\nbetter hyperparameters, but do not generate novel hypotheses, algorithms,\narchitectures, or substantial improvements. We open-source our framework and\nbenchmark to facilitate future research in advancing the AI research\ncapabilities of LLM agents."
                },
                "authors": [
                    {
                        "name": "Deepak Nathani"
                    },
                    {
                        "name": "Lovish Madaan"
                    },
                    {
                        "name": "Nicholas Roberts"
                    },
                    {
                        "name": "Nikolay Bashlykov"
                    },
                    {
                        "name": "Ajay Menon"
                    },
                    {
                        "name": "Vincent Moens"
                    },
                    {
                        "name": "Amar Budhiraja"
                    },
                    {
                        "name": "Despoina Magka"
                    },
                    {
                        "name": "Vladislav Vorotilov"
                    },
                    {
                        "name": "Gaurav Chaurasia"
                    },
                    {
                        "name": "Dieuwke Hupkes"
                    },
                    {
                        "name": "Ricardo Silveira Cabral"
                    },
                    {
                        "name": "Tatiana Shavrina"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Yoram Bachrach"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Roberta Raileanu"
                    }
                ],
                "author_detail": {
                    "name": "Roberta Raileanu"
                },
                "author": "Roberta Raileanu",
                "arxiv_comment": "35 pages, 12 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14496v1",
                "updated": "2025-02-20T12:26:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    26,
                    15,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:26:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    26,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "Enhancing Language Multi-Agent Learning with Multi-Agent Credit\n  Re-Assignment for Interactive Environment Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Language Multi-Agent Learning with Multi-Agent Credit\n  Re-Assignment for Interactive Environment Generalization"
                },
                "summary": "LLM-based agents have made significant advancements in interactive\nenvironments, such as mobile operations and web browsing, and other domains\nbeyond computer using. Current multi-agent systems universally excel in\nperformance, compared to single agents, but struggle with generalization across\nenvironments due to predefined roles and inadequate strategies for generalizing\nlanguage agents. The challenge of achieving both strong performance and good\ngeneralization has hindered the progress of multi-agent systems for interactive\nenvironments. To address these issues, we propose CollabUIAgents, a multi-agent\nreinforcement learning framework with a novel multi-agent credit re-assignment\n(CR) strategy, assigning process rewards with LLMs rather than\nenvironment-specific rewards and learning with synthesized preference data, in\norder to foster generalizable, collaborative behaviors among the role-free\nagents' policies. Empirical results show that our framework improves both\nperformance and cross-environment generalizability of multi-agent systems.\nMoreover, our 7B-parameter system achieves results on par with or exceed strong\nclosed-source models, and the LLM that guides the CR. We also provide insights\nin using granular CR rewards effectively for environment generalization, and\naccommodating trained LLMs in multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have made significant advancements in interactive\nenvironments, such as mobile operations and web browsing, and other domains\nbeyond computer using. Current multi-agent systems universally excel in\nperformance, compared to single agents, but struggle with generalization across\nenvironments due to predefined roles and inadequate strategies for generalizing\nlanguage agents. The challenge of achieving both strong performance and good\ngeneralization has hindered the progress of multi-agent systems for interactive\nenvironments. To address these issues, we propose CollabUIAgents, a multi-agent\nreinforcement learning framework with a novel multi-agent credit re-assignment\n(CR) strategy, assigning process rewards with LLMs rather than\nenvironment-specific rewards and learning with synthesized preference data, in\norder to foster generalizable, collaborative behaviors among the role-free\nagents' policies. Empirical results show that our framework improves both\nperformance and cross-environment generalizability of multi-agent systems.\nMoreover, our 7B-parameter system achieves results on par with or exceed strong\nclosed-source models, and the LLM that guides the CR. We also provide insights\nin using granular CR rewards effectively for environment generalization, and\naccommodating trained LLMs in multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Zhitao He"
                    },
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "May Fung"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "24 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14494v1",
                "updated": "2025-02-20T12:22:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    22,
                    18,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:22:18Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    22,
                    18,
                    3,
                    51,
                    0
                ],
                "title": "StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction\n  Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction\n  Following"
                },
                "summary": "Multi-turn instruction following capability constitutes a core competency of\nlarge language models (LLMs) in real-world applications. Existing evaluation\nbenchmarks predominantly focus on fine-grained constraint satisfaction and\ndomain-specific capability assessment, yet overlook the crucial structural\ndependency between dialogue turns that distinguishes multi-turn from\nsingle-turn interactions. This structural dependency not only reflects user\nintent but also establishes a second dimension for instruction following\nevaluation beyond constraint satisfaction. To address this gap, we propose\nStructFlowBench, a multi-turn instruction following benchmark with structural\nflow modeling. The benchmark innovatively defines a structural flow framework\ncomprising six fundamental inter-turn relationships, which not only introduces\nnovel structural constraints for model evaluation but also serves as generation\nparameters for creating customized dialogue flows tailored to specific\nscenarios. Adopting established LLM-based automatic evaluation methodologies,\nwe conduct systematic evaluations of 13 leading open-source and closed-source\nLLMs. Experimental results reveal significant deficiencies in current models'\ncomprehension of multi-turn dialogue structures. The code is available at\n\\url{https://github.com/MLGroupJLU/StructFlowBench}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn instruction following capability constitutes a core competency of\nlarge language models (LLMs) in real-world applications. Existing evaluation\nbenchmarks predominantly focus on fine-grained constraint satisfaction and\ndomain-specific capability assessment, yet overlook the crucial structural\ndependency between dialogue turns that distinguishes multi-turn from\nsingle-turn interactions. This structural dependency not only reflects user\nintent but also establishes a second dimension for instruction following\nevaluation beyond constraint satisfaction. To address this gap, we propose\nStructFlowBench, a multi-turn instruction following benchmark with structural\nflow modeling. The benchmark innovatively defines a structural flow framework\ncomprising six fundamental inter-turn relationships, which not only introduces\nnovel structural constraints for model evaluation but also serves as generation\nparameters for creating customized dialogue flows tailored to specific\nscenarios. Adopting established LLM-based automatic evaluation methodologies,\nwe conduct systematic evaluations of 13 leading open-source and closed-source\nLLMs. Experimental results reveal significant deficiencies in current models'\ncomprehension of multi-turn dialogue structures. The code is available at\n\\url{https://github.com/MLGroupJLU/StructFlowBench}."
                },
                "authors": [
                    {
                        "name": "Jinnan Li"
                    },
                    {
                        "name": "Jinzhe Li"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "18 pages, 8 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14493v1",
                "updated": "2025-02-20T12:19:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    19,
                    30,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:19:30Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    19,
                    30,
                    3,
                    51,
                    0
                ],
                "title": "CrossFuse: Learning Infrared and Visible Image Fusion by Cross-Sensor\n  Top-K Vision Alignment and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrossFuse: Learning Infrared and Visible Image Fusion by Cross-Sensor\n  Top-K Vision Alignment and Beyond"
                },
                "summary": "Infrared and visible image fusion (IVIF) is increasingly applied in critical\nfields such as video surveillance and autonomous driving systems. Significant\nprogress has been made in deep learning-based fusion methods. However, these\nmodels frequently encounter out-of-distribution (OOD) scenes in real-world\napplications, which severely impact their performance and reliability.\nTherefore, addressing the challenge of OOD data is crucial for the safe\ndeployment of these models in open-world environments. Unlike existing\nresearch, our focus is on the challenges posed by OOD data in real-world\napplications and on enhancing the robustness and generalization of models. In\nthis paper, we propose an infrared-visible fusion framework based on Multi-View\nAugmentation. For external data augmentation, Top-k Selective Vision Alignment\nis employed to mitigate distribution shifts between datasets by performing\nRGB-wise transformations on visible images. This strategy effectively\nintroduces augmented samples, enhancing the adaptability of the model to\ncomplex real-world scenarios. Additionally, for internal data augmentation,\nself-supervised learning is established using Weak-Aggressive Augmentation.\nThis enables the model to learn more robust and general feature representations\nduring the fusion process, thereby improving robustness and generalization.\nExtensive experiments demonstrate that the proposed method exhibits superior\nperformance and robustness across various conditions and environments. Our\napproach significantly enhances the reliability and stability of IVIF tasks in\npractical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infrared and visible image fusion (IVIF) is increasingly applied in critical\nfields such as video surveillance and autonomous driving systems. Significant\nprogress has been made in deep learning-based fusion methods. However, these\nmodels frequently encounter out-of-distribution (OOD) scenes in real-world\napplications, which severely impact their performance and reliability.\nTherefore, addressing the challenge of OOD data is crucial for the safe\ndeployment of these models in open-world environments. Unlike existing\nresearch, our focus is on the challenges posed by OOD data in real-world\napplications and on enhancing the robustness and generalization of models. In\nthis paper, we propose an infrared-visible fusion framework based on Multi-View\nAugmentation. For external data augmentation, Top-k Selective Vision Alignment\nis employed to mitigate distribution shifts between datasets by performing\nRGB-wise transformations on visible images. This strategy effectively\nintroduces augmented samples, enhancing the adaptability of the model to\ncomplex real-world scenarios. Additionally, for internal data augmentation,\nself-supervised learning is established using Weak-Aggressive Augmentation.\nThis enables the model to learn more robust and general feature representations\nduring the fusion process, thereby improving robustness and generalization.\nExtensive experiments demonstrate that the proposed method exhibits superior\nperformance and robustness across various conditions and environments. Our\napproach significantly enhances the reliability and stability of IVIF tasks in\npractical applications."
                },
                "authors": [
                    {
                        "name": "Yukai Shi"
                    },
                    {
                        "name": "Cidan Shi"
                    },
                    {
                        "name": "Zhipeng Weng"
                    },
                    {
                        "name": "Yin Tian"
                    },
                    {
                        "name": "Xiaoyu Xian"
                    },
                    {
                        "name": "Liang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liang Lin"
                },
                "author": "Liang Lin",
                "arxiv_comment": "IEEE T-CSVT. We mainly discuss the out-of-distribution challenges in\n  infrared and visible image fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v2",
                "updated": "2025-02-20T12:14:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    14,
                    49,
                    3,
                    51,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14482v1",
                "updated": "2025-02-20T12:01:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    1,
                    11,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:01:11Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    1,
                    11,
                    3,
                    51,
                    0
                ],
                "title": "NLoRA: Nystrm-Initiated Low-Rank Adaptation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLoRA: Nystrm-Initiated Low-Rank Adaptation for Large Language Models"
                },
                "summary": "Parameter-efficient fine-tuning (PEFT) is essential for adapting large\nlanguage models (LLMs), with low-rank adaptation (LoRA) being the most popular\napproach. However, LoRA suffers from slow convergence, and some recent LoRA\nvariants, such as PiSSA, primarily rely on Singular Value Decomposition (SVD)\nfor initialization, leading to expensive computation. To mitigate these\nproblems, we use the Nystr\\\"om method, which follows a three-matrix\nmanipulation. We first introduce StructuredLoRA (SLoRA), which investigates\nadding a small intermediate matrix between the low-rank matrices A and B.\nSecondly, we propose Nystr\\\"omLoRA (NLoRA), which leverages Nystr\\\"om-based\ninitialization for SLoRA to improve its effectiveness and efficiency. Finally,\nwe propose IntermediateTune (IntTune), which explores fine-tuning exclusively\non the intermediate matrix of NLoRA to further boost LLM efficiency. We\nevaluate our methods on five natural language generation (NLG) tasks and eight\nnatural language understanding (NLU) tasks. On GSM8K, SLoRA and NLoRA achieve\naccuracies of 56.48% and 57.70%, surpassing LoRA by 33.52% and 36.41%, with\nonly 3.67 million additional trainable parameters. IntTune improves average NLG\nperformance over LoRA by 7.45% while using only 1.25% of its parameters. These\nresults demonstrate the efficiency and effectiveness of our approach in\nenhancing model performance with minimal parameter overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning (PEFT) is essential for adapting large\nlanguage models (LLMs), with low-rank adaptation (LoRA) being the most popular\napproach. However, LoRA suffers from slow convergence, and some recent LoRA\nvariants, such as PiSSA, primarily rely on Singular Value Decomposition (SVD)\nfor initialization, leading to expensive computation. To mitigate these\nproblems, we use the Nystr\\\"om method, which follows a three-matrix\nmanipulation. We first introduce StructuredLoRA (SLoRA), which investigates\nadding a small intermediate matrix between the low-rank matrices A and B.\nSecondly, we propose Nystr\\\"omLoRA (NLoRA), which leverages Nystr\\\"om-based\ninitialization for SLoRA to improve its effectiveness and efficiency. Finally,\nwe propose IntermediateTune (IntTune), which explores fine-tuning exclusively\non the intermediate matrix of NLoRA to further boost LLM efficiency. We\nevaluate our methods on five natural language generation (NLG) tasks and eight\nnatural language understanding (NLU) tasks. On GSM8K, SLoRA and NLoRA achieve\naccuracies of 56.48% and 57.70%, surpassing LoRA by 33.52% and 36.41%, with\nonly 3.67 million additional trainable parameters. IntTune improves average NLG\nperformance over LoRA by 7.45% while using only 1.25% of its parameters. These\nresults demonstrate the efficiency and effectiveness of our approach in\nenhancing model performance with minimal parameter overhead."
                },
                "authors": [
                    {
                        "name": "Chenlu Guo"
                    },
                    {
                        "name": "Yuan Wu"
                    },
                    {
                        "name": "Yi Chang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Chang"
                },
                "author": "Yi Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14477v1",
                "updated": "2025-02-20T11:52:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    11,
                    52,
                    36,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T11:52:36Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    11,
                    52,
                    36,
                    3,
                    51,
                    0
                ],
                "title": "Unshackling Context Length: An Efficient Selective Attention Approach\n  through Query-Key Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unshackling Context Length: An Efficient Selective Attention Approach\n  through Query-Key Compression"
                },
                "summary": "Handling long-context sequences efficiently remains a significant challenge\nin large language models (LLMs). Existing methods for token selection in\nsequence extrapolation either employ a permanent eviction strategy or select\ntokens by chunk, which may lead to the loss of critical information. We propose\nEfficient Selective Attention (ESA), a novel approach that extends context\nlength by efficiently selecting the most critical tokens at the token level to\ncompute attention. ESA reduces the computational complexity of token selection\nby compressing query and key vectors into lower-dimensional representations. We\nevaluate ESA on long sequence benchmarks with maximum lengths up to 256k using\nopen-source LLMs with context lengths of 8k and 32k. ESA outperforms other\nselective attention methods, especially in tasks requiring the retrieval of\nmultiple pieces of information, achieving comparable performance to\nfull-attention extrapolation methods across various tasks, with superior\nresults in certain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling long-context sequences efficiently remains a significant challenge\nin large language models (LLMs). Existing methods for token selection in\nsequence extrapolation either employ a permanent eviction strategy or select\ntokens by chunk, which may lead to the loss of critical information. We propose\nEfficient Selective Attention (ESA), a novel approach that extends context\nlength by efficiently selecting the most critical tokens at the token level to\ncompute attention. ESA reduces the computational complexity of token selection\nby compressing query and key vectors into lower-dimensional representations. We\nevaluate ESA on long sequence benchmarks with maximum lengths up to 256k using\nopen-source LLMs with context lengths of 8k and 32k. ESA outperforms other\nselective attention methods, especially in tasks requiring the retrieval of\nmultiple pieces of information, achieving comparable performance to\nfull-attention extrapolation methods across various tasks, with superior\nresults in certain tasks."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Tong Teng"
                    },
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "An Xiao"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "arxiv_comment": "14 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14476v1",
                "updated": "2025-02-20T11:52:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    11,
                    52,
                    26,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T11:52:26Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    11,
                    52,
                    26,
                    3,
                    51,
                    0
                ],
                "title": "Argument-Based Comparative Question Answering Evaluation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argument-Based Comparative Question Answering Evaluation Benchmark"
                },
                "summary": "In this paper, we aim to solve the problems standing in the way of automatic\ncomparative question answering. To this end, we propose an evaluation framework\nto assess the quality of comparative question answering summaries. We formulate\n15 criteria for assessing comparative answers created using manual annotation\nand annotation from 6 large language models and two comparative question\nasnwering datasets. We perform our tests using several LLMs and manual\nannotation under different settings and demonstrate the constituency of both\nevaluations. Our results demonstrate that the Llama-3 70B Instruct model\ndemonstrates the best results for summary evaluation, while GPT-4 is the best\nfor answering comparative questions. All used data, code, and evaluation\nresults are publicly\navailable\\footnote{\\url{https://anonymous.4open.science/r/cqa-evaluation-benchmark-4561/README.md}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we aim to solve the problems standing in the way of automatic\ncomparative question answering. To this end, we propose an evaluation framework\nto assess the quality of comparative question answering summaries. We formulate\n15 criteria for assessing comparative answers created using manual annotation\nand annotation from 6 large language models and two comparative question\nasnwering datasets. We perform our tests using several LLMs and manual\nannotation under different settings and demonstrate the constituency of both\nevaluations. Our results demonstrate that the Llama-3 70B Instruct model\ndemonstrates the best results for summary evaluation, while GPT-4 is the best\nfor answering comparative questions. All used data, code, and evaluation\nresults are publicly\navailable\\footnote{\\url{https://anonymous.4open.science/r/cqa-evaluation-benchmark-4561/README.md}}."
                },
                "authors": [
                    {
                        "name": "Irina Nikishina"
                    },
                    {
                        "name": "Saba Anwar"
                    },
                    {
                        "name": "Nikolay Dolgov"
                    },
                    {
                        "name": "Maria Manina"
                    },
                    {
                        "name": "Daria Ignatenko"
                    },
                    {
                        "name": "Viktor Moskvoretskii"
                    },
                    {
                        "name": "Artem Shelmanov"
                    },
                    {
                        "name": "Tim Baldwin"
                    },
                    {
                        "name": "Chris Biemann"
                    }
                ],
                "author_detail": {
                    "name": "Chris Biemann"
                },
                "author": "Chris Biemann",
                "arxiv_comment": "8 pages, 7 Tables, 13 Figures, 18 pages with Appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14469v1",
                "updated": "2025-02-20T11:46:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    11,
                    46,
                    51,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T11:46:51Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    11,
                    46,
                    51,
                    3,
                    51,
                    0
                ],
                "title": "Enhancing Smart Environments with Context-Aware Chatbots using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Smart Environments with Context-Aware Chatbots using Large\n  Language Models"
                },
                "summary": "This work presents a novel architecture for context-aware interactions within\nsmart environments, leveraging Large Language Models (LLMs) to enhance user\nexperiences. Our system integrates user location data obtained through UWB tags\nand sensor-equipped smart homes with real-time human activity recognition (HAR)\nto provide a comprehensive understanding of user context. This contextual\ninformation is then fed to an LLM-powered chatbot, enabling it to generate\npersonalised interactions and recommendations based on the user's current\nactivity and environment. This approach moves beyond traditional static chatbot\ninteractions by dynamically adapting to the user's real-time situation. A case\nstudy conducted from a real-world dataset demonstrates the feasibility and\neffectiveness of our proposed architecture, showcasing its potential to create\nmore intuitive and helpful interactions within smart homes. The results\nhighlight the significant benefits of integrating LLM with real-time activity\nand location data to deliver personalised and contextually relevant user\nexperiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel architecture for context-aware interactions within\nsmart environments, leveraging Large Language Models (LLMs) to enhance user\nexperiences. Our system integrates user location data obtained through UWB tags\nand sensor-equipped smart homes with real-time human activity recognition (HAR)\nto provide a comprehensive understanding of user context. This contextual\ninformation is then fed to an LLM-powered chatbot, enabling it to generate\npersonalised interactions and recommendations based on the user's current\nactivity and environment. This approach moves beyond traditional static chatbot\ninteractions by dynamically adapting to the user's real-time situation. A case\nstudy conducted from a real-world dataset demonstrates the feasibility and\neffectiveness of our proposed architecture, showcasing its potential to create\nmore intuitive and helpful interactions within smart homes. The results\nhighlight the significant benefits of integrating LLM with real-time activity\nand location data to deliver personalised and contextually relevant user\nexperiences."
                },
                "authors": [
                    {
                        "name": "Aurora Polo-Rodrguez"
                    },
                    {
                        "name": "Laura Fiorini"
                    },
                    {
                        "name": "Erika Rovini"
                    },
                    {
                        "name": "Filippo Cavallo"
                    },
                    {
                        "name": "Javier Medina-Quero"
                    }
                ],
                "author_detail": {
                    "name": "Javier Medina-Quero"
                },
                "author": "Javier Medina-Quero",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14451v1",
                "updated": "2025-02-20T11:06:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    11,
                    6,
                    16,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T11:06:16Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    11,
                    6,
                    16,
                    3,
                    51,
                    0
                ],
                "title": "Optimal word order for non-causal text generation with Large Language\n  Models: the Spanish case",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal word order for non-causal text generation with Large Language\n  Models: the Spanish case"
                },
                "summary": "Natural Language Generation (NLG) popularity has increased owing to the\nprogress in Large Language Models (LLMs), with zero-shot inference\ncapabilities. However, most neural systems utilize decoder-only causal\n(unidirectional) transformer models, which are effective for English but may\nreduce the richness of languages with less strict word order, subject omission,\nor different relative clause attachment preferences. This is the first work\nthat analytically addresses optimal text generation order for non-causal\nlanguage models. We present a novel Viterbi algorithm-based methodology for\nmaximum likelihood word order estimation. We analyze the non-causal\nmost-likelihood order probability for NLG in Spanish and, then, the probability\nof generating the same phrases with Spanish causal NLG. This comparative\nanalysis reveals that causal NLG prefers English-like SVO structures. We also\nanalyze the relationship between optimal generation order and causal\nleft-to-right generation order using Spearman's rank correlation. Our results\ndemonstrate that the ideal order predicted by the maximum likelihood estimator\nis not closely related to the causal order and may be influenced by the\nsyntactic structure of the target sentence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Generation (NLG) popularity has increased owing to the\nprogress in Large Language Models (LLMs), with zero-shot inference\ncapabilities. However, most neural systems utilize decoder-only causal\n(unidirectional) transformer models, which are effective for English but may\nreduce the richness of languages with less strict word order, subject omission,\nor different relative clause attachment preferences. This is the first work\nthat analytically addresses optimal text generation order for non-causal\nlanguage models. We present a novel Viterbi algorithm-based methodology for\nmaximum likelihood word order estimation. We analyze the non-causal\nmost-likelihood order probability for NLG in Spanish and, then, the probability\nof generating the same phrases with Spanish causal NLG. This comparative\nanalysis reveals that causal NLG prefers English-like SVO structures. We also\nanalyze the relationship between optimal generation order and causal\nleft-to-right generation order using Spearman's rank correlation. Our results\ndemonstrate that the ideal order predicted by the maximum likelihood estimator\nis not closely related to the causal order and may be influenced by the\nsyntactic structure of the target sentence."
                },
                "authors": [
                    {
                        "name": "Andrea Busto-Castieira"
                    },
                    {
                        "name": "Silvia Garca-Mndez"
                    },
                    {
                        "name": "Francisco de Arriba-Prez"
                    },
                    {
                        "name": "Francisco J. Gonzlez-Castao"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Gonzlez-Castao"
                },
                "author": "Francisco J. Gonzlez-Castao",
                "arxiv_doi": "10.1016/j.patrec.2025.02.010",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.patrec.2025.02.010",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.14451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14450v1",
                "updated": "2025-02-20T11:05:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    11,
                    5,
                    10,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T11:05:10Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    11,
                    5,
                    10,
                    3,
                    51,
                    0
                ],
                "title": "LLM4FaaS: No-Code Application Development using LLMs and FaaS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4FaaS: No-Code Application Development using LLMs and FaaS"
                },
                "summary": "Large language models (LLMs) are powerful tools that can generate code from\nnatural language descriptions. While this theoretically enables non-technical\nusers to develop their own applications, they typically lack the expertise to\nexecute, deploy, and operate generated code. This poses a barrier for such\nusers to leverage the power of LLMs for application development.\n  In this paper, we propose leveraging the high levels of abstraction of the\nFunction-as-a-Service (FaaS) paradigm to handle code execution and operation\nfor non-technical users. FaaS offers function deployment without handling the\nunderlying infrastructure, enabling users to execute LLM-generated code without\nconcern for its operation and without requiring any technical expertise. We\npropose LLM4FaaS, a novel no-code application development approach that\ncombines LLMs and FaaS platforms to enable non-technical users to build and run\ntheir own applications using only natural language descriptions. Specifically,\nLLM4FaaS takes user prompts, uses LLMs to generate function code based on those\nprompts, and deploys these functions through a FaaS platform that handles the\napplication's operation. LLM4FaaS also leverages the FaaS infrastructure\nabstractions to reduce the task complexity for the LLM, improving result\naccuracy.\n  We evaluate LLM4FaaS with a proof-of-concept implementation based on GPT-4o\nand an open-source FaaS platform, using real prompts from non-technical users.\nOur evaluation based on these real user prompts demonstrates the feasibility of\nour approach and shows that LLM4FaaS can reliably build and deploy code in\n71.47% of cases, up from 43.48% in a baseline without FaaS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are powerful tools that can generate code from\nnatural language descriptions. While this theoretically enables non-technical\nusers to develop their own applications, they typically lack the expertise to\nexecute, deploy, and operate generated code. This poses a barrier for such\nusers to leverage the power of LLMs for application development.\n  In this paper, we propose leveraging the high levels of abstraction of the\nFunction-as-a-Service (FaaS) paradigm to handle code execution and operation\nfor non-technical users. FaaS offers function deployment without handling the\nunderlying infrastructure, enabling users to execute LLM-generated code without\nconcern for its operation and without requiring any technical expertise. We\npropose LLM4FaaS, a novel no-code application development approach that\ncombines LLMs and FaaS platforms to enable non-technical users to build and run\ntheir own applications using only natural language descriptions. Specifically,\nLLM4FaaS takes user prompts, uses LLMs to generate function code based on those\nprompts, and deploys these functions through a FaaS platform that handles the\napplication's operation. LLM4FaaS also leverages the FaaS infrastructure\nabstractions to reduce the task complexity for the LLM, improving result\naccuracy.\n  We evaluate LLM4FaaS with a proof-of-concept implementation based on GPT-4o\nand an open-source FaaS platform, using real prompts from non-technical users.\nOur evaluation based on these real user prompts demonstrates the feasibility of\nour approach and shows that LLM4FaaS can reliably build and deploy code in\n71.47% of cases, up from 43.48% in a baseline without FaaS."
                },
                "authors": [
                    {
                        "name": "Minghe Wang"
                    },
                    {
                        "name": "Tobias Pfandzelter"
                    },
                    {
                        "name": "Trever Schirmer"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14445v1",
                "updated": "2025-02-20T10:52:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    10,
                    52,
                    38,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T10:52:38Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    10,
                    52,
                    38,
                    3,
                    51,
                    0
                ],
                "title": "PredictaBoard: Benchmarking LLM Score Predictability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PredictaBoard: Benchmarking LLM Score Predictability"
                },
                "summary": "Despite possessing impressive skills, Large Language Models (LLMs) often fail\nunpredictably, demonstrating inconsistent success in even basic common sense\nreasoning tasks. This unpredictability poses a significant challenge to\nensuring their safe deployment, as identifying and operating within a reliable\n\"safe zone\" is essential for mitigating risks. To address this, we present\nPredictaBoard, a novel collaborative benchmarking framework designed to\nevaluate the ability of score predictors (referred to as assessors) to\nanticipate LLM errors on specific task instances (i.e., prompts) from existing\ndatasets. PredictaBoard evaluates pairs of LLMs and assessors by considering\nthe rejection rate at different tolerance errors. As such, PredictaBoard\nstimulates research into developing better assessors and making LLMs more\npredictable, not only with a higher average performance. We conduct\nillustrative experiments using baseline assessors and state-of-the-art LLMs.\nPredictaBoard highlights the critical need to evaluate predictability alongside\nperformance, paving the way for safer AI systems where errors are not only\nminimised but also anticipated and effectively mitigated. Code for our\nbenchmark can be found at\nhttps://github.com/Kinds-of-Intelligence-CFI/PredictaBoard",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite possessing impressive skills, Large Language Models (LLMs) often fail\nunpredictably, demonstrating inconsistent success in even basic common sense\nreasoning tasks. This unpredictability poses a significant challenge to\nensuring their safe deployment, as identifying and operating within a reliable\n\"safe zone\" is essential for mitigating risks. To address this, we present\nPredictaBoard, a novel collaborative benchmarking framework designed to\nevaluate the ability of score predictors (referred to as assessors) to\nanticipate LLM errors on specific task instances (i.e., prompts) from existing\ndatasets. PredictaBoard evaluates pairs of LLMs and assessors by considering\nthe rejection rate at different tolerance errors. As such, PredictaBoard\nstimulates research into developing better assessors and making LLMs more\npredictable, not only with a higher average performance. We conduct\nillustrative experiments using baseline assessors and state-of-the-art LLMs.\nPredictaBoard highlights the critical need to evaluate predictability alongside\nperformance, paving the way for safer AI systems where errors are not only\nminimised but also anticipated and effectively mitigated. Code for our\nbenchmark can be found at\nhttps://github.com/Kinds-of-Intelligence-CFI/PredictaBoard"
                },
                "authors": [
                    {
                        "name": "Lorenzo Pacchiardi"
                    },
                    {
                        "name": "Konstantinos Voudouris"
                    },
                    {
                        "name": "Ben Slater"
                    },
                    {
                        "name": "Fernando Martnez-Plumed"
                    },
                    {
                        "name": "Jos Hernndez-Orallo"
                    },
                    {
                        "name": "Lexin Zhou"
                    },
                    {
                        "name": "Wout Schellaert"
                    }
                ],
                "author_detail": {
                    "name": "Wout Schellaert"
                },
                "author": "Wout Schellaert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08638v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08638v3",
                "updated": "2025-02-20T10:51:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    10,
                    51,
                    46,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-12T18:54:37Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    54,
                    37,
                    2,
                    43,
                    0
                ],
                "title": "Examining Multilingual Embedding Models Cross-Lingually Through\n  LLM-Generated Adversarial Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining Multilingual Embedding Models Cross-Lingually Through\n  LLM-Generated Adversarial Examples"
                },
                "summary": "The evaluation of cross-lingual semantic search capabilities of models is\noften limited to existing datasets from tasks such as information retrieval and\nsemantic textual similarity. To allow for domain-specific evaluation, we\nintroduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual\nsemantic search task that does not require a large evaluation corpus, only\nparallel sentences of the language pair of interest within the target domain.\nThis task focuses on the ability of a model to cross-lingually rank the true\nparallel sentence higher than challenging distractors generated by a large\nlanguage model. We create a case study of our introduced CLSD task for the\nlanguage pair German-French in the news domain. Within this case study, we find\nthat models that are also fine-tuned for retrieval tasks benefit from pivoting\nthrough English, while bitext mining models perform best directly\ncross-lingually. A fine-grained similarity analysis enabled by our distractor\ngeneration strategy indicate that different embedding models are sensitive to\ndifferent types of perturbations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of cross-lingual semantic search capabilities of models is\noften limited to existing datasets from tasks such as information retrieval and\nsemantic textual similarity. To allow for domain-specific evaluation, we\nintroduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual\nsemantic search task that does not require a large evaluation corpus, only\nparallel sentences of the language pair of interest within the target domain.\nThis task focuses on the ability of a model to cross-lingually rank the true\nparallel sentence higher than challenging distractors generated by a large\nlanguage model. We create a case study of our introduced CLSD task for the\nlanguage pair German-French in the news domain. Within this case study, we find\nthat models that are also fine-tuned for retrieval tasks benefit from pivoting\nthrough English, while bitext mining models perform best directly\ncross-lingually. A fine-grained similarity analysis enabled by our distractor\ngeneration strategy indicate that different embedding models are sensitive to\ndifferent types of perturbations."
                },
                "authors": [
                    {
                        "name": "Andrianos Michail"
                    },
                    {
                        "name": "Simon Clematide"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08638v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08638v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12769v2",
                "updated": "2025-02-20T10:50:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    10,
                    50,
                    9,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-18T11:32:43Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    11,
                    32,
                    43,
                    1,
                    49,
                    0
                ],
                "title": "How Much Do LLMs Hallucinate across Languages? On Multilingual\n  Estimation of LLM Hallucination in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Do LLMs Hallucinate across Languages? On Multilingual\n  Estimation of LLM Hallucination in the Wild"
                },
                "summary": "In the age of misinformation, hallucination -- the tendency of Large Language\nModels (LLMs) to generate non-factual or unfaithful responses -- represents the\nmain risk for their global utility. Despite LLMs becoming increasingly\nmultilingual, the vast majority of research on detecting and quantifying LLM\nhallucination are (a) English-centric and (b) focus on machine translation (MT)\nand summarization, tasks that are less common ``in the wild'' than open\ninformation seeking. In contrast, we aim to quantify the extent of LLM\nhallucination across languages in knowledge-intensive long-form question\nanswering. To this end, we train a multilingual hallucination detection model\nand conduct a large-scale study across 30 languages and 6 open-source LLM\nfamilies. We start from an English hallucination detection dataset and rely on\nMT to generate (noisy) training data in other languages. We also manually\nannotate gold data for five high-resource languages; we then demonstrate, for\nthese languages, that the estimates of hallucination rates are similar between\nsilver (LLM-generated) and gold test sets, validating the use of silver data\nfor estimating hallucination rates for other languages. For the final rates\nestimation, we build a knowledge-intensive QA dataset for 30 languages with\nLLM-generated prompts and Wikipedia articles as references. We find that, while\nLLMs generate longer responses with more hallucinated tokens for\nhigher-resource languages, there is no correlation between length-normalized\nhallucination rates of languages and their digital representation. Further, we\nfind that smaller LLMs exhibit larger hallucination rates than larger models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the age of misinformation, hallucination -- the tendency of Large Language\nModels (LLMs) to generate non-factual or unfaithful responses -- represents the\nmain risk for their global utility. Despite LLMs becoming increasingly\nmultilingual, the vast majority of research on detecting and quantifying LLM\nhallucination are (a) English-centric and (b) focus on machine translation (MT)\nand summarization, tasks that are less common ``in the wild'' than open\ninformation seeking. In contrast, we aim to quantify the extent of LLM\nhallucination across languages in knowledge-intensive long-form question\nanswering. To this end, we train a multilingual hallucination detection model\nand conduct a large-scale study across 30 languages and 6 open-source LLM\nfamilies. We start from an English hallucination detection dataset and rely on\nMT to generate (noisy) training data in other languages. We also manually\nannotate gold data for five high-resource languages; we then demonstrate, for\nthese languages, that the estimates of hallucination rates are similar between\nsilver (LLM-generated) and gold test sets, validating the use of silver data\nfor estimating hallucination rates for other languages. For the final rates\nestimation, we build a knowledge-intensive QA dataset for 30 languages with\nLLM-generated prompts and Wikipedia articles as references. We find that, while\nLLMs generate longer responses with more hallucinated tokens for\nhigher-resource languages, there is no correlation between length-normalized\nhallucination rates of languages and their digital representation. Further, we\nfind that smaller LLMs exhibit larger hallucination rates than larger models."
                },
                "authors": [
                    {
                        "name": "Saad Obaid ul Islam"
                    },
                    {
                        "name": "Anne Lauscher"
                    },
                    {
                        "name": "Goran Glava"
                    }
                ],
                "author_detail": {
                    "name": "Goran Glava"
                },
                "author": "Goran Glava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14437v1",
                "updated": "2025-02-20T10:41:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    10,
                    41,
                    34,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T10:41:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    10,
                    41,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "Natural Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Generation"
                },
                "summary": "This book provides a broad overview of Natural Language Generation (NLG),\nincluding technology, user requirements, evaluation, and real-world\napplications. The focus is on concepts and insights which hopefully will remain\nrelevant for many years, not on the latest LLM innovations. It draws on decades\nof work by the author and others on NLG.\n  The book has the following chapters: Introduction to NLG; Rule-Based NLG;\nMachine Learning and Neural NLG; Requirements; Evaluation; Safety, Maintenance,\nand Testing; and Applications. All chapters include examples and anecdotes from\nthe author's personal experiences, and end with a Further Reading section.\n  The book should be especially useful to people working on applied NLG,\nincluding NLG researchers, people in other fields who want to use NLG, and\ncommercial developers. It will not however be useful to people who want to\nunderstand the latest LLM technology.\n  There is a companion site with more information at\nhttps://ehudreiter.com/book/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This book provides a broad overview of Natural Language Generation (NLG),\nincluding technology, user requirements, evaluation, and real-world\napplications. The focus is on concepts and insights which hopefully will remain\nrelevant for many years, not on the latest LLM innovations. It draws on decades\nof work by the author and others on NLG.\n  The book has the following chapters: Introduction to NLG; Rule-Based NLG;\nMachine Learning and Neural NLG; Requirements; Evaluation; Safety, Maintenance,\nand Testing; and Applications. All chapters include examples and anecdotes from\nthe author's personal experiences, and end with a Further Reading section.\n  The book should be especially useful to people working on applied NLG,\nincluding NLG researchers, people in other fields who want to use NLG, and\ncommercial developers. It will not however be useful to people who want to\nunderstand the latest LLM technology.\n  There is a companion site with more information at\nhttps://ehudreiter.com/book/"
                },
                "authors": [
                    {
                        "name": "Ehud Reiter"
                    }
                ],
                "author_detail": {
                    "name": "Ehud Reiter"
                },
                "author": "Ehud Reiter",
                "arxiv_doi": "10.1007/978-3-031-68582-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-68582-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.14437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is a preprint of the following work: Ehud Reiter, Natural\n  Language Generation, 2024, Springer reproduced with permission of Springer\n  Nature Switzerland AG. The final authenticated version is available online\n  at: http://dx.doi.org/10.1007/978-3-031-68582-8",
                "arxiv_journal_ref": "Book published by Springer in 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06490v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06490v4",
                "updated": "2025-02-20T10:35:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    10,
                    35,
                    34,
                    3,
                    51,
                    0
                ],
                "published": "2024-09-09T13:27:53Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    27,
                    53,
                    0,
                    253,
                    0
                ],
                "title": "UAVDB: Trajectory-Guided Adaptable Bounding Boxes for UAV Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAVDB: Trajectory-Guided Adaptable Bounding Boxes for UAV Detection"
                },
                "summary": "The widespread deployment of Unmanned Aerial Vehicles (UAVs) in surveillance,\nsecurity, and airspace management has created an urgent demand for precise,\nscalable, and efficient UAV detection. However, existing datasets often suffer\nfrom limited scale diversity and inaccurate annotations, hindering robust model\ndevelopment. This paper introduces UAVDB, a high-resolution UAV detection\ndataset constructed using Patch Intensity Convergence (PIC). This novel\ntechnique automatically generates high-fidelity bounding box annotations from\nUAV trajectory data~\\cite{li2020reconstruction}, eliminating the need for\nmanual labeling. UAVDB features single-class annotations with a fixed-camera\nsetup and consists of RGB frames capturing UAVs across various scales, from\nlarge-scale UAVs to near-single-pixel representations, along with challenging\nbackgrounds that pose difficulties for modern detectors. We first validate the\naccuracy and efficiency of PIC-generated bounding boxes by comparing\nIntersection over Union (IoU) performance and runtime against alternative\nannotation methods, demonstrating that PIC achieves higher annotation accuracy\nwhile being more efficient. Subsequently, we benchmark UAVDB using\nstate-of-the-art (SOTA) YOLO-series detectors, establishing UAVDB as a valuable\nresource for advancing long-range and high-resolution UAV detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of Unmanned Aerial Vehicles (UAVs) in surveillance,\nsecurity, and airspace management has created an urgent demand for precise,\nscalable, and efficient UAV detection. However, existing datasets often suffer\nfrom limited scale diversity and inaccurate annotations, hindering robust model\ndevelopment. This paper introduces UAVDB, a high-resolution UAV detection\ndataset constructed using Patch Intensity Convergence (PIC). This novel\ntechnique automatically generates high-fidelity bounding box annotations from\nUAV trajectory data~\\cite{li2020reconstruction}, eliminating the need for\nmanual labeling. UAVDB features single-class annotations with a fixed-camera\nsetup and consists of RGB frames capturing UAVs across various scales, from\nlarge-scale UAVs to near-single-pixel representations, along with challenging\nbackgrounds that pose difficulties for modern detectors. We first validate the\naccuracy and efficiency of PIC-generated bounding boxes by comparing\nIntersection over Union (IoU) performance and runtime against alternative\nannotation methods, demonstrating that PIC achieves higher annotation accuracy\nwhile being more efficient. Subsequently, we benchmark UAVDB using\nstate-of-the-art (SOTA) YOLO-series detectors, establishing UAVDB as a valuable\nresource for advancing long-range and high-resolution UAV detection."
                },
                "authors": [
                    {
                        "name": "Yu-Hsi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Hsi Chen"
                },
                "author": "Yu-Hsi Chen",
                "arxiv_comment": "9 pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06490v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06490v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]