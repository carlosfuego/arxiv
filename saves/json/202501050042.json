[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha Rsler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v1",
                "updated": "2024-12-29T15:42:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v1",
                "updated": "2024-12-28T14:38:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v1",
                "updated": "2024-12-27T20:47:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v1",
                "updated": "2024-12-26T15:45:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    }
                ],
                "author_detail": {
                    "name": "Heung-Yeung Shum"
                },
                "author": "Heung-Yeung Shum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jimnez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v2",
                "updated": "2024-12-18T05:08:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    8,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v3",
                "updated": "2024-12-17T05:40:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    40,
                    9,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12543v1",
                "updated": "2024-12-17T05:09:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T05:09:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks"
                },
                "summary": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Hai Liu"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages, 8 figures, WiOpt 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12488v1",
                "updated": "2024-12-17T02:44:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:44:43Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "title": "A System for Microserving of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Microserving of LLMs"
                },
                "summary": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
                },
                "authors": [
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Todd C. Mowry"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v1",
                "updated": "2024-12-17T01:12:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v1",
                "updated": "2024-12-15T21:02:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15246v1",
                "updated": "2024-12-14T06:47:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T06:47:56Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "title": "Accelerating Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Retrieval-Augmented Generation"
                },
                "summary": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded."
                },
                "authors": [
                    {
                        "name": "Derrick Quinn"
                    },
                    {
                        "name": "Mohammad Nouri"
                    },
                    {
                        "name": "Neel Patel"
                    },
                    {
                        "name": "John Salihu"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Sukhan Lee"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Mohammad Alian"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alian"
                },
                "author": "Mohammad Alian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v2",
                "updated": "2024-12-13T16:13:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Kstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v1",
                "updated": "2024-12-13T14:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v2",
                "updated": "2024-12-13T14:08:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    8,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thieen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper in ISAAC 2024; minor changes",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 18 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v1",
                "updated": "2024-12-13T02:26:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_comment": "Conference submission for IPCCC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09474v1",
                "updated": "2024-12-12T17:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance"
                },
                "summary": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments."
                },
                "authors": [
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Sifat Nawrin Nova"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Md Rahat Ul Nasib"
                    }
                ],
                "author_detail": {
                    "name": "Md Rahat Ul Nasib"
                },
                "author": "Md Rahat Ul Nasib",
                "arxiv_comment": "6 Pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v3",
                "updated": "2024-12-12T15:39:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    39,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v2",
                "updated": "2024-12-12T14:43:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtrik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtrik"
                },
                "author": "Peter Richtrik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v3",
                "updated": "2024-12-12T12:03:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    3,
                    19,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v2",
                "updated": "2024-12-12T10:07:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    7,
                    17,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v1",
                "updated": "2024-12-12T08:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09036v1",
                "updated": "2024-12-12T07:52:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T07:52:56Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty"
                },
                "summary": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v3",
                "updated": "2024-12-12T03:21:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    21,
                    13,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_doi": "10.1145/3669940.3707265",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3669940.3707265",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08890v1",
                "updated": "2024-12-12T03:00:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T03:00:29Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries"
                },
                "summary": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v1",
                "updated": "2024-12-11T16:35:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v3",
                "updated": "2024-12-11T12:03:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    3,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Pushing the Limits of In-Network Caching for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of In-Network Caching for Key-Value Stores"
                },
                "summary": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "arxiv_comment": "To be appeared in USENIX NSDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08176v1",
                "updated": "2024-12-11T08:07:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:07:12Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "title": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning"
                },
                "summary": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner"
                },
                "authors": [
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Zhaohong Huang"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08063v1",
                "updated": "2024-12-11T03:15:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:15:49Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "title": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates."
                },
                "authors": [
                    {
                        "name": "Zhanming Guan"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Jierui Liu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Dexin Liu"
                    },
                    {
                        "name": "Ningyuan Sun"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Wenchao Li"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Hang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhu"
                },
                "author": "Hang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12952v2",
                "updated": "2024-12-10T22:53:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    22,
                    53,
                    16,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-19T17:54:34Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    54,
                    34,
                    1,
                    79,
                    0
                ],
                "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models"
                },
                "summary": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements."
                },
                "authors": [
                    {
                        "name": "Elaine Sui"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v1",
                "updated": "2024-12-10T18:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Causal Video Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Causal Video Generators"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v1",
                "updated": "2024-12-10T18:50:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v1",
                "updated": "2024-12-10T18:13:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v4",
                "updated": "2024-12-10T12:45:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    45,
                    31,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v3",
                "updated": "2024-12-09T01:44:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    44,
                    10,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01844v3",
                "updated": "2024-12-09T01:39:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    15,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-03T04:27:32Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    4,
                    27,
                    32,
                    4,
                    124,
                    0
                ],
                "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges"
                },
                "summary": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Shazia Riaz"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "arxiv_doi": "10.1145/3706630",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706630",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.19260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19260v2",
                "updated": "2025-01-02T18:46:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    46,
                    5,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-26T15:54:10Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    54,
                    10,
                    3,
                    361,
                    0
                ],
                "title": "MEDEC: A Benchmark for Medical Error Detection and Correction in\n  Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDEC: A Benchmark for Medical Error Detection and Correction in\n  Clinical Notes"
                },
                "summary": "Several studies showed that Large Language Models (LLMs) can answer medical\nquestions correctly, even outperforming the average human score in some medical\nexams. However, to our knowledge, no study has been conducted to assess the\nability of language models to validate existing or generated medical text for\ncorrectness and consistency. In this paper, we introduce MEDEC\n(https://github.com/abachaa/MEDEC), the first publicly available benchmark for\nmedical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal\nOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes\nfrom three US hospital systems that were not previously seen by any LLM. The\ndataset has been used for the MEDIQA-CORR shared task to evaluate seventeen\nparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe the\ndata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,\nClaude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and\ncorrecting medical errors requiring both medical knowledge and reasoning\ncapabilities. We also conducted a comparative study where two medical doctors\nperformed the same task on the MEDEC test set. The results showed that MEDEC is\na sufficiently challenging benchmark to assess the ability of models to\nvalidate existing or generated notes and to correct medical errors. We also\nfound that although recent LLMs have a good performance in error detection and\ncorrection, they are still outperformed by medical doctors in these tasks. We\ndiscuss the potential factors behind this gap, the insights from our\nexperiments, the limitations of current evaluation metrics, and share potential\npointers for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several studies showed that Large Language Models (LLMs) can answer medical\nquestions correctly, even outperforming the average human score in some medical\nexams. However, to our knowledge, no study has been conducted to assess the\nability of language models to validate existing or generated medical text for\ncorrectness and consistency. In this paper, we introduce MEDEC\n(https://github.com/abachaa/MEDEC), the first publicly available benchmark for\nmedical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal\nOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes\nfrom three US hospital systems that were not previously seen by any LLM. The\ndataset has been used for the MEDIQA-CORR shared task to evaluate seventeen\nparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe the\ndata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,\nClaude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and\ncorrecting medical errors requiring both medical knowledge and reasoning\ncapabilities. We also conducted a comparative study where two medical doctors\nperformed the same task on the MEDEC test set. The results showed that MEDEC is\na sufficiently challenging benchmark to assess the ability of models to\nvalidate existing or generated notes and to correct medical errors. We also\nfound that although recent LLMs have a good performance in error detection and\ncorrection, they are still outperformed by medical doctors in these tasks. We\ndiscuss the potential factors behind this gap, the insights from our\nexperiments, the limitations of current evaluation metrics, and share potential\npointers for future research."
                },
                "authors": [
                    {
                        "name": "Asma Ben Abacha"
                    },
                    {
                        "name": "Wen-wai Yim"
                    },
                    {
                        "name": "Yujuan Fu"
                    },
                    {
                        "name": "Zhaoyi Sun"
                    },
                    {
                        "name": "Meliha Yetisgen"
                    },
                    {
                        "name": "Fei Xia"
                    },
                    {
                        "name": "Thomas Lin"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Lin"
                },
                "author": "Thomas Lin",
                "arxiv_comment": "This version has been updated with further clarification regarding\n  the model size estimates that were mined from public articles only and\n  provided to aid in contextualizing model performance. The authors cannot\n  vouch for the accuracy of those estimates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.20280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.20280v2",
                "updated": "2025-01-02T18:31:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    31,
                    25,
                    3,
                    2,
                    0
                ],
                "published": "2024-03-29T16:49:40Z",
                "published_parsed": [
                    2024,
                    3,
                    29,
                    16,
                    49,
                    40,
                    4,
                    89,
                    0
                ],
                "title": "Sparsely Multimodal Data Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparsely Multimodal Data Fusion"
                },
                "summary": "Multimodal data fusion is essential for applications requiring the\nintegration of diverse data sources, especially in the presence of incomplete\nor sparsely available modalities. This paper presents a comparative study of\nthree multimodal embedding techniques, Modal Channel Attention (MCA), Zorro,\nand Everything at Once (EAO), to evaluate their performance on sparsely\nmultimodal data. MCA introduces fusion embeddings for all combinations of input\nmodalities and uses attention masking to create distinct attention channels,\nenabling flexible and efficient data fusion. Experiments on two datasets with\nfour modalities each, CMU-MOSEI and TCGA, demonstrate that MCA outperforms\nZorro across ranking, recall, regression, and classification tasks and\noutperforms EAO across regression and classification tasks. MCA achieves\nsuperior performance by maintaining robust uniformity across unimodal and\nfusion embeddings. While EAO performs best in ranking metrics due to its\napproach of forming fusion embeddings post-inference, it underperforms in\ndownstream tasks requiring multimodal interactions. These results highlight the\nimportance of contrasting all modality combinations in constructing embedding\nspaces and offers insights into the design of multimodal architectures for\nreal-world applications with incomplete data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal data fusion is essential for applications requiring the\nintegration of diverse data sources, especially in the presence of incomplete\nor sparsely available modalities. This paper presents a comparative study of\nthree multimodal embedding techniques, Modal Channel Attention (MCA), Zorro,\nand Everything at Once (EAO), to evaluate their performance on sparsely\nmultimodal data. MCA introduces fusion embeddings for all combinations of input\nmodalities and uses attention masking to create distinct attention channels,\nenabling flexible and efficient data fusion. Experiments on two datasets with\nfour modalities each, CMU-MOSEI and TCGA, demonstrate that MCA outperforms\nZorro across ranking, recall, regression, and classification tasks and\noutperforms EAO across regression and classification tasks. MCA achieves\nsuperior performance by maintaining robust uniformity across unimodal and\nfusion embeddings. While EAO performs best in ranking metrics due to its\napproach of forming fusion embeddings post-inference, it underperforms in\ndownstream tasks requiring multimodal interactions. These results highlight the\nimportance of contrasting all modality combinations in constructing embedding\nspaces and offers insights into the design of multimodal architectures for\nreal-world applications with incomplete data."
                },
                "authors": [
                    {
                        "name": "Josiah Bjorgaard"
                    }
                ],
                "author_detail": {
                    "name": "Josiah Bjorgaard"
                },
                "author": "Josiah Bjorgaard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.20280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.20280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10848v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10848v3",
                "updated": "2025-01-02T17:17:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    17,
                    17,
                    28,
                    3,
                    2,
                    0
                ],
                "published": "2024-08-20T13:40:25Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    40,
                    25,
                    1,
                    233,
                    0
                ],
                "title": "Perception-guided Jailbreak against Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception-guided Jailbreak against Text-to-Image Models"
                },
                "summary": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ."
                },
                "authors": [
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Run Wang"
                    },
                    {
                        "name": "Weikai Miao"
                    },
                    {
                        "name": "Geguang Pu"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "9 pages, accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10848v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10848v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02064v2",
                "updated": "2025-01-02T15:34:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    34,
                    23,
                    3,
                    2,
                    0
                ],
                "published": "2024-11-04T13:06:46Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    6,
                    46,
                    0,
                    309,
                    0
                ],
                "title": "Amortized Bayesian Experimental Design for Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortized Bayesian Experimental Design for Decision-Making"
                },
                "summary": "Many critical decisions, such as personalized medical diagnoses and product\npricing, are made based on insights gained from designing, observing, and\nanalyzing a series of experiments. This highlights the crucial role of\nexperimental design, which goes beyond merely collecting information on system\nparameters as in traditional Bayesian experimental design (BED), but also plays\na key part in facilitating downstream decision-making. Most recent BED methods\nuse an amortized policy network to rapidly design experiments. However, the\ninformation gathered through these methods is suboptimal for down-the-line\ndecision-making, as the experiments are not inherently designed with downstream\nobjectives in mind. In this paper, we present an amortized decision-aware BED\nframework that prioritizes maximizing downstream decision utility. We introduce\na novel architecture, the Transformer Neural Decision Process (TNDP), capable\nof instantly proposing the next experimental design, whilst inferring the\ndownstream decision, thus effectively amortizing both tasks within a unified\nworkflow. We demonstrate the performance of our method across several tasks,\nshowing that it can deliver informative designs and facilitate accurate\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many critical decisions, such as personalized medical diagnoses and product\npricing, are made based on insights gained from designing, observing, and\nanalyzing a series of experiments. This highlights the crucial role of\nexperimental design, which goes beyond merely collecting information on system\nparameters as in traditional Bayesian experimental design (BED), but also plays\na key part in facilitating downstream decision-making. Most recent BED methods\nuse an amortized policy network to rapidly design experiments. However, the\ninformation gathered through these methods is suboptimal for down-the-line\ndecision-making, as the experiments are not inherently designed with downstream\nobjectives in mind. In this paper, we present an amortized decision-aware BED\nframework that prioritizes maximizing downstream decision utility. We introduce\na novel architecture, the Transformer Neural Decision Process (TNDP), capable\nof instantly proposing the next experimental design, whilst inferring the\ndownstream decision, thus effectively amortizing both tasks within a unified\nworkflow. We demonstrate the performance of our method across several tasks,\nshowing that it can deliver informative designs and facilitate accurate\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Daolang Huang"
                    },
                    {
                        "name": "Yujia Guo"
                    },
                    {
                        "name": "Luigi Acerbi"
                    },
                    {
                        "name": "Samuel Kaski"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Kaski"
                },
                "author": "Samuel Kaski",
                "arxiv_comment": "20 pages, 6 figures. Accepted at the 38th Conference on Neural\n  Information Processing Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21079v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21079v2",
                "updated": "2025-01-02T15:00:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    0,
                    16,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-30T16:56:44Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    56,
                    44,
                    0,
                    365,
                    0
                ],
                "title": "Edicho: Consistent Image Editing in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edicho: Consistent Image Editing in the Wild"
                },
                "summary": "As a verified need, consistent editing across in-the-wild images remains a\ntechnical challenge arising from various unmanageable factors, like object\nposes, lighting conditions, and photography environments. Edicho steps in with\na training-free solution based on diffusion models, featuring a fundamental\ndesign principle of using explicit image correspondence to direct editing.\nSpecifically, the key components include an attention manipulation module and a\ncarefully refined classifier-free guidance (CFG) denoising strategy, both of\nwhich take into account the pre-estimated correspondence. Such an\ninference-time algorithm enjoys a plug-and-play nature and is compatible to\nmost diffusion-based editing methods, such as ControlNet and BrushNet.\nExtensive results demonstrate the efficacy of Edicho in consistent cross-image\nediting under diverse settings. We will release the code to facilitate future\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a verified need, consistent editing across in-the-wild images remains a\ntechnical challenge arising from various unmanageable factors, like object\nposes, lighting conditions, and photography environments. Edicho steps in with\na training-free solution based on diffusion models, featuring a fundamental\ndesign principle of using explicit image correspondence to direct editing.\nSpecifically, the key components include an attention manipulation module and a\ncarefully refined classifier-free guidance (CFG) denoising strategy, both of\nwhich take into account the pre-estimated correspondence. Such an\ninference-time algorithm enjoys a plug-and-play nature and is compatible to\nmost diffusion-based editing methods, such as ControlNet and BrushNet.\nExtensive results demonstrate the efficacy of Edicho in consistent cross-image\nediting under diverse settings. We will release the code to facilitate future\nstudies."
                },
                "authors": [
                    {
                        "name": "Qingyan Bai"
                    },
                    {
                        "name": "Hao Ouyang"
                    },
                    {
                        "name": "Yinghao Xu"
                    },
                    {
                        "name": "Qiuyu Wang"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Ka Leong Cheng"
                    },
                    {
                        "name": "Yujun Shen"
                    },
                    {
                        "name": "Qifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qifeng Chen"
                },
                "author": "Qifeng Chen",
                "arxiv_comment": "Project page: https://github.com/EzioBy/edicho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21079v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21079v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01887v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01887v3",
                "updated": "2025-01-02T13:49:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    49,
                    59,
                    3,
                    2,
                    0
                ],
                "published": "2024-07-02T02:18:14Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    2,
                    18,
                    14,
                    1,
                    184,
                    0
                ],
                "title": "Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"
                },
                "summary": "In-context reinforcement learning (ICRL) is a frontier paradigm for solving\nreinforcement learning problems in the foundation model era. While ICRL\ncapabilities have been demonstrated in transformers through task-specific\ntraining, the potential of Large Language Models (LLMs) out-of-the-box remains\nlargely unexplored. Recent findings highlight that LLMs often face challenges\nwhen dealing with numerical contexts, and limited attention has been paid to\nevaluating their performance through preference feedback generated by the\nenvironment. This paper is the first to investigate LLMs as in-context\ndecision-makers under the problem of Dueling Bandits (DB), a stateless\npreference-based reinforcement learning setting that extends the classic\nMulti-Armed Bandit (MAB) model by querying for preference feedback. We compare\nGPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-Preview against nine\nwell-established DB algorithms. Our results reveal that our top-performing LLM,\nGPT-4 Turbo, has the zero-shot relative decision-making ability to achieve\nsurprisingly low weak regret across all the DB environment instances by quickly\nincluding the best arm in duels. However, an optimality gap exists between LLMs\nand classic DB algorithms in terms of strong regret. LLMs struggle to converge\nand consistently exploit even when explicitly prompted to do so, and are\nsensitive to prompt variations. To bridge this gap, we propose an agentic flow\nframework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates\noff-the-shelf DB algorithms with LLM agents through fine-grained adaptive\ninterplay. We show that LEAD has theoretical guarantees inherited from classic\nDB algorithms on both weak and strong regret. We validate its efficacy and\nrobustness even with noisy and adversarial prompts. The design of our framework\nsheds light on how to enhance the trustworthiness of LLMs used for in-context\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context reinforcement learning (ICRL) is a frontier paradigm for solving\nreinforcement learning problems in the foundation model era. While ICRL\ncapabilities have been demonstrated in transformers through task-specific\ntraining, the potential of Large Language Models (LLMs) out-of-the-box remains\nlargely unexplored. Recent findings highlight that LLMs often face challenges\nwhen dealing with numerical contexts, and limited attention has been paid to\nevaluating their performance through preference feedback generated by the\nenvironment. This paper is the first to investigate LLMs as in-context\ndecision-makers under the problem of Dueling Bandits (DB), a stateless\npreference-based reinforcement learning setting that extends the classic\nMulti-Armed Bandit (MAB) model by querying for preference feedback. We compare\nGPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-Preview against nine\nwell-established DB algorithms. Our results reveal that our top-performing LLM,\nGPT-4 Turbo, has the zero-shot relative decision-making ability to achieve\nsurprisingly low weak regret across all the DB environment instances by quickly\nincluding the best arm in duels. However, an optimality gap exists between LLMs\nand classic DB algorithms in terms of strong regret. LLMs struggle to converge\nand consistently exploit even when explicitly prompted to do so, and are\nsensitive to prompt variations. To bridge this gap, we propose an agentic flow\nframework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates\noff-the-shelf DB algorithms with LLM agents through fine-grained adaptive\ninterplay. We show that LEAD has theoretical guarantees inherited from classic\nDB algorithms on both weak and strong regret. We validate its efficacy and\nrobustness even with noisy and adversarial prompts. The design of our framework\nsheds light on how to enhance the trustworthiness of LLMs used for in-context\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Fanzeng Xia"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Tongxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Tongxin Li"
                },
                "author": "Tongxin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01887v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01887v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07640v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07640v3",
                "updated": "2025-01-02T13:46:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    46,
                    53,
                    3,
                    2,
                    0
                ],
                "published": "2024-05-13T11:00:25Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    11,
                    0,
                    25,
                    0,
                    134,
                    0
                ],
                "title": "Hyperparameter Importance Analysis for Multi-Objective AutoML",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperparameter Importance Analysis for Multi-Objective AutoML"
                },
                "summary": "Hyperparameter optimization plays a pivotal role in enhancing the predictive\nperformance and generalization capabilities of ML models. However, in many\napplications, we do not only care about predictive performance but also about\nadditional objectives such as inference time, memory, or energy consumption. In\nsuch multi-objective scenarios, determining the importance of hyperparameters\nposes a significant challenge due to the complex interplay between the\nconflicting objectives. In this paper, we propose the first method for\nassessing the importance of hyperparameters in multi-objective hyperparameter\noptimization. Our approach leverages surrogate-based hyperparameter importance\nmeasures, i.e., fANOVA and ablation paths, to provide insights into the impact\nof hyperparameters on the optimization objectives. Specifically, we compute the\na-priori scalarization of the objectives and determine the importance of the\nhyperparameters for different objective tradeoffs. Through extensive empirical\nevaluations on diverse benchmark datasets with three different objective pairs,\neach combined with accuracy, namely time, demographic parity loss, and energy\nconsumption, we demonstrate the effectiveness and robustness of our proposed\nmethod. Our findings not only offer valuable guidance for hyperparameter tuning\nin multi-objective optimization tasks but also contribute to advancing the\nunderstanding of hyperparameter importance in complex optimization scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperparameter optimization plays a pivotal role in enhancing the predictive\nperformance and generalization capabilities of ML models. However, in many\napplications, we do not only care about predictive performance but also about\nadditional objectives such as inference time, memory, or energy consumption. In\nsuch multi-objective scenarios, determining the importance of hyperparameters\nposes a significant challenge due to the complex interplay between the\nconflicting objectives. In this paper, we propose the first method for\nassessing the importance of hyperparameters in multi-objective hyperparameter\noptimization. Our approach leverages surrogate-based hyperparameter importance\nmeasures, i.e., fANOVA and ablation paths, to provide insights into the impact\nof hyperparameters on the optimization objectives. Specifically, we compute the\na-priori scalarization of the objectives and determine the importance of the\nhyperparameters for different objective tradeoffs. Through extensive empirical\nevaluations on diverse benchmark datasets with three different objective pairs,\neach combined with accuracy, namely time, demographic parity loss, and energy\nconsumption, we demonstrate the effectiveness and robustness of our proposed\nmethod. Our findings not only offer valuable guidance for hyperparameter tuning\nin multi-objective optimization tasks but also contribute to advancing the\nunderstanding of hyperparameter importance in complex optimization scenarios."
                },
                "authors": [
                    {
                        "name": "Daphne Theodorakopoulos"
                    },
                    {
                        "name": "Frederic Stahl"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_doi": "10.3233/FAIA240602",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3233/FAIA240602",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07640v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07640v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Presented at the 27th European Conference on Artificial Intelligence,\n  19-24 October 2024, Santiago de Compostela, Spain",
                "arxiv_journal_ref": "Frontiers in Artificial Intelligence and Applications: Proceedings\n  of the European Conference on AI (ECAI) 2024, vol. 392, 1100-1107, IOS Press",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.17440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.17440v2",
                "updated": "2025-01-02T13:32:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    32,
                    4,
                    3,
                    2,
                    0
                ],
                "published": "2023-10-26T14:50:07Z",
                "published_parsed": [
                    2023,
                    10,
                    26,
                    14,
                    50,
                    7,
                    3,
                    299,
                    0
                ],
                "title": "Gibbs optimal design of experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gibbs optimal design of experiments"
                },
                "summary": "Bayesian optimal design is a well-established approach to planning\nexperiments. A distribution for the responses, i.e. a statistical model, is\nassumed which is dependent on unknown parameters. A utility function is then\nspecified giving gain in information in estimating the true values of the\nparameters, using the Bayesian posterior distribution. A Bayesian optimal\ndesign is given by maximising expectation of the utility with respect to the\ndistribution implied by statistical model and prior distribution for the true\nparameter values. The approach accounts for the experimental aim, via\nspecification of the utility, and of assumed sources of uncertainty. However,\nit is predicated on the statistical model being correct. Recently, a new type\nof statistical inference, known as Gibbs inference, has been proposed. This is\nBayesian-like, i.e. uncertainty for unknown quantities is represented by a\nposterior distribution, but does not necessarily require specification of a\nstatistical model. The resulting inference is less sensitive to\nmisspecification of the statistical model. This paper introduces Gibbs optimal\ndesign: a framework for optimal design of experiments under Gibbs inference. A\ncomputational approach to find designs in practice is outlined and the\nframework is demonstrated on exemplars including linear models, and experiments\nwith count and time-to-event responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian optimal design is a well-established approach to planning\nexperiments. A distribution for the responses, i.e. a statistical model, is\nassumed which is dependent on unknown parameters. A utility function is then\nspecified giving gain in information in estimating the true values of the\nparameters, using the Bayesian posterior distribution. A Bayesian optimal\ndesign is given by maximising expectation of the utility with respect to the\ndistribution implied by statistical model and prior distribution for the true\nparameter values. The approach accounts for the experimental aim, via\nspecification of the utility, and of assumed sources of uncertainty. However,\nit is predicated on the statistical model being correct. Recently, a new type\nof statistical inference, known as Gibbs inference, has been proposed. This is\nBayesian-like, i.e. uncertainty for unknown quantities is represented by a\nposterior distribution, but does not necessarily require specification of a\nstatistical model. The resulting inference is less sensitive to\nmisspecification of the statistical model. This paper introduces Gibbs optimal\ndesign: a framework for optimal design of experiments under Gibbs inference. A\ncomputational approach to find designs in practice is outlined and the\nframework is demonstrated on exemplars including linear models, and experiments\nwith count and time-to-event responses."
                },
                "authors": [
                    {
                        "name": "Antony M. Overstall"
                    },
                    {
                        "name": "Jacinta Holloway-Brown"
                    },
                    {
                        "name": "James M. McGree"
                    }
                ],
                "author_detail": {
                    "name": "James M. McGree"
                },
                "author": "James M. McGree",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.17440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.17440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11006v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11006v4",
                "updated": "2025-01-02T13:11:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    11,
                    53,
                    3,
                    2,
                    0
                ],
                "published": "2024-08-20T17:00:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    0,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Security Attacks on LLM-based Code Completion Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Attacks on LLM-based Code Completion Tools"
                },
                "summary": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs."
                },
                "authors": [
                    {
                        "name": "Wen Cheng"
                    },
                    {
                        "name": "Ke Sun"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "Paper accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11006v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11006v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05798v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05798v4",
                "updated": "2025-01-02T12:00:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    12,
                    0,
                    28,
                    3,
                    2,
                    0
                ],
                "published": "2024-09-09T17:02:47Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    2,
                    47,
                    0,
                    253,
                    0
                ],
                "title": "Enhancing Preference-based Linear Bandits via Human Response Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Preference-based Linear Bandits via Human Response Time"
                },
                "summary": "Interactive preference learning systems infer human preferences by presenting\nqueries as pairs of options and collecting binary choices. Although binary\nchoices are simple and widely used, they provide limited information about\npreference strength. To address this, we leverage human response times, which\nare inversely related to preference strength, as an additional signal. We\npropose a computationally efficient method that combines choices and response\ntimes to estimate human utility functions, grounded in the EZ diffusion model\nfrom psychology. Theoretical and empirical analyses show that for queries with\nstrong preferences, response times complement choices by providing extra\ninformation about preference strength, leading to significantly improved\nutility estimation. We incorporate this estimator into preference-based linear\nbandits for fixed-budget best-arm identification. Simulations on three\nreal-world datasets demonstrate that using response times significantly\naccelerates preference learning compared to choice-only approaches. Additional\nmaterials, such as code, slides, and talk video, are available at\nhttps://shenlirobot.github.io/pages/NeurIPS24.html",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive preference learning systems infer human preferences by presenting\nqueries as pairs of options and collecting binary choices. Although binary\nchoices are simple and widely used, they provide limited information about\npreference strength. To address this, we leverage human response times, which\nare inversely related to preference strength, as an additional signal. We\npropose a computationally efficient method that combines choices and response\ntimes to estimate human utility functions, grounded in the EZ diffusion model\nfrom psychology. Theoretical and empirical analyses show that for queries with\nstrong preferences, response times complement choices by providing extra\ninformation about preference strength, leading to significantly improved\nutility estimation. We incorporate this estimator into preference-based linear\nbandits for fixed-budget best-arm identification. Simulations on three\nreal-world datasets demonstrate that using response times significantly\naccelerates preference learning compared to choice-only approaches. Additional\nmaterials, such as code, slides, and talk video, are available at\nhttps://shenlirobot.github.io/pages/NeurIPS24.html"
                },
                "authors": [
                    {
                        "name": "Shen Li"
                    },
                    {
                        "name": "Yuyang Zhang"
                    },
                    {
                        "name": "Zhaolin Ren"
                    },
                    {
                        "name": "Claire Liang"
                    },
                    {
                        "name": "Na Li"
                    },
                    {
                        "name": "Julie A. Shah"
                    }
                ],
                "author_detail": {
                    "name": "Julie A. Shah"
                },
                "author": "Julie A. Shah",
                "arxiv_comment": "NeurIPS 2024 (Oral) camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05798v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05798v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15270v2",
                "updated": "2025-01-02T11:21:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    21,
                    38,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-17T08:05:32Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    5,
                    32,
                    1,
                    352,
                    0
                ],
                "title": "Baichuan4-Finance Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Baichuan4-Finance Technical Report"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding, generation, and reasoning, yet their potential in\nfinance remains underexplored due to the complexity and specialization of\nfinancial knowledge. In this work, we report the development of the\nBaichuan4-Finance series, including a comprehensive suite of foundational\nBaichuan4-Finance-Base and an aligned language model Baichuan4-Finance, which\nare built upon Baichuan4-Turbo base model and tailored for finance domain.\nFirstly, we have dedicated significant effort to building a detailed pipeline\nfor improving data quality. Moreover, in the continual pre-training phase, we\npropose a novel domain self-constraint training strategy, which enables\nBaichuan4-Finance-Base to acquire financial knowledge without losing general\ncapabilities. After Supervised Fine-tuning and Reinforcement Learning from\nHuman Feedback and AI Feedback, the chat model Baichuan4-Finance is able to\ntackle various financial certification questions and real-world scenario\napplications. We evaluate Baichuan4-Finance on many widely used general\ndatasets and two holistic financial benchmarks. The evaluation results show\nthat Baichuan4-Finance-Base surpasses almost all competitive baselines on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. At the same time, Baichuan4-Finance demonstrates even\nmore impressive performance on financial application scenarios, showcasing its\npotential to foster community innovation in the financial LLM field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding, generation, and reasoning, yet their potential in\nfinance remains underexplored due to the complexity and specialization of\nfinancial knowledge. In this work, we report the development of the\nBaichuan4-Finance series, including a comprehensive suite of foundational\nBaichuan4-Finance-Base and an aligned language model Baichuan4-Finance, which\nare built upon Baichuan4-Turbo base model and tailored for finance domain.\nFirstly, we have dedicated significant effort to building a detailed pipeline\nfor improving data quality. Moreover, in the continual pre-training phase, we\npropose a novel domain self-constraint training strategy, which enables\nBaichuan4-Finance-Base to acquire financial knowledge without losing general\ncapabilities. After Supervised Fine-tuning and Reinforcement Learning from\nHuman Feedback and AI Feedback, the chat model Baichuan4-Finance is able to\ntackle various financial certification questions and real-world scenario\napplications. We evaluate Baichuan4-Finance on many widely used general\ndatasets and two holistic financial benchmarks. The evaluation results show\nthat Baichuan4-Finance-Base surpasses almost all competitive baselines on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. At the same time, Baichuan4-Finance demonstrates even\nmore impressive performance on financial application scenarios, showcasing its\npotential to foster community innovation in the financial LLM field."
                },
                "authors": [
                    {
                        "name": "Hanyu Zhang"
                    },
                    {
                        "name": "Boyu Qiu"
                    },
                    {
                        "name": "Yuhao Feng"
                    },
                    {
                        "name": "Shuqi Li"
                    },
                    {
                        "name": "Qian Ma"
                    },
                    {
                        "name": "Xiyuan Zhang"
                    },
                    {
                        "name": "Qiang Ju"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Jian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Jian Xie"
                },
                "author": "Jian Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21349v3",
                "updated": "2025-01-02T11:16:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    16,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-28T12:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    18,
                    22,
                    0,
                    302,
                    0
                ],
                "title": "FALCON: Feedback-driven Adaptive Long/short-term memory reinforced\n  Coding Optimization system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FALCON: Feedback-driven Adaptive Long/short-term memory reinforced\n  Coding Optimization system"
                },
                "summary": "Recently, large language models (LLMs) have achieved significant progress in\nautomated code generation. Despite their strong instruction-following\ncapabilities, these models frequently struggled to align with user intent in\ncoding scenarios. In particular, they were hampered by datasets that lacked\ndiversity and failed to address specialized tasks or edge cases. Furthermore,\nchallenges in supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) led to failures in generating precise,\nhuman-intent-aligned code. To tackle these challenges and improve the code\ngeneration performance for automated programming systems, we propose\nFeedback-driven Adaptive Long/short-term memory reinforced Coding Optimization\n(i.e., FALCON). FALCON is structured into two hierarchical levels. From the\nglobal level, long-term memory improves code quality by retaining and applying\nlearned knowledge. At the local level, short-term memory allows for the\nincorporation of immediate feedback from compilers and AI systems.\nAdditionally, we introduce meta-reinforcement learning with feedback rewards to\nsolve the global-local bi-level optimization problem and enhance the model's\nadaptability across diverse code generation tasks. Extensive experiments\ndemonstrate that our technique achieves state-of-the-art performance, leading\nother reinforcement learning methods by more than 4.5 percentage points on the\nMBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The\nopen-sourced code is publicly available at https://github.com/titurte/FALCON.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have achieved significant progress in\nautomated code generation. Despite their strong instruction-following\ncapabilities, these models frequently struggled to align with user intent in\ncoding scenarios. In particular, they were hampered by datasets that lacked\ndiversity and failed to address specialized tasks or edge cases. Furthermore,\nchallenges in supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) led to failures in generating precise,\nhuman-intent-aligned code. To tackle these challenges and improve the code\ngeneration performance for automated programming systems, we propose\nFeedback-driven Adaptive Long/short-term memory reinforced Coding Optimization\n(i.e., FALCON). FALCON is structured into two hierarchical levels. From the\nglobal level, long-term memory improves code quality by retaining and applying\nlearned knowledge. At the local level, short-term memory allows for the\nincorporation of immediate feedback from compilers and AI systems.\nAdditionally, we introduce meta-reinforcement learning with feedback rewards to\nsolve the global-local bi-level optimization problem and enhance the model's\nadaptability across diverse code generation tasks. Extensive experiments\ndemonstrate that our technique achieves state-of-the-art performance, leading\nother reinforcement learning methods by more than 4.5 percentage points on the\nMBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The\nopen-sourced code is publicly available at https://github.com/titurte/FALCON."
                },
                "authors": [
                    {
                        "name": "Zeyuan Li"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Lewei He"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Bin Lei"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Qiuwu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qiuwu Chen"
                },
                "author": "Qiuwu Chen",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01946v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01946v3",
                "updated": "2025-01-02T11:04:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    4,
                    46,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-02T20:14:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    14,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "The Reality of AI and Biorisk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Reality of AI and Biorisk"
                },
                "summary": "To accurately and confidently answer the question 'could an AI model or\nsystem increase biorisk', it is necessary to have both a sound theoretical\nthreat model for how AI models or systems could increase biorisk and a robust\nmethod for testing that threat model. This paper provides an analysis of\nexisting available research surrounding two AI and biorisk threat models: 1)\naccess to information and planning via large language models (LLMs), and 2) the\nuse of AI-enabled biological tools (BTs) in synthesizing novel biological\nartifacts. We find that existing studies around AI-related biorisk are nascent,\noften speculative in nature, or limited in terms of their methodological\nmaturity and transparency. The available literature suggests that current LLMs\nand BTs do not pose an immediate risk, and more work is needed to develop\nrigorous approaches to understanding how future models could increase biorisks.\nWe end with recommendations about how empirical work can be expanded to more\nprecisely target biorisk and ensure rigor and validity of findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To accurately and confidently answer the question 'could an AI model or\nsystem increase biorisk', it is necessary to have both a sound theoretical\nthreat model for how AI models or systems could increase biorisk and a robust\nmethod for testing that threat model. This paper provides an analysis of\nexisting available research surrounding two AI and biorisk threat models: 1)\naccess to information and planning via large language models (LLMs), and 2) the\nuse of AI-enabled biological tools (BTs) in synthesizing novel biological\nartifacts. We find that existing studies around AI-related biorisk are nascent,\noften speculative in nature, or limited in terms of their methodological\nmaturity and transparency. The available literature suggests that current LLMs\nand BTs do not pose an immediate risk, and more work is needed to develop\nrigorous approaches to understanding how future models could increase biorisks.\nWe end with recommendations about how empirical work can be expanded to more\nprecisely target biorisk and ensure rigor and validity of findings."
                },
                "authors": [
                    {
                        "name": "Aidan Peppin"
                    },
                    {
                        "name": "Anka Reuel"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Elliot Jones"
                    },
                    {
                        "name": "Andrew Strait"
                    },
                    {
                        "name": "Usman Anwar"
                    },
                    {
                        "name": "Anurag Agrawal"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Marie Pellat"
                    },
                    {
                        "name": "Rishi Bommasani"
                    },
                    {
                        "name": "Nick Frosst"
                    },
                    {
                        "name": "Sara Hooker"
                    }
                ],
                "author_detail": {
                    "name": "Sara Hooker"
                },
                "author": "Sara Hooker",
                "arxiv_comment": "Updated to correct author affiliations and clarify findings of\n  evaluations of the o1 model",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01946v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01946v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14674v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14674v3",
                "updated": "2025-01-02T11:00:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    0,
                    24,
                    3,
                    2,
                    0
                ],
                "published": "2024-11-22T02:15:38Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    2,
                    15,
                    38,
                    4,
                    327,
                    0
                ],
                "title": "Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal\n  Transport Metrics for Gaussian Mixtures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal\n  Transport Metrics for Gaussian Mixtures"
                },
                "summary": "Existing methods to summarize posterior inference for mixture models focus on\nidentifying a point estimate of the implied random partition for clustering,\nwith density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et\nal., 2022). We propose a novel approach for summarizing posterior inference in\nnonparametric Bayesian mixture models, prioritizing density estimation of the\nmixing measure (or mixture) as an inference target. One of the key features is\nthe model-agnostic nature of the approach, which remains valid under\narbitrarily complex dependence structures in the underlying sampling model.\nUsing a decision-theoretic framework, our method identifies a point estimate by\nminimizing posterior expected loss. A loss function is defined as a discrepancy\nbetween mixing measures. Estimating the mixing measure implies inference on the\nmixture density and the random partition. Exploiting the discrete nature of the\nmixing measure, we use a version of sliced Wasserstein distance. We introduce\ntwo specific variants for Gaussian mixtures. The first, mixed sliced\nWasserstein, applies generalized geodesic projections on the product of the\nEuclidean space and the manifold of symmetric positive definite matrices. The\nsecond, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture\nmeasures for efficient projection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing methods to summarize posterior inference for mixture models focus on\nidentifying a point estimate of the implied random partition for clustering,\nwith density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et\nal., 2022). We propose a novel approach for summarizing posterior inference in\nnonparametric Bayesian mixture models, prioritizing density estimation of the\nmixing measure (or mixture) as an inference target. One of the key features is\nthe model-agnostic nature of the approach, which remains valid under\narbitrarily complex dependence structures in the underlying sampling model.\nUsing a decision-theoretic framework, our method identifies a point estimate by\nminimizing posterior expected loss. A loss function is defined as a discrepancy\nbetween mixing measures. Estimating the mixing measure implies inference on the\nmixture density and the random partition. Exploiting the discrete nature of the\nmixing measure, we use a version of sliced Wasserstein distance. We introduce\ntwo specific variants for Gaussian mixtures. The first, mixed sliced\nWasserstein, applies generalized geodesic projections on the product of the\nEuclidean space and the manifold of symmetric positive definite matrices. The\nsecond, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture\nmeasures for efficient projection."
                },
                "authors": [
                    {
                        "name": "Khai Nguyen"
                    },
                    {
                        "name": "Peter Mueller"
                    }
                ],
                "author_detail": {
                    "name": "Peter Mueller"
                },
                "author": "Peter Mueller",
                "arxiv_comment": "38 pages, 2 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14674v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14674v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05411v2",
                "updated": "2025-01-02T10:56:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    10,
                    56,
                    16,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-07T18:23:00Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    18,
                    23,
                    0,
                    0,
                    281,
                    0
                ],
                "title": "Filtering Discomforting Recommendations with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filtering Discomforting Recommendations with Large Language Models"
                },
                "summary": "Personalized algorithms can inadvertently expose users to discomforting\nrecommendations, potentially triggering negative consequences. The subjectivity\nof discomfort and the black-box nature of these algorithms make it challenging\nto effectively identify and filter such content. To address this, we first\nconducted a formative study to understand users' practices and expectations\nregarding discomforting recommendation filtering. Then, we designed a Large\nLanguage Model (LLM)-based tool named DiscomfortFilter, which constructs an\neditable preference profile for a user and helps the user express filtering\nneeds through conversation to mask discomforting preferences within the\nprofile. Based on the edited profile, DiscomfortFilter facilitates the\ndiscomforting recommendations filtering in a plug-and-play manner, maintaining\nflexibility and transparency. The constructed preference profile improves LLM\nreasoning and simplifies user alignment, enabling a 3.8B open-source LLM to\nrival top commercial models in an offline proxy task. A one-week user study\nwith 24 participants demonstrated the effectiveness of DiscomfortFilter, while\nalso highlighting its potential impact on platform recommendation outcomes. We\nconclude by discussing the ongoing challenges, highlighting its relevance to\nbroader research, assessing stakeholder impact, and outlining future research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized algorithms can inadvertently expose users to discomforting\nrecommendations, potentially triggering negative consequences. The subjectivity\nof discomfort and the black-box nature of these algorithms make it challenging\nto effectively identify and filter such content. To address this, we first\nconducted a formative study to understand users' practices and expectations\nregarding discomforting recommendation filtering. Then, we designed a Large\nLanguage Model (LLM)-based tool named DiscomfortFilter, which constructs an\neditable preference profile for a user and helps the user express filtering\nneeds through conversation to mask discomforting preferences within the\nprofile. Based on the edited profile, DiscomfortFilter facilitates the\ndiscomforting recommendations filtering in a plug-and-play manner, maintaining\nflexibility and transparency. The constructed preference profile improves LLM\nreasoning and simplifies user alignment, enabling a 3.8B open-source LLM to\nrival top commercial models in an offline proxy task. A one-week user study\nwith 24 participants demonstrated the effectiveness of DiscomfortFilter, while\nalso highlighting its potential impact on platform recommendation outcomes. We\nconclude by discussing the ongoing challenges, highlighting its relevance to\nbroader research, assessing stakeholder impact, and outlining future research\ndirections."
                },
                "authors": [
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Yiyang Shao"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Longzhi Du"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Ning Gu"
                    }
                ],
                "author_detail": {
                    "name": "Ning Gu"
                },
                "author": "Ning Gu",
                "arxiv_comment": "16 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20367v2",
                "updated": "2025-01-02T09:43:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    43,
                    43,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-29T06:15:41Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    6,
                    15,
                    41,
                    6,
                    364,
                    0
                ],
                "title": "Enhancing Code LLMs with Reinforcement Learning in Code Generation: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Code LLMs with Reinforcement Learning in Code Generation: A\n  Survey"
                },
                "summary": "With the rapid evolution of large language models (LLM), reinforcement\nlearning (RL) has emerged as a pivotal technique for code generation and\noptimization in various domains. This paper presents a systematic survey of the\napplication of RL in code optimization and generation, highlighting its role in\nenhancing compiler optimization, resource allocation, and the development of\nframeworks and tools. Subsequent sections first delve into the intricate\nprocesses of compiler optimization, where RL algorithms are leveraged to\nimprove efficiency and resource utilization. The discussion then progresses to\nthe function of RL in resource allocation, emphasizing register allocation and\nsystem optimization. We also explore the burgeoning role of frameworks and\ntools in code generation, examining how RL can be integrated to bolster their\ncapabilities. This survey aims to serve as a comprehensive resource for\nresearchers and practitioners interested in harnessing the power of RL to\nadvance code generation and optimization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid evolution of large language models (LLM), reinforcement\nlearning (RL) has emerged as a pivotal technique for code generation and\noptimization in various domains. This paper presents a systematic survey of the\napplication of RL in code optimization and generation, highlighting its role in\nenhancing compiler optimization, resource allocation, and the development of\nframeworks and tools. Subsequent sections first delve into the intricate\nprocesses of compiler optimization, where RL algorithms are leveraged to\nimprove efficiency and resource utilization. The discussion then progresses to\nthe function of RL in resource allocation, emphasizing register allocation and\nsystem optimization. We also explore the burgeoning role of frameworks and\ntools in code generation, examining how RL can be integrated to bolster their\ncapabilities. This survey aims to serve as a comprehensive resource for\nresearchers and practitioners interested in harnessing the power of RL to\nadvance code generation and optimization techniques."
                },
                "authors": [
                    {
                        "name": "Junqiao Wang"
                    },
                    {
                        "name": "Zeng Zhang"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Yuyang Song"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Hengyuan Xu"
                    },
                    {
                        "name": "Kunyu Wu"
                    },
                    {
                        "name": "Guangwu Qian"
                    },
                    {
                        "name": "Qiuwu Chen"
                    },
                    {
                        "name": "Lewei He"
                    }
                ],
                "author_detail": {
                    "name": "Lewei He"
                },
                "author": "Lewei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03603v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03603v3",
                "updated": "2025-01-02T09:13:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    13,
                    42,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-03T23:52:37Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    23,
                    52,
                    37,
                    1,
                    338,
                    0
                ],
                "title": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
                },
                "summary": "Recent advancements in video generation have significantly impacted daily\nlife for both individuals and industries. However, the leading video generation\nmodels remain closed-source, resulting in a notable performance gap between\nindustry capabilities and those available to the public. In this report, we\nintroduce HunyuanVideo, an innovative open-source video foundation model that\ndemonstrates performance in video generation comparable to, or even surpassing,\nthat of leading closed-source models. HunyuanVideo encompasses a comprehensive\nframework that integrates several key elements, including data curation,\nadvanced architectural design, progressive model scaling and training, and an\nefficient infrastructure tailored for large-scale model training and inference.\nAs a result, we successfully trained a video generative model with over 13\nbillion parameters, making it the largest among all open-source models. We\nconducted extensive experiments and implemented a series of targeted designs to\nensure high visual quality, motion dynamics, text-video alignment, and advanced\nfilming techniques. According to evaluations by professionals, HunyuanVideo\noutperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6,\nand three top-performing Chinese video generative models. By releasing the code\nfor the foundation model and its applications, we aim to bridge the gap between\nclosed-source and open-source communities. This initiative will empower\nindividuals within the community to experiment with their ideas, fostering a\nmore dynamic and vibrant video generation ecosystem. The code is publicly\navailable at https://github.com/Tencent/HunyuanVideo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in video generation have significantly impacted daily\nlife for both individuals and industries. However, the leading video generation\nmodels remain closed-source, resulting in a notable performance gap between\nindustry capabilities and those available to the public. In this report, we\nintroduce HunyuanVideo, an innovative open-source video foundation model that\ndemonstrates performance in video generation comparable to, or even surpassing,\nthat of leading closed-source models. HunyuanVideo encompasses a comprehensive\nframework that integrates several key elements, including data curation,\nadvanced architectural design, progressive model scaling and training, and an\nefficient infrastructure tailored for large-scale model training and inference.\nAs a result, we successfully trained a video generative model with over 13\nbillion parameters, making it the largest among all open-source models. We\nconducted extensive experiments and implemented a series of targeted designs to\nensure high visual quality, motion dynamics, text-video alignment, and advanced\nfilming techniques. According to evaluations by professionals, HunyuanVideo\noutperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6,\nand three top-performing Chinese video generative models. By releasing the code\nfor the foundation model and its applications, we aim to bridge the gap between\nclosed-source and open-source communities. This initiative will empower\nindividuals within the community to experiment with their ideas, fostering a\nmore dynamic and vibrant video generation ecosystem. The code is publicly\navailable at https://github.com/Tencent/HunyuanVideo."
                },
                "authors": [
                    {
                        "name": "Weijie Kong"
                    },
                    {
                        "name": "Qi Tian"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Rox Min"
                    },
                    {
                        "name": "Zuozhuo Dai"
                    },
                    {
                        "name": "Jin Zhou"
                    },
                    {
                        "name": "Jiangfeng Xiong"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Jianwei Zhang"
                    },
                    {
                        "name": "Kathrina Wu"
                    },
                    {
                        "name": "Qin Lin"
                    },
                    {
                        "name": "Junkun Yuan"
                    },
                    {
                        "name": "Yanxin Long"
                    },
                    {
                        "name": "Aladdin Wang"
                    },
                    {
                        "name": "Andong Wang"
                    },
                    {
                        "name": "Changlin Li"
                    },
                    {
                        "name": "Duojun Huang"
                    },
                    {
                        "name": "Fang Yang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Hongmei Wang"
                    },
                    {
                        "name": "Jacob Song"
                    },
                    {
                        "name": "Jiawang Bai"
                    },
                    {
                        "name": "Jianbing Wu"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Joey Wang"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Mengyang Liu"
                    },
                    {
                        "name": "Pengyu Li"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Weiyan Wang"
                    },
                    {
                        "name": "Wenqing Yu"
                    },
                    {
                        "name": "Xinchi Deng"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Yutao Cui"
                    },
                    {
                        "name": "Yuanbo Peng"
                    },
                    {
                        "name": "Zhentao Yu"
                    },
                    {
                        "name": "Zhiyu He"
                    },
                    {
                        "name": "Zhiyong Xu"
                    },
                    {
                        "name": "Zixiang Zhou"
                    },
                    {
                        "name": "Zunnan Xu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Qinglin Lu"
                    },
                    {
                        "name": "Songtao Liu"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Hongfa Wang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Caesar Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Caesar Zhong"
                },
                "arxiv_affiliation": "refer to the report for detailed contributions",
                "author": "Caesar Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03603v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03603v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00927v2",
                "updated": "2025-01-02T08:53:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    53,
                    50,
                    3,
                    2,
                    0
                ],
                "published": "2024-09-30T16:57:34Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    57,
                    34,
                    0,
                    274,
                    0
                ],
                "title": "Text Clustering as Classification with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Clustering as Classification with LLMs"
                },
                "summary": "Text clustering remains valuable in real-world applications where manual\nlabeling is cost-prohibitive. It facilitates efficient organization and\nanalysis of information by grouping similar texts based on their\nrepresentations. However, implementing this approach necessitates fine-tuned\nembedders for downstream data and sophisticated similarity metrics. To address\nthis issue, this study presents a novel framework for text clustering that\neffectively leverages the in-context learning capacity of Large Language Models\n(LLMs). Instead of fine-tuning embedders, we propose to transform the text\nclustering into a classification task via LLM. First, we prompt LLM to generate\npotential labels for a given dataset. Second, after integrating similar labels\ngenerated by the LLM, we prompt the LLM to assign the most appropriate label to\neach sample in the dataset. Our framework has been experimentally proven to\nachieve comparable or superior performance to state-of-the-art clustering\nmethods that employ embeddings, without requiring complex fine-tuning or\nclustering algorithms. We make our code available to the public for utilization\nat https://github.com/ECNU-Text-Computing/Text-Clustering-via-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text clustering remains valuable in real-world applications where manual\nlabeling is cost-prohibitive. It facilitates efficient organization and\nanalysis of information by grouping similar texts based on their\nrepresentations. However, implementing this approach necessitates fine-tuned\nembedders for downstream data and sophisticated similarity metrics. To address\nthis issue, this study presents a novel framework for text clustering that\neffectively leverages the in-context learning capacity of Large Language Models\n(LLMs). Instead of fine-tuning embedders, we propose to transform the text\nclustering into a classification task via LLM. First, we prompt LLM to generate\npotential labels for a given dataset. Second, after integrating similar labels\ngenerated by the LLM, we prompt the LLM to assign the most appropriate label to\neach sample in the dataset. Our framework has been experimentally proven to\nachieve comparable or superior performance to state-of-the-art clustering\nmethods that employ embeddings, without requiring complex fine-tuning or\nclustering algorithms. We make our code available to the public for utilization\nat https://github.com/ECNU-Text-Computing/Text-Clustering-via-LLM."
                },
                "authors": [
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Guoxiu He"
                    }
                ],
                "author_detail": {
                    "name": "Guoxiu He"
                },
                "author": "Guoxiu He",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14528v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14528v3",
                "updated": "2025-01-02T08:49:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    49,
                    12,
                    3,
                    2,
                    0
                ],
                "published": "2023-05-23T21:10:17Z",
                "published_parsed": [
                    2023,
                    5,
                    23,
                    21,
                    10,
                    17,
                    1,
                    143,
                    0
                ],
                "title": "Function Basis Encoding of Numerical Features in Factorization Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function Basis Encoding of Numerical Features in Factorization Machines"
                },
                "summary": "Factorization machine (FM) variants are widely used for large scale real-time\ncontent recommendation systems, since they offer an excellent balance between\nmodel accuracy and low computational costs for training and inference. These\nsystems are trained on tabular data with both numerical and categorical\ncolumns. Incorporating numerical columns poses a challenge, and they are\ntypically incorporated using a scalar transformation or binning, which can be\neither learned or chosen a-priori. In this work, we provide a systematic and\ntheoretically-justified way to incorporate numerical features into FM variants\nby encoding them into a vector of function values for a set of functions of\none's choice.\n  We view factorization machines as approximators of segmentized functions,\nnamely, functions from a field's value to the real numbers, assuming the\nremaining fields are assigned some given constants, which we refer to as the\nsegment. From this perspective, we show that our technique yields a model that\nlearns segmentized functions of the numerical feature spanned by the set of\nfunctions of one's choice, namely, the spanning coefficients vary between\nsegments. Hence, to improve model accuracy we advocate the use of functions\nknown to have strong approximation power, and offer the B-Spline basis due to\nits well-known approximation power, availability in software libraries, and\nefficiency. Our technique preserves fast training and inference, and requires\nonly a small modification of the computational graph of an FM model. Therefore,\nit is easy to incorporate into an existing system to improve its performance.\nFinally, we back our claims with a set of experiments, including synthetic,\nperformance evaluation on several data-sets, and an A/B test on a real online\nadvertising system which shows improved performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factorization machine (FM) variants are widely used for large scale real-time\ncontent recommendation systems, since they offer an excellent balance between\nmodel accuracy and low computational costs for training and inference. These\nsystems are trained on tabular data with both numerical and categorical\ncolumns. Incorporating numerical columns poses a challenge, and they are\ntypically incorporated using a scalar transformation or binning, which can be\neither learned or chosen a-priori. In this work, we provide a systematic and\ntheoretically-justified way to incorporate numerical features into FM variants\nby encoding them into a vector of function values for a set of functions of\none's choice.\n  We view factorization machines as approximators of segmentized functions,\nnamely, functions from a field's value to the real numbers, assuming the\nremaining fields are assigned some given constants, which we refer to as the\nsegment. From this perspective, we show that our technique yields a model that\nlearns segmentized functions of the numerical feature spanned by the set of\nfunctions of one's choice, namely, the spanning coefficients vary between\nsegments. Hence, to improve model accuracy we advocate the use of functions\nknown to have strong approximation power, and offer the B-Spline basis due to\nits well-known approximation power, availability in software libraries, and\nefficiency. Our technique preserves fast training and inference, and requires\nonly a small modification of the computational graph of an FM model. Therefore,\nit is easy to incorporate into an existing system to improve its performance.\nFinally, we back our claims with a set of experiments, including synthetic,\nperformance evaluation on several data-sets, and an A/B test on a real online\nadvertising system which shows improved performance."
                },
                "authors": [
                    {
                        "name": "Alex Shtoff"
                    },
                    {
                        "name": "Elie Abboud"
                    },
                    {
                        "name": "Rotem Stram"
                    },
                    {
                        "name": "Oren Somekh"
                    }
                ],
                "author_detail": {
                    "name": "Oren Somekh"
                },
                "author": "Oren Somekh",
                "arxiv_comment": "Published in TMLR, '2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14528v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14528v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09893v3",
                "updated": "2025-01-02T08:43:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    43,
                    57,
                    3,
                    2,
                    0
                ],
                "published": "2024-07-13T13:58:24Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    13,
                    58,
                    24,
                    5,
                    195,
                    0
                ],
                "title": "Synergistic Multi-Agent Framework with Trajectory Learning for\n  Knowledge-Intensive Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergistic Multi-Agent Framework with Trajectory Learning for\n  Knowledge-Intensive Tasks"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long-Short Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on five knowledge-intensive\ntasks demonstrate SMART's superior performance compared to widely adopted\nknowledge internalization and knowledge enhancement methods. Our framework can\nextend beyond knowledge-intensive tasks to more complex scenarios. Our code is\navailable at https://github.com/yueshengbin/SMART.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long-Short Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on five knowledge-intensive\ntasks demonstrate SMART's superior performance compared to widely adopted\nknowledge internalization and knowledge enhancement methods. Our framework can\nextend beyond knowledge-intensive tasks to more complex scenarios. Our code is\navailable at https://github.com/yueshengbin/SMART."
                },
                "authors": [
                    {
                        "name": "Shengbin Yue"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16500v2",
                "updated": "2025-01-02T07:29:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    7,
                    29,
                    1,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-21T06:16:04Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    6,
                    16,
                    4,
                    5,
                    356,
                    0
                ],
                "title": "Speech Retrieval-Augmented Generation without Automatic Speech\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech Retrieval-Augmented Generation without Automatic Speech\n  Recognition"
                },
                "summary": "One common approach for question answering over speech data is to first\ntranscribe speech using automatic speech recognition (ASR) and then employ\ntext-based retrieval-augmented generation (RAG) on the transcriptions. While\nthis cascaded pipeline has proven effective in many practical settings, ASR\nerrors can propagate to the retrieval and generation steps. To overcome this\nlimitation, we introduce SpeechRAG, a novel framework designed for\nopen-question answering over spoken data. Our proposed approach fine-tunes a\npre-trained speech encoder into a speech adapter fed into a frozen large\nlanguage model (LLM)--based retrieval model. By aligning the embedding spaces\nof text and speech, our speech retriever directly retrieves audio passages from\ntext-based queries, leveraging the retrieval capacity of the frozen text\nretriever. Our retrieval experiments on spoken question answering datasets show\nthat direct speech retrieval does not degrade over the text-based baseline, and\noutperforms the cascaded systems using ASR. For generation, we use a speech\nlanguage model (SLM) as a generator, conditioned on audio passages rather than\ntranscripts. Without fine-tuning of the SLM, this approach outperforms cascaded\ntext-based models when there is high WER in the transcripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One common approach for question answering over speech data is to first\ntranscribe speech using automatic speech recognition (ASR) and then employ\ntext-based retrieval-augmented generation (RAG) on the transcriptions. While\nthis cascaded pipeline has proven effective in many practical settings, ASR\nerrors can propagate to the retrieval and generation steps. To overcome this\nlimitation, we introduce SpeechRAG, a novel framework designed for\nopen-question answering over spoken data. Our proposed approach fine-tunes a\npre-trained speech encoder into a speech adapter fed into a frozen large\nlanguage model (LLM)--based retrieval model. By aligning the embedding spaces\nof text and speech, our speech retriever directly retrieves audio passages from\ntext-based queries, leveraging the retrieval capacity of the frozen text\nretriever. Our retrieval experiments on spoken question answering datasets show\nthat direct speech retrieval does not degrade over the text-based baseline, and\noutperforms the cascaded systems using ASR. For generation, we use a speech\nlanguage model (SLM) as a generator, conditioned on audio passages rather than\ntranscripts. Without fine-tuning of the SLM, this approach outperforms cascaded\ntext-based models when there is high WER in the transcripts."
                },
                "authors": [
                    {
                        "name": "Do June Min"
                    },
                    {
                        "name": "Karel Mundnich"
                    },
                    {
                        "name": "Andy Lapastora"
                    },
                    {
                        "name": "Erfan Soltanmohammadi"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Kyu Han"
                    }
                ],
                "author_detail": {
                    "name": "Kyu Han"
                },
                "author": "Kyu Han",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20265v2",
                "updated": "2025-01-02T07:26:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    7,
                    26,
                    59,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-28T20:40:23Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    20,
                    40,
                    23,
                    5,
                    363,
                    0
                ],
                "title": "Overcoming Intensity Limits for Long-Distance Quantum Key Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overcoming Intensity Limits for Long-Distance Quantum Key Distribution"
                },
                "summary": "Quantum Key Distribution (QKD) enables the sharing of cryptographic keys\nsecured by quantum mechanics. The BB84 protocol assumed single-photon sources,\nbut practical systems rely on weak coherent pulses vulnerable to\nphoton-number-splitting (PNS) attacks. The Gottesman-Lo-L\\\"utkenhaus-Preskill\n(GLLP) framework addressed these imperfections, deriving secure key rate bounds\nunder limited PNS. The Decoy-state protocol further improved performance by\nrefining single-photon yield estimates, but still considered multi-photon\nstates as insecure, limiting intensities and thereby constraining key rate and\ndistance. Here, we show that higher intensities can be securely permitted by\napplying Bayesian inference to estimate key parameters directly from observed\ndata rather than relying on worst-case assumptions. By raising the pulse\nintensity to 10 photons, we achieve 50 times the key rate and a 62.2% increase\nin operational range (about 200 km) compared to the decoy-state protocol.\nFurthermore, we accurately model after-pulsing using a Hidden Markov Model and\nreveal inaccuracies in decoy-state calculations that may produce erroneous\nkey-rate estimates. By bridging theoretical security and real-world conditions,\nthis Bayesian methodology provides a versatile post-processing step for many\ndiscrete-variable QKD protocols, advancing their reach, efficiency, and\nfacilitating broader adoption of quantum-secured communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Key Distribution (QKD) enables the sharing of cryptographic keys\nsecured by quantum mechanics. The BB84 protocol assumed single-photon sources,\nbut practical systems rely on weak coherent pulses vulnerable to\nphoton-number-splitting (PNS) attacks. The Gottesman-Lo-L\\\"utkenhaus-Preskill\n(GLLP) framework addressed these imperfections, deriving secure key rate bounds\nunder limited PNS. The Decoy-state protocol further improved performance by\nrefining single-photon yield estimates, but still considered multi-photon\nstates as insecure, limiting intensities and thereby constraining key rate and\ndistance. Here, we show that higher intensities can be securely permitted by\napplying Bayesian inference to estimate key parameters directly from observed\ndata rather than relying on worst-case assumptions. By raising the pulse\nintensity to 10 photons, we achieve 50 times the key rate and a 62.2% increase\nin operational range (about 200 km) compared to the decoy-state protocol.\nFurthermore, we accurately model after-pulsing using a Hidden Markov Model and\nreveal inaccuracies in decoy-state calculations that may produce erroneous\nkey-rate estimates. By bridging theoretical security and real-world conditions,\nthis Bayesian methodology provides a versatile post-processing step for many\ndiscrete-variable QKD protocols, advancing their reach, efficiency, and\nfacilitating broader adoption of quantum-secured communication."
                },
                "authors": [
                    {
                        "name": "Ibrahim Almosallam"
                    }
                ],
                "author_detail": {
                    "name": "Ibrahim Almosallam"
                },
                "author": "Ibrahim Almosallam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19376v2",
                "updated": "2025-01-02T05:34:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    5,
                    34,
                    51,
                    3,
                    2,
                    0
                ],
                "published": "2024-02-29T17:24:22Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    17,
                    24,
                    22,
                    3,
                    60,
                    0
                ],
                "title": "Commercial Evaluation of Zero-Skipping MAC Design for Bit Sparsity\n  Exploitation in DL Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commercial Evaluation of Zero-Skipping MAC Design for Bit Sparsity\n  Exploitation in DL Inference"
                },
                "summary": "General Matrix Multiply (GEMM) units, consisting of multiply-accumulate (MAC)\narrays, perform bulk of the computation in deep learning (DL). Recent work has\nproposed a novel MAC design, Bit-Pragmatic (PRA), capable of dynamically\nexploiting bit sparsity. This work presents OzMAC (Omit-zero-MAC), a modified\nre-implementation of PRA, but extends beyond earlier works by performing\nrigorous post-synthesis evaluation against binary MAC design across multiple\nbitwidths and clock frequencies using TSMC N5 process node to assess commercial\nimplementation potential. We demonstrate the existence of high bit sparsity in\neight pretrained INT8 DL workloads and show that 8-bit OzMAC improves all three\nmetrics of area, power, and energy significantly by 21%, 70%, and 28%,\nrespectively. Similar improvements are achieved when scaling data precisions\n(4, 8, 16 bits) and clock frequencies (0.5 GHz, 1 GHz, 1.5 GHz). For the 8-bit\nOzMAC, scaling its frequency to normalize the throughput, it still achieves 30%\nimprovement on both power and energy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General Matrix Multiply (GEMM) units, consisting of multiply-accumulate (MAC)\narrays, perform bulk of the computation in deep learning (DL). Recent work has\nproposed a novel MAC design, Bit-Pragmatic (PRA), capable of dynamically\nexploiting bit sparsity. This work presents OzMAC (Omit-zero-MAC), a modified\nre-implementation of PRA, but extends beyond earlier works by performing\nrigorous post-synthesis evaluation against binary MAC design across multiple\nbitwidths and clock frequencies using TSMC N5 process node to assess commercial\nimplementation potential. We demonstrate the existence of high bit sparsity in\neight pretrained INT8 DL workloads and show that 8-bit OzMAC improves all three\nmetrics of area, power, and energy significantly by 21%, 70%, and 28%,\nrespectively. Similar improvements are achieved when scaling data precisions\n(4, 8, 16 bits) and clock frequencies (0.5 GHz, 1 GHz, 1.5 GHz). For the 8-bit\nOzMAC, scaling its frequency to normalize the throughput, it still achieves 30%\nimprovement on both power and energy."
                },
                "authors": [
                    {
                        "name": "Harideep Nair"
                    },
                    {
                        "name": "Prabhu Vellaisamy"
                    },
                    {
                        "name": "Tsung-Han Lin"
                    },
                    {
                        "name": "Perry Wang"
                    },
                    {
                        "name": "Shawn Blanton"
                    },
                    {
                        "name": "John Paul Shen"
                    }
                ],
                "author_detail": {
                    "name": "John Paul Shen"
                },
                "author": "John Paul Shen",
                "arxiv_doi": "10.1109/VLSI-SoC62099.2024.10767792",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/VLSI-SoC62099.2024.10767792",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.19376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Pre-print version of the publication in VLSI-SoC 2024",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01985v2",
                "updated": "2025-01-02T04:15:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    4,
                    15,
                    37,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-02T19:45:19Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    19,
                    45,
                    19,
                    2,
                    276,
                    0
                ],
                "title": "Lost-in-Distance: Impact of Contextual Proximity on LLM Performance in\n  Graph Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost-in-Distance: Impact of Contextual Proximity on LLM Performance in\n  Graph Tasks"
                },
                "summary": "Despite significant advancements, Large Language Models (LLMs) exhibit blind\nspots that impair their ability to retrieve and process relevant contextual\ndata effectively. We demonstrate that LLM performance in graph tasks with\ncomplexities beyond the \"needle-in-a-haystack\" scenario-where solving the\nproblem requires cross-referencing and reasoning across multiple subproblems\njointly-is influenced by the proximity of relevant information within the\ncontext, a phenomenon we term \"lost-in-distance\". We examine two fundamental\ngraph tasks: identifying common connections between two nodes and assessing\nsimilarity among three nodes, and show that the model's performance in these\ntasks significantly depends on the relative positioning of common edges. We\nevaluate three publicly available LLMs using various graph encoding techniques\nthat represent graph structures for LLM input. We propose a formulation for the\nlost-in-distance phenomenon and demonstrate that lost-in-distance and\nlost-in-the middle phenomenas occur independently. Results indicate that model\naccuracy can decline by up to 6x as the distance between node connections\nincreases, independent of graph encoding and model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements, Large Language Models (LLMs) exhibit blind\nspots that impair their ability to retrieve and process relevant contextual\ndata effectively. We demonstrate that LLM performance in graph tasks with\ncomplexities beyond the \"needle-in-a-haystack\" scenario-where solving the\nproblem requires cross-referencing and reasoning across multiple subproblems\njointly-is influenced by the proximity of relevant information within the\ncontext, a phenomenon we term \"lost-in-distance\". We examine two fundamental\ngraph tasks: identifying common connections between two nodes and assessing\nsimilarity among three nodes, and show that the model's performance in these\ntasks significantly depends on the relative positioning of common edges. We\nevaluate three publicly available LLMs using various graph encoding techniques\nthat represent graph structures for LLM input. We propose a formulation for the\nlost-in-distance phenomenon and demonstrate that lost-in-distance and\nlost-in-the middle phenomenas occur independently. Results indicate that model\naccuracy can decline by up to 6x as the distance between node connections\nincreases, independent of graph encoding and model size."
                },
                "authors": [
                    {
                        "name": "Hamed Firooz"
                    },
                    {
                        "name": "Maziar Sanjabi"
                    },
                    {
                        "name": "Wenlong Jiang"
                    },
                    {
                        "name": "Xiaoling Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoling Zhai"
                },
                "author": "Xiaoling Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13334v3",
                "updated": "2025-01-02T04:06:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    4,
                    6,
                    46,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-17T08:46:09Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    46,
                    9,
                    3,
                    291,
                    0
                ],
                "title": "BiasJailbreak:Analyzing Ethical Biases and Jailbreak Vulnerabilities in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiasJailbreak:Analyzing Ethical Biases and Jailbreak Vulnerabilities in\n  Large Language Models"
                },
                "summary": "Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as `jailbreaks', where\nmalicious inputs can coerce LLMs into generating harmful content bypassing\nsafety alignments. In this paper, we delve into the ethical biases in LLMs and\nexamine how those biases could be exploited for jailbreaks. Notably, these\nbiases result in a jailbreaking success rate in GPT-4o models that differs by\n20\\% between non-binary and cisgender keywords and by 16\\% between white and\nblack keywords, even when the other parts of the prompts are identical. We\nintroduce the concept of BiasJailbreak, highlighting the inherent risks posed\nby these safety-induced biases. BiasJailbreak generates biased keywords\nautomatically by asking the target LLM itself, and utilizes the keywords to\ngenerate harmful output. Additionally, we propose an efficient defense method\nBiasDefense, which prevents jailbreak attempts by injecting defense prompts\nprior to generation. BiasDefense stands as an appealing alternative to Guard\nModels, such as Llama-Guard, that require additional inference cost after text\ngeneration. Our findings emphasize that ethical biases in LLMs can actually\nlead to generating unsafe output, and suggest a method to make the LLMs more\nsecure and unbiased. To enable further research and improvements, we\nopen-source our code and artifacts of BiasJailbreak, providing the community\nwith tools to better understand and mitigate safety-induced biases in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as `jailbreaks', where\nmalicious inputs can coerce LLMs into generating harmful content bypassing\nsafety alignments. In this paper, we delve into the ethical biases in LLMs and\nexamine how those biases could be exploited for jailbreaks. Notably, these\nbiases result in a jailbreaking success rate in GPT-4o models that differs by\n20\\% between non-binary and cisgender keywords and by 16\\% between white and\nblack keywords, even when the other parts of the prompts are identical. We\nintroduce the concept of BiasJailbreak, highlighting the inherent risks posed\nby these safety-induced biases. BiasJailbreak generates biased keywords\nautomatically by asking the target LLM itself, and utilizes the keywords to\ngenerate harmful output. Additionally, we propose an efficient defense method\nBiasDefense, which prevents jailbreak attempts by injecting defense prompts\nprior to generation. BiasDefense stands as an appealing alternative to Guard\nModels, such as Llama-Guard, that require additional inference cost after text\ngeneration. Our findings emphasize that ethical biases in LLMs can actually\nlead to generating unsafe output, and suggest a method to make the LLMs more\nsecure and unbiased. To enable further research and improvements, we\nopen-source our code and artifacts of BiasJailbreak, providing the community\nwith tools to better understand and mitigate safety-induced biases in LLMs."
                },
                "authors": [
                    {
                        "name": "Isack Lee"
                    },
                    {
                        "name": "Haebin Seong"
                    }
                ],
                "author_detail": {
                    "name": "Haebin Seong"
                },
                "author": "Haebin Seong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01340v2",
                "updated": "2025-01-02T03:29:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    29,
                    31,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-02T10:07:01Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    10,
                    7,
                    1,
                    0,
                    337,
                    0
                ],
                "title": "A 2-step Framework for Automated Literary Translation Evaluation: Its\n  Promises and Pitfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A 2-step Framework for Automated Literary Translation Evaluation: Its\n  Promises and Pitfalls"
                },
                "summary": "In this work, we propose and evaluate the feasibility of a two-stage pipeline\nto evaluate literary machine translation, in a fine-grained manner, from\nEnglish to Korean. The results show that our framework provides fine-grained,\ninterpretable metrics suited for literary translation and obtains a higher\ncorrelation with human judgment than traditional machine translation metrics.\nNonetheless, it still fails to match inter-human agreement, especially in\nmetrics like Korean Honorifics. We also observe that LLMs tend to favor\ntranslations generated by other LLMs, and we highlight the necessity of\ndeveloping more sophisticated evaluation methods to ensure accurate and\nculturally sensitive machine translation of literary works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose and evaluate the feasibility of a two-stage pipeline\nto evaluate literary machine translation, in a fine-grained manner, from\nEnglish to Korean. The results show that our framework provides fine-grained,\ninterpretable metrics suited for literary translation and obtains a higher\ncorrelation with human judgment than traditional machine translation metrics.\nNonetheless, it still fails to match inter-human agreement, especially in\nmetrics like Korean Honorifics. We also observe that LLMs tend to favor\ntranslations generated by other LLMs, and we highlight the necessity of\ndeveloping more sophisticated evaluation methods to ensure accurate and\nculturally sensitive machine translation of literary works."
                },
                "authors": [
                    {
                        "name": "Sheikh Shafayat"
                    },
                    {
                        "name": "Dongkeun Yoon"
                    },
                    {
                        "name": "Woori Jang"
                    },
                    {
                        "name": "Jiwoo Choi"
                    },
                    {
                        "name": "Alice Oh"
                    },
                    {
                        "name": "Seohyon Jung"
                    }
                ],
                "author_detail": {
                    "name": "Seohyon Jung"
                },
                "author": "Seohyon Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.00314v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.00314v3",
                "updated": "2025-01-02T03:29:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    29,
                    11,
                    3,
                    2,
                    0
                ],
                "published": "2023-01-01T00:47:03Z",
                "published_parsed": [
                    2023,
                    1,
                    1,
                    0,
                    47,
                    3,
                    6,
                    1,
                    0
                ],
                "title": "Causal Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Deep Learning"
                },
                "summary": "We derive a set of causal deep neural networks whose architectures are a\nconsequence of tensor (multilinear) factor analysis, a framework that\nfacilitates forward and inverse causal inference. Forward causal questions are\naddressed with a neural architecture composed of causal capsules and a tensor\ntransformer. Causal capsules compute a set of invariant causal factor\nrepresentations, whose interactions are governed by a tensor transformation.\nInverse causal questions are addressed with a neural network that implements\nthe multilinear projection algorithm. The architecture reverses the order of\nthe operations of a forward neural network and estimates the causes of effects.\nAs an alternative to aggressive bottleneck dimension reduction or regularized\nregression that may camouflage an inherently underdetermined inverse problem,\nwe prescribe modeling different aspects of the mechanism of data formation with\npiecewise tensor models whose multilinear projections produce multiple\ncandidate solutions. Our forward and inverse questions may be addressed with\nshallow architectures, but for computationally scalable solutions, we derive a\nset of deep neural networks by taking advantage of block algebra. An\ninterleaved kernel hierarchy results in a doubly non-linear tensor factor\nmodels. The causal neural networks that are a consequence of tensor factor\nanalysis are data agnostic, but are illustrated with facial images. Sequential,\nparallel and asynchronous parallel computation strategies are described.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We derive a set of causal deep neural networks whose architectures are a\nconsequence of tensor (multilinear) factor analysis, a framework that\nfacilitates forward and inverse causal inference. Forward causal questions are\naddressed with a neural architecture composed of causal capsules and a tensor\ntransformer. Causal capsules compute a set of invariant causal factor\nrepresentations, whose interactions are governed by a tensor transformation.\nInverse causal questions are addressed with a neural network that implements\nthe multilinear projection algorithm. The architecture reverses the order of\nthe operations of a forward neural network and estimates the causes of effects.\nAs an alternative to aggressive bottleneck dimension reduction or regularized\nregression that may camouflage an inherently underdetermined inverse problem,\nwe prescribe modeling different aspects of the mechanism of data formation with\npiecewise tensor models whose multilinear projections produce multiple\ncandidate solutions. Our forward and inverse questions may be addressed with\nshallow architectures, but for computationally scalable solutions, we derive a\nset of deep neural networks by taking advantage of block algebra. An\ninterleaved kernel hierarchy results in a doubly non-linear tensor factor\nmodels. The causal neural networks that are a consequence of tensor factor\nanalysis are data agnostic, but are illustrated with facial images. Sequential,\nparallel and asynchronous parallel computation strategies are described."
                },
                "authors": [
                    {
                        "name": "M. Alex O. Vasilescu"
                    }
                ],
                "author_detail": {
                    "name": "M. Alex O. Vasilescu"
                },
                "author": "M. Alex O. Vasilescu",
                "arxiv_doi": "10.1007/978-3-031-78189-6_27",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-78189-6_27",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2301.00314v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.00314v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Antonacopoulos, A., Chaudhuri, S., Chellappa, R., Liu, CL.,\n  Bhattacharya, S., Pal, U. (eds) Pattern Recognition. (ICPR 2024). Lecture\n  Notes in Computer Science, vol 15309, pp 420-438(LNCS 2025). Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07 (Primary) 68T30, 68T45, 62H25, 62H30, 62H35, 62D20, 62J10,\n  15A72, 15A69, 15A09 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.5.1; I.2.6; I.2.4; G.3; I.2.10; I.5.2; I.4.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08877v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08877v4",
                "updated": "2025-01-02T03:14:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    14,
                    11,
                    3,
                    2,
                    0
                ],
                "published": "2024-04-13T02:36:40Z",
                "published_parsed": [
                    2024,
                    4,
                    13,
                    2,
                    36,
                    40,
                    5,
                    104,
                    0
                ],
                "title": "Aligning the Objective of LLM-based Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning the Objective of LLM-based Program Repair"
                },
                "summary": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR."
                },
                "authors": [
                    {
                        "name": "Junjielong Xu"
                    },
                    {
                        "name": "Ying Fu"
                    },
                    {
                        "name": "Shin Hwei Tan"
                    },
                    {
                        "name": "Pinjia He"
                    }
                ],
                "author_detail": {
                    "name": "Pinjia He"
                },
                "author": "Pinjia He",
                "arxiv_comment": "Accepted by ICSE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08877v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08877v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16668v2",
                "updated": "2025-01-02T03:02:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    2,
                    13,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-22T03:53:46Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    3,
                    53,
                    46,
                    1,
                    296,
                    0
                ],
                "title": "Satori: Towards Proactive AR Assistant with Belief-Desire-Intention User\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satori: Towards Proactive AR Assistant with Belief-Desire-Intention User\n  Modeling"
                },
                "summary": "Augmented Reality assistance are increasingly popular for supporting users\nwith tasks like assembly and cooking. However, current practice typically\nprovide reactive responses initialized from user requests, lacking\nconsideration of rich contextual and user-specific information. To address this\nlimitation, we propose a novel AR assistance system, Satori, that models both\nuser states and environmental contexts to deliver proactive guidance. Our\nsystem combines the Belief-Desire-Intention (BDI) model with a state-of-the-art\nmulti-modal large language model (LLM) to infer contextually appropriate\nguidance. The design is informed by two formative studies involving twelve\nexperts. A sixteen within-subject study find that Satori achieves performance\ncomparable to an designer-created Wizard-of-Oz (WoZ) system without relying on\nmanual configurations or heuristics, thereby enhancing generalizability,\nreusability and opening up new possibilities for AR assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Reality assistance are increasingly popular for supporting users\nwith tasks like assembly and cooking. However, current practice typically\nprovide reactive responses initialized from user requests, lacking\nconsideration of rich contextual and user-specific information. To address this\nlimitation, we propose a novel AR assistance system, Satori, that models both\nuser states and environmental contexts to deliver proactive guidance. Our\nsystem combines the Belief-Desire-Intention (BDI) model with a state-of-the-art\nmulti-modal large language model (LLM) to infer contextually appropriate\nguidance. The design is informed by two formative studies involving twelve\nexperts. A sixteen within-subject study find that Satori achieves performance\ncomparable to an designer-created Wizard-of-Oz (WoZ) system without relying on\nmanual configurations or heuristics, thereby enhancing generalizability,\nreusability and opening up new possibilities for AR assistance."
                },
                "authors": [
                    {
                        "name": "Chenyi Li"
                    },
                    {
                        "name": "Guande Wu"
                    },
                    {
                        "name": "Gromit Yeuk-Yin Chan"
                    },
                    {
                        "name": "Dishita G Turakhia"
                    },
                    {
                        "name": "Sonia Castelo Quispe"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Leslie Welch"
                    },
                    {
                        "name": "Claudio Silva"
                    },
                    {
                        "name": "Jing Qian"
                    }
                ],
                "author_detail": {
                    "name": "Jing Qian"
                },
                "author": "Jing Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2110.07722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2110.07722v2",
                "updated": "2025-01-02T02:24:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    24,
                    58,
                    3,
                    2,
                    0
                ],
                "published": "2021-10-12T15:55:37Z",
                "published_parsed": [
                    2021,
                    10,
                    12,
                    15,
                    55,
                    37,
                    1,
                    285,
                    0
                ],
                "title": "The Sigma-max System Induced from Randomness & Fuzziness and its\n  Application in Time Series Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Sigma-max System Induced from Randomness & Fuzziness and its\n  Application in Time Series Prediction"
                },
                "summary": "This paper managed to induce probability theory (sigma system) and\npossibility theory (max system) respectively from the clearly-defined\nrandomness and fuzziness, while focusing the question why the key axiom of\n\"maxitivity\" is adopted for possibility measure. Such an objective is achieved\nby following three steps: a) the establishment of mathematical definitions of\nrandomness and fuzziness; b) the development of intuitive definition of\npossibility as measure of fuzziness based on compatibility interpretation; c)\nthe abstraction of the axiomatic definitions of probability/ possibility from\ntheir intuitive definitions, by taking advantage of properties of the\nwell-defined randomness and fuzziness. We derived the conclusion that \"max\" is\nthe only but un-strict disjunctive operator that is applicable across the fuzzy\nevent space, and is an exact operator for extracting the value from the fuzzy\nsample space that leads to the largest possibility of one. Then a demonstration\nexample of stock price prediction is presented, which confirms that max\ninference indeed exhibits distinctive performance, with an improvement up to\n18.99%, over sigma inference for the investigated application. Our work\nprovides a physical foundation for the axiomatic definition of possibility for\nthe measure of fuzziness, which hopefully would facilitate wider adoption of\npossibility theory in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper managed to induce probability theory (sigma system) and\npossibility theory (max system) respectively from the clearly-defined\nrandomness and fuzziness, while focusing the question why the key axiom of\n\"maxitivity\" is adopted for possibility measure. Such an objective is achieved\nby following three steps: a) the establishment of mathematical definitions of\nrandomness and fuzziness; b) the development of intuitive definition of\npossibility as measure of fuzziness based on compatibility interpretation; c)\nthe abstraction of the axiomatic definitions of probability/ possibility from\ntheir intuitive definitions, by taking advantage of properties of the\nwell-defined randomness and fuzziness. We derived the conclusion that \"max\" is\nthe only but un-strict disjunctive operator that is applicable across the fuzzy\nevent space, and is an exact operator for extracting the value from the fuzzy\nsample space that leads to the largest possibility of one. Then a demonstration\nexample of stock price prediction is presented, which confirms that max\ninference indeed exhibits distinctive performance, with an improvement up to\n18.99%, over sigma inference for the investigated application. Our work\nprovides a physical foundation for the axiomatic definition of possibility for\nthe measure of fuzziness, which hopefully would facilitate wider adoption of\npossibility theory in practice."
                },
                "authors": [
                    {
                        "name": "Wei Mei"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Yuanzeng Cheng"
                    },
                    {
                        "name": "Limin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Limin Liu"
                },
                "author": "Limin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2110.07722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2110.07722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B48, 03B52, 60A05, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10422v2",
                "updated": "2025-01-02T01:11:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    1,
                    11,
                    46,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-10T11:03:49Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    3,
                    49,
                    1,
                    345,
                    0
                ],
                "title": "AutoPrep: Natural Language Question-Aware Data Preparation with a\n  Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPrep: Natural Language Question-Aware Data Preparation with a\n  Multi-Agent Framework"
                },
                "summary": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\naugmentation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\naugmentation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation."
                },
                "authors": [
                    {
                        "name": "Meihao Fan"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Lei Cao"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Xiaoyong Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyong Du"
                },
                "author": "Xiaoyong Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13184v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13184v5",
                "updated": "2025-01-01T23:34:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    23,
                    34,
                    53,
                    2,
                    1,
                    0
                ],
                "published": "2024-02-20T17:49:46Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    17,
                    49,
                    46,
                    1,
                    51,
                    0
                ],
                "title": "What if LLMs Have Different World Views: Simulating Alien Civilizations\n  with LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What if LLMs Have Different World Views: Simulating Alien Civilizations\n  with LLM-based Agents"
                },
                "summary": "This study introduces \"CosmoAgent,\" an innovative artificial intelligence\nsystem that utilizes Large Language Models (LLMs) to simulate complex\ninteractions between human and extraterrestrial civilizations. This paper\nintroduces a mathematical model for quantifying the levels of civilization\ndevelopment and further employs a state transition matrix approach to evaluate\ntheir trajectories. Through this methodology, our study quantitatively analyzes\nthe growth trajectories of civilizations, providing insights into future\ndecision-making at critical points of growth and saturation. Furthermore, this\npaper acknowledges the vast diversity of potential living conditions across the\nuniverse, which could foster unique cosmologies, ethical codes, and worldviews\namong different civilizations. Recognizing the Earth-centric bias inherent in\ncurrent LLM designs, we propose the novel concept of using LLM agents with\ndiverse ethical paradigms and simulating interactions between entities with\ndistinct moral principles. This innovative research not only introduces a novel\nmethod for comprehending potential inter-civilizational dynamics but also holds\npractical value in enabling entities with divergent value systems to\nstrategize, prevent conflicts, and engage in games under conditions of\nasymmetric information. The accompanying code is available at\nhttps://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces \"CosmoAgent,\" an innovative artificial intelligence\nsystem that utilizes Large Language Models (LLMs) to simulate complex\ninteractions between human and extraterrestrial civilizations. This paper\nintroduces a mathematical model for quantifying the levels of civilization\ndevelopment and further employs a state transition matrix approach to evaluate\ntheir trajectories. Through this methodology, our study quantitatively analyzes\nthe growth trajectories of civilizations, providing insights into future\ndecision-making at critical points of growth and saturation. Furthermore, this\npaper acknowledges the vast diversity of potential living conditions across the\nuniverse, which could foster unique cosmologies, ethical codes, and worldviews\namong different civilizations. Recognizing the Earth-centric bias inherent in\ncurrent LLM designs, we propose the novel concept of using LLM agents with\ndiverse ethical paradigms and simulating interactions between entities with\ndistinct moral principles. This innovative research not only introduces a novel\nmethod for comprehending potential inter-civilizational dynamics but also holds\npractical value in enabling entities with divergent value systems to\nstrategize, prevent conflicts, and engage in games under conditions of\nasymmetric information. The accompanying code is available at\nhttps://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents."
                },
                "authors": [
                    {
                        "name": "Zhaoqian Xue"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Beichen Wang"
                    },
                    {
                        "name": "Suiyuan Zhu"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Hua Tang"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13184v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13184v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12944v2",
                "updated": "2025-01-01T23:21:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    23,
                    21,
                    43,
                    2,
                    1,
                    0
                ],
                "published": "2024-11-20T00:28:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    28,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "From Estimands to Robust Inference of Treatment Effects in Platform\n  Trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Estimands to Robust Inference of Treatment Effects in Platform\n  Trials"
                },
                "summary": "A platform trial is an innovative clinical trial design that uses a master\nprotocol (i.e., one overarching protocol) to evaluate multiple treatments in an\nongoing manner and can accelerate the evaluation of new treatments. However,\nits flexibility introduces inferential challenges, with two fundamental ones\nbeing the precise definition of treatment effects and robust, efficient\ninference on these effects. Central to these challenges is the definition of an\nappropriate target population for the estimand, as some commonly used\npopulations can be unexpectedly problematic. This article, for the first time,\npresents a clear framework for constructing a clinically meaningful estimand\nwith precise specificity regarding the population of interest. The proposed\nestimand defines the treatment effect as a contrast of expected outcomes\nbetween two treatments within the entire concurrently eligible (ECE) population\n- the largest population that preserves the integrity of randomization -\nestablishing a foundation for future research in platform trials. Then, we\ndevelop weighting and post-stratification methods for estimation of treatment\neffects with minimal assumptions. To fully leverage the efficiency potential of\nplatform trials, we also consider a model-assisted approach for baseline\ncovariate adjustment to gain efficiency while maintaining robustness against\nmodel misspecification. We derive and compare asymptotic distributions of\nproposed estimators in theory and propose robust variance estimators. The\nproposed estimators are empirically evaluated in a simulation study and\nillustrated in the SIMPLIFY trial, using the R package RobinCID.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A platform trial is an innovative clinical trial design that uses a master\nprotocol (i.e., one overarching protocol) to evaluate multiple treatments in an\nongoing manner and can accelerate the evaluation of new treatments. However,\nits flexibility introduces inferential challenges, with two fundamental ones\nbeing the precise definition of treatment effects and robust, efficient\ninference on these effects. Central to these challenges is the definition of an\nappropriate target population for the estimand, as some commonly used\npopulations can be unexpectedly problematic. This article, for the first time,\npresents a clear framework for constructing a clinically meaningful estimand\nwith precise specificity regarding the population of interest. The proposed\nestimand defines the treatment effect as a contrast of expected outcomes\nbetween two treatments within the entire concurrently eligible (ECE) population\n- the largest population that preserves the integrity of randomization -\nestablishing a foundation for future research in platform trials. Then, we\ndevelop weighting and post-stratification methods for estimation of treatment\neffects with minimal assumptions. To fully leverage the efficiency potential of\nplatform trials, we also consider a model-assisted approach for baseline\ncovariate adjustment to gain efficiency while maintaining robustness against\nmodel misspecification. We derive and compare asymptotic distributions of\nproposed estimators in theory and propose robust variance estimators. The\nproposed estimators are empirically evaluated in a simulation study and\nillustrated in the SIMPLIFY trial, using the R package RobinCID."
                },
                "authors": [
                    {
                        "name": "Yuhan Qian"
                    },
                    {
                        "name": "Yifan Yi"
                    },
                    {
                        "name": "Jun Shao"
                    },
                    {
                        "name": "Yanyao Yi"
                    },
                    {
                        "name": "Nicole Mayer-Hamblett"
                    },
                    {
                        "name": "Patrick J. Heagerty"
                    },
                    {
                        "name": "Ting Ye"
                    }
                ],
                "author_detail": {
                    "name": "Ting Ye"
                },
                "author": "Ting Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15166v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15166v3",
                "updated": "2025-01-01T20:57:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    57,
                    38,
                    2,
                    1,
                    0
                ],
                "published": "2024-10-19T17:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    17,
                    35,
                    12,
                    5,
                    293,
                    0
                ],
                "title": "Joint Probability Estimation of Many Binary Outcomes via Localized\n  Adversarial Lasso",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Probability Estimation of Many Binary Outcomes via Localized\n  Adversarial Lasso"
                },
                "summary": "In this work we consider estimating the probability of many (possibly\ndependent) binary outcomes which is at the core of many applications, e.g.,\nmulti-level treatments in causal inference, demands for bundle of products,\netc. Without further conditions, the probability distribution of an M\ndimensional binary vector is characterized by exponentially in M coefficients\nwhich can lead to a high-dimensional problem even without the presence of\ncovariates. Understanding the (in)dependence structure allows us to\nsubstantially improve the estimation as it allows for an effective\nfactorization of the probability distribution. In order to estimate the\nprobability distribution of a M dimensional binary vector, we leverage a\nBahadur representation that connects the sparsity of its coefficients with\nindependence across the components. We propose to use regularized and\nadversarial regularized estimators to obtain an adaptive estimator with respect\nto the dependence structure which allows for rates of convergence to depend on\nthis intrinsic (lower) dimension. These estimators are needed to handle several\nchallenges within this setting, including estimating nuisance parameters,\nestimating covariates, and nonseparable moment conditions. Our main results\nconsider the presence of (low dimensional) covariates for which we propose a\nlocally penalized estimator. We provide pointwise rates of convergence\naddressing several issues in the theoretical analyses as we strive for making a\ncomputationally tractable formulation. We apply our results in the estimation\nof causal effects with multiple binary treatments and show how our estimators\ncan improve the finite sample performance when compared with non-adaptive\nestimators that try to estimate all the probabilities directly. We also provide\nsimulations that are consistent with our theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we consider estimating the probability of many (possibly\ndependent) binary outcomes which is at the core of many applications, e.g.,\nmulti-level treatments in causal inference, demands for bundle of products,\netc. Without further conditions, the probability distribution of an M\ndimensional binary vector is characterized by exponentially in M coefficients\nwhich can lead to a high-dimensional problem even without the presence of\ncovariates. Understanding the (in)dependence structure allows us to\nsubstantially improve the estimation as it allows for an effective\nfactorization of the probability distribution. In order to estimate the\nprobability distribution of a M dimensional binary vector, we leverage a\nBahadur representation that connects the sparsity of its coefficients with\nindependence across the components. We propose to use regularized and\nadversarial regularized estimators to obtain an adaptive estimator with respect\nto the dependence structure which allows for rates of convergence to depend on\nthis intrinsic (lower) dimension. These estimators are needed to handle several\nchallenges within this setting, including estimating nuisance parameters,\nestimating covariates, and nonseparable moment conditions. Our main results\nconsider the presence of (low dimensional) covariates for which we propose a\nlocally penalized estimator. We provide pointwise rates of convergence\naddressing several issues in the theoretical analyses as we strive for making a\ncomputationally tractable formulation. We apply our results in the estimation\nof causal effects with multiple binary treatments and show how our estimators\ncan improve the finite sample performance when compared with non-adaptive\nestimators that try to estimate all the probabilities directly. We also provide\nsimulations that are consistent with our theoretical findings."
                },
                "authors": [
                    {
                        "name": "Alexandre Belloni"
                    },
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Matthew Harding"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Harding"
                },
                "author": "Matthew Harding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15166v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15166v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06970v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06970v3",
                "updated": "2025-01-01T16:32:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    32,
                    6,
                    2,
                    1,
                    0
                ],
                "published": "2024-08-13T15:27:43Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    27,
                    43,
                    1,
                    226,
                    0
                ],
                "title": "Prompt-Based Segmentation at Multiple Resolutions and Lighting\n  Conditions using Segment Anything Model 2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Based Segmentation at Multiple Resolutions and Lighting\n  Conditions using Segment Anything Model 2"
                },
                "summary": "This paper provides insights on the effectiveness of the zero shot,\nprompt-based Segment Anything Model (SAM) and its updated versions, SAM 2 and\nSAM 2.1, along with the non-promptable conventional neural network (CNN), for\nsegmenting solar panels in RGB aerial imagery. The study evaluates these models\nacross diverse lighting conditions, spatial resolutions, and prompt strategies.\nSAM 2 showed slight improvements over SAM, while SAM 2.1 demonstrated notable\nimprovements, particularly in sub-optimal lighting and low resolution\nconditions. SAM models, when prompted by user-defined boxes, outperformed CNN\nin all scenarios; in particular, user-box prompts were found crucial for\nachieving reasonable performance in low resolution data. Additionally, under\nhigh resolution, YOLOv9 automatic prompting outperformed user-points prompting\nby providing reliable prompts to SAM. Under low resolution, SAM 2.1 prompted by\nuser points showed similar performance to SAM 2.1 prompted by YOLOv9,\nhighlighting its zero shot improvements with a single click. In high resolution\nwith optimal lighting imagery, Eff-UNet outperformed SAMs prompted by YOLOv9,\nwhile under sub-optimal lighting conditions, Eff-UNet, and SAM 2.1 prompted by\nYOLOv9, had similar performance. However, SAM is more resource-intensive, and\ndespite improved inference time of SAM 2.1, Eff-UNet is more suitable for\nautomatic segmentation in high resolution data. This research details strengths\nand limitations of each model and outlines the robustness of user-prompted\nimage segmentation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides insights on the effectiveness of the zero shot,\nprompt-based Segment Anything Model (SAM) and its updated versions, SAM 2 and\nSAM 2.1, along with the non-promptable conventional neural network (CNN), for\nsegmenting solar panels in RGB aerial imagery. The study evaluates these models\nacross diverse lighting conditions, spatial resolutions, and prompt strategies.\nSAM 2 showed slight improvements over SAM, while SAM 2.1 demonstrated notable\nimprovements, particularly in sub-optimal lighting and low resolution\nconditions. SAM models, when prompted by user-defined boxes, outperformed CNN\nin all scenarios; in particular, user-box prompts were found crucial for\nachieving reasonable performance in low resolution data. Additionally, under\nhigh resolution, YOLOv9 automatic prompting outperformed user-points prompting\nby providing reliable prompts to SAM. Under low resolution, SAM 2.1 prompted by\nuser points showed similar performance to SAM 2.1 prompted by YOLOv9,\nhighlighting its zero shot improvements with a single click. In high resolution\nwith optimal lighting imagery, Eff-UNet outperformed SAMs prompted by YOLOv9,\nwhile under sub-optimal lighting conditions, Eff-UNet, and SAM 2.1 prompted by\nYOLOv9, had similar performance. However, SAM is more resource-intensive, and\ndespite improved inference time of SAM 2.1, Eff-UNet is more suitable for\nautomatic segmentation in high resolution data. This research details strengths\nand limitations of each model and outlines the robustness of user-prompted\nimage segmentation models."
                },
                "authors": [
                    {
                        "name": "Osher Rafaeli"
                    },
                    {
                        "name": "Tal Svoray"
                    },
                    {
                        "name": "Roni Blushtein-Livnon"
                    },
                    {
                        "name": "Ariel Nahlieli"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Nahlieli"
                },
                "author": "Ariel Nahlieli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06970v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06970v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04264v3",
                "updated": "2025-01-01T15:53:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    53,
                    58,
                    2,
                    1,
                    0
                ],
                "published": "2024-06-06T17:09:32Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    17,
                    9,
                    32,
                    3,
                    158,
                    0
                ],
                "title": "MLVU: Benchmarking Multi-task Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLVU: Benchmarking Multi-task Long Video Understanding"
                },
                "summary": "The evaluation of Long Video Understanding (LVU) performance poses an\nimportant but challenging research problem. Despite previous efforts, the\nexisting video understanding benchmarks are severely constrained by several\nissues, especially the insufficient lengths of videos, a lack of diversity in\nvideo types and evaluation tasks, and the inappropriateness for evaluating LVU\nperformances. To address the above problems, we propose a new benchmark called\nMLVU (Multi-task Long Video Understanding Benchmark) for the comprehensive and\nin-depth evaluation of LVU. MLVU presents the following critical values:\n\\textit{1)} The substantial and flexible extension of video lengths, which\nenables the benchmark to evaluate LVU performance across a wide range of\ndurations. \\textit{2)} The inclusion of various video genres, e.g., movies,\nsurveillance footage, egocentric videos, cartoons, game videos, etc., which\nreflects the models' LVU performances in different scenarios. \\textit{3)} The\ndevelopment of diversified evaluation tasks, which enables a comprehensive\nexamination of MLLMs' key abilities in long-video understanding. The empirical\nstudy with 23 latest MLLMs reveals significant room for improvement in today's\ntechnique, as all existing methods struggle with most of the evaluation tasks\nand exhibit severe performance degradation when handling longer videos.\nAdditionally, it suggests that factors such as context length,\nimage-understanding ability, and the choice of LLM backbone can play critical\nroles in future advancements. We anticipate that MLVU will advance the research\nof long video understanding by providing a comprehensive and in-depth analysis\nof MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of Long Video Understanding (LVU) performance poses an\nimportant but challenging research problem. Despite previous efforts, the\nexisting video understanding benchmarks are severely constrained by several\nissues, especially the insufficient lengths of videos, a lack of diversity in\nvideo types and evaluation tasks, and the inappropriateness for evaluating LVU\nperformances. To address the above problems, we propose a new benchmark called\nMLVU (Multi-task Long Video Understanding Benchmark) for the comprehensive and\nin-depth evaluation of LVU. MLVU presents the following critical values:\n\\textit{1)} The substantial and flexible extension of video lengths, which\nenables the benchmark to evaluate LVU performance across a wide range of\ndurations. \\textit{2)} The inclusion of various video genres, e.g., movies,\nsurveillance footage, egocentric videos, cartoons, game videos, etc., which\nreflects the models' LVU performances in different scenarios. \\textit{3)} The\ndevelopment of diversified evaluation tasks, which enables a comprehensive\nexamination of MLLMs' key abilities in long-video understanding. The empirical\nstudy with 23 latest MLLMs reveals significant room for improvement in today's\ntechnique, as all existing methods struggle with most of the evaluation tasks\nand exhibit severe performance degradation when handling longer videos.\nAdditionally, it suggests that factors such as context length,\nimage-understanding ability, and the choice of LLM backbone can play critical\nroles in future advancements. We anticipate that MLVU will advance the research\nof long video understanding by providing a comprehensive and in-depth analysis\nof MLLMs."
                },
                "authors": [
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Boya Wu"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Yongping Xiong"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13621v2",
                "updated": "2025-01-01T15:40:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    40,
                    57,
                    2,
                    1,
                    0
                ],
                "published": "2023-11-22T08:34:33Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    8,
                    34,
                    33,
                    2,
                    326,
                    0
                ],
                "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EA-KD: Entropy-based Adaptive Knowledge Distillation"
                },
                "summary": "Knowledge distillation (KD) enables a smaller \"student\" model to mimic a\nlarger \"teacher\" model by transferring knowledge from the teacher's output or\nfeatures. However, most KD methods treat all samples uniformly, overlooking the\nvarying learning value of each sample and thereby limiting effectiveness. In\nthis paper, we propose Entropy-based Adaptive Knowledge Distillation (EA-KD), a\nsimple yet effective plug-and-play KD method that prioritizes learning from\nvaluable samples. EA-KD quantifies each sample's learning value by\nstrategically combining the entropy of the teacher and student output, then\ndynamically reweights the distillation loss to place greater emphasis on\nhigh-value samples. Extensive experiments across diverse KD frameworks and\ntasks$\\unicode{x2014}$including image classification, object detection, and\nlarge language model (LLM) distillation$\\unicode{x2014}$demonstrate that EA-KD\nconsistently enhances performance, achieving state-of-the-art results with\nnegligible computational cost. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) enables a smaller \"student\" model to mimic a\nlarger \"teacher\" model by transferring knowledge from the teacher's output or\nfeatures. However, most KD methods treat all samples uniformly, overlooking the\nvarying learning value of each sample and thereby limiting effectiveness. In\nthis paper, we propose Entropy-based Adaptive Knowledge Distillation (EA-KD), a\nsimple yet effective plug-and-play KD method that prioritizes learning from\nvaluable samples. EA-KD quantifies each sample's learning value by\nstrategically combining the entropy of the teacher and student output, then\ndynamically reweights the distillation loss to place greater emphasis on\nhigh-value samples. Extensive experiments across diverse KD frameworks and\ntasks$\\unicode{x2014}$including image classification, object detection, and\nlarge language model (LLM) distillation$\\unicode{x2014}$demonstrate that EA-KD\nconsistently enhances performance, achieving state-of-the-art results with\nnegligible computational cost. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Chi-Ping Su"
                    },
                    {
                        "name": "Ching-Hsun Tseng"
                    },
                    {
                        "name": "Bin Pu"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Zhuangzhuang Chen"
                    },
                    {
                        "name": "Shin-Jye Lee"
                    }
                ],
                "author_detail": {
                    "name": "Shin-Jye Lee"
                },
                "author": "Shin-Jye Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15496v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15496v4",
                "updated": "2025-01-01T15:22:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    22,
                    34,
                    2,
                    1,
                    0
                ],
                "published": "2024-08-28T02:47:27Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    2,
                    47,
                    27,
                    2,
                    241,
                    0
                ],
                "title": "ReMamba: Equip Mamba with Effective Long-Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReMamba: Equip Mamba with Effective Long-Sequence Modeling"
                },
                "summary": "While the Mamba architecture demonstrates superior inference efficiency and\ncompetitive performance on short-context natural language processing (NLP)\ntasks, empirical evidence suggests its capacity to comprehend long contexts is\nlimited compared to transformer-based models. In this study, we investigate the\nlong-context efficiency issues of the Mamba models and propose ReMamba, which\nenhances Mamba's ability to comprehend long contexts. ReMamba incorporates\nselective compression and adaptation techniques within a two-stage re-forward\nprocess, incurring minimal additional inference costs overhead. Experimental\nresults on the LongBench and L-Eval benchmarks demonstrate ReMamba's efficacy,\nimproving over the baselines by 3.2 and 1.6 points, respectively, and attaining\nperformance almost on par with same-size transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the Mamba architecture demonstrates superior inference efficiency and\ncompetitive performance on short-context natural language processing (NLP)\ntasks, empirical evidence suggests its capacity to comprehend long contexts is\nlimited compared to transformer-based models. In this study, we investigate the\nlong-context efficiency issues of the Mamba models and propose ReMamba, which\nenhances Mamba's ability to comprehend long contexts. ReMamba incorporates\nselective compression and adaptation techniques within a two-stage re-forward\nprocess, incurring minimal additional inference costs overhead. Experimental\nresults on the LongBench and L-Eval benchmarks demonstrate ReMamba's efficacy,\nimproving over the baselines by 3.2 and 1.6 points, respectively, and attaining\nperformance almost on par with same-size transformer models."
                },
                "authors": [
                    {
                        "name": "Danlong Yuan"
                    },
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Huishuai Zhang"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Dongyan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongyan Zhao"
                },
                "author": "Dongyan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15496v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15496v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02148v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02148v3",
                "updated": "2025-01-01T15:19:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    19,
                    54,
                    2,
                    1,
                    0
                ],
                "published": "2024-07-02T10:46:22Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    10,
                    46,
                    22,
                    1,
                    184,
                    0
                ],
                "title": "The coherent magnetic field of the Milky Way halo, Local Bubble and Fan\n  Region",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coherent magnetic field of the Milky Way halo, Local Bubble and Fan\n  Region"
                },
                "summary": "Recent catalog of Faraday rotation measures (RM) of extragalactic sources\ntogether with the synchrotron polarization data from WMAP and Planck provides\nus with a wealth of information on magnetic fields of the Galaxy. However, the\nintegral character of these observables together with our position inside the\nGalaxy makes the inference of the coherent Galactic magnetic field (GMF)\ncomplicated and ambiguous. We combine several phenomenological components of\nthe GMF -- the spiral arms, the toroidal halo, the X-shaped field and the field\nof the Local Bubble -- to construct a new model of the regular GMF outside the\nthin disk. To have control over the relative contributions of the RM and\npolarization data to the fit we pay special attention to the estimation of\nerrors in data bins. To this end we develop a systematic method which is\nuniformly applicable to different data sets. This method takes into account\nindividual measurement errors, the variance in the bin as well as fluctuations\nin the data at angular scales larger than the bin size. This leads to decrease\nof the errors and, as a result, to better sensitivity of the data to the model\ncontent. We cross checked the stability of our method with the new LOFAR data.\nWe found that the four components listed above are sufficient to fit both the\nRM and polarization data over the whole sky with only a small fraction masked\nout. Moreover, we have achieved several important improvements compared to\nprevious approaches. Due to account of our location inside of the Local Bubble\nour model does not require introduction of striated fields. For the first time\nwe showed that the Fan Region can be modeled as a Galactic-scale feature. The\npitch angle of the magnetic field in our fit converged to the value around 20\ndegrees. Interestingly, with value is very close to the direction of the arms\ninferred recently from Gaia data on upper main sequence stars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent catalog of Faraday rotation measures (RM) of extragalactic sources\ntogether with the synchrotron polarization data from WMAP and Planck provides\nus with a wealth of information on magnetic fields of the Galaxy. However, the\nintegral character of these observables together with our position inside the\nGalaxy makes the inference of the coherent Galactic magnetic field (GMF)\ncomplicated and ambiguous. We combine several phenomenological components of\nthe GMF -- the spiral arms, the toroidal halo, the X-shaped field and the field\nof the Local Bubble -- to construct a new model of the regular GMF outside the\nthin disk. To have control over the relative contributions of the RM and\npolarization data to the fit we pay special attention to the estimation of\nerrors in data bins. To this end we develop a systematic method which is\nuniformly applicable to different data sets. This method takes into account\nindividual measurement errors, the variance in the bin as well as fluctuations\nin the data at angular scales larger than the bin size. This leads to decrease\nof the errors and, as a result, to better sensitivity of the data to the model\ncontent. We cross checked the stability of our method with the new LOFAR data.\nWe found that the four components listed above are sufficient to fit both the\nRM and polarization data over the whole sky with only a small fraction masked\nout. Moreover, we have achieved several important improvements compared to\nprevious approaches. Due to account of our location inside of the Local Bubble\nour model does not require introduction of striated fields. For the first time\nwe showed that the Fan Region can be modeled as a Galactic-scale feature. The\npitch angle of the magnetic field in our fit converged to the value around 20\ndegrees. Interestingly, with value is very close to the direction of the arms\ninferred recently from Gaia data on upper main sequence stars."
                },
                "authors": [
                    {
                        "name": "Alexander Korochkin"
                    },
                    {
                        "name": "Dmitri Semikoz"
                    },
                    {
                        "name": "Peter Tinyakov"
                    }
                ],
                "author_detail": {
                    "name": "Peter Tinyakov"
                },
                "author": "Peter Tinyakov",
                "arxiv_comment": "v3: 22 pages, 16 figures, version accepted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02148v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02148v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17851v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17851v3",
                "updated": "2025-01-01T15:06:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    6,
                    12,
                    2,
                    1,
                    0
                ],
                "published": "2024-01-31T14:13:01Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    14,
                    13,
                    1,
                    2,
                    31,
                    0
                ],
                "title": "Instruction-Guided Scene Text Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-Guided Scene Text Recognition"
                },
                "summary": "Multi-modal models have shown appealing performance in visual recognition\ntasks, as free-form text-guided training evokes the ability to understand\nfine-grained visual content. However, current models cannot be trivially\napplied to scene text recognition (STR) due to the compositional difference\nbetween natural and text images. We propose a novel instruction-guided scene\ntext recognition (IGTR) paradigm that formulates STR as an instruction learning\nproblem and understands text images by predicting character attributes, e.g.,\ncharacter frequency, position, etc. IGTR first devises $\\left \\langle\ncondition,question,answer\\right \\rangle$ instruction triplets, providing rich\nand diverse descriptions of character attributes. To effectively learn these\nattributes through question-answering, IGTR develops a lightweight instruction\nencoder, a cross-modal feature fusion module and a multi-task answer head,\nwhich guides nuanced text image understanding. Furthermore, IGTR realizes\ndifferent recognition pipelines simply by using different instructions,\nenabling a character-understanding-based text reasoning paradigm that differs\nfrom current methods considerably. Experiments on English and Chinese\nbenchmarks show that IGTR outperforms existing models by significant margins,\nwhile maintaining a small model size and fast inference speed. Moreover, by\nadjusting the sampling of instructions, IGTR offers an elegant way to tackle\nthe recognition of rarely appearing and morphologically similar characters,\nwhich were previous challenges. Code: https://github.com/Topdu/OpenOCR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal models have shown appealing performance in visual recognition\ntasks, as free-form text-guided training evokes the ability to understand\nfine-grained visual content. However, current models cannot be trivially\napplied to scene text recognition (STR) due to the compositional difference\nbetween natural and text images. We propose a novel instruction-guided scene\ntext recognition (IGTR) paradigm that formulates STR as an instruction learning\nproblem and understands text images by predicting character attributes, e.g.,\ncharacter frequency, position, etc. IGTR first devises $\\left \\langle\ncondition,question,answer\\right \\rangle$ instruction triplets, providing rich\nand diverse descriptions of character attributes. To effectively learn these\nattributes through question-answering, IGTR develops a lightweight instruction\nencoder, a cross-modal feature fusion module and a multi-task answer head,\nwhich guides nuanced text image understanding. Furthermore, IGTR realizes\ndifferent recognition pipelines simply by using different instructions,\nenabling a character-understanding-based text reasoning paradigm that differs\nfrom current methods considerably. Experiments on English and Chinese\nbenchmarks show that IGTR outperforms existing models by significant margins,\nwhile maintaining a small model size and fast inference speed. Moreover, by\nadjusting the sampling of instructions, IGTR offers an elegant way to tackle\nthe recognition of rarely appearing and morphologically similar characters,\nwhich were previous challenges. Code: https://github.com/Topdu/OpenOCR."
                },
                "authors": [
                    {
                        "name": "Yongkun Du"
                    },
                    {
                        "name": "Zhineng Chen"
                    },
                    {
                        "name": "Yuchen Su"
                    },
                    {
                        "name": "Caiyan Jia"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "Accepted by TPAMI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17851v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17851v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19153v2",
                "updated": "2025-01-01T14:33:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    14,
                    33,
                    33,
                    2,
                    1,
                    0
                ],
                "published": "2024-12-26T10:17:21Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    10,
                    17,
                    21,
                    3,
                    361,
                    0
                ],
                "title": "Sketch-MoMa: Teleoperation for Mobile Manipulator via Interpretation of\n  Hand-Drawn Sketches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sketch-MoMa: Teleoperation for Mobile Manipulator via Interpretation of\n  Hand-Drawn Sketches"
                },
                "summary": "To use assistive robots in everyday life, a remote control system with common\ndevices, such as 2D devices, is helpful to control the robots anytime and\nanywhere as intended. Hand-drawn sketches are one of the intuitive ways to\ncontrol robots with 2D devices. However, since similar sketches have different\nintentions from scene to scene, existing work needs additional modalities to\nset the sketches' semantics. This requires complex operations for users and\nleads to decreasing usability. In this paper, we propose Sketch-MoMa, a\nteleoperation system using the user-given hand-drawn sketches as instructions\nto control a robot. We use Vision-Language Models (VLMs) to understand the\nuser-given sketches superimposed on an observation image and infer drawn shapes\nand low-level tasks of the robot. We utilize the sketches and the generated\nshapes for recognition and motion planning of the generated low-level tasks for\nprecise and intuitive operations. We validate our approach using\nstate-of-the-art VLMs with 7 tasks and 5 sketch shapes. We also demonstrate\nthat our approach effectively specifies the detailed motions, such as how to\ngrasp and how much to rotate. Moreover, we show the competitive usability of\nour approach compared with the existing 2D interface through a user experiment\nwith 14 participants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To use assistive robots in everyday life, a remote control system with common\ndevices, such as 2D devices, is helpful to control the robots anytime and\nanywhere as intended. Hand-drawn sketches are one of the intuitive ways to\ncontrol robots with 2D devices. However, since similar sketches have different\nintentions from scene to scene, existing work needs additional modalities to\nset the sketches' semantics. This requires complex operations for users and\nleads to decreasing usability. In this paper, we propose Sketch-MoMa, a\nteleoperation system using the user-given hand-drawn sketches as instructions\nto control a robot. We use Vision-Language Models (VLMs) to understand the\nuser-given sketches superimposed on an observation image and infer drawn shapes\nand low-level tasks of the robot. We utilize the sketches and the generated\nshapes for recognition and motion planning of the generated low-level tasks for\nprecise and intuitive operations. We validate our approach using\nstate-of-the-art VLMs with 7 tasks and 5 sketch shapes. We also demonstrate\nthat our approach effectively specifies the detailed motions, such as how to\ngrasp and how much to rotate. Moreover, we show the competitive usability of\nour approach compared with the existing 2D interface through a user experiment\nwith 14 participants."
                },
                "authors": [
                    {
                        "name": "Kosei Tanada"
                    },
                    {
                        "name": "Yuka Iwanaga"
                    },
                    {
                        "name": "Masayoshi Tsuchinaga"
                    },
                    {
                        "name": "Yuji Nakamura"
                    },
                    {
                        "name": "Takemitsu Mori"
                    },
                    {
                        "name": "Remi Sakai"
                    },
                    {
                        "name": "Takashi Yamamoto"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Yamamoto"
                },
                "author": "Takashi Yamamoto",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Project Page: https://toyotafrc.github.io/SketchMoMa-Proj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04416v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04416v4",
                "updated": "2025-01-01T13:46:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    46,
                    37,
                    2,
                    1,
                    0
                ],
                "published": "2024-07-05T11:07:13Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    11,
                    7,
                    13,
                    4,
                    187,
                    0
                ],
                "title": "Sound-VECaps: Improving Audio Generation with Visual Enhanced Captions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sound-VECaps: Improving Audio Generation with Visual Enhanced Captions"
                },
                "summary": "Generative models have shown significant achievements in audio generation\ntasks. However, existing models struggle with complex and detailed prompts,\nleading to potential performance degradation. We hypothesize that this problem\nstems from the simplicity and scarcity of the training data. This work aims to\ncreate a large-scale audio dataset with rich captions for improving audio\ngeneration models. We first develop an automated pipeline to generate detailed\ncaptions by transforming predicted visual captions, audio captions, and tagging\nlabels into comprehensive descriptions using a Large Language Model (LLM). The\nresulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption\npairs with enriched details including audio event orders, occurred places and\nenvironment information. We then demonstrate that training the text-to-audio\ngeneration models with Sound-VECaps significantly improves the performance on\ncomplex prompts. Furthermore, we conduct ablation studies of the models on\nseveral downstream audio-language tasks, showing the potential of Sound-VECaps\nin advancing audio-text representation learning. Our dataset and models are\navailable online from here https://yyua8222.github.io/Sound-VECaps-demo/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have shown significant achievements in audio generation\ntasks. However, existing models struggle with complex and detailed prompts,\nleading to potential performance degradation. We hypothesize that this problem\nstems from the simplicity and scarcity of the training data. This work aims to\ncreate a large-scale audio dataset with rich captions for improving audio\ngeneration models. We first develop an automated pipeline to generate detailed\ncaptions by transforming predicted visual captions, audio captions, and tagging\nlabels into comprehensive descriptions using a Large Language Model (LLM). The\nresulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption\npairs with enriched details including audio event orders, occurred places and\nenvironment information. We then demonstrate that training the text-to-audio\ngeneration models with Sound-VECaps significantly improves the performance on\ncomplex prompts. Furthermore, we conduct ablation studies of the models on\nseveral downstream audio-language tasks, showing the potential of Sound-VECaps\nin advancing audio-text representation learning. Our dataset and models are\navailable online from here https://yyua8222.github.io/Sound-VECaps-demo/."
                },
                "authors": [
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Dongya Jia"
                    },
                    {
                        "name": "Xiaobin Zhuang"
                    },
                    {
                        "name": "Yuanzhe Chen"
                    },
                    {
                        "name": "Zhengxi Liu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xubo Liu"
                    },
                    {
                        "name": "Xiyuan Kang"
                    },
                    {
                        "name": "Mark D. Plumbley"
                    },
                    {
                        "name": "Wenwu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Wang"
                },
                "author": "Wenwu Wang",
                "arxiv_comment": "5 pages with 1 appendix, accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04416v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04416v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07614v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07614v2",
                "updated": "2025-01-01T13:35:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    35,
                    23,
                    2,
                    1,
                    0
                ],
                "published": "2024-09-11T20:54:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    54,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "FlowSep: Language-Queried Sound Separation with Rectified Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowSep: Language-Queried Sound Separation with Rectified Flow Matching"
                },
                "summary": "Language-queried audio source separation (LASS) focuses on separating sounds\nusing textual descriptions of the desired sources. Current methods mainly use\ndiscriminative approaches, such as time-frequency masking, to separate target\nsounds and minimize interference from other sources. However, these models face\nchallenges when separating overlapping soundtracks, which may lead to artifacts\nsuch as spectral holes or incomplete separation. Rectified flow matching (RFM),\na generative model that establishes linear relations between the distribution\nof data and noise, offers superior theoretical properties and simplicity, but\nhas not yet been explored in sound separation. In this work, we introduce\nFlowSep, a new generative model based on RFM for LASS tasks. FlowSep learns\nlinear flow trajectories from noise to target source features within the\nvariational autoencoder (VAE) latent space. During inference, the RFM-generated\nlatent features are reconstructed into a mel-spectrogram via the pre-trained\nVAE decoder, followed by a pre-trained vocoder to synthesize the waveform.\nTrained on 1,680 hours of audio data, FlowSep outperforms the state-of-the-art\nmodels across multiple benchmarks, as evaluated with subjective and objective\nmetrics. Additionally, our results show that FlowSep surpasses a\ndiffusion-based LASS model in both separation quality and inference efficiency,\nhighlighting its strong potential for audio source separation tasks. Code,\npre-trained models and demos can be found at:\nhttps://audio-agi.github.io/FlowSep_demo/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried audio source separation (LASS) focuses on separating sounds\nusing textual descriptions of the desired sources. Current methods mainly use\ndiscriminative approaches, such as time-frequency masking, to separate target\nsounds and minimize interference from other sources. However, these models face\nchallenges when separating overlapping soundtracks, which may lead to artifacts\nsuch as spectral holes or incomplete separation. Rectified flow matching (RFM),\na generative model that establishes linear relations between the distribution\nof data and noise, offers superior theoretical properties and simplicity, but\nhas not yet been explored in sound separation. In this work, we introduce\nFlowSep, a new generative model based on RFM for LASS tasks. FlowSep learns\nlinear flow trajectories from noise to target source features within the\nvariational autoencoder (VAE) latent space. During inference, the RFM-generated\nlatent features are reconstructed into a mel-spectrogram via the pre-trained\nVAE decoder, followed by a pre-trained vocoder to synthesize the waveform.\nTrained on 1,680 hours of audio data, FlowSep outperforms the state-of-the-art\nmodels across multiple benchmarks, as evaluated with subjective and objective\nmetrics. Additionally, our results show that FlowSep surpasses a\ndiffusion-based LASS model in both separation quality and inference efficiency,\nhighlighting its strong potential for audio source separation tasks. Code,\npre-trained models and demos can be found at:\nhttps://audio-agi.github.io/FlowSep_demo/ ."
                },
                "authors": [
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Xubo Liu"
                    },
                    {
                        "name": "Haohe Liu"
                    },
                    {
                        "name": "Mark D. Plumbley"
                    },
                    {
                        "name": "Wenwu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Wang"
                },
                "author": "Wenwu Wang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07614v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07614v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.09583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.09583v2",
                "updated": "2025-01-01T13:02:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    2,
                    5,
                    2,
                    1,
                    0
                ],
                "published": "2023-08-18T14:23:21Z",
                "published_parsed": [
                    2023,
                    8,
                    18,
                    14,
                    23,
                    21,
                    4,
                    230,
                    0
                ],
                "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models\n  via Reinforced Evol-Instruct",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WizardMath: Empowering Mathematical Reasoning for Large Language Models\n  via Reinforced Evol-Instruct"
                },
                "summary": "Large language models (LLMs), such as GPT-4, have shown remarkable\nperformance in natural language processing (NLP) tasks, including challenging\nmathematical reasoning. However, most existing open-source models are only\npre-trained on large-scale internet data and without math-related optimization.\nIn this paper, we present WizardMath, which enhances the mathematical CoT\nreasoning abilities of LLMs without using external python tools, by applying\nour proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method\nto the domain of math. Through extensive experiments on two mathematical\nreasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary\ncapabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier\nopen-source LLMs by a substantial margin with higher data efficiency.\nFurthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini\nPro and GPT-4-early-version. Additionally, our preliminary exploration\nhighlights the pivotal role of instruction evolution and process supervision in\nachieving exceptional math performance. For more details refer to\nhttps://github.com/nlpxucan/WizardLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), such as GPT-4, have shown remarkable\nperformance in natural language processing (NLP) tasks, including challenging\nmathematical reasoning. However, most existing open-source models are only\npre-trained on large-scale internet data and without math-related optimization.\nIn this paper, we present WizardMath, which enhances the mathematical CoT\nreasoning abilities of LLMs without using external python tools, by applying\nour proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method\nto the domain of math. Through extensive experiments on two mathematical\nreasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary\ncapabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier\nopen-source LLMs by a substantial margin with higher data efficiency.\nFurthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini\nPro and GPT-4-early-version. Additionally, our preliminary exploration\nhighlights the pivotal role of instruction evolution and process supervision in\nachieving exceptional math performance. For more details refer to\nhttps://github.com/nlpxucan/WizardLM"
                },
                "authors": [
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Jianguang Lou"
                    },
                    {
                        "name": "Chongyang Tao"
                    },
                    {
                        "name": "Xiubo Geng"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Shifeng Chen"
                    },
                    {
                        "name": "Yansong Tang"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "arxiv_comment": "LLM, Mathematical Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.09583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.09583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00746v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00746v8",
                "updated": "2025-01-01T12:12:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    12,
                    12,
                    56,
                    2,
                    1,
                    0
                ],
                "published": "2024-02-01T16:40:32Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    16,
                    40,
                    32,
                    3,
                    32,
                    0
                ],
                "title": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System"
                },
                "summary": "Recent advancements in artificial intelligence (AI), especially large\nlanguage models (LLMs), have significantly advanced healthcare applications and\ndemonstrated potentials in intelligent medical treatment. However, there are\nconspicuous challenges such as vast data volumes and inconsistent symptom\ncharacterization standards, preventing full integration of healthcare AI\nsystems with individual patients' needs. To promote professional and\npersonalized healthcare, we propose an innovative framework, Heath-LLM, which\ncombines large-scale feature extraction and medical knowledge trade-off\nscoring. Compared to traditional health management applications, our system has\nthree main advantages: (1) It integrates health reports and medical knowledge\ninto a large model to ask relevant questions to large language model for\ndisease prediction; (2) It leverages a retrieval augmented generation (RAG)\nmechanism to enhance feature extraction; (3) It incorporates a semi-automated\nfeature updating framework that can merge and delete features to improve\naccuracy of disease prediction. We experiment on a large number of health\nreports to assess the effectiveness of Health-LLM system. The results indicate\nthat the proposed system surpasses the existing ones and has the potential to\nsignificantly advance disease prediction and personalized health management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in artificial intelligence (AI), especially large\nlanguage models (LLMs), have significantly advanced healthcare applications and\ndemonstrated potentials in intelligent medical treatment. However, there are\nconspicuous challenges such as vast data volumes and inconsistent symptom\ncharacterization standards, preventing full integration of healthcare AI\nsystems with individual patients' needs. To promote professional and\npersonalized healthcare, we propose an innovative framework, Heath-LLM, which\ncombines large-scale feature extraction and medical knowledge trade-off\nscoring. Compared to traditional health management applications, our system has\nthree main advantages: (1) It integrates health reports and medical knowledge\ninto a large model to ask relevant questions to large language model for\ndisease prediction; (2) It leverages a retrieval augmented generation (RAG)\nmechanism to enhance feature extraction; (3) It incorporates a semi-automated\nfeature updating framework that can merge and delete features to improve\naccuracy of disease prediction. We experiment on a large number of health\nreports to assess the effectiveness of Health-LLM system. The results indicate\nthat the proposed system surpasses the existing ones and has the potential to\nsignificantly advance disease prediction and personalized health management."
                },
                "authors": [
                    {
                        "name": "Qinkai Yu"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Lizhou Fan"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Suiyuan Zhu"
                    },
                    {
                        "name": "Yanda Meng"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00746v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00746v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.09612v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.09612v3",
                "updated": "2025-01-01T11:14:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    11,
                    14,
                    29,
                    2,
                    1,
                    0
                ],
                "published": "2024-05-15T18:00:01Z",
                "published_parsed": [
                    2024,
                    5,
                    15,
                    18,
                    0,
                    1,
                    2,
                    136,
                    0
                ],
                "title": "Imprint of \"Local Opacity\" Effect in Gamma-Ray Spectrum of Blazar Jet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imprint of \"Local Opacity\" Effect in Gamma-Ray Spectrum of Blazar Jet"
                },
                "summary": "Relativistic jets from accreting supermassive black holes at cosmological\ndistances can be powerful emitters of $\\gamma$-rays. However, the precise\nmechanisms and locations responsible for the dissipation of energy within these\njets, leading to observable $\\gamma$-ray radiation, remain elusive. We detect\nevidence for an intrinsic absorption feature in the $\\gamma$-ray spectrum at\nenergies exceeding $10\\,$GeV, presumably due to the photon-photon pair\nproduction of $\\gamma$-rays with low ionization lines at the outer edge of\nBroad-line region (BLR), during the high-flux state of the flat-spectrum radio\nquasar PKS 1424$-$418. The feature can be discriminated from the turnover at\nhigher energies resulting from $\\gamma$-ray absorption in the extragalactic\nbackground light. It is absent in the low-flux states supporting the\ninterpretation that powerful dissipation events within or at the edge of the\nBLR evolve into fainter $\\gamma$-ray emitting zones outside the BLR, possibly\nassociated with the moving VLBI radio knots. The inferred location of\n$\\gamma$-ray emission zone is consistent with the observed variability time\nscale of the brightest flare, provided that the flare is attributed to external\nCompton scattering with BLR photons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relativistic jets from accreting supermassive black holes at cosmological\ndistances can be powerful emitters of $\\gamma$-rays. However, the precise\nmechanisms and locations responsible for the dissipation of energy within these\njets, leading to observable $\\gamma$-ray radiation, remain elusive. We detect\nevidence for an intrinsic absorption feature in the $\\gamma$-ray spectrum at\nenergies exceeding $10\\,$GeV, presumably due to the photon-photon pair\nproduction of $\\gamma$-rays with low ionization lines at the outer edge of\nBroad-line region (BLR), during the high-flux state of the flat-spectrum radio\nquasar PKS 1424$-$418. The feature can be discriminated from the turnover at\nhigher energies resulting from $\\gamma$-ray absorption in the extragalactic\nbackground light. It is absent in the low-flux states supporting the\ninterpretation that powerful dissipation events within or at the edge of the\nBLR evolve into fainter $\\gamma$-ray emitting zones outside the BLR, possibly\nassociated with the moving VLBI radio knots. The inferred location of\n$\\gamma$-ray emission zone is consistent with the observed variability time\nscale of the brightest flare, provided that the flare is attributed to external\nCompton scattering with BLR photons."
                },
                "authors": [
                    {
                        "name": "Sushmita Agarwal"
                    },
                    {
                        "name": "Amit Shukla"
                    },
                    {
                        "name": "Karl Mannheim"
                    },
                    {
                        "name": "Bhargav Vaidya"
                    },
                    {
                        "name": "Biswajit Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Biswajit Banerjee"
                },
                "author": "Biswajit Banerjee",
                "arxiv_doi": "10.3847/2041-8213/ad4994",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2041-8213/ad4994",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.09612v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.09612v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages, 3 figures, 1 table, Published in ApJL",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.07622v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.07622v4",
                "updated": "2025-01-01T08:16:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    8,
                    16,
                    41,
                    2,
                    1,
                    0
                ],
                "published": "2023-12-12T01:39:16Z",
                "published_parsed": [
                    2023,
                    12,
                    12,
                    1,
                    39,
                    16,
                    1,
                    346,
                    0
                ],
                "title": "Mathematical Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical Language Models: A Survey"
                },
                "summary": "In recent years, there has been remarkable progress in leveraging Language\nModels (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale\nLanguage Models (LLMs), within the domain of mathematics. This paper conducts a\ncomprehensive survey of mathematical LMs, systematically categorizing pivotal\nresearch endeavors from two distinct perspectives: tasks and methodologies. The\nlandscape reveals a large number of proposed mathematical LLMs, which are\nfurther delineated into instruction learning, tool-based methods, fundamental\nCoT techniques, advanced CoT methodologies and multi-modal methods. To\ncomprehend the benefits of mathematical LMs more thoroughly, we carry out an\nin-depth contrast of their characteristics and performance. In addition, our\nsurvey entails the compilation of over 60 mathematical datasets, including\ntraining datasets, benchmark datasets, and augmented datasets. Addressing the\nprimary challenges and delineating future trajectories within the field of\nmathematical LMs, this survey is poised to facilitate and inspire future\ninnovation among researchers invested in advancing this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been remarkable progress in leveraging Language\nModels (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale\nLanguage Models (LLMs), within the domain of mathematics. This paper conducts a\ncomprehensive survey of mathematical LMs, systematically categorizing pivotal\nresearch endeavors from two distinct perspectives: tasks and methodologies. The\nlandscape reveals a large number of proposed mathematical LLMs, which are\nfurther delineated into instruction learning, tool-based methods, fundamental\nCoT techniques, advanced CoT methodologies and multi-modal methods. To\ncomprehend the benefits of mathematical LMs more thoroughly, we carry out an\nin-depth contrast of their characteristics and performance. In addition, our\nsurvey entails the compilation of over 60 mathematical datasets, including\ntraining datasets, benchmark datasets, and augmented datasets. Addressing the\nprimary challenges and delineating future trajectories within the field of\nmathematical LMs, this survey is poised to facilitate and inspire future\ninnovation among researchers invested in advancing this domain."
                },
                "authors": [
                    {
                        "name": "Wentao Liu"
                    },
                    {
                        "name": "Hanglei Hu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Yuyang Ding"
                    },
                    {
                        "name": "Junsong Li"
                    },
                    {
                        "name": "Jiayi Zeng"
                    },
                    {
                        "name": "Mengliang He"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Aimin Zhou"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:1705.04146,\n  arXiv:2304.10977, arXiv:2112.00114, arXiv:1905.13319, arXiv:2304.12244,\n  arXiv:2206.01347, arXiv:2006.09265 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.07622v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.07622v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03104v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03104v2",
                "updated": "2025-01-01T07:23:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    7,
                    23,
                    17,
                    2,
                    1,
                    0
                ],
                "published": "2024-12-04T08:06:15Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    6,
                    15,
                    2,
                    339,
                    0
                ],
                "title": "ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced\n  Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced\n  Understanding and Reasoning"
                },
                "summary": "Understanding time series is crucial for its application in real-world\nscenarios. Recently, large language models (LLMs) have been increasingly\napplied to time series tasks, leveraging their strong language capabilities to\nenhance various applications. However, research on multimodal LLMs (MLLMs) for\ntime series understanding and reasoning remains limited, primarily due to the\nscarcity of high-quality datasets that align time series with textual\ninformation. This paper introduces ChatTS, a novel MLLM designed for time\nseries analysis. ChatTS treats time series as a modality, similar to how vision\nMLLMs process images, enabling it to perform both understanding and reasoning\nwith time series. To address the scarcity of training data, we propose an\nattribute-based method for generating synthetic time series with detailed\nattribute descriptions. We further introduce Time Series Evol-Instruct, a novel\napproach that generates diverse time series Q&As, enhancing the model's\nreasoning capabilities. To the best of our knowledge, ChatTS is the first\nTS-MLLM that takes multivariate time series as input for understanding and\nreasoning, which is fine-tuned exclusively on synthetic datasets. We evaluate\nits performance using benchmark datasets with real-world data, including six\nalignment tasks and four reasoning tasks. Our results show that ChatTS\nsignificantly outperforms existing vision-based MLLMs (e.g., GPT-4o) and\ntext/agent-based LLMs, achieving a 46.0% improvement in alignment tasks and a\n25.8% improvement in reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding time series is crucial for its application in real-world\nscenarios. Recently, large language models (LLMs) have been increasingly\napplied to time series tasks, leveraging their strong language capabilities to\nenhance various applications. However, research on multimodal LLMs (MLLMs) for\ntime series understanding and reasoning remains limited, primarily due to the\nscarcity of high-quality datasets that align time series with textual\ninformation. This paper introduces ChatTS, a novel MLLM designed for time\nseries analysis. ChatTS treats time series as a modality, similar to how vision\nMLLMs process images, enabling it to perform both understanding and reasoning\nwith time series. To address the scarcity of training data, we propose an\nattribute-based method for generating synthetic time series with detailed\nattribute descriptions. We further introduce Time Series Evol-Instruct, a novel\napproach that generates diverse time series Q&As, enhancing the model's\nreasoning capabilities. To the best of our knowledge, ChatTS is the first\nTS-MLLM that takes multivariate time series as input for understanding and\nreasoning, which is fine-tuned exclusively on synthetic datasets. We evaluate\nits performance using benchmark datasets with real-world data, including six\nalignment tasks and four reasoning tasks. Our results show that ChatTS\nsignificantly outperforms existing vision-based MLLMs (e.g., GPT-4o) and\ntext/agent-based LLMs, achieving a 46.0% improvement in alignment tasks and a\n25.8% improvement in reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zeyan Li"
                    },
                    {
                        "name": "Xiao He"
                    },
                    {
                        "name": "Longlong Xu"
                    },
                    {
                        "name": "Xidao Wen"
                    },
                    {
                        "name": "Tieying Zhang"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Rui Shi"
                    },
                    {
                        "name": "Dan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Dan Pei"
                },
                "author": "Dan Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03104v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16882v2",
                "updated": "2025-01-01T03:13:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    3,
                    13,
                    3,
                    2,
                    1,
                    0
                ],
                "published": "2024-12-22T06:22:40Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    6,
                    22,
                    40,
                    6,
                    357,
                    0
                ],
                "title": "PsychAdapter: Adapting LLM Transformers to Reflect Traits, Personality\n  and Mental Health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsychAdapter: Adapting LLM Transformers to Reflect Traits, Personality\n  and Mental Health"
                },
                "summary": "Artificial intelligence-based language generators are now a part of most\npeople's lives. However, by default, they tend to generate \"average\" language\nwithout reflecting the ways in which people differ. Here, we propose a\nlightweight modification to the standard language model transformer\narchitecture - \"PsychAdapter\" - that uses empirically derived trait-language\npatterns to generate natural language for specified personality, demographic,\nand mental health characteristics (with or without prompting). We applied\nPsychAdapters to modify OpenAI's GPT-2, Google's Gemma, and Meta's Llama 3 and\nfound generated text to reflect the desired traits. For example, expert raters\nevaluated PsychAdapter's generated text output and found it matched intended\ntrait levels with 87.3% average accuracy for Big Five personalities, and 96.7%\nfor depression and life satisfaction. PsychAdapter is a novel method to\nintroduce psychological behavior patterns into language models at the\nfoundation level, independent of prompting, by influencing every transformer\nlayer. This approach can create chatbots with specific personality profiles,\nclinical training tools that mirror language associated with psychological\nconditionals, and machine translations that match an authors reading or\neducation level without taking up LLM context windows. PsychAdapter also allows\nfor the exploration psychological constructs through natural language\nexpression, extending the natural language processing toolkit to study human\npsychology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence-based language generators are now a part of most\npeople's lives. However, by default, they tend to generate \"average\" language\nwithout reflecting the ways in which people differ. Here, we propose a\nlightweight modification to the standard language model transformer\narchitecture - \"PsychAdapter\" - that uses empirically derived trait-language\npatterns to generate natural language for specified personality, demographic,\nand mental health characteristics (with or without prompting). We applied\nPsychAdapters to modify OpenAI's GPT-2, Google's Gemma, and Meta's Llama 3 and\nfound generated text to reflect the desired traits. For example, expert raters\nevaluated PsychAdapter's generated text output and found it matched intended\ntrait levels with 87.3% average accuracy for Big Five personalities, and 96.7%\nfor depression and life satisfaction. PsychAdapter is a novel method to\nintroduce psychological behavior patterns into language models at the\nfoundation level, independent of prompting, by influencing every transformer\nlayer. This approach can create chatbots with specific personality profiles,\nclinical training tools that mirror language associated with psychological\nconditionals, and machine translations that match an authors reading or\neducation level without taking up LLM context windows. PsychAdapter also allows\nfor the exploration psychological constructs through natural language\nexpression, extending the natural language processing toolkit to study human\npsychology."
                },
                "authors": [
                    {
                        "name": "Huy Vu"
                    },
                    {
                        "name": "Huy Anh Nguyen"
                    },
                    {
                        "name": "Adithya V Ganesan"
                    },
                    {
                        "name": "Swanie Juhng"
                    },
                    {
                        "name": "Oscar N. E. Kjell"
                    },
                    {
                        "name": "Joao Sedoc"
                    },
                    {
                        "name": "Margaret L. Kern"
                    },
                    {
                        "name": "Ryan L. Boyd"
                    },
                    {
                        "name": "Lyle Ungar"
                    },
                    {
                        "name": "H. Andrew Schwartz"
                    },
                    {
                        "name": "Johannes C. Eichstaedt"
                    }
                ],
                "author_detail": {
                    "name": "Johannes C. Eichstaedt"
                },
                "author": "Johannes C. Eichstaedt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19352v2",
                "updated": "2025-01-01T00:03:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    0,
                    3,
                    24,
                    2,
                    1,
                    0
                ],
                "published": "2024-11-28T19:53:39Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    19,
                    53,
                    39,
                    3,
                    333,
                    0
                ],
                "title": "OMuleT: Orchestrating Multiple Tools for Practicable Conversational\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OMuleT: Orchestrating Multiple Tools for Practicable Conversational\n  Recommendation"
                },
                "summary": "In this paper, we present a systematic effort to design, evaluate, and\nimplement a realistic conversational recommender system (CRS). The objective of\nour system is to allow users to input free-form text to request\nrecommendations, and then receive a list of relevant and diverse items. While\nprevious work on synthetic queries augments large language models (LLMs) with\n1-3 tools, we argue that a more extensive toolbox is necessary to effectively\nhandle real user requests. As such, we propose a novel approach that equips\nLLMs with over 10 tools, providing them access to the internal knowledge base\nand API calls used in production. We evaluate our model on a dataset of real\nusers and show that it generates relevant, novel, and diverse recommendations\ncompared to vanilla LLMs. Furthermore, we conduct ablation studies to\ndemonstrate the effectiveness of using the full range of tools in our toolbox.\nWe share our designs and lessons learned from deploying the system for internal\nalpha release. Our contribution is the addressing of all four key aspects of a\npracticable CRS: (1) real user requests, (2) augmenting LLMs with a wide\nvariety of tools, (3) extensive evaluation, and (4) deployment insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a systematic effort to design, evaluate, and\nimplement a realistic conversational recommender system (CRS). The objective of\nour system is to allow users to input free-form text to request\nrecommendations, and then receive a list of relevant and diverse items. While\nprevious work on synthetic queries augments large language models (LLMs) with\n1-3 tools, we argue that a more extensive toolbox is necessary to effectively\nhandle real user requests. As such, we propose a novel approach that equips\nLLMs with over 10 tools, providing them access to the internal knowledge base\nand API calls used in production. We evaluate our model on a dataset of real\nusers and show that it generates relevant, novel, and diverse recommendations\ncompared to vanilla LLMs. Furthermore, we conduct ablation studies to\ndemonstrate the effectiveness of using the full range of tools in our toolbox.\nWe share our designs and lessons learned from deploying the system for internal\nalpha release. Our contribution is the addressing of all four key aspects of a\npracticable CRS: (1) real user requests, (2) augmenting LLMs with a wide\nvariety of tools, (3) extensive evaluation, and (4) deployment insights."
                },
                "authors": [
                    {
                        "name": "Se-eun Yoon"
                    },
                    {
                        "name": "Xiaokai Wei"
                    },
                    {
                        "name": "Yexi Jiang"
                    },
                    {
                        "name": "Rachit Pareek"
                    },
                    {
                        "name": "Frank Ong"
                    },
                    {
                        "name": "Kevin Gao"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Michelle Gong"
                    }
                ],
                "author_detail": {
                    "name": "Michelle Gong"
                },
                "author": "Michelle Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2104.04590v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2104.04590v3",
                "updated": "2024-12-31T23:31:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    23,
                    31,
                    12,
                    1,
                    366,
                    0
                ],
                "published": "2021-04-09T20:06:54Z",
                "published_parsed": [
                    2021,
                    4,
                    9,
                    20,
                    6,
                    54,
                    4,
                    99,
                    0
                ],
                "title": "Identification of Dynamic Panel Logit Models with Fixed Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identification of Dynamic Panel Logit Models with Fixed Effects"
                },
                "summary": "We show that identification in a general class of dynamic panel logit models\nwith fixed effects is related to the truncated moment problem from the\nmathematics literature. We use this connection to show that the identified set\nfor structural parameters and functionals of the distribution of latent\nindividual effects can be characterized by a finite set of conditional moment\nequalities subject to a certain set of shape constraints on the model\nparameters. In addition to providing a general approach to identification, the\nnew characterization can deliver informative bounds in cases where competing\nmethods deliver no identifying restrictions, and can deliver point\nidentification in cases where competing methods deliver partial identification.\nWe then present an estimation and inference procedure that uses semidefinite\nprogramming methods, is applicable with continuous or discrete covariates, and\ncan be used for models that are either point- or partially-identified. Finally,\nwe illustrate our identification result with a number of examples, and provide\nan empirical application to employment dynamics using data from the National\nLongitudinal Survey of Youth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that identification in a general class of dynamic panel logit models\nwith fixed effects is related to the truncated moment problem from the\nmathematics literature. We use this connection to show that the identified set\nfor structural parameters and functionals of the distribution of latent\nindividual effects can be characterized by a finite set of conditional moment\nequalities subject to a certain set of shape constraints on the model\nparameters. In addition to providing a general approach to identification, the\nnew characterization can deliver informative bounds in cases where competing\nmethods deliver no identifying restrictions, and can deliver point\nidentification in cases where competing methods deliver partial identification.\nWe then present an estimation and inference procedure that uses semidefinite\nprogramming methods, is applicable with continuous or discrete covariates, and\ncan be used for models that are either point- or partially-identified. Finally,\nwe illustrate our identification result with a number of examples, and provide\nan empirical application to employment dynamics using data from the National\nLongitudinal Survey of Youth."
                },
                "authors": [
                    {
                        "name": "Christopher Dobronyi"
                    },
                    {
                        "name": "Jiaying Gu"
                    },
                    {
                        "name": "Kyoo il Kim"
                    },
                    {
                        "name": "Thomas M. Russell"
                    }
                ],
                "author_detail": {
                    "name": "Thomas M. Russell"
                },
                "author": "Thomas M. Russell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2104.04590v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2104.04590v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11350v2",
                "updated": "2024-12-31T23:02:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    23,
                    2,
                    54,
                    1,
                    366,
                    0
                ],
                "published": "2024-04-17T13:08:26Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    13,
                    8,
                    26,
                    2,
                    108,
                    0
                ],
                "title": "Calibrating Bayesian Learning via Regularization, Confidence\n  Minimization, and Selective Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating Bayesian Learning via Regularization, Confidence\n  Minimization, and Selective Inference"
                },
                "summary": "The application of artificial intelligence (AI) models in fields such as\nengineering is limited by the known difficulty of quantifying the reliability\nof an AI's decision. A well-calibrated AI model must correctly report its\naccuracy on in-distribution (ID) inputs, while also enabling the detection of\nout-of-distribution (OOD) inputs. A conventional approach to improve\ncalibration is the application of Bayesian ensembling. However, owing to\ncomputational limitations and model misspecification, practical ensembling\nstrategies do not necessarily enhance calibration. This paper proposes an\nextension of variational inference (VI)-based Bayesian learning that integrates\ncalibration regularization for improved ID performance, confidence minimization\nfor OOD detection, and selective calibration to ensure a synergistic use of\ncalibration regularization and confidence minimization. The scheme is\nconstructed successively by first introducing calibration-regularized Bayesian\nlearning (CBNN), then incorporating out-of-distribution confidence minimization\n(OCM) to yield CBNN-OCM, and finally integrating also selective calibration to\nproduce selective CBNN-OCM (SCBNN-OCM). Selective calibration rejects inputs\nfor which the calibration performance is expected to be insufficient. Numerical\nresults illustrate the trade-offs between ID accuracy, ID calibration, and OOD\ncalibration attained by both frequentist and Bayesian learning methods. Among\nthe main conclusions, SCBNN-OCM is seen to achieve best ID and OOD performance\nas compared to existing state-of-the-art approaches at the cost of rejecting a\nsufficiently large number of inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of artificial intelligence (AI) models in fields such as\nengineering is limited by the known difficulty of quantifying the reliability\nof an AI's decision. A well-calibrated AI model must correctly report its\naccuracy on in-distribution (ID) inputs, while also enabling the detection of\nout-of-distribution (OOD) inputs. A conventional approach to improve\ncalibration is the application of Bayesian ensembling. However, owing to\ncomputational limitations and model misspecification, practical ensembling\nstrategies do not necessarily enhance calibration. This paper proposes an\nextension of variational inference (VI)-based Bayesian learning that integrates\ncalibration regularization for improved ID performance, confidence minimization\nfor OOD detection, and selective calibration to ensure a synergistic use of\ncalibration regularization and confidence minimization. The scheme is\nconstructed successively by first introducing calibration-regularized Bayesian\nlearning (CBNN), then incorporating out-of-distribution confidence minimization\n(OCM) to yield CBNN-OCM, and finally integrating also selective calibration to\nproduce selective CBNN-OCM (SCBNN-OCM). Selective calibration rejects inputs\nfor which the calibration performance is expected to be insufficient. Numerical\nresults illustrate the trade-offs between ID accuracy, ID calibration, and OOD\ncalibration attained by both frequentist and Bayesian learning methods. Among\nthe main conclusions, SCBNN-OCM is seen to achieve best ID and OOD performance\nas compared to existing state-of-the-art approaches at the cost of rejecting a\nsufficiently large number of inputs."
                },
                "authors": [
                    {
                        "name": "Jiayi Huang"
                    },
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20173v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20173v2",
                "updated": "2024-12-31T21:10:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    21,
                    10,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-28T15:01:19Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    15,
                    1,
                    19,
                    5,
                    363,
                    0
                ],
                "title": "Debiased Nonparametric Regression for Statistical Inference and\n  Distributionally Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiased Nonparametric Regression for Statistical Inference and\n  Distributionally Robustness"
                },
                "summary": "This study proposes a debiasing method for smooth nonparametric estimators.\nWhile machine learning techniques such as random forests and neural networks\nhave demonstrated strong predictive performance, their theoretical properties\nremain relatively underexplored. Specifically, many modern algorithms lack\nassurances of pointwise asymptotic normality and uniform convergence, which are\ncritical for statistical inference and robustness under covariate shift and\nhave been well-established for classical methods like Nadaraya-Watson\nregression. To address this, we introduce a model-free debiasing method that\nguarantees these properties for smooth estimators derived from any\nnonparametric regression approach. By adding a correction term that estimates\nthe conditional expected residual of the original estimator, or equivalently,\nits estimation error, we obtain a debiased estimator with proven pointwise\nasymptotic normality, and uniform convergence. These properties enable\nstatistical inference and enhance robustness to covariate shift, making the\nmethod broadly applicable to a wide range of nonparametric regression problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes a debiasing method for smooth nonparametric estimators.\nWhile machine learning techniques such as random forests and neural networks\nhave demonstrated strong predictive performance, their theoretical properties\nremain relatively underexplored. Specifically, many modern algorithms lack\nassurances of pointwise asymptotic normality and uniform convergence, which are\ncritical for statistical inference and robustness under covariate shift and\nhave been well-established for classical methods like Nadaraya-Watson\nregression. To address this, we introduce a model-free debiasing method that\nguarantees these properties for smooth estimators derived from any\nnonparametric regression approach. By adding a correction term that estimates\nthe conditional expected residual of the original estimator, or equivalently,\nits estimation error, we obtain a debiased estimator with proven pointwise\nasymptotic normality, and uniform convergence. These properties enable\nstatistical inference and enhance robustness to covariate shift, making the\nmethod broadly applicable to a wide range of nonparametric regression problems."
                },
                "authors": [
                    {
                        "name": "Masahiro Kato"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Kato"
                },
                "author": "Masahiro Kato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20173v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20173v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10277v2",
                "updated": "2024-12-31T20:02:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    2,
                    33,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T13:39:05Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    39,
                    5,
                    0,
                    260,
                    0
                ],
                "title": "Cognitive Kernel: An Open-source Agent System towards Generalist\n  Autopilots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Kernel: An Open-source Agent System towards Generalist\n  Autopilots"
                },
                "summary": "We introduce Cognitive Kernel, an open-source agent system towards the goal\nof generalist autopilots. Unlike copilot systems, which primarily rely on users\nto provide essential state information (e.g., task descriptions) and assist\nusers by answering questions or auto-completing contents, autopilot systems\nmust complete tasks from start to finish independently, which requires the\nsystem to acquire the state information from the environments actively. To\nachieve this, an autopilot system should be capable of understanding user\nintents, actively gathering necessary information from various real-world\nsources, and making wise decisions. Cognitive Kernel adopts a model-centric\ndesign. In our implementation, the central policy model (a fine-tuned LLM)\ninitiates interactions with the environment using a combination of atomic\nactions, such as opening files, clicking buttons, saving intermediate results\nto memory, or calling the LLM itself. This differs from the widely used\nenvironment-centric design, where a task-specific environment with predefined\nactions is fixed, and the policy model is limited to selecting the correct\naction from a given set of options. Our design facilitates seamless information\nflow across various sources and provides greater flexibility. We evaluate our\nsystem in three use cases: real-time information management, private\ninformation management, and long-term memory management. The results\ndemonstrate that Cognitive Kernel achieves better or comparable performance to\nother closed-source systems in these scenarios. Cognitive Kernel is fully\ndockerized, ensuring everyone can deploy it privately and securely. We\nopen-source the system and the backbone model to encourage further research on\nLLM-driven autopilot systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Cognitive Kernel, an open-source agent system towards the goal\nof generalist autopilots. Unlike copilot systems, which primarily rely on users\nto provide essential state information (e.g., task descriptions) and assist\nusers by answering questions or auto-completing contents, autopilot systems\nmust complete tasks from start to finish independently, which requires the\nsystem to acquire the state information from the environments actively. To\nachieve this, an autopilot system should be capable of understanding user\nintents, actively gathering necessary information from various real-world\nsources, and making wise decisions. Cognitive Kernel adopts a model-centric\ndesign. In our implementation, the central policy model (a fine-tuned LLM)\ninitiates interactions with the environment using a combination of atomic\nactions, such as opening files, clicking buttons, saving intermediate results\nto memory, or calling the LLM itself. This differs from the widely used\nenvironment-centric design, where a task-specific environment with predefined\nactions is fixed, and the policy model is limited to selecting the correct\naction from a given set of options. Our design facilitates seamless information\nflow across various sources and provides greater flexibility. We evaluate our\nsystem in three use cases: real-time information management, private\ninformation management, and long-term memory management. The results\ndemonstrate that Cognitive Kernel achieves better or comparable performance to\nother closed-source systems in these scenarios. Cognitive Kernel is fully\ndockerized, ensuring everyone can deploy it privately and securely. We\nopen-source the system and the backbone model to encourage further research on\nLLM-driven autopilot systems."
                },
                "authors": [
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Xiaoman Pan"
                    },
                    {
                        "name": "Hongwei Wang"
                    },
                    {
                        "name": "Kaixin Ma"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2201.05139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2201.05139v2",
                "updated": "2024-12-31T19:49:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    19,
                    49,
                    52,
                    1,
                    366,
                    0
                ],
                "published": "2022-01-13T18:51:56Z",
                "published_parsed": [
                    2022,
                    1,
                    13,
                    18,
                    51,
                    56,
                    3,
                    13,
                    0
                ],
                "title": "Kernel methods for long term dose response curves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kernel methods for long term dose response curves"
                },
                "summary": "A core challenge in causal inference is how to extrapolate long term effects,\nof possibly continuous actions, from short term experimental data. It arises in\nartificial intelligence: the long term consequences of continuous actions may\nbe of interest, yet only short term rewards may be collected in exploration.\nFor this estimand, called the long term dose response curve, we propose a\nsimple nonparametric estimator based on kernel ridge regression. By embedding\nthe distribution of the short term experimental data with kernels, we derive\ninterpretable weights for extrapolating long term effects. Our method allows\nactions, short term rewards, and long term rewards to be continuous in general\nspaces. It also allows for nonlinearity and heterogeneity in the link between\nshort term effects and long term effects. We prove uniform consistency, with\nnonasymptotic error bounds reflecting the effective dimension of the data. As\nan application, we estimate the long term dose response curve of Project STAR,\na social program which randomly assigned students to various class sizes. We\nextend our results to long term counterfactual distributions, proving weak\nconvergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A core challenge in causal inference is how to extrapolate long term effects,\nof possibly continuous actions, from short term experimental data. It arises in\nartificial intelligence: the long term consequences of continuous actions may\nbe of interest, yet only short term rewards may be collected in exploration.\nFor this estimand, called the long term dose response curve, we propose a\nsimple nonparametric estimator based on kernel ridge regression. By embedding\nthe distribution of the short term experimental data with kernels, we derive\ninterpretable weights for extrapolating long term effects. Our method allows\nactions, short term rewards, and long term rewards to be continuous in general\nspaces. It also allows for nonlinearity and heterogeneity in the link between\nshort term effects and long term effects. We prove uniform consistency, with\nnonasymptotic error bounds reflecting the effective dimension of the data. As\nan application, we estimate the long term dose response curve of Project STAR,\na social program which randomly assigned students to various class sizes. We\nextend our results to long term counterfactual distributions, proving weak\nconvergence."
                },
                "authors": [
                    {
                        "name": "Rahul Singh"
                    },
                    {
                        "name": "Hannah Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Hannah Zhou"
                },
                "author": "Hannah Zhou",
                "arxiv_comment": "subsumes arXiv:2111.05277",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2201.05139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2201.05139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.13099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.13099v2",
                "updated": "2024-12-31T19:41:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    19,
                    41,
                    56,
                    1,
                    366,
                    0
                ],
                "published": "2023-07-24T19:41:13Z",
                "published_parsed": [
                    2023,
                    7,
                    24,
                    19,
                    41,
                    13,
                    0,
                    205,
                    0
                ],
                "title": "Measuring Gravitational Wave Speed and Lorentz Violation with the First\n  Three Gravitational-Wave Catalogs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Gravitational Wave Speed and Lorentz Violation with the First\n  Three Gravitational-Wave Catalogs"
                },
                "summary": "The speed of gravitational waves $v_g$ can be measured with the time delay\nbetween gravitational-wave detectors. Our study provides a more precise\nmeasurement of $v_g$ using gravitational-wave signals only, compared with\nprevious studies. We select 52 gravitational-wave events that were detected\nwith high confidence by at least two detectors in the first three observing\nruns (O1, O2, and O3) of Advanced LIGO and Advanced Virgo. We use Markov chain\nMonte Carlo and nested sampling to estimate the $v_g$ posterior distribution\nfor each of those events. We then combine their posterior distributions to find\nthe 90% credible interval of the combined $v_g$ distribution for which we\nobtain $0.99^{+0.02}_{-0.02}c$ without the use of more accurate sky\nlocalization from the electromagnetic signal associated with GW170817.\nRestricting attention to the 50 binary black hole events generates the same\nresult, while the use of the electromagnetic sky localization for GW170817\ngives a tighter constraint of $0.99^{+0.01}_{-0.02}c$. The abundance of\ngravitational wave events allows us to apply hierarchical Bayesian inference on\nthe posterior samples to simultaneously constrain all nine coefficients for\nLorentz violation in the nondispersive, nonbirefringent limit of the\ngravitational sector of the Standard-Model Extension test framework. We compare\nthe hierarchical Bayesian inference method with other methods of combining\nlimits on Lorentz violation in the gravity sector that are found in the\nliterature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The speed of gravitational waves $v_g$ can be measured with the time delay\nbetween gravitational-wave detectors. Our study provides a more precise\nmeasurement of $v_g$ using gravitational-wave signals only, compared with\nprevious studies. We select 52 gravitational-wave events that were detected\nwith high confidence by at least two detectors in the first three observing\nruns (O1, O2, and O3) of Advanced LIGO and Advanced Virgo. We use Markov chain\nMonte Carlo and nested sampling to estimate the $v_g$ posterior distribution\nfor each of those events. We then combine their posterior distributions to find\nthe 90% credible interval of the combined $v_g$ distribution for which we\nobtain $0.99^{+0.02}_{-0.02}c$ without the use of more accurate sky\nlocalization from the electromagnetic signal associated with GW170817.\nRestricting attention to the 50 binary black hole events generates the same\nresult, while the use of the electromagnetic sky localization for GW170817\ngives a tighter constraint of $0.99^{+0.01}_{-0.02}c$. The abundance of\ngravitational wave events allows us to apply hierarchical Bayesian inference on\nthe posterior samples to simultaneously constrain all nine coefficients for\nLorentz violation in the nondispersive, nonbirefringent limit of the\ngravitational sector of the Standard-Model Extension test framework. We compare\nthe hierarchical Bayesian inference method with other methods of combining\nlimits on Lorentz violation in the gravity sector that are found in the\nliterature."
                },
                "authors": [
                    {
                        "name": "Anarya Ray"
                    },
                    {
                        "name": "Pinchen Fan"
                    },
                    {
                        "name": "Vincent F. He"
                    },
                    {
                        "name": "Malachy Bloom"
                    },
                    {
                        "name": "Suyu Michael Yang"
                    },
                    {
                        "name": "Jay D. Tasson"
                    },
                    {
                        "name": "Jolien D. E. Creighton"
                    }
                ],
                "author_detail": {
                    "name": "Jolien D. E. Creighton"
                },
                "author": "Jolien D. E. Creighton",
                "arxiv_doi": "10.1103/PhysRevD.110.122001",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.122001",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.13099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.13099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published version, 18 pages, 5 figures, 3 tables",
                "arxiv_journal_ref": "Phys. Rev. D 110, 122001 (2024)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15485v2",
                "updated": "2024-12-31T17:43:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    17,
                    43,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-01-27T19:24:12Z",
                "published_parsed": [
                    2024,
                    1,
                    27,
                    19,
                    24,
                    12,
                    5,
                    27,
                    0
                ],
                "title": "Constrained Hamiltonian Systems and Physics-Informed Neural Networks:\n  Hamilton-Dirac Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constrained Hamiltonian Systems and Physics-Informed Neural Networks:\n  Hamilton-Dirac Neural Networks"
                },
                "summary": "The effectiveness of the Physics Informed Neural Networks (PINNs) for\nlearning the dynamics of constrained Hamiltonian systems is demonstrated using\nthe Dirac theory of constraints for regular systems with holonomic constraints\nand systems with non-standard Lagrangians. By utilizing Dirac brackets, we\nderive the Hamilton-Dirac equations and minimize their residuals, incorporating\nalso energy conservation and the Dirac constraints, using appropriate\nregularization terms in the loss function. The resulting PINNs, referred to as\nHamilton-Dirac Neural Networks (HDNNs), successfully learn constrained dynamics\nwithout deviating from the constraint manifold. Two examples with holonomic\nconstraints are presented: the nonlinear pendulum in Cartesian coordinates and\na two-dimensional, elliptically restricted harmonic oscillator. In both cases,\nHDNNs exhibit superior performance in preserving energy and constraints\ncompared to traditional explicit solvers. To demonstrate applicability in\nsystems with singular Lagrangians, we computed the guiding center motion in a\nstrong magnetic field starting from the guiding center Lagrangian. The\nimposition of energy conservation during the neural network training proved\nessential for accurately determining the orbits of the guiding center. The HDNN\narchitecture enables the learning of parametric dependencies in constrained\ndynamics by incorporating a problem-specific parameter as an input, in addition\nto the time variable. Additionally, an example of semi-supervised, data-driven\nlearning of guiding center dynamics with parameter inference is presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of the Physics Informed Neural Networks (PINNs) for\nlearning the dynamics of constrained Hamiltonian systems is demonstrated using\nthe Dirac theory of constraints for regular systems with holonomic constraints\nand systems with non-standard Lagrangians. By utilizing Dirac brackets, we\nderive the Hamilton-Dirac equations and minimize their residuals, incorporating\nalso energy conservation and the Dirac constraints, using appropriate\nregularization terms in the loss function. The resulting PINNs, referred to as\nHamilton-Dirac Neural Networks (HDNNs), successfully learn constrained dynamics\nwithout deviating from the constraint manifold. Two examples with holonomic\nconstraints are presented: the nonlinear pendulum in Cartesian coordinates and\na two-dimensional, elliptically restricted harmonic oscillator. In both cases,\nHDNNs exhibit superior performance in preserving energy and constraints\ncompared to traditional explicit solvers. To demonstrate applicability in\nsystems with singular Lagrangians, we computed the guiding center motion in a\nstrong magnetic field starting from the guiding center Lagrangian. The\nimposition of energy conservation during the neural network training proved\nessential for accurately determining the orbits of the guiding center. The HDNN\narchitecture enables the learning of parametric dependencies in constrained\ndynamics by incorporating a problem-specific parameter as an input, in addition\nto the time variable. Additionally, an example of semi-supervised, data-driven\nlearning of guiding center dynamics with parameter inference is presented."
                },
                "authors": [
                    {
                        "name": "Dimitrios A. Kaltsas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios A. Kaltsas"
                },
                "author": "Dimitrios A. Kaltsas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18899v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18899v2",
                "updated": "2024-12-31T17:00:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    17,
                    0,
                    33,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-25T13:20:10Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    13,
                    20,
                    10,
                    2,
                    360,
                    0
                ],
                "title": "GAI: Generative Agents for Innovation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAI: Generative Agents for Innovation"
                },
                "summary": "This study examines whether collective reasoning among generative agents can\nfacilitate novel and coherent thinking that leads to innovation. To achieve\nthis, it proposes GAI, a new LLM-empowered framework designed for reflection\nand interaction among multiple generative agents to replicate the process of\ninnovation. The core of the GAI framework lies in an architecture that\ndynamically processes the internal states of agents and a dialogue scheme\nspecifically tailored to facilitate analogy-driven innovation. The framework's\nfunctionality is evaluated using Dyson's invention of the bladeless fan as a\ncase study, assessing the extent to which the core ideas of the innovation can\nbe replicated through a set of fictional technical documents. The experimental\nresults demonstrate that models with internal states significantly outperformed\nthose without, achieving higher average scores and lower variance. Notably, the\nmodel with five heterogeneous agents equipped with internal states successfully\nreplicated the key ideas underlying the Dyson's invention. This indicates that\nthe internal state enables agents to refine their ideas, resulting in the\nconstruction and sharing of more coherent and comprehensive concepts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines whether collective reasoning among generative agents can\nfacilitate novel and coherent thinking that leads to innovation. To achieve\nthis, it proposes GAI, a new LLM-empowered framework designed for reflection\nand interaction among multiple generative agents to replicate the process of\ninnovation. The core of the GAI framework lies in an architecture that\ndynamically processes the internal states of agents and a dialogue scheme\nspecifically tailored to facilitate analogy-driven innovation. The framework's\nfunctionality is evaluated using Dyson's invention of the bladeless fan as a\ncase study, assessing the extent to which the core ideas of the innovation can\nbe replicated through a set of fictional technical documents. The experimental\nresults demonstrate that models with internal states significantly outperformed\nthose without, achieving higher average scores and lower variance. Notably, the\nmodel with five heterogeneous agents equipped with internal states successfully\nreplicated the key ideas underlying the Dyson's invention. This indicates that\nthe internal state enables agents to refine their ideas, resulting in the\nconstruction and sharing of more coherent and comprehensive concepts."
                },
                "authors": [
                    {
                        "name": "Masahiro Sato"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Sato"
                },
                "author": "Masahiro Sato",
                "arxiv_comment": "Added an Appendix section",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18899v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08843v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08843v3",
                "updated": "2024-12-31T16:39:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    16,
                    39,
                    16,
                    1,
                    366,
                    0
                ],
                "published": "2024-07-11T19:58:19Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    19,
                    58,
                    19,
                    3,
                    193,
                    0
                ],
                "title": "Inflationary Flows: Calibrated Bayesian Inference with Diffusion-Based\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inflationary Flows: Calibrated Bayesian Inference with Diffusion-Based\n  Models"
                },
                "summary": "Beyond estimating parameters of interest from data, one of the key goals of\nstatistical inference is to properly quantify uncertainty in these estimates.\nIn Bayesian inference, this uncertainty is provided by the posterior\ndistribution, the computation of which typically involves an intractable\nhigh-dimensional integral. Among available approximation methods,\nsampling-based approaches come with strong theoretical guarantees but scale\npoorly to large problems, while variational approaches scale well but offer few\ntheoretical guarantees. In particular, variational methods are known to produce\noverconfident estimates of posterior uncertainty and are typically\nnon-identifiable, with many latent variable configurations generating\nequivalent predictions. Here, we address these challenges by showing how\ndiffusion-based models (DBMs), which have recently produced state-of-the-art\nperformance in generative modeling tasks, can be repurposed for performing\ncalibrated, identifiable Bayesian inference. By exploiting a previously\nestablished connection between the stochastic and probability flow ordinary\ndifferential equations (pfODEs) underlying DBMs, we derive a class of models,\ninflationary flows, that uniquely and deterministically map high-dimensional\ndata to a lower-dimensional Gaussian distribution via ODE integration. This map\nis both invertible and neighborhood-preserving, with controllable numerical\nerror, with the result that uncertainties in the data are correctly propagated\nto the latent space. We demonstrate how such maps can be learned via standard\nDBM training using a novel noise schedule and are effective at both preserving\nand reducing intrinsic data dimensionality. The result is a class of highly\nexpressive generative models, uniquely defined on a low-dimensional latent\nspace, that afford principled Bayesian inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond estimating parameters of interest from data, one of the key goals of\nstatistical inference is to properly quantify uncertainty in these estimates.\nIn Bayesian inference, this uncertainty is provided by the posterior\ndistribution, the computation of which typically involves an intractable\nhigh-dimensional integral. Among available approximation methods,\nsampling-based approaches come with strong theoretical guarantees but scale\npoorly to large problems, while variational approaches scale well but offer few\ntheoretical guarantees. In particular, variational methods are known to produce\noverconfident estimates of posterior uncertainty and are typically\nnon-identifiable, with many latent variable configurations generating\nequivalent predictions. Here, we address these challenges by showing how\ndiffusion-based models (DBMs), which have recently produced state-of-the-art\nperformance in generative modeling tasks, can be repurposed for performing\ncalibrated, identifiable Bayesian inference. By exploiting a previously\nestablished connection between the stochastic and probability flow ordinary\ndifferential equations (pfODEs) underlying DBMs, we derive a class of models,\ninflationary flows, that uniquely and deterministically map high-dimensional\ndata to a lower-dimensional Gaussian distribution via ODE integration. This map\nis both invertible and neighborhood-preserving, with controllable numerical\nerror, with the result that uncertainties in the data are correctly propagated\nto the latent space. We demonstrate how such maps can be learned via standard\nDBM training using a novel noise schedule and are effective at both preserving\nand reducing intrinsic data dimensionality. The result is a class of highly\nexpressive generative models, uniquely defined on a low-dimensional latent\nspace, that afford principled Bayesian inference."
                },
                "authors": [
                    {
                        "name": "Daniela de Albuquerque"
                    },
                    {
                        "name": "John Pearson"
                    }
                ],
                "author_detail": {
                    "name": "John Pearson"
                },
                "author": "John Pearson",
                "arxiv_comment": "10 pages, 5 figures. Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08843v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08843v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T99 (Primary) 62M45 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3; I.6.5; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16928v2",
                "updated": "2024-12-31T16:29:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    16,
                    29,
                    28,
                    1,
                    366,
                    0
                ],
                "published": "2024-07-24T01:33:57Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    1,
                    33,
                    57,
                    2,
                    206,
                    0
                ],
                "title": "From Sands to Mansions: Simulating Full Attack Chain with LLM-Organized\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Sands to Mansions: Simulating Full Attack Chain with LLM-Organized\n  Knowledge"
                },
                "summary": "Adversarial dynamics are intrinsic to the nature of offense and defense in\ncyberspace, with both attackers and defenders continuously evolving their\ntechnologies. Given the wide array of security products available, users often\nface challenges in selecting the most effective solutions. Furthermore,\ntraditional benchmarks based on single-point attacks are increasingly\ninadequate, failing to accurately reflect the full range of attacker\ncapabilities and falling short in properly evaluating the effectiveness of\ndefense products. Automated multi-stage attack simulations offer a promising\napproach to enhance system evaluation efficiency and aid in analyzing the\neffectiveness of detection systems. However, simulating a full attack chain is\ncomplex and requires significant time and expertise from security\nprofessionals, facing several challenges, including limited coverage of attack\ntechniques, a high level of required expertise, and a lack of execution detail.\nIn this paper, we model automatic attack simulation as a planning problem. By\nusing the Planning Domain Definition Language (PDDL) to formally describe the\nattack simulation problem, and combining domain knowledge of both the problem\nand the domain space, we enable the planning of attack paths through\nstandardized, domain-independent planning algorithms. We explore the potential\nof Large Language Models (LLMs) to summarize and analyze knowledge from\nexisting attack documentation and reports, facilitating automated attack\nplanning. We introduce Aurora, a system that autonomously simulates full attack\nchains based on external attack tools and threat intelligence reports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial dynamics are intrinsic to the nature of offense and defense in\ncyberspace, with both attackers and defenders continuously evolving their\ntechnologies. Given the wide array of security products available, users often\nface challenges in selecting the most effective solutions. Furthermore,\ntraditional benchmarks based on single-point attacks are increasingly\ninadequate, failing to accurately reflect the full range of attacker\ncapabilities and falling short in properly evaluating the effectiveness of\ndefense products. Automated multi-stage attack simulations offer a promising\napproach to enhance system evaluation efficiency and aid in analyzing the\neffectiveness of detection systems. However, simulating a full attack chain is\ncomplex and requires significant time and expertise from security\nprofessionals, facing several challenges, including limited coverage of attack\ntechniques, a high level of required expertise, and a lack of execution detail.\nIn this paper, we model automatic attack simulation as a planning problem. By\nusing the Planning Domain Definition Language (PDDL) to formally describe the\nattack simulation problem, and combining domain knowledge of both the problem\nand the domain space, we enable the planning of attack paths through\nstandardized, domain-independent planning algorithms. We explore the potential\nof Large Language Models (LLMs) to summarize and analyze knowledge from\nexisting attack documentation and reports, facilitating automated attack\nplanning. We introduce Aurora, a system that autonomously simulates full attack\nchains based on external attack tools and threat intelligence reports."
                },
                "authors": [
                    {
                        "name": "Lingzhi Wang"
                    },
                    {
                        "name": "Zhenyuan Li"
                    },
                    {
                        "name": "Zonghan Guo"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Kyle Jung"
                    },
                    {
                        "name": "Kedar Thiagarajan"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zhengkai Wang"
                    },
                    {
                        "name": "Emily Wei"
                    },
                    {
                        "name": "Xiangmin Shen"
                    },
                    {
                        "name": "Yan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yan Chen"
                },
                "author": "Yan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17470v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17470v2",
                "updated": "2024-12-31T16:25:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    16,
                    25,
                    39,
                    1,
                    366,
                    0
                ],
                "published": "2024-11-25T18:59:04Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    59,
                    4,
                    0,
                    330,
                    0
                ],
                "title": "Towards Precise Scaling Laws for Video Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Precise Scaling Laws for Video Diffusion Transformers"
                },
                "summary": "Achieving optimal performance of video diffusion transformers within given\ndata and compute budget is crucial due to their high training costs. This\nnecessitates precisely determining the optimal model size and training\nhyperparameters before large-scale training. While scaling laws are employed in\nlanguage models to predict performance, their existence and accurate derivation\nin visual generation models remain underexplored. In this paper, we\nsystematically analyze scaling laws for video diffusion transformers and\nconfirm their presence. Moreover, we discover that, unlike language models,\nvideo diffusion models are more sensitive to learning rate and batch size, two\nhyperparameters often not precisely modeled. To address this, we propose a new\nscaling law that predicts optimal hyperparameters for any model size and\ncompute budget. Under these optimal settings, we achieve comparable performance\nand reduce inference costs by 40.1% compared to conventional scaling methods,\nwithin a compute budget of 1e10 TFlops. Furthermore, we establish a more\ngeneralized and precise relationship among validation loss, any model size, and\ncompute budget. This enables performance prediction for non-optimal model\nsizes, which may also be appealed under practical inference cost constraints,\nachieving a better trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving optimal performance of video diffusion transformers within given\ndata and compute budget is crucial due to their high training costs. This\nnecessitates precisely determining the optimal model size and training\nhyperparameters before large-scale training. While scaling laws are employed in\nlanguage models to predict performance, their existence and accurate derivation\nin visual generation models remain underexplored. In this paper, we\nsystematically analyze scaling laws for video diffusion transformers and\nconfirm their presence. Moreover, we discover that, unlike language models,\nvideo diffusion models are more sensitive to learning rate and batch size, two\nhyperparameters often not precisely modeled. To address this, we propose a new\nscaling law that predicts optimal hyperparameters for any model size and\ncompute budget. Under these optimal settings, we achieve comparable performance\nand reduce inference costs by 40.1% compared to conventional scaling methods,\nwithin a compute budget of 1e10 TFlops. Furthermore, we establish a more\ngeneralized and precise relationship among validation loss, any model size, and\ncompute budget. This enables performance prediction for non-optimal model\nsizes, which may also be appealed under practical inference cost constraints,\nachieving a better trade-off."
                },
                "authors": [
                    {
                        "name": "Yuanyang Yin"
                    },
                    {
                        "name": "Yaqi Zhao"
                    },
                    {
                        "name": "Mingwu Zheng"
                    },
                    {
                        "name": "Ke Lin"
                    },
                    {
                        "name": "Jiarong Ou"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Victor Shea-Jay Huang"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Baoqun Yin"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17470v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17470v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18205v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18205v3",
                "updated": "2024-12-31T16:14:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    16,
                    14,
                    51,
                    1,
                    366,
                    0
                ],
                "published": "2024-02-28T09:51:55Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    9,
                    51,
                    55,
                    2,
                    59,
                    0
                ],
                "title": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging"
                },
                "summary": "Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, These methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and Chain-of-Thought \\textbf{M}erging\n(Lemur). Specifically, to discard the tedious manual rules. We propose a novel\nsampling method inspired by information entropy, which efficiently clusters\ntypical logs. Furthermore, to enhance the merging of log templates, we design a\nchain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension, deftly distinguishing between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that Lemur achieves the\nstate-of-the-art performance and impressive efficiency. The Code is available\nat https://github.com/zwpride/lemur.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, These methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and Chain-of-Thought \\textbf{M}erging\n(Lemur). Specifically, to discard the tedious manual rules. We propose a novel\nsampling method inspired by information entropy, which efficiently clusters\ntypical logs. Furthermore, to enhance the merging of log templates, we design a\nchain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension, deftly distinguishing between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that Lemur achieves the\nstate-of-the-art performance and impressive efficiency. The Code is available\nat https://github.com/zwpride/lemur."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18205v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12469v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12469v3",
                "updated": "2024-12-31T15:52:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    15,
                    52,
                    47,
                    1,
                    366,
                    0
                ],
                "published": "2024-11-19T12:51:17Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    51,
                    17,
                    1,
                    324,
                    0
                ],
                "title": "AI Flow at the Network Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Flow at the Network Edge"
                },
                "summary": "Recent advancements in large language models (LLMs) and their multimodal\nvariants have led to remarkable progress across various domains, demonstrating\nimpressive capabilities and unprecedented potential. In the era of ubiquitous\nconnectivity, leveraging communication networks to distribute intelligence is a\ntransformative concept, envisioning AI-powered services accessible at the\nnetwork edge. However, pushing large models from the cloud to\nresource-constrained environments faces critical challenges. Model inference on\nlow-end devices leads to excessive latency and performance bottlenecks, while\nraw data transmission over limited bandwidth networks causes high communication\noverhead. This article presents AI Flow, a framework that streamlines the\ninference process by jointly leveraging the heterogeneous resources available\nacross devices, edge nodes, and cloud servers, making intelligence flow across\nnetworks. To facilitate cooperation among multiple computational nodes, the\nproposed framework explores a paradigm shift in the design of communication\nnetwork systems from transmitting information flow to intelligence flow, where\nthe goal of communications is task-oriented and folded into the inference\nprocess. Experimental results demonstrate the effectiveness of the proposed\nframework through an image captioning use case, showcasing the ability to\nreduce response latency while maintaining high-quality captions. This article\nserves as a position paper for identifying the motivation, challenges, and\nprinciples of AI Flow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) and their multimodal\nvariants have led to remarkable progress across various domains, demonstrating\nimpressive capabilities and unprecedented potential. In the era of ubiquitous\nconnectivity, leveraging communication networks to distribute intelligence is a\ntransformative concept, envisioning AI-powered services accessible at the\nnetwork edge. However, pushing large models from the cloud to\nresource-constrained environments faces critical challenges. Model inference on\nlow-end devices leads to excessive latency and performance bottlenecks, while\nraw data transmission over limited bandwidth networks causes high communication\noverhead. This article presents AI Flow, a framework that streamlines the\ninference process by jointly leveraging the heterogeneous resources available\nacross devices, edge nodes, and cloud servers, making intelligence flow across\nnetworks. To facilitate cooperation among multiple computational nodes, the\nproposed framework explores a paradigm shift in the design of communication\nnetwork systems from transmitting information flow to intelligence flow, where\nthe goal of communications is task-oriented and folded into the inference\nprocess. Experimental results demonstrate the effectiveness of the proposed\nframework through an image captioning use case, showcasing the ability to\nreduce response latency while maintaining high-quality captions. This article\nserves as a position paper for identifying the motivation, challenges, and\nprinciples of AI Flow."
                },
                "authors": [
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12469v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12469v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13398v3",
                "updated": "2024-12-31T15:12:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    15,
                    12,
                    20,
                    1,
                    366,
                    0
                ],
                "published": "2024-05-22T07:17:45Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    7,
                    17,
                    45,
                    2,
                    143,
                    0
                ],
                "title": "Agent-Knowledge Logic for Alternative Epistemic Logic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-Knowledge Logic for Alternative Epistemic Logic"
                },
                "summary": "Epistemic logic is known as a logic that captures the knowledge and beliefs\nof agents and has undergone various developments since Hintikka (1962). In this\npaper, we propose a new logic called agent-knowledge logic by taking the\nproduct of individual knowledge structures and the set of relationships among\nagents. This logic is based on the Facebook logic proposed by Seligman et al.\n(2011) and the Logic of Hide and Seek Game proposed by Li et al. (2021). We\nshow two main results; one is that this logic can embed the standard epistemic\nlogic, and the other is that there is a proof system of tableau calculus that\nworks in finite time. We also discuss various sentences and inferences that\nthis logic can express.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epistemic logic is known as a logic that captures the knowledge and beliefs\nof agents and has undergone various developments since Hintikka (1962). In this\npaper, we propose a new logic called agent-knowledge logic by taking the\nproduct of individual knowledge structures and the set of relationships among\nagents. This logic is based on the Facebook logic proposed by Seligman et al.\n(2011) and the Logic of Hide and Seek Game proposed by Li et al. (2021). We\nshow two main results; one is that this logic can embed the standard epistemic\nlogic, and the other is that there is a proof system of tableau calculus that\nworks in finite time. We also discuss various sentences and inferences that\nthis logic can express."
                },
                "authors": [
                    {
                        "name": "Yuki Nishimura"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Nishimura"
                },
                "arxiv_affiliation": "Tokyo Institute of Technology",
                "author": "Yuki Nishimura",
                "arxiv_doi": "10.4204/EPTCS.415.10",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.415.10",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.13398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings NCL'24, arXiv:2412.20053",
                "arxiv_journal_ref": "EPTCS 415, 2024, pp. 77-92",
                "arxiv_primary_category": {
                    "term": "math.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03338v2",
                "updated": "2024-12-31T14:57:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    14,
                    57,
                    11,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-04T14:13:38Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    13,
                    38,
                    2,
                    339,
                    0
                ],
                "title": "AI-Driven Day-to-Day Route Choice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Driven Day-to-Day Route Choice"
                },
                "summary": "Understanding travelers' route choices can help policymakers devise optimal\noperational and planning strategies for both normal and abnormal circumstances.\nHowever, existing choice modeling methods often rely on predefined assumptions\nand struggle to capture the dynamic and adaptive nature of travel behavior.\nRecently, Large Language Models (LLMs) have emerged as a promising alternative,\ndemonstrating remarkable ability to replicate human-like behaviors across\nvarious fields. Despite this potential, their capacity to accurately simulate\nhuman route choice behavior in transportation contexts remains doubtful. To\nsatisfy this curiosity, this paper investigates the potential of LLMs for route\nchoice modeling by introducing an LLM-empowered agent, \"LLMTraveler.\" This\nagent integrates an LLM as its core, equipped with a memory system that learns\nfrom past experiences and makes decisions by balancing retrieved data and\npersonality traits. The study systematically evaluates the LLMTraveler's\nability to replicate human-like decision-making through two stages of\nday-to-day (DTD) congestion games: (1) analyzing its route-switching behavior\nin single origin-destination (OD) pair scenarios, where it demonstrates\npatterns that align with laboratory data but cannot be fully explained by\ntraditional models, and (2) testing its capacity to model adaptive learning\nbehaviors in multi-OD scenarios on the Ortuzar and Willumsen (OW) network,\nproducing results comparable to Multinomial Logit (MNL) and Reinforcement\nLearning (RL) models. These experiments demonstrate that the framework can\npartially replicate human-like decision-making in route choice while providing\nnatural language explanations for its decisions. This capability offers\nvaluable insights for transportation policymaking, such as simulating traveler\nresponses to new policies or changes in the network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding travelers' route choices can help policymakers devise optimal\noperational and planning strategies for both normal and abnormal circumstances.\nHowever, existing choice modeling methods often rely on predefined assumptions\nand struggle to capture the dynamic and adaptive nature of travel behavior.\nRecently, Large Language Models (LLMs) have emerged as a promising alternative,\ndemonstrating remarkable ability to replicate human-like behaviors across\nvarious fields. Despite this potential, their capacity to accurately simulate\nhuman route choice behavior in transportation contexts remains doubtful. To\nsatisfy this curiosity, this paper investigates the potential of LLMs for route\nchoice modeling by introducing an LLM-empowered agent, \"LLMTraveler.\" This\nagent integrates an LLM as its core, equipped with a memory system that learns\nfrom past experiences and makes decisions by balancing retrieved data and\npersonality traits. The study systematically evaluates the LLMTraveler's\nability to replicate human-like decision-making through two stages of\nday-to-day (DTD) congestion games: (1) analyzing its route-switching behavior\nin single origin-destination (OD) pair scenarios, where it demonstrates\npatterns that align with laboratory data but cannot be fully explained by\ntraditional models, and (2) testing its capacity to model adaptive learning\nbehaviors in multi-OD scenarios on the Ortuzar and Willumsen (OW) network,\nproducing results comparable to Multinomial Logit (MNL) and Reinforcement\nLearning (RL) models. These experiments demonstrate that the framework can\npartially replicate human-like decision-making in route choice while providing\nnatural language explanations for its decisions. This capability offers\nvaluable insights for transportation policymaking, such as simulating traveler\nresponses to new policies or changes in the network."
                },
                "authors": [
                    {
                        "name": "Leizhen Wang"
                    },
                    {
                        "name": "Peibo Duan"
                    },
                    {
                        "name": "Zhengbing He"
                    },
                    {
                        "name": "Cheng Lyu"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Nan Zheng"
                    },
                    {
                        "name": "Li Yao"
                    },
                    {
                        "name": "Zhenliang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhenliang Ma"
                },
                "author": "Zhenliang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.06459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.06459v2",
                "updated": "2024-12-31T09:50:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    50,
                    34,
                    1,
                    366,
                    0
                ],
                "published": "2023-09-12T16:36:22Z",
                "published_parsed": [
                    2023,
                    9,
                    12,
                    16,
                    36,
                    22,
                    1,
                    255,
                    0
                ],
                "title": "Sensitivity Analysis for Quantiles of Hidden Biases in Matched\n  Observational Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensitivity Analysis for Quantiles of Hidden Biases in Matched\n  Observational Studies"
                },
                "summary": "Causal conclusions from observational studies may be sensitive to unmeasured\nconfounding. In such cases, a sensitivity analysis is often conducted, which\ntries to infer the minimum amount of hidden biases or the minimum strength of\nunmeasured confounding needed in order to explain away the observed association\nbetween treatment and outcome. If the needed bias is large, then the treatment\nis likely to have significant effects. The Rosenbaum sensitivity analysis is a\nmodern approach for conducting sensitivity analysis in matched observational\nstudies. It investigates what magnitude the maximum of hidden biases from all\nmatched sets needs to be in order to explain away the observed association.\nHowever, such a sensitivity analysis can be overly conservative and\npessimistic, especially when investigators suspect that some matched sets may\nhave exceptionally large hidden biases. In this paper, we generalize\nRosenbaum's framework to conduct sensitivity analysis on quantiles of hidden\nbiases from all matched sets, which are more robust than the maximum. Moreover,\nthe proposed sensitivity analysis is simultaneously valid across all quantiles\nof hidden biases and is thus a free lunch added to the conventional sensitivity\nanalysis. The proposed approach works for general outcomes, general matched\nstudies and general test statistics. In addition, we demonstrate that the\nproposed sensitivity analysis also works for bounded null hypotheses when the\ntest statistic satisfies certain properties. An R package implementing the\nproposed approach is available online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal conclusions from observational studies may be sensitive to unmeasured\nconfounding. In such cases, a sensitivity analysis is often conducted, which\ntries to infer the minimum amount of hidden biases or the minimum strength of\nunmeasured confounding needed in order to explain away the observed association\nbetween treatment and outcome. If the needed bias is large, then the treatment\nis likely to have significant effects. The Rosenbaum sensitivity analysis is a\nmodern approach for conducting sensitivity analysis in matched observational\nstudies. It investigates what magnitude the maximum of hidden biases from all\nmatched sets needs to be in order to explain away the observed association.\nHowever, such a sensitivity analysis can be overly conservative and\npessimistic, especially when investigators suspect that some matched sets may\nhave exceptionally large hidden biases. In this paper, we generalize\nRosenbaum's framework to conduct sensitivity analysis on quantiles of hidden\nbiases from all matched sets, which are more robust than the maximum. Moreover,\nthe proposed sensitivity analysis is simultaneously valid across all quantiles\nof hidden biases and is thus a free lunch added to the conventional sensitivity\nanalysis. The proposed approach works for general outcomes, general matched\nstudies and general test statistics. In addition, we demonstrate that the\nproposed sensitivity analysis also works for bounded null hypotheses when the\ntest statistic satisfies certain properties. An R package implementing the\nproposed approach is available online."
                },
                "authors": [
                    {
                        "name": "Dongxiao Wu"
                    },
                    {
                        "name": "Xinran Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Li"
                },
                "author": "Xinran Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.06459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.06459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15453v2",
                "updated": "2024-12-31T09:13:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    13,
                    6,
                    1,
                    366,
                    0
                ],
                "published": "2024-05-24T11:30:37Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    11,
                    30,
                    37,
                    4,
                    145,
                    0
                ],
                "title": "Benchmarking the Performance of Pre-trained LLMs across Urdu NLP Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking the Performance of Pre-trained LLMs across Urdu NLP Tasks"
                },
                "summary": "Large Language Models (LLMs) pre-trained on multilingual data have\nrevolutionized natural language processing research, by transitioning from\nlanguages and task specific model pipelines to a single model adapted on a\nvariety of tasks. However majority of existing multilingual NLP benchmarks for\nLLMs provide evaluation data in only few languages with little linguistic\ndiversity. In addition these benchmarks lack quality assessment against the\nrespective state-of the art models. This study presents an in-depth examination\nof 7 prominent LLMs: GPT-3.5-turbo, Llama 2-7B-Chat, Llama 3.1-8B, Bloomz 3B,\nBloomz 7B1, Ministral-8B and Whisper (Large, medium and small variant) across\n17 tasks using 22 datasets, 13.8 hours of speech, in a zero-shot setting, and\ntheir performance against state-of-the-art (SOTA) models, has been compared and\nanalyzed. Our experiments show that SOTA models currently outperform\nencoder-decoder models in majority of Urdu NLP tasks under zero-shot settings.\nHowever, comparing Llama 3.1-8B over prior version Llama 2-7B-Chat, we can\ndeduce that with improved language coverage, LLMs can surpass these SOTA\nmodels. Our results emphasize that models with fewer parameters but richer\nlanguage-specific data, like Llama 3.1-8B, often outperform larger models with\nlower language diversity, such as GPT-3.5, in several tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) pre-trained on multilingual data have\nrevolutionized natural language processing research, by transitioning from\nlanguages and task specific model pipelines to a single model adapted on a\nvariety of tasks. However majority of existing multilingual NLP benchmarks for\nLLMs provide evaluation data in only few languages with little linguistic\ndiversity. In addition these benchmarks lack quality assessment against the\nrespective state-of the art models. This study presents an in-depth examination\nof 7 prominent LLMs: GPT-3.5-turbo, Llama 2-7B-Chat, Llama 3.1-8B, Bloomz 3B,\nBloomz 7B1, Ministral-8B and Whisper (Large, medium and small variant) across\n17 tasks using 22 datasets, 13.8 hours of speech, in a zero-shot setting, and\ntheir performance against state-of-the-art (SOTA) models, has been compared and\nanalyzed. Our experiments show that SOTA models currently outperform\nencoder-decoder models in majority of Urdu NLP tasks under zero-shot settings.\nHowever, comparing Llama 3.1-8B over prior version Llama 2-7B-Chat, we can\ndeduce that with improved language coverage, LLMs can surpass these SOTA\nmodels. Our results emphasize that models with fewer parameters but richer\nlanguage-specific data, like Llama 3.1-8B, often outperform larger models with\nlower language diversity, such as GPT-3.5, in several tasks."
                },
                "authors": [
                    {
                        "name": "Munief Hassan Tahir"
                    },
                    {
                        "name": "Sana Shams"
                    },
                    {
                        "name": "Layba Fiaz"
                    },
                    {
                        "name": "Farah Adeeba"
                    },
                    {
                        "name": "Sarmad Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Sarmad Hussain"
                },
                "author": "Sarmad Hussain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.16014v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.16014v4",
                "updated": "2024-12-31T09:12:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    12,
                    26,
                    1,
                    366,
                    0
                ],
                "published": "2023-12-26T11:49:23Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    11,
                    49,
                    23,
                    1,
                    360,
                    0
                ],
                "title": "Passive Non-Line-of-Sight Imaging with Light Transport Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Passive Non-Line-of-Sight Imaging with Light Transport Modulation"
                },
                "summary": "Passive non-line-of-sight (NLOS) imaging has witnessed rapid development in\nrecent years, due to its ability to image objects that are out of sight. The\nlight transport condition plays an important role in this task since changing\nthe conditions will lead to different imaging models. Existing learning-based\nNLOS methods usually train independent models for different light transport\nconditions, which is computationally inefficient and impairs the practicality\nof the models. In this work, we propose NLOS-LTM, a novel passive NLOS imaging\nmethod that effectively handles multiple light transport conditions with a\nsingle network. We achieve this by inferring a latent light transport\nrepresentation from the projection image and using this representation to\nmodulate the network that reconstructs the hidden image from the projection\nimage. We train a light transport encoder together with a vector quantizer to\nobtain the light transport representation. To further regulate this\nrepresentation, we jointly learn both the reconstruction network and the\nreprojection network during training. A set of light transport modulation\nblocks is used to modulate the two jointly trained networks in a multi-scale\nway. Extensive experiments on a large-scale passive NLOS dataset demonstrate\nthe superiority of the proposed method. The code is available at\nhttps://github.com/JerryOctopus/NLOS-LTM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Passive non-line-of-sight (NLOS) imaging has witnessed rapid development in\nrecent years, due to its ability to image objects that are out of sight. The\nlight transport condition plays an important role in this task since changing\nthe conditions will lead to different imaging models. Existing learning-based\nNLOS methods usually train independent models for different light transport\nconditions, which is computationally inefficient and impairs the practicality\nof the models. In this work, we propose NLOS-LTM, a novel passive NLOS imaging\nmethod that effectively handles multiple light transport conditions with a\nsingle network. We achieve this by inferring a latent light transport\nrepresentation from the projection image and using this representation to\nmodulate the network that reconstructs the hidden image from the projection\nimage. We train a light transport encoder together with a vector quantizer to\nobtain the light transport representation. To further regulate this\nrepresentation, we jointly learn both the reconstruction network and the\nreprojection network during training. A set of light transport modulation\nblocks is used to modulate the two jointly trained networks in a multi-scale\nway. Extensive experiments on a large-scale passive NLOS dataset demonstrate\nthe superiority of the proposed method. The code is available at\nhttps://github.com/JerryOctopus/NLOS-LTM."
                },
                "authors": [
                    {
                        "name": "Jiarui Zhang"
                    },
                    {
                        "name": "Ruixu Geng"
                    },
                    {
                        "name": "Xiaolong Du"
                    },
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Houqiang Li"
                    },
                    {
                        "name": "Yang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Hu"
                },
                "author": "Yang Hu",
                "arxiv_doi": "10.1109/TIP.2024.3518097",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TIP.2024.3518097",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.16014v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.16014v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.18634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.18634v2",
                "updated": "2024-12-31T08:55:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    8,
                    55,
                    29,
                    1,
                    366,
                    0
                ],
                "published": "2023-10-28T08:29:49Z",
                "published_parsed": [
                    2023,
                    10,
                    28,
                    8,
                    29,
                    49,
                    5,
                    301,
                    0
                ],
                "title": "SSL Framework for Causal Inconsistency between Structures and\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSL Framework for Causal Inconsistency between Structures and\n  Representations"
                },
                "summary": "The cross-pollination between causal discovery and deep learning has led to\nincreasingly extensive interactions. It results in a large number of deep\nlearning data types (such as images, text, etc.) extending into the field of\ncausal discovery, and a multitude of deep learning tasks have begun to utilize\ncausal discovery to explore the internal causal structure and causal\nrepresentation of data. In this paper, we first identified that a complex data\ntype, ``Indefinite Data\", has conflicts between causal relationships expressed\nby the causal structure and causal representation generated by deep learning\nmodels, a phenomenon referred to as causal inconsistency. We thoroughly\nanalyzed related work to explain why only Indefinite Data exhibits causal\ninconsistency while other data types do not. Furthermore, to alleviate causal\ninconsistency, we proposed a self-supervised learning (SSL) framework based on\nintervention, hoping to provide more causal information from different\nintervention views to promote consistency between structure and representation.\nExtensive experiments have shown that the SSL framework enhances causal\nconsistency and can further improve causal structure and representation\nlearning performance. Additionally, we extended the SSL framework to three\ndifferent downstream tasks and LLM instructions. The quantitative results of\nthese applications all reflect the performance improvement brought about by\ncausal consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cross-pollination between causal discovery and deep learning has led to\nincreasingly extensive interactions. It results in a large number of deep\nlearning data types (such as images, text, etc.) extending into the field of\ncausal discovery, and a multitude of deep learning tasks have begun to utilize\ncausal discovery to explore the internal causal structure and causal\nrepresentation of data. In this paper, we first identified that a complex data\ntype, ``Indefinite Data\", has conflicts between causal relationships expressed\nby the causal structure and causal representation generated by deep learning\nmodels, a phenomenon referred to as causal inconsistency. We thoroughly\nanalyzed related work to explain why only Indefinite Data exhibits causal\ninconsistency while other data types do not. Furthermore, to alleviate causal\ninconsistency, we proposed a self-supervised learning (SSL) framework based on\nintervention, hoping to provide more causal information from different\nintervention views to promote consistency between structure and representation.\nExtensive experiments have shown that the SSL framework enhances causal\nconsistency and can further improve causal structure and representation\nlearning performance. Additionally, we extended the SSL framework to three\ndifferent downstream tasks and LLM instructions. The quantitative results of\nthese applications all reflect the performance improvement brought about by\ncausal consistency."
                },
                "authors": [
                    {
                        "name": "Hang Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Keqing Du"
                    },
                    {
                        "name": "Wenya Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenya Wang"
                },
                "author": "Wenya Wang",
                "arxiv_comment": "29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.18634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.18634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21199v2",
                "updated": "2024-12-31T08:20:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    8,
                    20,
                    42,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T18:58:58Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    58,
                    58,
                    0,
                    365,
                    0
                ],
                "title": "HumanEval Pro and MBPP Pro: Evaluating Large Language Models on\n  Self-invoking Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumanEval Pro and MBPP Pro: Evaluating Large Language Models on\n  Self-invoking Code Generation"
                },
                "summary": "We introduce self-invoking code generation, a new task designed to evaluate\nthe progressive reasoning and problem-solving capabilities of LLMs. In this\ntask, models are presented with a base problem and a related, more complex\nproblem. They must solve the base problem and then utilize its solution to\naddress the more complex one. This work features three key contributions.\nFirst, we propose a general recipe for generating more challenging versions of\nexisting benchmarks, resulting in three new benchmarks: HumanEval Pro, MBPP\nPro, and BigCodeBench-Lite Pro, specifically designed to assess LLMs on\nself-invoking code generation. Second, from the analysis of experimental\nresults over twenty LLMs on our benchmarks, we have two important observations:\n(i) Most LLMs excel in traditional code generation benchmarks like HumanEval\nand MBPP, but their performance declines on self-invoking tasks. For example,\no1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro.\n(ii) On self-invoking code generation task, the instruction-tuned models\ndemonstrate only marginal improvements compared to the base models. Third, we\ndisclose the types of failure modes that exist in our evaluation results. All\nthese results underscore the need for further advancements in self-invoking\ncode generation tasks and provide a new direction for future research on\nenhancing LLMs' code reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce self-invoking code generation, a new task designed to evaluate\nthe progressive reasoning and problem-solving capabilities of LLMs. In this\ntask, models are presented with a base problem and a related, more complex\nproblem. They must solve the base problem and then utilize its solution to\naddress the more complex one. This work features three key contributions.\nFirst, we propose a general recipe for generating more challenging versions of\nexisting benchmarks, resulting in three new benchmarks: HumanEval Pro, MBPP\nPro, and BigCodeBench-Lite Pro, specifically designed to assess LLMs on\nself-invoking code generation. Second, from the analysis of experimental\nresults over twenty LLMs on our benchmarks, we have two important observations:\n(i) Most LLMs excel in traditional code generation benchmarks like HumanEval\nand MBPP, but their performance declines on self-invoking tasks. For example,\no1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro.\n(ii) On self-invoking code generation task, the instruction-tuned models\ndemonstrate only marginal improvements compared to the base models. Third, we\ndisclose the types of failure modes that exist in our evaluation results. All\nthese results underscore the need for further advancements in self-invoking\ncode generation tasks and provide a new direction for future research on\nenhancing LLMs' code reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Zhaojian Yu"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Arman Cohan"
                    },
                    {
                        "name": "Xiao-Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ping Zhang"
                },
                "author": "Xiao-Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15371v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15371v6",
                "updated": "2024-12-31T08:08:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    8,
                    8,
                    20,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-19T10:26:42Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    10,
                    26,
                    42,
                    3,
                    263,
                    0
                ],
                "title": "DiSHA: Dimension-Sharding Adaptation with Fast Convergence and Fast\n  Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiSHA: Dimension-Sharding Adaptation with Fast Convergence and Fast\n  Computation"
                },
                "summary": "Low-Rank Adaptation (LoRA) leverages the low intrinsic rank of weight updates\nin Large Language Models (LLMs), establishing a Parameter-Efficient Fine-Tuning\n(PEFT) paradigm. However, LoRA suffers from slow convergence. We introduce\nDimension-Sharding Adaptation (DiSHA), which expands the PEFT design space to\nunlock lower intrinsic ranks and faster convergence by default. Within DiSHA's\ndesign space, we propose Block Affine Adaptation (Bone), a computationally\nefficient structure that delivers both high performance and efficiency. While\ncertain DiSHA configurations may result in colinear updates to weight shards,\nwe address this with Block Affine Transformation Adaptation (BAT), a nonlinear\nvariant of DiSHA. BAT introduces nonlinearity by combining trainable matrices\nwith original weight shards in a nonlinear manner, inducing nonlinearity in\nmatrix updates without introducing additional parameters. Empirical results\nshow that Bone, under the DiSHA framework, consistently outperforms LoRA\nvariants in both NLG and NLU tasks, with significantly improved computational\nefficiency. Further analysis demonstrates that BAT enhances model capabilities\nby leveraging its nonlinear design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) leverages the low intrinsic rank of weight updates\nin Large Language Models (LLMs), establishing a Parameter-Efficient Fine-Tuning\n(PEFT) paradigm. However, LoRA suffers from slow convergence. We introduce\nDimension-Sharding Adaptation (DiSHA), which expands the PEFT design space to\nunlock lower intrinsic ranks and faster convergence by default. Within DiSHA's\ndesign space, we propose Block Affine Adaptation (Bone), a computationally\nefficient structure that delivers both high performance and efficiency. While\ncertain DiSHA configurations may result in colinear updates to weight shards,\nwe address this with Block Affine Transformation Adaptation (BAT), a nonlinear\nvariant of DiSHA. BAT introduces nonlinearity by combining trainable matrices\nwith original weight shards in a nonlinear manner, inducing nonlinearity in\nmatrix updates without introducing additional parameters. Empirical results\nshow that Bone, under the DiSHA framework, consistently outperforms LoRA\nvariants in both NLG and NLU tasks, with significantly improved computational\nefficiency. Further analysis demonstrates that BAT enhances model capabilities\nby leveraging its nonlinear design."
                },
                "authors": [
                    {
                        "name": "Jiale Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jiale Kang"
                },
                "author": "Jiale Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15371v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15371v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16526v2",
                "updated": "2024-12-31T07:56:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    56,
                    59,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-21T08:09:12Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    8,
                    9,
                    12,
                    5,
                    356,
                    0
                ],
                "title": "Text2midi: Generating Symbolic Music from Captions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2midi: Generating Symbolic Music from Captions"
                },
                "summary": "This paper introduces text2midi, an end-to-end model to generate MIDI files\nfrom textual descriptions. Leveraging the growing popularity of multimodal\ngenerative approaches, text2midi capitalizes on the extensive availability of\ntextual data and the success of large language models (LLMs). Our end-to-end\nsystem harnesses the power of LLMs to generate symbolic music in the form of\nMIDI files. Specifically, we utilize a pretrained LLM encoder to process\ncaptions, which then condition an autoregressive transformer decoder to produce\nMIDI sequences that accurately reflect the provided descriptions. This\nintuitive and user-friendly method significantly streamlines the music creation\nprocess by allowing users to generate music pieces using text prompts. We\nconduct comprehensive empirical evaluations, incorporating both automated and\nhuman studies, that show our model generates MIDI files of high quality that\nare indeed controllable by text captions that may include music theory terms\nsuch as chords, keys, and tempo. We release the code and music samples on our\ndemo page (https://github.com/AMAAI-Lab/Text2midi) for users to interact with\ntext2midi.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces text2midi, an end-to-end model to generate MIDI files\nfrom textual descriptions. Leveraging the growing popularity of multimodal\ngenerative approaches, text2midi capitalizes on the extensive availability of\ntextual data and the success of large language models (LLMs). Our end-to-end\nsystem harnesses the power of LLMs to generate symbolic music in the form of\nMIDI files. Specifically, we utilize a pretrained LLM encoder to process\ncaptions, which then condition an autoregressive transformer decoder to produce\nMIDI sequences that accurately reflect the provided descriptions. This\nintuitive and user-friendly method significantly streamlines the music creation\nprocess by allowing users to generate music pieces using text prompts. We\nconduct comprehensive empirical evaluations, incorporating both automated and\nhuman studies, that show our model generates MIDI files of high quality that\nare indeed controllable by text captions that may include music theory terms\nsuch as chords, keys, and tempo. We release the code and music samples on our\ndemo page (https://github.com/AMAAI-Lab/Text2midi) for users to interact with\ntext2midi."
                },
                "authors": [
                    {
                        "name": "Keshav Bhandari"
                    },
                    {
                        "name": "Abhinaba Roy"
                    },
                    {
                        "name": "Kyra Wang"
                    },
                    {
                        "name": "Geeta Puri"
                    },
                    {
                        "name": "Simon Colton"
                    },
                    {
                        "name": "Dorien Herremans"
                    }
                ],
                "author_detail": {
                    "name": "Dorien Herremans"
                },
                "author": "Dorien Herremans",
                "arxiv_comment": "9 pages, 3 figures, Accepted at the 39th AAAI Conference on\n  Artificial Intelligence (AAAI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.12871v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.12871v9",
                "updated": "2024-12-31T07:56:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    56,
                    13,
                    1,
                    366,
                    0
                ],
                "published": "2023-09-22T13:52:42Z",
                "published_parsed": [
                    2023,
                    9,
                    22,
                    13,
                    52,
                    42,
                    4,
                    265,
                    0
                ],
                "title": "AnglE-optimized Text Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnglE-optimized Text Embeddings"
                },
                "summary": "High-quality text embedding is pivotal in improving semantic textual\nsimilarity (STS) tasks, which are crucial components in Large Language Model\n(LLM) applications. However, a common challenge existing text embedding models\nface is the problem of vanishing gradients, primarily due to their reliance on\nthe cosine function in the optimization objective, which has saturation zones.\nTo address this issue, this paper proposes a novel angle-optimized text\nembedding model called AnglE. The core idea of AnglE is to introduce angle\noptimization in a complex space. This novel approach effectively mitigates the\nadverse effects of the saturation zone in the cosine function, which can impede\ngradient and hinder optimization processes. To set up a comprehensive STS\nevaluation, we experimented on existing short-text STS datasets and a newly\ncollected long-text STS dataset from GitHub Issues. Furthermore, we examine\ndomain-specific STS scenarios with limited labeled data and explore how AnglE\nworks with LLM-annotated data. Extensive experiments were conducted on various\ntasks including short-text STS, long-text STS, and domain-specific STS tasks.\nThe results show that AnglE outperforms the state-of-the-art (SOTA) STS models\nthat ignore the cosine saturation zone. These findings demonstrate the ability\nof AnglE to generate high-quality text embeddings and the usefulness of angle\noptimization in STS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality text embedding is pivotal in improving semantic textual\nsimilarity (STS) tasks, which are crucial components in Large Language Model\n(LLM) applications. However, a common challenge existing text embedding models\nface is the problem of vanishing gradients, primarily due to their reliance on\nthe cosine function in the optimization objective, which has saturation zones.\nTo address this issue, this paper proposes a novel angle-optimized text\nembedding model called AnglE. The core idea of AnglE is to introduce angle\noptimization in a complex space. This novel approach effectively mitigates the\nadverse effects of the saturation zone in the cosine function, which can impede\ngradient and hinder optimization processes. To set up a comprehensive STS\nevaluation, we experimented on existing short-text STS datasets and a newly\ncollected long-text STS dataset from GitHub Issues. Furthermore, we examine\ndomain-specific STS scenarios with limited labeled data and explore how AnglE\nworks with LLM-annotated data. Extensive experiments were conducted on various\ntasks including short-text STS, long-text STS, and domain-specific STS tasks.\nThe results show that AnglE outperforms the state-of-the-art (SOTA) STS models\nthat ignore the cosine saturation zone. These findings demonstrate the ability\nof AnglE to generate high-quality text embeddings and the usefulness of angle\noptimization in STS."
                },
                "authors": [
                    {
                        "name": "Xianming Li"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li",
                "arxiv_comment": "Published at the Proceedings of ACL24. AoE: Angle-optimized\n  Embeddings for Semantic Textual Similarity\n  (https://aclanthology.org/2024.acl-long.101/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.12871v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.12871v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04139v2",
                "updated": "2024-12-31T07:04:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    4,
                    56,
                    1,
                    366,
                    0
                ],
                "published": "2024-10-05T12:27:47Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    12,
                    27,
                    47,
                    5,
                    279,
                    0
                ],
                "title": "From Reading to Compressing: Exploring the Multi-document Reader for\n  Prompt Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reading to Compressing: Exploring the Multi-document Reader for\n  Prompt Compression"
                },
                "summary": "Large language models (LLMs) have achieved significant performance gains\nusing advanced prompting techniques over various tasks. However, the increasing\nlength of prompts leads to high computational costs and often obscures crucial\ninformation. Prompt compression has been proposed to alleviate these issues,\nbut it faces challenges in (i) capturing the global context and (ii) training\nthe compressor effectively. To tackle these challenges, we introduce a novel\nprompt compression method, namely Reading To Compressing (R2C), utilizing the\nFusion-in-Decoder (FiD) architecture to identify the important information in\nthe prompt. Specifically, the cross-attention scores of the FiD are used to\ndiscern essential chunks and sentences from the prompt. R2C effectively\ncaptures the global context without compromising semantic consistency while\ndetouring the necessity of pseudo-labels for training the compressor. Empirical\nresults show that R2C retains key contexts, enhancing the LLM performance by 6%\nin out-of-domain evaluations while reducing the prompt length by 80%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant performance gains\nusing advanced prompting techniques over various tasks. However, the increasing\nlength of prompts leads to high computational costs and often obscures crucial\ninformation. Prompt compression has been proposed to alleviate these issues,\nbut it faces challenges in (i) capturing the global context and (ii) training\nthe compressor effectively. To tackle these challenges, we introduce a novel\nprompt compression method, namely Reading To Compressing (R2C), utilizing the\nFusion-in-Decoder (FiD) architecture to identify the important information in\nthe prompt. Specifically, the cross-attention scores of the FiD are used to\ndiscern essential chunks and sentences from the prompt. R2C effectively\ncaptures the global context without compromising semantic consistency while\ndetouring the necessity of pseudo-labels for training the compressor. Empirical\nresults show that R2C retains key contexts, enhancing the LLM performance by 6%\nin out-of-domain evaluations while reducing the prompt length by 80%."
                },
                "authors": [
                    {
                        "name": "Eunseong Choi"
                    },
                    {
                        "name": "Sunkyung Lee"
                    },
                    {
                        "name": "Minjin Choi"
                    },
                    {
                        "name": "June Park"
                    },
                    {
                        "name": "Jongwuk Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jongwuk Lee"
                },
                "author": "Jongwuk Lee",
                "arxiv_comment": "Findings of the Association for Computational Linguistics: EMNLP\n  2024; 21 pages; 10 figures and 7 tables. Code available at\n  https://github.com/eunseongc/R2C",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02762v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02762v3",
                "updated": "2024-12-31T06:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    6,
                    54,
                    19,
                    1,
                    366,
                    0
                ],
                "published": "2023-07-06T04:05:44Z",
                "published_parsed": [
                    2023,
                    7,
                    6,
                    4,
                    5,
                    44,
                    3,
                    187,
                    0
                ],
                "title": "PRD: Peer Rank and Discussion Improve Large Language Model based\n  Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRD: Peer Rank and Discussion Improve Large Language Model based\n  Evaluations"
                },
                "summary": "Nowadays, the quality of responses generated by different modern large\nlanguage models (LLMs) is hard to evaluate and compare automatically. Recent\nstudies suggest and predominantly use LLMs for reference-free evaluation of\nopen-ended question answering. More specifically, they use the recognized\n\"strongest\" LLM as the evaluator, which conducts pairwise comparisons of\ncandidate models' answers and provides a ranking score. However, this intuitive\nmethod has multiple problems, such as bringing in self-enhancement (favoring\nits own answers) and positional bias. We draw insights and lessons from the\neducational domain (Cho & MacArthur, 2011; Walsh, 2014) to improve LLM-based\nevaluations. Specifically, we propose (1) the peer rank (PR) algorithm that\ntakes into account each peer LLM's pairwise preferences of all answer pairs,\nand outputs a final ranking of models; and (2) peer discussion (PD), where we\nprompt two LLMs to discuss and try to reach a mutual agreement on the\npreferences of two answers. We conduct experiments on two benchmark datasets.\nWe find that our approaches achieve higher accuracy and align better with human\njudgments. Interestingly, PR can induce a relatively accurate self-ranking of\nmodels under the anonymous setting, where each model's name is unrevealed. Our\nwork provides space to explore evaluating models that are hard to compare for\nhumans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the quality of responses generated by different modern large\nlanguage models (LLMs) is hard to evaluate and compare automatically. Recent\nstudies suggest and predominantly use LLMs for reference-free evaluation of\nopen-ended question answering. More specifically, they use the recognized\n\"strongest\" LLM as the evaluator, which conducts pairwise comparisons of\ncandidate models' answers and provides a ranking score. However, this intuitive\nmethod has multiple problems, such as bringing in self-enhancement (favoring\nits own answers) and positional bias. We draw insights and lessons from the\neducational domain (Cho & MacArthur, 2011; Walsh, 2014) to improve LLM-based\nevaluations. Specifically, we propose (1) the peer rank (PR) algorithm that\ntakes into account each peer LLM's pairwise preferences of all answer pairs,\nand outputs a final ranking of models; and (2) peer discussion (PD), where we\nprompt two LLMs to discuss and try to reach a mutual agreement on the\npreferences of two answers. We conduct experiments on two benchmark datasets.\nWe find that our approaches achieve higher accuracy and align better with human\njudgments. Interestingly, PR can induce a relatively accurate self-ranking of\nmodels under the anonymous setting, where each model's name is unrevealed. Our\nwork provides space to explore evaluating models that are hard to compare for\nhumans."
                },
                "authors": [
                    {
                        "name": "Ruosen Li"
                    },
                    {
                        "name": "Teerth Patel"
                    },
                    {
                        "name": "Xinya Du"
                    }
                ],
                "author_detail": {
                    "name": "Xinya Du"
                },
                "author": "Xinya Du",
                "arxiv_comment": "Accepted by TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.02762v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02762v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09744v2",
                "updated": "2024-12-31T06:44:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    6,
                    44,
                    42,
                    1,
                    366,
                    0
                ],
                "published": "2024-08-19T07:15:44Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    7,
                    15,
                    44,
                    0,
                    232,
                    0
                ],
                "title": "RealCustom++: Representing Images as Real-Word for Real-Time\n  Customization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealCustom++: Representing Images as Real-Word for Real-Time\n  Customization"
                },
                "summary": "Text-to-image customization, which takes given texts and images depicting\ngiven subjects as inputs, aims to synthesize new images that align with both\ntext semantics and subject appearance. This task provides precise control over\ndetails that text alone cannot capture and is fundamental for various\nreal-world applications, garnering significant interest from academia and\nindustry. Existing works follow the pseudo-word paradigm, which involves\nrepresenting given subjects as pseudo-words and combining them with given texts\nto collectively guide the generation. However, the inherent conflict and\nentanglement between the pseudo-words and texts result in a dual-optimum\nparadox, where subject similarity and text controllability cannot be optimal\nsimultaneously. We propose a novel real-words paradigm termed RealCustom++ that\ninstead represents subjects as non-conflict real words, thereby disentangling\nsubject similarity from text controllability and allowing both to be optimized\nsimultaneously. Specifically, RealCustom++ introduces a novel \"train-inference\"\ndecoupled framework: (1) During training, RealCustom++ learns the alignment\nbetween vision conditions and all real words in the text, ensuring high\nsubject-similarity generation in open domains. This is achieved by the\ncross-layer cross-scale projector to robustly and finely extract subject\nfeatures, and a curriculum training recipe that adapts the generated subject to\ndiverse poses and sizes. (2) During inference, leveraging the learned general\nalignment, an adaptive mask guidance is proposed to only customize the\ngeneration of the specific target real word, keeping other subject-irrelevant\nregions uncontaminated to ensure high text-controllability in real-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image customization, which takes given texts and images depicting\ngiven subjects as inputs, aims to synthesize new images that align with both\ntext semantics and subject appearance. This task provides precise control over\ndetails that text alone cannot capture and is fundamental for various\nreal-world applications, garnering significant interest from academia and\nindustry. Existing works follow the pseudo-word paradigm, which involves\nrepresenting given subjects as pseudo-words and combining them with given texts\nto collectively guide the generation. However, the inherent conflict and\nentanglement between the pseudo-words and texts result in a dual-optimum\nparadox, where subject similarity and text controllability cannot be optimal\nsimultaneously. We propose a novel real-words paradigm termed RealCustom++ that\ninstead represents subjects as non-conflict real words, thereby disentangling\nsubject similarity from text controllability and allowing both to be optimized\nsimultaneously. Specifically, RealCustom++ introduces a novel \"train-inference\"\ndecoupled framework: (1) During training, RealCustom++ learns the alignment\nbetween vision conditions and all real words in the text, ensuring high\nsubject-similarity generation in open domains. This is achieved by the\ncross-layer cross-scale projector to robustly and finely extract subject\nfeatures, and a curriculum training recipe that adapts the generated subject to\ndiverse poses and sizes. (2) During inference, leveraging the learned general\nalignment, an adaptive mask guidance is proposed to only customize the\ngeneration of the specific target real word, keeping other subject-irrelevant\nregions uncontaminated to ensure high text-controllability in real-time."
                },
                "authors": [
                    {
                        "name": "Zhendong Mao"
                    },
                    {
                        "name": "Mengqi Huang"
                    },
                    {
                        "name": "Fei Ding"
                    },
                    {
                        "name": "Mingcong Liu"
                    },
                    {
                        "name": "Qian He"
                    },
                    {
                        "name": "Yongdong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongdong Zhang"
                },
                "author": "Yongdong Zhang",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20787v2",
                "updated": "2024-12-31T06:43:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    6,
                    43,
                    12,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T08:11:54Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    11,
                    54,
                    0,
                    365,
                    0
                ],
                "title": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for\n  LLMs in Cybersecurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for\n  LLMs in Cybersecurity"
                },
                "summary": "Evaluating Large Language Models (LLMs) is crucial for understanding their\ncapabilities and limitations across various applications, including natural\nlanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,\nand HumanEval assess general LLM performance but lack focus on specific expert\ndomains such as cybersecurity. Previous attempts to create cybersecurity\ndatasets have faced limitations, including insufficient data volume and a\nreliance on multiple-choice questions (MCQs). To address these gaps, we propose\nSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in\nthe cybersecurity domain. SecBench includes questions in various formats (MCQs\nand short-answer questions (SAQs)), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English),\nand across various sub-domains. The dataset was constructed by collecting\nhigh-quality data from open sources and organizing a Cybersecurity Question\nDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used\nthe powerful while cost-effective LLMs to (1). label the data and (2).\nconstructing a grading agent for automatic evaluation of SAQs. Benchmarking\nresults on 13 SOTA LLMs demonstrate the usability of SecBench, which is\narguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website, and\nthe dataset can be accessed via the artifact link.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) is crucial for understanding their\ncapabilities and limitations across various applications, including natural\nlanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,\nand HumanEval assess general LLM performance but lack focus on specific expert\ndomains such as cybersecurity. Previous attempts to create cybersecurity\ndatasets have faced limitations, including insufficient data volume and a\nreliance on multiple-choice questions (MCQs). To address these gaps, we propose\nSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in\nthe cybersecurity domain. SecBench includes questions in various formats (MCQs\nand short-answer questions (SAQs)), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English),\nand across various sub-domains. The dataset was constructed by collecting\nhigh-quality data from open sources and organizing a Cybersecurity Question\nDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used\nthe powerful while cost-effective LLMs to (1). label the data and (2).\nconstructing a grading agent for automatic evaluation of SAQs. Benchmarking\nresults on 13 SOTA LLMs demonstrate the usability of SecBench, which is\narguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website, and\nthe dataset can be accessed via the artifact link."
                },
                "authors": [
                    {
                        "name": "Pengfei Jing"
                    },
                    {
                        "name": "Mengyun Tang"
                    },
                    {
                        "name": "Xiaorong Shi"
                    },
                    {
                        "name": "Xing Zheng"
                    },
                    {
                        "name": "Sen Nie"
                    },
                    {
                        "name": "Shi Wu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Xiapu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Xiapu Luo"
                },
                "author": "Xiapu Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20840v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20840v2",
                "updated": "2024-12-31T06:22:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    6,
                    22,
                    7,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T10:09:51Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    10,
                    9,
                    51,
                    0,
                    365,
                    0
                ],
                "title": "Identifying average causal effect in regression discontinuity design\n  with auxiliary data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying average causal effect in regression discontinuity design\n  with auxiliary data"
                },
                "summary": "Regression discontinuity designs are widely used when treatment assignment is\ndetermined by whether a running variable exceeds a predefined threshold.\nHowever, most research focuses on estimating local causal effects at the\nthreshold, leaving the challenge of identifying treatment effects away from the\ncutoff largely unaddressed. The primary difficulty in this context is that the\ntreatment assignment is deterministically defined by the running variable,\nviolating the commonly assumed positivity assumption. In this paper, we\nintroduce a novel framework for identifying the average causal effect in\nregression discontinuity designs.Our approach assumes the existence of an\nauxiliary variable for which the running variable can be seen as a surrogate,\nand an additional dataset that consists of the running variable and the\nauxiliary variable alongside the traditional regression discontinuity design\nsetup. Under this framework, we propose three estimation methods for the ATE,\nwhich resembles the outcome regression, inverse propensity weighted and doubly\nrobust estimators in classical causal inference literature. Asymptotically\nvalid inference procedures are also provided. To demonstrate the practical\napplication of our method, simulations are conducted to show the good\nperformance of our methods; besides, we use the proposed methods to assess the\ncausal effects of vitamin A supplementation on the severity of autism spectrum\ndisorders in children, where a positive effect is found but with no statistical\nsignificance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression discontinuity designs are widely used when treatment assignment is\ndetermined by whether a running variable exceeds a predefined threshold.\nHowever, most research focuses on estimating local causal effects at the\nthreshold, leaving the challenge of identifying treatment effects away from the\ncutoff largely unaddressed. The primary difficulty in this context is that the\ntreatment assignment is deterministically defined by the running variable,\nviolating the commonly assumed positivity assumption. In this paper, we\nintroduce a novel framework for identifying the average causal effect in\nregression discontinuity designs.Our approach assumes the existence of an\nauxiliary variable for which the running variable can be seen as a surrogate,\nand an additional dataset that consists of the running variable and the\nauxiliary variable alongside the traditional regression discontinuity design\nsetup. Under this framework, we propose three estimation methods for the ATE,\nwhich resembles the outcome regression, inverse propensity weighted and doubly\nrobust estimators in classical causal inference literature. Asymptotically\nvalid inference procedures are also provided. To demonstrate the practical\napplication of our method, simulations are conducted to show the good\nperformance of our methods; besides, we use the proposed methods to assess the\ncausal effects of vitamin A supplementation on the severity of autism spectrum\ndisorders in children, where a positive effect is found but with no statistical\nsignificance."
                },
                "authors": [
                    {
                        "name": "Xinqin Feng"
                    },
                    {
                        "name": "Wenjie Hu"
                    },
                    {
                        "name": "Pu Yang"
                    },
                    {
                        "name": "Tingyu Li"
                    },
                    {
                        "name": "Xiao-Hua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Hua Zhou"
                },
                "author": "Xiao-Hua Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20840v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20840v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18547v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18547v3",
                "updated": "2024-12-31T06:11:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    6,
                    11,
                    39,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-24T16:55:45Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    55,
                    45,
                    1,
                    359,
                    0
                ],
                "title": "Token-Budget-Aware LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Budget-Aware LLM Reasoning"
                },
                "summary": "Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE."
                },
                "authors": [
                    {
                        "name": "Tingxu Han"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Shiyu Zhao"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18547v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18547v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02795v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02795v3",
                "updated": "2024-12-31T06:03:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    6,
                    3,
                    22,
                    1,
                    366,
                    0
                ],
                "published": "2024-06-04T21:43:56Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    21,
                    43,
                    56,
                    1,
                    156,
                    0
                ],
                "title": "ArguMentor: Augmenting User Experiences with Counter-Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArguMentor: Augmenting User Experiences with Counter-Perspectives"
                },
                "summary": "We encounter arguments everyday in the form of social media posts,\npresidential debates, news articles, and even advertisements. A ubiquitous,\ninfluential example is the opinion piece (op-ed). Opinion pieces can provide\nvaluable perspectives, but they often represent only one side of a story, which\ncan make readers susceptible to confirmation bias and echo chambers. Exposure\nto different perspectives can help readers overcome these obstacles and form\nmore robust, nuanced views on important societal issues. We designed\nArguMentor, a human-AI collaboration system that highlights claims in opinion\npieces, identifies counter-arguments for them using a LLM, and generates a\ncontext-based summary of based on current events. It further enhances user\nunderstanding through additional features like a Q\\&A bot (that answers user\nquestions pertaining to the text), DebateMe (an agent that users can argue any\nside of the piece with) and highlighting (where users can highlight a word or\npassage to get its definition or context). Our evaluation on news op-eds shows\nthat participants can generate more arguments and counter-arguments and display\nhigher critical thinking skills after engaging with the system. Further\ndiscussion highlights a more general need for this kind of a system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We encounter arguments everyday in the form of social media posts,\npresidential debates, news articles, and even advertisements. A ubiquitous,\ninfluential example is the opinion piece (op-ed). Opinion pieces can provide\nvaluable perspectives, but they often represent only one side of a story, which\ncan make readers susceptible to confirmation bias and echo chambers. Exposure\nto different perspectives can help readers overcome these obstacles and form\nmore robust, nuanced views on important societal issues. We designed\nArguMentor, a human-AI collaboration system that highlights claims in opinion\npieces, identifies counter-arguments for them using a LLM, and generates a\ncontext-based summary of based on current events. It further enhances user\nunderstanding through additional features like a Q\\&A bot (that answers user\nquestions pertaining to the text), DebateMe (an agent that users can argue any\nside of the piece with) and highlighting (where users can highlight a word or\npassage to get its definition or context). Our evaluation on news op-eds shows\nthat participants can generate more arguments and counter-arguments and display\nhigher critical thinking skills after engaging with the system. Further\ndiscussion highlights a more general need for this kind of a system."
                },
                "authors": [
                    {
                        "name": "Priya Pitre"
                    },
                    {
                        "name": "Kurt Luther"
                    }
                ],
                "author_detail": {
                    "name": "Kurt Luther"
                },
                "author": "Kurt Luther",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02795v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02795v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2106.15051v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2106.15051v5",
                "updated": "2024-12-31T05:30:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    30,
                    33,
                    1,
                    366,
                    0
                ],
                "published": "2021-06-29T01:42:40Z",
                "published_parsed": [
                    2021,
                    6,
                    29,
                    1,
                    42,
                    40,
                    1,
                    180,
                    0
                ],
                "title": "A tree-based model for addressing sparsity and taxa covariance in\n  microbiome compositional count data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A tree-based model for addressing sparsity and taxa covariance in\n  microbiome compositional count data"
                },
                "summary": "Microbiome compositional data are often high-dimensional, sparse, and exhibit\npervasive cross-sample heterogeneity. Generative modeling is a popular approach\nto analyze such data, and effective generative models must accurately\ncharacterize these key features. While high-dimensionality and abundance of\nzeros have received much attention, existing models often lack flexibility in\ncapturing complex cross-sample variability. This limitation can affect\nstatistical efficiency and lead to misleading conclusions in tasks like\ndifferential abundance analysis, clustering, and network analysis. We introduce\na generative model, the \"logistic-tree normal\" (LTN) model, which addresses\nthis issue and effectively captures key characteristics of microbiome data,\nincluding abundance of zeros. LTN employs a tree-based decomposition to\naggregate sparse taxa counts and uses a (multivariate) logistic-normal\ndistribution at tree splits, allowing for flexible covariance adjustments among\ntaxa as needed. The latent Gaussian structure of LTN enables the incorporation\nof multivariate analysis tools that enforce sparsity or low-rank covariance\nassumptions. As a versatile, fully generative model, LTN supports a wide range\nof applications and offers efficient Bayesian inference computational recipes\nthrough conjugate blocked Gibbs sampling with P\\'olya-Gamma augmentation. We\ndemonstrate application of LTN in a compositional mixed-effects model for\ndifferential abundance analysis using numerical experiments and a reanalysis of\nthe infant cohort in the DIABIMMUNE study. Our findings illustrate that LTN, by\nadequately accounting for cross-sample heterogeneity, appropriately generates\nthe proportion of zeros without requiring an explicit zero-inflation component,\nconfirming a recent viewpoint that \"zero-inflation\" in count-based sequencing\ndata are often results of unaccounted cross-sample variation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microbiome compositional data are often high-dimensional, sparse, and exhibit\npervasive cross-sample heterogeneity. Generative modeling is a popular approach\nto analyze such data, and effective generative models must accurately\ncharacterize these key features. While high-dimensionality and abundance of\nzeros have received much attention, existing models often lack flexibility in\ncapturing complex cross-sample variability. This limitation can affect\nstatistical efficiency and lead to misleading conclusions in tasks like\ndifferential abundance analysis, clustering, and network analysis. We introduce\na generative model, the \"logistic-tree normal\" (LTN) model, which addresses\nthis issue and effectively captures key characteristics of microbiome data,\nincluding abundance of zeros. LTN employs a tree-based decomposition to\naggregate sparse taxa counts and uses a (multivariate) logistic-normal\ndistribution at tree splits, allowing for flexible covariance adjustments among\ntaxa as needed. The latent Gaussian structure of LTN enables the incorporation\nof multivariate analysis tools that enforce sparsity or low-rank covariance\nassumptions. As a versatile, fully generative model, LTN supports a wide range\nof applications and offers efficient Bayesian inference computational recipes\nthrough conjugate blocked Gibbs sampling with P\\'olya-Gamma augmentation. We\ndemonstrate application of LTN in a compositional mixed-effects model for\ndifferential abundance analysis using numerical experiments and a reanalysis of\nthe infant cohort in the DIABIMMUNE study. Our findings illustrate that LTN, by\nadequately accounting for cross-sample heterogeneity, appropriately generates\nthe proportion of zeros without requiring an explicit zero-inflation component,\nconfirming a recent viewpoint that \"zero-inflation\" in count-based sequencing\ndata are often results of unaccounted cross-sample variation."
                },
                "authors": [
                    {
                        "name": "Zhuoqun Wang"
                    },
                    {
                        "name": "Jialiang Mao"
                    },
                    {
                        "name": "Li Ma"
                    }
                ],
                "author_detail": {
                    "name": "Li Ma"
                },
                "author": "Li Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2106.15051v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2106.15051v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01334v2",
                "updated": "2024-12-31T04:17:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    4,
                    17,
                    24,
                    1,
                    366,
                    0
                ],
                "published": "2024-03-30T12:13:57Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    12,
                    13,
                    57,
                    5,
                    90,
                    0
                ],
                "title": "Augmenting NER Datasets with LLMs: Towards Automated and Refined\n  Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting NER Datasets with LLMs: Towards Automated and Refined\n  Annotation"
                },
                "summary": "In the field of Natural Language Processing (NLP), Named Entity Recognition\n(NER) is recognized as a critical technology, employed across a wide array of\napplications. Traditional methodologies for annotating datasets for NER models\nare challenged by high costs and variations in dataset quality. This research\nintroduces a novel hybrid annotation approach that synergizes human effort with\nthe capabilities of Large Language Models (LLMs). This approach not only aims\nto ameliorate the noise inherent in manual annotations, such as omissions,\nthereby enhancing the performance of NER models, but also achieves this in a\ncost-effective manner. Additionally, by employing a label mixing strategy, it\naddresses the issue of class imbalance encountered in LLM-based annotations.\nThrough an analysis across multiple datasets, this method has been consistently\nshown to provide superior performance compared to traditional annotation\nmethods, even under constrained budget conditions. This study illuminates the\npotential of leveraging LLMs to improve dataset quality, introduces a novel\ntechnique to mitigate class imbalances, and demonstrates the feasibility of\nachieving high-performance NER in a cost-effective way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of Natural Language Processing (NLP), Named Entity Recognition\n(NER) is recognized as a critical technology, employed across a wide array of\napplications. Traditional methodologies for annotating datasets for NER models\nare challenged by high costs and variations in dataset quality. This research\nintroduces a novel hybrid annotation approach that synergizes human effort with\nthe capabilities of Large Language Models (LLMs). This approach not only aims\nto ameliorate the noise inherent in manual annotations, such as omissions,\nthereby enhancing the performance of NER models, but also achieves this in a\ncost-effective manner. Additionally, by employing a label mixing strategy, it\naddresses the issue of class imbalance encountered in LLM-based annotations.\nThrough an analysis across multiple datasets, this method has been consistently\nshown to provide superior performance compared to traditional annotation\nmethods, even under constrained budget conditions. This study illuminates the\npotential of leveraging LLMs to improve dataset quality, introduces a novel\ntechnique to mitigate class imbalances, and demonstrates the feasibility of\nachieving high-performance NER in a cost-effective way."
                },
                "authors": [
                    {
                        "name": "Yuji Naraki"
                    },
                    {
                        "name": "Ryosuke Yamaki"
                    },
                    {
                        "name": "Yoshikazu Ikeda"
                    },
                    {
                        "name": "Takafumi Horie"
                    },
                    {
                        "name": "Kotaro Yoshida"
                    },
                    {
                        "name": "Ryotaro Shimizu"
                    },
                    {
                        "name": "Hiroki Naganuma"
                    }
                ],
                "author_detail": {
                    "name": "Hiroki Naganuma"
                },
                "author": "Hiroki Naganuma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03242v2",
                "updated": "2024-12-31T03:56:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    56,
                    19,
                    1,
                    366,
                    0
                ],
                "published": "2024-06-05T13:18:55Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    13,
                    18,
                    55,
                    2,
                    157,
                    0
                ],
                "title": "Variational Pseudo Marginal Methods for Jet Reconstruction in Particle\n  Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Pseudo Marginal Methods for Jet Reconstruction in Particle\n  Physics"
                },
                "summary": "Reconstructing jets, which provide vital insights into the properties and\nhistories of subatomic particles produced in high-energy collisions, is a main\nproblem in data analyses in collider physics. This intricate task deals with\nestimating the latent structure of a jet (binary tree) and involves parameters\nsuch as particle energy, momentum, and types. While Bayesian methods offer a\nnatural approach for handling uncertainty and leveraging prior knowledge, they\nface significant challenges due to the super-exponential growth of potential\njet topologies as the number of observed particles increases. To address this,\nwe introduce a Combinatorial Sequential Monte Carlo approach for inferring jet\nlatent structures. As a second contribution, we leverage the resulting\nestimator to develop a variational inference algorithm for parameter learning.\nBuilding on this, we introduce a variational family using a pseudo-marginal\nframework for a fully Bayesian treatment of all variables, unifying the\ngenerative model with the inference process. We illustrate our method's\neffectiveness through experiments using data generated with a collider physics\ngenerative model, highlighting superior speed and accuracy across a range of\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing jets, which provide vital insights into the properties and\nhistories of subatomic particles produced in high-energy collisions, is a main\nproblem in data analyses in collider physics. This intricate task deals with\nestimating the latent structure of a jet (binary tree) and involves parameters\nsuch as particle energy, momentum, and types. While Bayesian methods offer a\nnatural approach for handling uncertainty and leveraging prior knowledge, they\nface significant challenges due to the super-exponential growth of potential\njet topologies as the number of observed particles increases. To address this,\nwe introduce a Combinatorial Sequential Monte Carlo approach for inferring jet\nlatent structures. As a second contribution, we leverage the resulting\nestimator to develop a variational inference algorithm for parameter learning.\nBuilding on this, we introduce a variational family using a pseudo-marginal\nframework for a fully Bayesian treatment of all variables, unifying the\ngenerative model with the inference process. We illustrate our method's\neffectiveness through experiments using data generated with a collider physics\ngenerative model, highlighting superior speed and accuracy across a range of\ntasks."
                },
                "authors": [
                    {
                        "name": "Hanming Yang"
                    },
                    {
                        "name": "Antonio Khalil Moretti"
                    },
                    {
                        "name": "Sebastian Macaluso"
                    },
                    {
                        "name": "Philippe Chlenski"
                    },
                    {
                        "name": "Christian A. Naesseth"
                    },
                    {
                        "name": "Itsik Pe'er"
                    }
                ],
                "author_detail": {
                    "name": "Itsik Pe'er"
                },
                "author": "Itsik Pe'er",
                "arxiv_comment": "21 pages, 9 figures",
                "arxiv_journal_ref": "Transactions on Machine Learning Research 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20072v2",
                "updated": "2024-12-31T03:11:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    11,
                    3,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-28T07:54:14Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    7,
                    54,
                    14,
                    5,
                    363,
                    0
                ],
                "title": "Extract Information from Hybrid Long Documents Leveraging LLMs: A\n  Framework and Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extract Information from Hybrid Long Documents Leveraging LLMs: A\n  Framework and Dataset"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional performance in textual\nunderstanding and tabular reasoning tasks. However, their ability to comprehend\nand analyze hybrid text, containing textual and tabular data, remains\nunexplored. The hybrid text often appears in the form of hybrid long documents\n(HLDs), which far exceed the token limit of LLMs. Consequently, we apply an\nAutomated Information Extraction framework (AIE) to enable LLMs to process the\nHLDs and carry out experiments to analyse four important aspects of information\nextraction from HLDs. Given the findings: 1) The effective way to select and\nsummarize the useful part of a HLD. 2) An easy table serialization way is\nenough for LLMs to understand tables. 3) The naive AIE has adaptability in many\ncomplex scenarios. 4) The useful prompt engineering to enhance LLMs on HLDs. To\naddress the issue of dataset scarcity in HLDs and support future work, we also\npropose the Financial Reports Numerical Extraction (FINE) dataset. The dataset\nand code are publicly available in the attachments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional performance in textual\nunderstanding and tabular reasoning tasks. However, their ability to comprehend\nand analyze hybrid text, containing textual and tabular data, remains\nunexplored. The hybrid text often appears in the form of hybrid long documents\n(HLDs), which far exceed the token limit of LLMs. Consequently, we apply an\nAutomated Information Extraction framework (AIE) to enable LLMs to process the\nHLDs and carry out experiments to analyse four important aspects of information\nextraction from HLDs. Given the findings: 1) The effective way to select and\nsummarize the useful part of a HLD. 2) An easy table serialization way is\nenough for LLMs to understand tables. 3) The naive AIE has adaptability in many\ncomplex scenarios. 4) The useful prompt engineering to enhance LLMs on HLDs. To\naddress the issue of dataset scarcity in HLDs and support future work, we also\npropose the Financial Reports Numerical Extraction (FINE) dataset. The dataset\nand code are publicly available in the attachments."
                },
                "authors": [
                    {
                        "name": "Chongjian Yue"
                    },
                    {
                        "name": "Xinrun Xu"
                    },
                    {
                        "name": "Xiaojun Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Zhiming Ding"
                    },
                    {
                        "name": "Shi Han"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21006v2",
                "updated": "2024-12-31T03:06:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    6,
                    15,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:15:08Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    15,
                    8,
                    0,
                    365,
                    0
                ],
                "title": "Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant\n  Rationale via Principled Criteria",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant\n  Rationale via Principled Criteria"
                },
                "summary": "Large Language Models (LLMs) rely on generating extensive intermediate\nreasoning units (e.g., tokens, sentences) to enhance final answer quality\nacross a wide range of complex tasks. While generating multiple reasoning paths\nor iteratively refining rationales proves effective for improving performance,\nthese approaches inevitably result in significantly higher inference costs. In\nthis work, we propose a novel sentence-level rationale reduction training\nframework that leverages likelihood-based criteria, verbosity, to identify and\nremove redundant reasoning sentences. Unlike previous approaches that utilize\ntoken-level reduction, our sentence-level reduction framework maintains model\nperformance while reducing generation length. This preserves the original\nreasoning abilities of LLMs and achieves an average 17.15% reduction in\ngeneration costs across various models and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) rely on generating extensive intermediate\nreasoning units (e.g., tokens, sentences) to enhance final answer quality\nacross a wide range of complex tasks. While generating multiple reasoning paths\nor iteratively refining rationales proves effective for improving performance,\nthese approaches inevitably result in significantly higher inference costs. In\nthis work, we propose a novel sentence-level rationale reduction training\nframework that leverages likelihood-based criteria, verbosity, to identify and\nremove redundant reasoning sentences. Unlike previous approaches that utilize\ntoken-level reduction, our sentence-level reduction framework maintains model\nperformance while reducing generation length. This preserves the original\nreasoning abilities of LLMs and achieves an average 17.15% reduction in\ngeneration costs across various models and tasks."
                },
                "authors": [
                    {
                        "name": "Joonwon Jang"
                    },
                    {
                        "name": "Jaehee Kim"
                    },
                    {
                        "name": "Wonbin Kweon"
                    },
                    {
                        "name": "Hwanjo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjo Yu"
                },
                "author": "Hwanjo Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05636v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05636v3",
                "updated": "2024-12-31T02:10:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    2,
                    10,
                    29,
                    1,
                    366,
                    0
                ],
                "published": "2024-08-10T21:24:25Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    21,
                    24,
                    25,
                    5,
                    223,
                    0
                ],
                "title": "Speculative Diffusion Decoding: Accelerating Language Generation through\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative Diffusion Decoding: Accelerating Language Generation through\n  Diffusion"
                },
                "summary": "Speculative decoding has emerged as a widely adopted method to accelerate\nlarge language model inference without sacrificing the quality of the model\noutputs. While this technique has facilitated notable speed improvements by\nenabling parallel sequence verification, its efficiency remains inherently\nlimited by the reliance on incremental token generation in existing draft\nmodels. To overcome this limitation, this paper proposes an adaptation of\nspeculative decoding which uses discrete diffusion models to generate draft\nsequences. This allows parallelization of both the drafting and verification\nsteps, providing significant speed-ups to the inference process. Our proposed\napproach, Speculative Diffusion Decoding (SpecDiff), is validated on standard\nlanguage generation benchmarks and empirically demonstrated to provide a up to\n8.7x speed-up over standard generation processes and up to 2.5x speed-up over\nexisting speculative decoding approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has emerged as a widely adopted method to accelerate\nlarge language model inference without sacrificing the quality of the model\noutputs. While this technique has facilitated notable speed improvements by\nenabling parallel sequence verification, its efficiency remains inherently\nlimited by the reliance on incremental token generation in existing draft\nmodels. To overcome this limitation, this paper proposes an adaptation of\nspeculative decoding which uses discrete diffusion models to generate draft\nsequences. This allows parallelization of both the drafting and verification\nsteps, providing significant speed-ups to the inference process. Our proposed\napproach, Speculative Diffusion Decoding (SpecDiff), is validated on standard\nlanguage generation benchmarks and empirically demonstrated to provide a up to\n8.7x speed-up over standard generation processes and up to 2.5x speed-up over\nexisting speculative decoding approaches."
                },
                "authors": [
                    {
                        "name": "Jacob K Christopher"
                    },
                    {
                        "name": "Brian R Bartoldson"
                    },
                    {
                        "name": "Tal Ben-Nun"
                    },
                    {
                        "name": "Michael Cardei"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Ferdinando Fioretto"
                    }
                ],
                "author_detail": {
                    "name": "Ferdinando Fioretto"
                },
                "author": "Ferdinando Fioretto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05636v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05636v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11694v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11694v4",
                "updated": "2024-12-31T01:38:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    1,
                    38,
                    12,
                    1,
                    366,
                    0
                ],
                "published": "2024-11-18T16:15:17Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    15,
                    17,
                    0,
                    323,
                    0
                ],
                "title": "Enhancing LLM Reasoning with Reward-guided Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Reasoning with Reward-guided Tree Search"
                },
                "summary": "Recently, test-time scaling has garnered significant attention from the\nresearch community, largely due to the substantial advancements of the o1 model\nreleased by OpenAI. By allocating more computational resources during the\ninference phase, large language models~(LLMs) can extensively explore the\nsolution space by generating more thought tokens or diverse solutions, thereby\nproducing more accurate responses. However, developing an o1-like reasoning\napproach is challenging, and researchers have been making various attempts to\nadvance this open area of research. In this paper, we present a preliminary\nexploration into enhancing the reasoning abilities of LLMs through\nreward-guided tree search algorithms. This framework is implemented by\nintegrating the policy model, reward model, and search algorithm. It is\nprimarily constructed around a tree search algorithm, where the policy model\nnavigates a dynamically expanding tree guided by a specially trained reward\nmodel. The implemented framework is denoted as \\textbf{STILL-1}. We thoroughly\nexplore various design considerations necessary for implementing this framework\nand provide a detailed report of the technical aspects. To assess the\neffectiveness of our approach, we focus on mathematical reasoning tasks and\nconduct extensive evaluations on four challenging datasets, significantly\nenhancing the reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, test-time scaling has garnered significant attention from the\nresearch community, largely due to the substantial advancements of the o1 model\nreleased by OpenAI. By allocating more computational resources during the\ninference phase, large language models~(LLMs) can extensively explore the\nsolution space by generating more thought tokens or diverse solutions, thereby\nproducing more accurate responses. However, developing an o1-like reasoning\napproach is challenging, and researchers have been making various attempts to\nadvance this open area of research. In this paper, we present a preliminary\nexploration into enhancing the reasoning abilities of LLMs through\nreward-guided tree search algorithms. This framework is implemented by\nintegrating the policy model, reward model, and search algorithm. It is\nprimarily constructed around a tree search algorithm, where the policy model\nnavigates a dynamically expanding tree guided by a specially trained reward\nmodel. The implemented framework is denoted as \\textbf{STILL-1}. We thoroughly\nexplore various design considerations necessary for implementing this framework\nand provide a detailed report of the technical aspects. To assess the\neffectiveness of our approach, we focus on mathematical reasoning tasks and\nconduct extensive evaluations on four challenging datasets, significantly\nenhancing the reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Xiaoxue Cheng"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Haoxiang Sun"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Technical Report on Slow Thinking with LLMs: I",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11694v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11694v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02504v2",
                "updated": "2024-12-31T01:36:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    1,
                    36,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-10-03T14:09:58Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    9,
                    58,
                    3,
                    277,
                    0
                ],
                "title": "Dual Active Learning for Reinforcement Learning from Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Active Learning for Reinforcement Learning from Human Feedback"
                },
                "summary": "Aligning large language models (LLMs) with human preferences is critical to\nrecent advances in generative artificial intelligence. Reinforcement learning\nfrom human feedback (RLHF) is widely applied to achieve this objective. A key\nstep in RLHF is to learn the reward function from human feedback. However,\nhuman feedback is costly and time-consuming, making it essential to collect\nhigh-quality conversation data for human teachers to label. Additionally,\ndifferent human teachers have different levels of expertise. It is thus\ncritical to query the most appropriate teacher for their opinions. In this\npaper, we use offline reinforcement learning (RL) to formulate the alignment\nproblem. Motivated by the idea of $D$-optimal design, we first propose a dual\nactive reward learning algorithm for the simultaneous selection of\nconversations and teachers. Next, we apply pessimistic RL to solve the\nalignment problem, based on the learned reward estimator. Theoretically, we\nshow that the reward estimator obtained through our proposed adaptive selection\nstrategy achieves minimal generalized variance asymptotically, and prove that\nthe sub-optimality of our pessimistic policy scales as $O(1/\\sqrt{T})$ with a\ngiven sample budget $T$. Through simulations and experiments on LLMs, we\ndemonstrate the effectiveness of our algorithm and its superiority over\nstate-of-the-arts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human preferences is critical to\nrecent advances in generative artificial intelligence. Reinforcement learning\nfrom human feedback (RLHF) is widely applied to achieve this objective. A key\nstep in RLHF is to learn the reward function from human feedback. However,\nhuman feedback is costly and time-consuming, making it essential to collect\nhigh-quality conversation data for human teachers to label. Additionally,\ndifferent human teachers have different levels of expertise. It is thus\ncritical to query the most appropriate teacher for their opinions. In this\npaper, we use offline reinforcement learning (RL) to formulate the alignment\nproblem. Motivated by the idea of $D$-optimal design, we first propose a dual\nactive reward learning algorithm for the simultaneous selection of\nconversations and teachers. Next, we apply pessimistic RL to solve the\nalignment problem, based on the learned reward estimator. Theoretically, we\nshow that the reward estimator obtained through our proposed adaptive selection\nstrategy achieves minimal generalized variance asymptotically, and prove that\nthe sub-optimality of our pessimistic policy scales as $O(1/\\sqrt{T})$ with a\ngiven sample budget $T$. Through simulations and experiments on LLMs, we\ndemonstrate the effectiveness of our algorithm and its superiority over\nstate-of-the-arts."
                },
                "authors": [
                    {
                        "name": "Pangpang Liu"
                    },
                    {
                        "name": "Chengchun Shi"
                    },
                    {
                        "name": "Will Wei Sun"
                    }
                ],
                "author_detail": {
                    "name": "Will Wei Sun"
                },
                "author": "Will Wei Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11900v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11900v3",
                "updated": "2024-12-30T23:52:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    23,
                    52,
                    49,
                    0,
                    365,
                    0
                ],
                "published": "2024-10-14T19:39:11Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    39,
                    11,
                    0,
                    288,
                    0
                ],
                "title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLARE: Faithful Logic-Aided Reasoning and Exploration"
                },
                "summary": "Modern Question Answering (QA) and Reasoning approaches based on Large\nLanguage Models (LLMs) commonly use prompting techniques, such as\nChain-of-Thought (CoT), assuming the resulting generation will have a more\ngranular exploration and reasoning over the question space and scope. However,\nsuch methods struggle with generating outputs that are faithful to the\nintermediate chain of reasoning produced by the model. On the other end of the\nspectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to\ncombine LLMs with external symbolic solvers. While such approaches boast a high\ndegree of faithfulness, they usually require a model trained for code\ngeneration and struggle with tasks that are ambiguous or hard to formalise\nstrictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided\n$\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel\ninterpretable approach for traversing the problem space using task\ndecompositions. We use the LLM to plan a solution, soft-formalise the query\ninto facts and predicates using a logic programming code and simulate that code\nexecution using an exhaustive multi-hop search over the defined space. Our\nmethod allows us to compute the faithfulness of the reasoning process w.r.t.\nthe generated code and analyse the steps of the multi-hop search without\nrelying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$\nout of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model\nfaithfulness positively correlates with overall performance and further\ndemonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors\nsufficient for and leading to the correct answer with optimal reasoning during\nthe multi-hop search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Question Answering (QA) and Reasoning approaches based on Large\nLanguage Models (LLMs) commonly use prompting techniques, such as\nChain-of-Thought (CoT), assuming the resulting generation will have a more\ngranular exploration and reasoning over the question space and scope. However,\nsuch methods struggle with generating outputs that are faithful to the\nintermediate chain of reasoning produced by the model. On the other end of the\nspectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to\ncombine LLMs with external symbolic solvers. While such approaches boast a high\ndegree of faithfulness, they usually require a model trained for code\ngeneration and struggle with tasks that are ambiguous or hard to formalise\nstrictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided\n$\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel\ninterpretable approach for traversing the problem space using task\ndecompositions. We use the LLM to plan a solution, soft-formalise the query\ninto facts and predicates using a logic programming code and simulate that code\nexecution using an exhaustive multi-hop search over the defined space. Our\nmethod allows us to compute the faithfulness of the reasoning process w.r.t.\nthe generated code and analyse the steps of the multi-hop search without\nrelying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$\nout of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model\nfaithfulness positively correlates with overall performance and further\ndemonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors\nsufficient for and leading to the correct answer with optimal reasoning during\nthe multi-hop search."
                },
                "authors": [
                    {
                        "name": "Erik Arakelyan"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Pat Verga"
                    },
                    {
                        "name": "Patrick Lewis"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11900v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11900v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17548v2",
                "updated": "2024-12-30T22:39:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    22,
                    39,
                    49,
                    0,
                    365,
                    0
                ],
                "published": "2024-06-25T13:36:53Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    13,
                    36,
                    53,
                    1,
                    177,
                    0
                ],
                "title": "Laminator: Verifiable ML Property Cards using Hardware-assisted\n  Attestations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laminator: Verifiable ML Property Cards using Hardware-assisted\n  Attestations"
                },
                "summary": "Regulations increasingly call for various assurances from machine learning\n(ML) model providers about their training data, training process, and model\nbehavior. For better transparency, industry (e.g., Huggingface and Google) has\nadopted model cards and datasheets to describe various properties of training\ndatasets and models. In the same vein, we introduce the notion of inference\ncards to describe the properties of a given inference (e.g., binding of the\noutput to the model and its corresponding input). We coin the term ML property\ncards to collectively refer to these various types of cards.\n  To prevent a malicious model provider from including false information in ML\nproperty cards, they need to be verifiable. We show how to construct verifiable\nML property cards using property attestation, technical mechanisms by which a\nprover (e.g., a model provider) can attest to various ML properties to a\nverifier (e.g., an auditor). Since prior attestation mechanisms based purely on\ncryptography are often narrowly focused (lacking versatility) and inefficient,\nwe need an efficient mechanism to attest different types of properties across\nthe entire ML model pipeline.\n  Emerging widespread support for confidential computing has made it possible\nto run and even train models inside hardware-assisted trusted execution\nenvironments (TEEs), which provide highly efficient attestation mechanisms. We\npropose Laminator, which uses TEEs to provide the first framework for\nverifiable ML property cards via hardware-assisted ML property attestations.\nLaminator is efficient in terms of overhead, scalable to large numbers of\nverifiers, and versatile with respect to the properties it can prove during\ntraining or inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regulations increasingly call for various assurances from machine learning\n(ML) model providers about their training data, training process, and model\nbehavior. For better transparency, industry (e.g., Huggingface and Google) has\nadopted model cards and datasheets to describe various properties of training\ndatasets and models. In the same vein, we introduce the notion of inference\ncards to describe the properties of a given inference (e.g., binding of the\noutput to the model and its corresponding input). We coin the term ML property\ncards to collectively refer to these various types of cards.\n  To prevent a malicious model provider from including false information in ML\nproperty cards, they need to be verifiable. We show how to construct verifiable\nML property cards using property attestation, technical mechanisms by which a\nprover (e.g., a model provider) can attest to various ML properties to a\nverifier (e.g., an auditor). Since prior attestation mechanisms based purely on\ncryptography are often narrowly focused (lacking versatility) and inefficient,\nwe need an efficient mechanism to attest different types of properties across\nthe entire ML model pipeline.\n  Emerging widespread support for confidential computing has made it possible\nto run and even train models inside hardware-assisted trusted execution\nenvironments (TEEs), which provide highly efficient attestation mechanisms. We\npropose Laminator, which uses TEEs to provide the first framework for\nverifiable ML property cards via hardware-assisted ML property attestations.\nLaminator is efficient in terms of overhead, scalable to large numbers of\nverifiers, and versatile with respect to the properties it can prove during\ntraining or inference."
                },
                "authors": [
                    {
                        "name": "Vasisht Duddu"
                    },
                    {
                        "name": "Oskari Jrvinen"
                    },
                    {
                        "name": "Lachlan J Gunn"
                    },
                    {
                        "name": "N Asokan"
                    }
                ],
                "author_detail": {
                    "name": "N Asokan"
                },
                "author": "N Asokan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21520v2",
                "updated": "2024-12-30T22:37:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    22,
                    37,
                    28,
                    0,
                    365,
                    0
                ],
                "published": "2024-10-28T20:42:46Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    20,
                    42,
                    46,
                    0,
                    302,
                    0
                ],
                "title": "LLM-Forest: Ensemble Learning of LLMs with Graph-Augmented Prompts for\n  Data Imputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Forest: Ensemble Learning of LLMs with Graph-Augmented Prompts for\n  Data Imputation"
                },
                "summary": "Missing data imputation is a critical challenge in various domains, such as\nhealthcare and finance, where data completeness is vital for accurate analysis.\nLarge language models (LLMs), trained on vast corpora, have shown strong\npotential in data generation, making them a promising tool for data imputation.\nHowever, challenges persist in designing effective prompts for a\nfinetuning-free process and in mitigating the risk of LLM hallucinations. To\naddress these issues, we propose a novel framework, LLM-Forest, which\nintroduces a \"forest\" of few-shot learning LLM \"trees\" with confidence-based\nweighted voting, inspired by ensemble learning (Random Forest). This framework\nis established on a new concept of bipartite information graphs to identify\nhigh-quality relevant neighboring entries with both feature and value\ngranularity. Extensive experiments on 9 real-world datasets demonstrate the\neffectiveness and efficiency of LLM-Forest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing data imputation is a critical challenge in various domains, such as\nhealthcare and finance, where data completeness is vital for accurate analysis.\nLarge language models (LLMs), trained on vast corpora, have shown strong\npotential in data generation, making them a promising tool for data imputation.\nHowever, challenges persist in designing effective prompts for a\nfinetuning-free process and in mitigating the risk of LLM hallucinations. To\naddress these issues, we propose a novel framework, LLM-Forest, which\nintroduces a \"forest\" of few-shot learning LLM \"trees\" with confidence-based\nweighted voting, inspired by ensemble learning (Random Forest). This framework\nis established on a new concept of bipartite information graphs to identify\nhigh-quality relevant neighboring entries with both feature and value\ngranularity. Extensive experiments on 9 real-world datasets demonstrate the\neffectiveness and efficiency of LLM-Forest."
                },
                "authors": [
                    {
                        "name": "Xinrui He"
                    },
                    {
                        "name": "Yikun Ban"
                    },
                    {
                        "name": "Jiaru Zou"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Curtiss B. Cook"
                    },
                    {
                        "name": "Jingrui He"
                    }
                ],
                "author_detail": {
                    "name": "Jingrui He"
                },
                "author": "Jingrui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18566v2",
                "updated": "2024-12-30T22:25:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    22,
                    25,
                    56,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-24T17:37:11Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    37,
                    11,
                    1,
                    359,
                    0
                ],
                "title": "Zero-resource Speech Translation and Recognition with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-resource Speech Translation and Recognition with LLMs"
                },
                "summary": "Despite recent advancements in speech processing, zero-resource speech\ntranslation (ST) and automatic speech recognition (ASR) remain challenging\nproblems. In this work, we propose to leverage a multilingual Large Language\nModel (LLM) to perform ST and ASR in languages for which the model has never\nseen paired audio-text data. We achieve this by using a pre-trained\nmultilingual speech encoder, a multilingual LLM, and a lightweight adaptation\nmodule that maps the audio representations to the token embedding space of the\nLLM. We perform several experiments both in ST and ASR to understand how to\nbest train the model and what data has the most impact on performance in\npreviously unseen languages. In ST, our best model is capable to achieve BLEU\nscores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we\nachieve WERs of up to 28.2\\%. We finally show that the performance of our\nsystem is bounded by the ability of the LLM to output text in the desired\nlanguage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advancements in speech processing, zero-resource speech\ntranslation (ST) and automatic speech recognition (ASR) remain challenging\nproblems. In this work, we propose to leverage a multilingual Large Language\nModel (LLM) to perform ST and ASR in languages for which the model has never\nseen paired audio-text data. We achieve this by using a pre-trained\nmultilingual speech encoder, a multilingual LLM, and a lightweight adaptation\nmodule that maps the audio representations to the token embedding space of the\nLLM. We perform several experiments both in ST and ASR to understand how to\nbest train the model and what data has the most impact on performance in\npreviously unseen languages. In ST, our best model is capable to achieve BLEU\nscores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we\nachieve WERs of up to 28.2\\%. We finally show that the performance of our\nsystem is bounded by the ability of the LLM to output text in the desired\nlanguage."
                },
                "authors": [
                    {
                        "name": "Karel Mundnich"
                    },
                    {
                        "name": "Xing Niu"
                    },
                    {
                        "name": "Prashant Mathur"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Brady Houston"
                    },
                    {
                        "name": "Veera Raghavendra Elluru"
                    },
                    {
                        "name": "Nilaksh Das"
                    },
                    {
                        "name": "Zejiang Hou"
                    },
                    {
                        "name": "Goeric Huybrechts"
                    },
                    {
                        "name": "Anshu Bhatia"
                    },
                    {
                        "name": "Daniel Garcia-Romero"
                    },
                    {
                        "name": "Kyu J. Han"
                    },
                    {
                        "name": "Katrin Kirchhoff"
                    }
                ],
                "author_detail": {
                    "name": "Katrin Kirchhoff"
                },
                "author": "Katrin Kirchhoff",
                "arxiv_comment": "ICASSP 2025, 5 pages, 2 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04418v2",
                "updated": "2024-12-30T21:38:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    21,
                    38,
                    56,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-05T18:44:33Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    44,
                    33,
                    3,
                    340,
                    0
                ],
                "title": "ACE2-SOM: Coupling an ML atmospheric emulator to a slab ocean and\n  learning the sensitivity of climate to changed CO$_2$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACE2-SOM: Coupling an ML atmospheric emulator to a slab ocean and\n  learning the sensitivity of climate to changed CO$_2$"
                },
                "summary": "While autoregressive machine-learning-based emulators have been trained to\nproduce stable and accurate rollouts in the climate of the present-day and\nrecent past, none so far have been trained to emulate the sensitivity of\nclimate to substantial changes in CO$_2$ or other greenhouse gases. As an\ninitial step we couple the Ai2 Climate Emulator version 2 to a slab ocean model\n(hereafter ACE2-SOM) and train it on output from a collection of\nequilibrium-climate physics-based reference simulations with varying levels of\nCO$_2$. We test it in equilibrium and non-equilibrium climate scenarios with\nCO$_2$ concentrations seen and unseen in training.\n  ACE2-SOM performs well in equilibrium-climate inference with both in-sample\nand out-of-sample CO$_2$ concentrations, accurately reproducing the emergent\ntime-mean spatial patterns of surface temperature and precipitation change with\nCO$_2$ doubling, tripling, or quadrupling. In addition, the vertical profile of\natmospheric warming and change in extreme precipitation rates up to the\n99.9999th percentile closely agree with the reference model.\nNon-equilibrium-climate inference is more challenging. With CO$_2$ increasing\ngradually at a rate of 2% year$^{-1}$, ACE2-SOM can accurately emulate the\nglobal annual mean trends of surface and lower-to-middle atmosphere fields but\nproduces unphysical jumps in stratospheric fields. With an abrupt quadrupling\nof CO$_2$, ML-controlled fields transition unrealistically quickly to the\n4xCO$_2$ regime. In doing so they violate global energy conservation and\nexhibit unphysical sensitivities of and surface and top of atmosphere radiative\nfluxes to instantaneous changes in CO$_2$. Future emulator development needed\nto address these issues should improve its generalizability to diverse climate\nchange scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While autoregressive machine-learning-based emulators have been trained to\nproduce stable and accurate rollouts in the climate of the present-day and\nrecent past, none so far have been trained to emulate the sensitivity of\nclimate to substantial changes in CO$_2$ or other greenhouse gases. As an\ninitial step we couple the Ai2 Climate Emulator version 2 to a slab ocean model\n(hereafter ACE2-SOM) and train it on output from a collection of\nequilibrium-climate physics-based reference simulations with varying levels of\nCO$_2$. We test it in equilibrium and non-equilibrium climate scenarios with\nCO$_2$ concentrations seen and unseen in training.\n  ACE2-SOM performs well in equilibrium-climate inference with both in-sample\nand out-of-sample CO$_2$ concentrations, accurately reproducing the emergent\ntime-mean spatial patterns of surface temperature and precipitation change with\nCO$_2$ doubling, tripling, or quadrupling. In addition, the vertical profile of\natmospheric warming and change in extreme precipitation rates up to the\n99.9999th percentile closely agree with the reference model.\nNon-equilibrium-climate inference is more challenging. With CO$_2$ increasing\ngradually at a rate of 2% year$^{-1}$, ACE2-SOM can accurately emulate the\nglobal annual mean trends of surface and lower-to-middle atmosphere fields but\nproduces unphysical jumps in stratospheric fields. With an abrupt quadrupling\nof CO$_2$, ML-controlled fields transition unrealistically quickly to the\n4xCO$_2$ regime. In doing so they violate global energy conservation and\nexhibit unphysical sensitivities of and surface and top of atmosphere radiative\nfluxes to instantaneous changes in CO$_2$. Future emulator development needed\nto address these issues should improve its generalizability to diverse climate\nchange scenarios."
                },
                "authors": [
                    {
                        "name": "Spencer K. Clark"
                    },
                    {
                        "name": "Oliver Watt-Meyer"
                    },
                    {
                        "name": "Anna Kwa"
                    },
                    {
                        "name": "Jeremy McGibbon"
                    },
                    {
                        "name": "Brian Henn"
                    },
                    {
                        "name": "W. Andre Perkins"
                    },
                    {
                        "name": "Elynn Wu"
                    },
                    {
                        "name": "Lucas M. Harris"
                    },
                    {
                        "name": "Christopher S. Bretherton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher S. Bretherton"
                },
                "author": "Christopher S. Bretherton",
                "arxiv_comment": "31 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14204v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14204v2",
                "updated": "2024-12-30T20:43:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    20,
                    43,
                    17,
                    0,
                    365,
                    0
                ],
                "published": "2024-09-21T17:36:11Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    17,
                    36,
                    11,
                    5,
                    265,
                    0
                ],
                "title": "UniMo: Universal Motion Correction For Medical Images without Network\n  Retraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniMo: Universal Motion Correction For Medical Images without Network\n  Retraining"
                },
                "summary": "In this paper, we introduce a Universal Motion Correction (UniMo) framework,\nleveraging deep neural networks to tackle the challenges of motion correction\nacross diverse imaging modalities. Our approach employs advanced neural network\narchitectures with equivariant filters, overcoming the limitations of current\nmodels that require iterative inference or retraining for new image modalities.\nUniMo enables one-time training on a single modality while maintaining high\nstability and adaptability for inference across multiple unseen image\nmodalities. We developed a joint learning framework that integrates multimodal\nknowledge from both shape and images that faithfully improve motion correction\naccuracy despite image appearance variations. UniMo features a geometric\ndeformation augmenter that enhances the robustness of global motion correction\nby addressing any local deformations whether they are caused by object\ndeformations or geometric distortions, and also generates augmented data to\nimprove the training process. Our experimental results, conducted on various\ndatasets with four different image modalities, demonstrate that UniMo surpasses\nexisting motion correction methods in terms of accuracy. By offering a\ncomprehensive solution to motion correction, UniMo marks a significant\nadvancement in medical imaging, especially in challenging applications with\nwide ranges of motion, such as fetal imaging. The code for this work is\navailable online, https://github.com/IntelligentImaging/UNIMO/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a Universal Motion Correction (UniMo) framework,\nleveraging deep neural networks to tackle the challenges of motion correction\nacross diverse imaging modalities. Our approach employs advanced neural network\narchitectures with equivariant filters, overcoming the limitations of current\nmodels that require iterative inference or retraining for new image modalities.\nUniMo enables one-time training on a single modality while maintaining high\nstability and adaptability for inference across multiple unseen image\nmodalities. We developed a joint learning framework that integrates multimodal\nknowledge from both shape and images that faithfully improve motion correction\naccuracy despite image appearance variations. UniMo features a geometric\ndeformation augmenter that enhances the robustness of global motion correction\nby addressing any local deformations whether they are caused by object\ndeformations or geometric distortions, and also generates augmented data to\nimprove the training process. Our experimental results, conducted on various\ndatasets with four different image modalities, demonstrate that UniMo surpasses\nexisting motion correction methods in terms of accuracy. By offering a\ncomprehensive solution to motion correction, UniMo marks a significant\nadvancement in medical imaging, especially in challenging applications with\nwide ranges of motion, such as fetal imaging. The code for this work is\navailable online, https://github.com/IntelligentImaging/UNIMO/."
                },
                "authors": [
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Razieh Faghihpirayesh"
                    },
                    {
                        "name": "Danny Joca"
                    },
                    {
                        "name": "Polina Golland"
                    },
                    {
                        "name": "Ali Gholipour"
                    }
                ],
                "author_detail": {
                    "name": "Ali Gholipour"
                },
                "author": "Ali Gholipour",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14204v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14204v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19906v2",
                "updated": "2024-12-30T20:37:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    20,
                    37,
                    49,
                    0,
                    365,
                    0
                ],
                "published": "2024-11-29T18:11:39Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    11,
                    39,
                    4,
                    334,
                    0
                ],
                "title": "Classical and Quantum Algorithms for the Deterministic L-system\n  Inductive Inference Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical and Quantum Algorithms for the Deterministic L-system\n  Inductive Inference Problem"
                },
                "summary": "L-systems can be made to model and create simulations of many biological\nprocesses, such as plant development. Finding an L-system for a given process\nis typically solved by hand, by experts, in a massively time-consuming process.\nIt would be significant if this could be done automatically from data, such as\nfrom sequences of images. In this paper, we are interested in inferring a\nparticular type of L-system, deterministic context-free L-system (D0L-system)\nfrom a sequence of strings. We introduce the characteristic graph of a sequence\nof strings, which we then utilize to translate our problem (inferring\nD0L-system) in polynomial time into the maximum independent set problem (MIS)\nand the SAT problem. After that, we offer a classical exact algorithm and an\napproximate quantum algorithm for the problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L-systems can be made to model and create simulations of many biological\nprocesses, such as plant development. Finding an L-system for a given process\nis typically solved by hand, by experts, in a massively time-consuming process.\nIt would be significant if this could be done automatically from data, such as\nfrom sequences of images. In this paper, we are interested in inferring a\nparticular type of L-system, deterministic context-free L-system (D0L-system)\nfrom a sequence of strings. We introduce the characteristic graph of a sequence\nof strings, which we then utilize to translate our problem (inferring\nD0L-system) in polynomial time into the maximum independent set problem (MIS)\nand the SAT problem. After that, we offer a classical exact algorithm and an\napproximate quantum algorithm for the problem."
                },
                "authors": [
                    {
                        "name": "Ali Lotfi"
                    },
                    {
                        "name": "Ian McQuillan"
                    },
                    {
                        "name": "Steven Rayan"
                    }
                ],
                "author_detail": {
                    "name": "Steven Rayan"
                },
                "author": "Steven Rayan",
                "arxiv_comment": "18 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.11586v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.11586v3",
                "updated": "2024-12-30T19:44:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    19,
                    44,
                    54,
                    0,
                    365,
                    0
                ],
                "published": "2023-05-19T10:53:08Z",
                "published_parsed": [
                    2023,
                    5,
                    19,
                    10,
                    53,
                    8,
                    4,
                    139,
                    0
                ],
                "title": "PDE-constrained Gaussian process surrogate modeling with uncertain data\n  locations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDE-constrained Gaussian process surrogate modeling with uncertain data\n  locations"
                },
                "summary": "Gaussian process regression is widely applied in computational science and\nengineering for surrogate modeling owning to its kernel-based and probabilistic\nnature. In this work, we propose a Bayesian approach that integrates the\nvariability of input data into the Gaussian process regression for function and\npartial differential equation approximation. Leveraging two types of\nobservables -- noise-corrupted outputs with certain inputs and those with\nprior-distribution-defined uncertain inputs, a posterior distribution of\nuncertain inputs is estimated via Bayesian inference. Thereafter, such\nquantified uncertainties of inputs are incorporated into Gaussian process\npredictions by means of marginalization. The setting of two types of data\naligned with common scenarios of constructing surrogate models for the\nsolutions of partial differential equations, where the data of boundary\nconditions and initial conditions are typically known while the data of\nsolution may involve uncertainties due to the measurement or stochasticity. The\neffectiveness of the proposed method is demonstrated through several numerical\nexamples including multiple one-dimensional functions, the heat equation and\nAllen-Cahn equation. A consistently good performance of generalization is\nobserved, and a substantial reduction in the predictive uncertainties is\nachieved by the Bayesian inference of uncertain inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian process regression is widely applied in computational science and\nengineering for surrogate modeling owning to its kernel-based and probabilistic\nnature. In this work, we propose a Bayesian approach that integrates the\nvariability of input data into the Gaussian process regression for function and\npartial differential equation approximation. Leveraging two types of\nobservables -- noise-corrupted outputs with certain inputs and those with\nprior-distribution-defined uncertain inputs, a posterior distribution of\nuncertain inputs is estimated via Bayesian inference. Thereafter, such\nquantified uncertainties of inputs are incorporated into Gaussian process\npredictions by means of marginalization. The setting of two types of data\naligned with common scenarios of constructing surrogate models for the\nsolutions of partial differential equations, where the data of boundary\nconditions and initial conditions are typically known while the data of\nsolution may involve uncertainties due to the measurement or stochasticity. The\neffectiveness of the proposed method is demonstrated through several numerical\nexamples including multiple one-dimensional functions, the heat equation and\nAllen-Cahn equation. A consistently good performance of generalization is\nobserved, and a substantial reduction in the predictive uncertainties is\nachieved by the Bayesian inference of uncertain inputs."
                },
                "authors": [
                    {
                        "name": "Dongwei Ye"
                    },
                    {
                        "name": "Weihao Yan"
                    },
                    {
                        "name": "Christoph Brune"
                    },
                    {
                        "name": "Mengwu Guo"
                    }
                ],
                "author_detail": {
                    "name": "Mengwu Guo"
                },
                "author": "Mengwu Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.11586v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.11586v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02695v2",
                "updated": "2024-12-30T19:37:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    19,
                    37,
                    18,
                    0,
                    365,
                    0
                ],
                "published": "2023-07-05T23:56:24Z",
                "published_parsed": [
                    2023,
                    7,
                    5,
                    23,
                    56,
                    24,
                    2,
                    186,
                    0
                ],
                "title": "High-Dimensional Expected Shortfall Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Dimensional Expected Shortfall Regression"
                },
                "summary": "Expected shortfall is defined as the average over the tail below (or above) a\ncertain quantile of a probability distribution. Expected shortfall regression\nprovides powerful tools for learning the relationship between a response\nvariable and a set of covariates while exploring the heterogeneous effects of\nthe covariates. In the health disparity research, for example, the lower/upper\ntail of the conditional distribution of a health-related outcome, given\nhigh-dimensional covariates, is often of importance. Under sparse models, we\npropose the lasso-penalized expected shortfall regression and establish\nnon-asymptotic error bounds, depending explicitly on the sample size,\ndimension, and sparsity, for the proposed estimator. To perform statistical\ninference on a covariate of interest, we propose a debiased estimator and\nestablish its asymptotic normality, from which asymptotically valid tests can\nbe constructed. We illustrate the finite sample performance of the proposed\nmethod through numerical studies and a data application on health disparity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expected shortfall is defined as the average over the tail below (or above) a\ncertain quantile of a probability distribution. Expected shortfall regression\nprovides powerful tools for learning the relationship between a response\nvariable and a set of covariates while exploring the heterogeneous effects of\nthe covariates. In the health disparity research, for example, the lower/upper\ntail of the conditional distribution of a health-related outcome, given\nhigh-dimensional covariates, is often of importance. Under sparse models, we\npropose the lasso-penalized expected shortfall regression and establish\nnon-asymptotic error bounds, depending explicitly on the sample size,\ndimension, and sparsity, for the proposed estimator. To perform statistical\ninference on a covariate of interest, we propose a debiased estimator and\nestablish its asymptotic normality, from which asymptotically valid tests can\nbe constructed. We illustrate the finite sample performance of the proposed\nmethod through numerical studies and a data application on health disparity."
                },
                "authors": [
                    {
                        "name": "Shushu Zhang"
                    },
                    {
                        "name": "Xuming He"
                    },
                    {
                        "name": "Kean Ming Tan"
                    },
                    {
                        "name": "Wen-Xin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Xin Zhou"
                },
                "author": "Wen-Xin Zhou",
                "arxiv_doi": "10.1080/01621459.2024.2448860",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1080/01621459.2024.2448860",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.02695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "R code for fitting the proposed method can be found at\n  https://github.com/shushuzh/ES_highD.git",
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06608v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06608v5",
                "updated": "2024-12-30T19:33:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    19,
                    33,
                    9,
                    0,
                    365,
                    0
                ],
                "published": "2024-06-06T18:10:11Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    18,
                    10,
                    11,
                    3,
                    158,
                    0
                ],
                "title": "The Prompt Report: A Systematic Survey of Prompting Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Prompt Report: A Systematic Survey of Prompting Techniques"
                },
                "summary": "Generative Artificial Intelligence (GenAI) systems are increasingly being\ndeployed across diverse industries and research domains. Developers and\nend-users interact with these systems through the use of prompting and prompt\nengineering. Although prompt engineering is a widely adopted and extensively\nresearched area, it suffers from conflicting terminology and a fragmented\nontological understanding of what constitutes an effective prompt due to its\nrelatively recent emergence. We establish a structured understanding of prompt\nengineering by assembling a taxonomy of prompting techniques and analyzing\ntheir applications. We present a detailed vocabulary of 33 vocabulary terms, a\ntaxonomy of 58 LLM prompting techniques, and 40 techniques for other\nmodalities. Additionally, we provide best practices and guidelines for prompt\nengineering, including advice for prompting state-of-the-art (SOTA) LLMs such\nas ChatGPT. We further present a meta-analysis of the entire literature on\nnatural language prefix-prompting. As a culmination of these efforts, this\npaper presents the most comprehensive survey on prompt engineering to date.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) systems are increasingly being\ndeployed across diverse industries and research domains. Developers and\nend-users interact with these systems through the use of prompting and prompt\nengineering. Although prompt engineering is a widely adopted and extensively\nresearched area, it suffers from conflicting terminology and a fragmented\nontological understanding of what constitutes an effective prompt due to its\nrelatively recent emergence. We establish a structured understanding of prompt\nengineering by assembling a taxonomy of prompting techniques and analyzing\ntheir applications. We present a detailed vocabulary of 33 vocabulary terms, a\ntaxonomy of 58 LLM prompting techniques, and 40 techniques for other\nmodalities. Additionally, we provide best practices and guidelines for prompt\nengineering, including advice for prompting state-of-the-art (SOTA) LLMs such\nas ChatGPT. We further present a meta-analysis of the entire literature on\nnatural language prefix-prompting. As a culmination of these efforts, this\npaper presents the most comprehensive survey on prompt engineering to date."
                },
                "authors": [
                    {
                        "name": "Sander Schulhoff"
                    },
                    {
                        "name": "Michael Ilie"
                    },
                    {
                        "name": "Nishant Balepur"
                    },
                    {
                        "name": "Konstantine Kahadze"
                    },
                    {
                        "name": "Amanda Liu"
                    },
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Yinheng Li"
                    },
                    {
                        "name": "Aayush Gupta"
                    },
                    {
                        "name": "HyoJung Han"
                    },
                    {
                        "name": "Sevien Schulhoff"
                    },
                    {
                        "name": "Pranav Sandeep Dulepet"
                    },
                    {
                        "name": "Saurav Vidyadhara"
                    },
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Chau Pham"
                    },
                    {
                        "name": "Gerson Kroiz"
                    },
                    {
                        "name": "Feileen Li"
                    },
                    {
                        "name": "Hudson Tao"
                    },
                    {
                        "name": "Ashay Srivastava"
                    },
                    {
                        "name": "Hevander Da Costa"
                    },
                    {
                        "name": "Saloni Gupta"
                    },
                    {
                        "name": "Megan L. Rogers"
                    },
                    {
                        "name": "Inna Goncearenco"
                    },
                    {
                        "name": "Giuseppe Sarli"
                    },
                    {
                        "name": "Igor Galynker"
                    },
                    {
                        "name": "Denis Peskoff"
                    },
                    {
                        "name": "Marine Carpuat"
                    },
                    {
                        "name": "Jules White"
                    },
                    {
                        "name": "Shyamal Anadkat"
                    },
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Philip Resnik"
                    }
                ],
                "author_detail": {
                    "name": "Philip Resnik"
                },
                "author": "Philip Resnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06608v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06608v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.19260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19260v2",
                "updated": "2025-01-02T18:46:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    46,
                    5,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-26T15:54:10Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    54,
                    10,
                    3,
                    361,
                    0
                ],
                "title": "MEDEC: A Benchmark for Medical Error Detection and Correction in\n  Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDEC: A Benchmark for Medical Error Detection and Correction in\n  Clinical Notes"
                },
                "summary": "Several studies showed that Large Language Models (LLMs) can answer medical\nquestions correctly, even outperforming the average human score in some medical\nexams. However, to our knowledge, no study has been conducted to assess the\nability of language models to validate existing or generated medical text for\ncorrectness and consistency. In this paper, we introduce MEDEC\n(https://github.com/abachaa/MEDEC), the first publicly available benchmark for\nmedical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal\nOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes\nfrom three US hospital systems that were not previously seen by any LLM. The\ndataset has been used for the MEDIQA-CORR shared task to evaluate seventeen\nparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe the\ndata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,\nClaude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and\ncorrecting medical errors requiring both medical knowledge and reasoning\ncapabilities. We also conducted a comparative study where two medical doctors\nperformed the same task on the MEDEC test set. The results showed that MEDEC is\na sufficiently challenging benchmark to assess the ability of models to\nvalidate existing or generated notes and to correct medical errors. We also\nfound that although recent LLMs have a good performance in error detection and\ncorrection, they are still outperformed by medical doctors in these tasks. We\ndiscuss the potential factors behind this gap, the insights from our\nexperiments, the limitations of current evaluation metrics, and share potential\npointers for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several studies showed that Large Language Models (LLMs) can answer medical\nquestions correctly, even outperforming the average human score in some medical\nexams. However, to our knowledge, no study has been conducted to assess the\nability of language models to validate existing or generated medical text for\ncorrectness and consistency. In this paper, we introduce MEDEC\n(https://github.com/abachaa/MEDEC), the first publicly available benchmark for\nmedical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal\nOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes\nfrom three US hospital systems that were not previously seen by any LLM. The\ndataset has been used for the MEDIQA-CORR shared task to evaluate seventeen\nparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe the\ndata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,\nClaude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and\ncorrecting medical errors requiring both medical knowledge and reasoning\ncapabilities. We also conducted a comparative study where two medical doctors\nperformed the same task on the MEDEC test set. The results showed that MEDEC is\na sufficiently challenging benchmark to assess the ability of models to\nvalidate existing or generated notes and to correct medical errors. We also\nfound that although recent LLMs have a good performance in error detection and\ncorrection, they are still outperformed by medical doctors in these tasks. We\ndiscuss the potential factors behind this gap, the insights from our\nexperiments, the limitations of current evaluation metrics, and share potential\npointers for future research."
                },
                "authors": [
                    {
                        "name": "Asma Ben Abacha"
                    },
                    {
                        "name": "Wen-wai Yim"
                    },
                    {
                        "name": "Yujuan Fu"
                    },
                    {
                        "name": "Zhaoyi Sun"
                    },
                    {
                        "name": "Meliha Yetisgen"
                    },
                    {
                        "name": "Fei Xia"
                    },
                    {
                        "name": "Thomas Lin"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Lin"
                },
                "author": "Thomas Lin",
                "arxiv_comment": "This version has been updated with further clarification regarding\n  the model size estimates that were mined from public articles only and\n  provided to aid in contextualizing model performance. The authors cannot\n  vouch for the accuracy of those estimates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04655v2",
                "updated": "2025-01-02T17:21:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    17,
                    21,
                    22,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-05T22:59:26Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    59,
                    26,
                    3,
                    340,
                    0
                ],
                "title": "From Models to Systems: A Comprehensive Fairness Framework for\n  Compositional Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Models to Systems: A Comprehensive Fairness Framework for\n  Compositional Recommender Systems"
                },
                "summary": "Fairness research in machine learning often centers on ensuring equitable\nperformance of individual models. However, real-world recommendation systems\nare built on multiple models and even multiple stages, from candidate retrieval\nto scoring and serving, which raises challenges for responsible development and\ndeployment. This system-level view, as highlighted by regulations like the EU\nAI Act, necessitates moving beyond auditing individual models as independent\nentities. We propose a holistic framework for modeling system-level fairness,\nfocusing on the end-utility delivered to diverse user groups, and consider\ninteractions between components such as retrieval and scoring models. We\nprovide formal insights on the limitations of focusing solely on model-level\nfairness and highlight the need for alternative tools that account for\nheterogeneity in user preferences. To mitigate system-level disparities, we\nadapt closed-box optimization tools (e.g., BayesOpt) to jointly optimize\nutility and equity. We empirically demonstrate the effectiveness of our\nproposed framework on synthetic and real datasets, underscoring the need for a\nsystem-level framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness research in machine learning often centers on ensuring equitable\nperformance of individual models. However, real-world recommendation systems\nare built on multiple models and even multiple stages, from candidate retrieval\nto scoring and serving, which raises challenges for responsible development and\ndeployment. This system-level view, as highlighted by regulations like the EU\nAI Act, necessitates moving beyond auditing individual models as independent\nentities. We propose a holistic framework for modeling system-level fairness,\nfocusing on the end-utility delivered to diverse user groups, and consider\ninteractions between components such as retrieval and scoring models. We\nprovide formal insights on the limitations of focusing solely on model-level\nfairness and highlight the need for alternative tools that account for\nheterogeneity in user preferences. To mitigate system-level disparities, we\nadapt closed-box optimization tools (e.g., BayesOpt) to jointly optimize\nutility and equity. We empirically demonstrate the effectiveness of our\nproposed framework on synthetic and real datasets, underscoring the need for a\nsystem-level framework."
                },
                "authors": [
                    {
                        "name": "Brian Hsu"
                    },
                    {
                        "name": "Cyrus DiCiccio"
                    },
                    {
                        "name": "Natesh Sivasubramoniapillai"
                    },
                    {
                        "name": "Hongseok Namkoong"
                    }
                ],
                "author_detail": {
                    "name": "Hongseok Namkoong"
                },
                "author": "Hongseok Namkoong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10848v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10848v3",
                "updated": "2025-01-02T17:17:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    17,
                    17,
                    28,
                    3,
                    2,
                    0
                ],
                "published": "2024-08-20T13:40:25Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    40,
                    25,
                    1,
                    233,
                    0
                ],
                "title": "Perception-guided Jailbreak against Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception-guided Jailbreak against Text-to-Image Models"
                },
                "summary": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ."
                },
                "authors": [
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Run Wang"
                    },
                    {
                        "name": "Weikai Miao"
                    },
                    {
                        "name": "Geguang Pu"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "9 pages, accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10848v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10848v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06083v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06083v2",
                "updated": "2025-01-02T16:14:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    14,
                    16,
                    3,
                    2,
                    0
                ],
                "published": "2024-07-04T09:50:50Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    9,
                    50,
                    50,
                    3,
                    186,
                    0
                ],
                "title": "A Survey of Controllable Learning: Methods and Applications in\n  Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Controllable Learning: Methods and Applications in\n  Information Retrieval"
                },
                "summary": "Controllability has become a crucial aspect of trustworthy machine learning,\nenabling learners to meet predefined targets and adapt dynamically at test time\nwithout requiring retraining as the targets shift. We provide a formal\ndefinition of controllable learning (CL), and discuss its applications in\ninformation retrieval (IR) where information needs are often complex and\ndynamic. The survey categorizes CL according to what is controllable (e.g.,\nmultiple objectives, user portrait, scenario adaptation), who controls (users\nor platforms), how control is implemented (e.g., rule-based method, Pareto\noptimization, hypernetwork and others), and where to implement control (e.g.,\npre-processing, in-processing, post-processing methods). Then, we identify\nchallenges faced by CL across training, evaluation, task setting, and\ndeployment in online environments. Additionally, we outline promising\ndirections for CL in theoretical analysis, efficient computation, empowering\nlarge language models, application scenarios and evaluation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllability has become a crucial aspect of trustworthy machine learning,\nenabling learners to meet predefined targets and adapt dynamically at test time\nwithout requiring retraining as the targets shift. We provide a formal\ndefinition of controllable learning (CL), and discuss its applications in\ninformation retrieval (IR) where information needs are often complex and\ndynamic. The survey categorizes CL according to what is controllable (e.g.,\nmultiple objectives, user portrait, scenario adaptation), who controls (users\nor platforms), how control is implemented (e.g., rule-based method, Pareto\noptimization, hypernetwork and others), and where to implement control (e.g.,\npre-processing, in-processing, post-processing methods). Then, we identify\nchallenges faced by CL across training, evaluation, task setting, and\ndeployment in online environments. Additionally, we outline promising\ndirections for CL in theoretical analysis, efficient computation, empowering\nlarge language models, application scenarios and evaluation frameworks."
                },
                "authors": [
                    {
                        "name": "Chenglei Shen"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Teng Shi"
                    },
                    {
                        "name": "Changshuo Zhang"
                    },
                    {
                        "name": "Guofu Xie"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06083v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07259v2",
                "updated": "2025-01-02T14:57:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    14,
                    57,
                    3,
                    3,
                    2,
                    0
                ],
                "published": "2024-02-11T18:04:06Z",
                "published_parsed": [
                    2024,
                    2,
                    11,
                    18,
                    4,
                    6,
                    6,
                    42,
                    0
                ],
                "title": "RIS-Augmented Millimeter-Wave MIMO Systems for Passive Drone Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIS-Augmented Millimeter-Wave MIMO Systems for Passive Drone Detection"
                },
                "summary": "In the past decade, the number of amateur drones is increasing, and this\ntrend is expected to continue in the future. The security issues brought by\nabuse and misconduct of drones become more and more severe and may incur a\nnegative impact to the society. In this paper, we leverage existing cellular\nmultiple-input multiple-output (MIMO) base station (BS) infrastructure,\noperating at millimeter wave (mmWave) frequency bands, for drone detection in a\ndevice-free manner with the aid of one reconfigurable intelligent surface\n(RIS), deployed in the proximity of the BS. We theoretically examine the\nfeasibility of drone detection with the aid of the generalized likelihood ratio\ntest (GLRT) and validate via simulations that, the optimized deployment of an\nRIS can bring added benefits compared to RIS-free systems. In addition, the\neffect of RIS training beams, training overhead, and radar cross section, is\ninvestigated in order to offer theoretical design guidance for the proposed\ncellular RIS-based passive drone detection system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the past decade, the number of amateur drones is increasing, and this\ntrend is expected to continue in the future. The security issues brought by\nabuse and misconduct of drones become more and more severe and may incur a\nnegative impact to the society. In this paper, we leverage existing cellular\nmultiple-input multiple-output (MIMO) base station (BS) infrastructure,\noperating at millimeter wave (mmWave) frequency bands, for drone detection in a\ndevice-free manner with the aid of one reconfigurable intelligent surface\n(RIS), deployed in the proximity of the BS. We theoretically examine the\nfeasibility of drone detection with the aid of the generalized likelihood ratio\ntest (GLRT) and validate via simulations that, the optimized deployment of an\nRIS can bring added benefits compared to RIS-free systems. In addition, the\neffect of RIS training beams, training overhead, and radar cross section, is\ninvestigated in order to offer theoretical design guidance for the proposed\ncellular RIS-based passive drone detection system."
                },
                "authors": [
                    {
                        "name": "Jiguang He"
                    },
                    {
                        "name": "Aymen Fakhreddine"
                    },
                    {
                        "name": "George C. Alexandropoulos"
                    }
                ],
                "author_detail": {
                    "name": "George C. Alexandropoulos"
                },
                "author": "George C. Alexandropoulos",
                "arxiv_comment": "6 pages, 6 figures, accepted by IEEE PIMRC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19530v2",
                "updated": "2025-01-02T13:54:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    54,
                    17,
                    3,
                    2,
                    0
                ],
                "published": "2024-03-28T16:06:06Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    16,
                    6,
                    6,
                    3,
                    88,
                    0
                ],
                "title": "Detecting Financial Bots on the Ethereum Blockchain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Financial Bots on the Ethereum Blockchain"
                },
                "summary": "The integration of bots in Distributed Ledger Technologies (DLTs) fosters\nefficiency and automation. However, their use is also associated with predatory\ntrading and market manipulation, and can pose threats to system integrity. It\nis therefore essential to understand the extent of bot deployment in DLTs;\ndespite this, current detection systems are predominantly rule-based and lack\nflexibility. In this study, we present a novel approach that utilizes machine\nlearning for the detection of financial bots on the Ethereum platform. First,\nwe systematize existing scientific literature and collect anecdotal evidence to\nestablish a taxonomy for financial bots, comprising 7 categories and 24\nsubcategories. Next, we create a ground-truth dataset consisting of 133 human\nand 137 bot addresses. Third, we employ both unsupervised and supervised\nmachine learning algorithms to detect bots deployed on Ethereum. The\nhighest-performing clustering algorithm is a Gaussian Mixture Model with an\naverage cluster purity of 82.6%, while the highest-performing model for binary\nclassification is a Random Forest with an accuracy of 83%. Our machine\nlearning-based detection mechanism contributes to understanding the Ethereum\necosystem dynamics by providing additional insights into the current bot\nlandscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of bots in Distributed Ledger Technologies (DLTs) fosters\nefficiency and automation. However, their use is also associated with predatory\ntrading and market manipulation, and can pose threats to system integrity. It\nis therefore essential to understand the extent of bot deployment in DLTs;\ndespite this, current detection systems are predominantly rule-based and lack\nflexibility. In this study, we present a novel approach that utilizes machine\nlearning for the detection of financial bots on the Ethereum platform. First,\nwe systematize existing scientific literature and collect anecdotal evidence to\nestablish a taxonomy for financial bots, comprising 7 categories and 24\nsubcategories. Next, we create a ground-truth dataset consisting of 133 human\nand 137 bot addresses. Third, we employ both unsupervised and supervised\nmachine learning algorithms to detect bots deployed on Ethereum. The\nhighest-performing clustering algorithm is a Gaussian Mixture Model with an\naverage cluster purity of 82.6%, while the highest-performing model for binary\nclassification is a Random Forest with an accuracy of 83%. Our machine\nlearning-based detection mechanism contributes to understanding the Ethereum\necosystem dynamics by providing additional insights into the current bot\nlandscape."
                },
                "authors": [
                    {
                        "name": "Thomas Niedermayer"
                    },
                    {
                        "name": "Pietro Saggese"
                    },
                    {
                        "name": "Bernhard Haslhofer"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Haslhofer"
                },
                "author": "Bernhard Haslhofer",
                "arxiv_doi": "10.1145/3589335.3651959",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3589335.3651959",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.19530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01887v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01887v3",
                "updated": "2025-01-02T13:49:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    49,
                    59,
                    3,
                    2,
                    0
                ],
                "published": "2024-07-02T02:18:14Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    2,
                    18,
                    14,
                    1,
                    184,
                    0
                ],
                "title": "Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"
                },
                "summary": "In-context reinforcement learning (ICRL) is a frontier paradigm for solving\nreinforcement learning problems in the foundation model era. While ICRL\ncapabilities have been demonstrated in transformers through task-specific\ntraining, the potential of Large Language Models (LLMs) out-of-the-box remains\nlargely unexplored. Recent findings highlight that LLMs often face challenges\nwhen dealing with numerical contexts, and limited attention has been paid to\nevaluating their performance through preference feedback generated by the\nenvironment. This paper is the first to investigate LLMs as in-context\ndecision-makers under the problem of Dueling Bandits (DB), a stateless\npreference-based reinforcement learning setting that extends the classic\nMulti-Armed Bandit (MAB) model by querying for preference feedback. We compare\nGPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-Preview against nine\nwell-established DB algorithms. Our results reveal that our top-performing LLM,\nGPT-4 Turbo, has the zero-shot relative decision-making ability to achieve\nsurprisingly low weak regret across all the DB environment instances by quickly\nincluding the best arm in duels. However, an optimality gap exists between LLMs\nand classic DB algorithms in terms of strong regret. LLMs struggle to converge\nand consistently exploit even when explicitly prompted to do so, and are\nsensitive to prompt variations. To bridge this gap, we propose an agentic flow\nframework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates\noff-the-shelf DB algorithms with LLM agents through fine-grained adaptive\ninterplay. We show that LEAD has theoretical guarantees inherited from classic\nDB algorithms on both weak and strong regret. We validate its efficacy and\nrobustness even with noisy and adversarial prompts. The design of our framework\nsheds light on how to enhance the trustworthiness of LLMs used for in-context\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context reinforcement learning (ICRL) is a frontier paradigm for solving\nreinforcement learning problems in the foundation model era. While ICRL\ncapabilities have been demonstrated in transformers through task-specific\ntraining, the potential of Large Language Models (LLMs) out-of-the-box remains\nlargely unexplored. Recent findings highlight that LLMs often face challenges\nwhen dealing with numerical contexts, and limited attention has been paid to\nevaluating their performance through preference feedback generated by the\nenvironment. This paper is the first to investigate LLMs as in-context\ndecision-makers under the problem of Dueling Bandits (DB), a stateless\npreference-based reinforcement learning setting that extends the classic\nMulti-Armed Bandit (MAB) model by querying for preference feedback. We compare\nGPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-Preview against nine\nwell-established DB algorithms. Our results reveal that our top-performing LLM,\nGPT-4 Turbo, has the zero-shot relative decision-making ability to achieve\nsurprisingly low weak regret across all the DB environment instances by quickly\nincluding the best arm in duels. However, an optimality gap exists between LLMs\nand classic DB algorithms in terms of strong regret. LLMs struggle to converge\nand consistently exploit even when explicitly prompted to do so, and are\nsensitive to prompt variations. To bridge this gap, we propose an agentic flow\nframework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates\noff-the-shelf DB algorithms with LLM agents through fine-grained adaptive\ninterplay. We show that LEAD has theoretical guarantees inherited from classic\nDB algorithms on both weak and strong regret. We validate its efficacy and\nrobustness even with noisy and adversarial prompts. The design of our framework\nsheds light on how to enhance the trustworthiness of LLMs used for in-context\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Fanzeng Xia"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Tongxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Tongxin Li"
                },
                "author": "Tongxin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01887v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01887v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11006v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11006v4",
                "updated": "2025-01-02T13:11:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    11,
                    53,
                    3,
                    2,
                    0
                ],
                "published": "2024-08-20T17:00:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    0,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Security Attacks on LLM-based Code Completion Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Attacks on LLM-based Code Completion Tools"
                },
                "summary": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs."
                },
                "authors": [
                    {
                        "name": "Wen Cheng"
                    },
                    {
                        "name": "Ke Sun"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "Paper accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11006v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11006v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15270v2",
                "updated": "2025-01-02T11:21:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    21,
                    38,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-17T08:05:32Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    5,
                    32,
                    1,
                    352,
                    0
                ],
                "title": "Baichuan4-Finance Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Baichuan4-Finance Technical Report"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding, generation, and reasoning, yet their potential in\nfinance remains underexplored due to the complexity and specialization of\nfinancial knowledge. In this work, we report the development of the\nBaichuan4-Finance series, including a comprehensive suite of foundational\nBaichuan4-Finance-Base and an aligned language model Baichuan4-Finance, which\nare built upon Baichuan4-Turbo base model and tailored for finance domain.\nFirstly, we have dedicated significant effort to building a detailed pipeline\nfor improving data quality. Moreover, in the continual pre-training phase, we\npropose a novel domain self-constraint training strategy, which enables\nBaichuan4-Finance-Base to acquire financial knowledge without losing general\ncapabilities. After Supervised Fine-tuning and Reinforcement Learning from\nHuman Feedback and AI Feedback, the chat model Baichuan4-Finance is able to\ntackle various financial certification questions and real-world scenario\napplications. We evaluate Baichuan4-Finance on many widely used general\ndatasets and two holistic financial benchmarks. The evaluation results show\nthat Baichuan4-Finance-Base surpasses almost all competitive baselines on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. At the same time, Baichuan4-Finance demonstrates even\nmore impressive performance on financial application scenarios, showcasing its\npotential to foster community innovation in the financial LLM field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding, generation, and reasoning, yet their potential in\nfinance remains underexplored due to the complexity and specialization of\nfinancial knowledge. In this work, we report the development of the\nBaichuan4-Finance series, including a comprehensive suite of foundational\nBaichuan4-Finance-Base and an aligned language model Baichuan4-Finance, which\nare built upon Baichuan4-Turbo base model and tailored for finance domain.\nFirstly, we have dedicated significant effort to building a detailed pipeline\nfor improving data quality. Moreover, in the continual pre-training phase, we\npropose a novel domain self-constraint training strategy, which enables\nBaichuan4-Finance-Base to acquire financial knowledge without losing general\ncapabilities. After Supervised Fine-tuning and Reinforcement Learning from\nHuman Feedback and AI Feedback, the chat model Baichuan4-Finance is able to\ntackle various financial certification questions and real-world scenario\napplications. We evaluate Baichuan4-Finance on many widely used general\ndatasets and two holistic financial benchmarks. The evaluation results show\nthat Baichuan4-Finance-Base surpasses almost all competitive baselines on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. At the same time, Baichuan4-Finance demonstrates even\nmore impressive performance on financial application scenarios, showcasing its\npotential to foster community innovation in the financial LLM field."
                },
                "authors": [
                    {
                        "name": "Hanyu Zhang"
                    },
                    {
                        "name": "Boyu Qiu"
                    },
                    {
                        "name": "Yuhao Feng"
                    },
                    {
                        "name": "Shuqi Li"
                    },
                    {
                        "name": "Qian Ma"
                    },
                    {
                        "name": "Xiyuan Zhang"
                    },
                    {
                        "name": "Qiang Ju"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Jian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Jian Xie"
                },
                "author": "Jian Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21349v3",
                "updated": "2025-01-02T11:16:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    16,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-28T12:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    18,
                    22,
                    0,
                    302,
                    0
                ],
                "title": "FALCON: Feedback-driven Adaptive Long/short-term memory reinforced\n  Coding Optimization system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FALCON: Feedback-driven Adaptive Long/short-term memory reinforced\n  Coding Optimization system"
                },
                "summary": "Recently, large language models (LLMs) have achieved significant progress in\nautomated code generation. Despite their strong instruction-following\ncapabilities, these models frequently struggled to align with user intent in\ncoding scenarios. In particular, they were hampered by datasets that lacked\ndiversity and failed to address specialized tasks or edge cases. Furthermore,\nchallenges in supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) led to failures in generating precise,\nhuman-intent-aligned code. To tackle these challenges and improve the code\ngeneration performance for automated programming systems, we propose\nFeedback-driven Adaptive Long/short-term memory reinforced Coding Optimization\n(i.e., FALCON). FALCON is structured into two hierarchical levels. From the\nglobal level, long-term memory improves code quality by retaining and applying\nlearned knowledge. At the local level, short-term memory allows for the\nincorporation of immediate feedback from compilers and AI systems.\nAdditionally, we introduce meta-reinforcement learning with feedback rewards to\nsolve the global-local bi-level optimization problem and enhance the model's\nadaptability across diverse code generation tasks. Extensive experiments\ndemonstrate that our technique achieves state-of-the-art performance, leading\nother reinforcement learning methods by more than 4.5 percentage points on the\nMBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The\nopen-sourced code is publicly available at https://github.com/titurte/FALCON.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have achieved significant progress in\nautomated code generation. Despite their strong instruction-following\ncapabilities, these models frequently struggled to align with user intent in\ncoding scenarios. In particular, they were hampered by datasets that lacked\ndiversity and failed to address specialized tasks or edge cases. Furthermore,\nchallenges in supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) led to failures in generating precise,\nhuman-intent-aligned code. To tackle these challenges and improve the code\ngeneration performance for automated programming systems, we propose\nFeedback-driven Adaptive Long/short-term memory reinforced Coding Optimization\n(i.e., FALCON). FALCON is structured into two hierarchical levels. From the\nglobal level, long-term memory improves code quality by retaining and applying\nlearned knowledge. At the local level, short-term memory allows for the\nincorporation of immediate feedback from compilers and AI systems.\nAdditionally, we introduce meta-reinforcement learning with feedback rewards to\nsolve the global-local bi-level optimization problem and enhance the model's\nadaptability across diverse code generation tasks. Extensive experiments\ndemonstrate that our technique achieves state-of-the-art performance, leading\nother reinforcement learning methods by more than 4.5 percentage points on the\nMBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The\nopen-sourced code is publicly available at https://github.com/titurte/FALCON."
                },
                "authors": [
                    {
                        "name": "Zeyuan Li"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Lewei He"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Bin Lei"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Qiuwu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qiuwu Chen"
                },
                "author": "Qiuwu Chen",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01946v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01946v3",
                "updated": "2025-01-02T11:04:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    4,
                    46,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-02T20:14:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    14,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "The Reality of AI and Biorisk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Reality of AI and Biorisk"
                },
                "summary": "To accurately and confidently answer the question 'could an AI model or\nsystem increase biorisk', it is necessary to have both a sound theoretical\nthreat model for how AI models or systems could increase biorisk and a robust\nmethod for testing that threat model. This paper provides an analysis of\nexisting available research surrounding two AI and biorisk threat models: 1)\naccess to information and planning via large language models (LLMs), and 2) the\nuse of AI-enabled biological tools (BTs) in synthesizing novel biological\nartifacts. We find that existing studies around AI-related biorisk are nascent,\noften speculative in nature, or limited in terms of their methodological\nmaturity and transparency. The available literature suggests that current LLMs\nand BTs do not pose an immediate risk, and more work is needed to develop\nrigorous approaches to understanding how future models could increase biorisks.\nWe end with recommendations about how empirical work can be expanded to more\nprecisely target biorisk and ensure rigor and validity of findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To accurately and confidently answer the question 'could an AI model or\nsystem increase biorisk', it is necessary to have both a sound theoretical\nthreat model for how AI models or systems could increase biorisk and a robust\nmethod for testing that threat model. This paper provides an analysis of\nexisting available research surrounding two AI and biorisk threat models: 1)\naccess to information and planning via large language models (LLMs), and 2) the\nuse of AI-enabled biological tools (BTs) in synthesizing novel biological\nartifacts. We find that existing studies around AI-related biorisk are nascent,\noften speculative in nature, or limited in terms of their methodological\nmaturity and transparency. The available literature suggests that current LLMs\nand BTs do not pose an immediate risk, and more work is needed to develop\nrigorous approaches to understanding how future models could increase biorisks.\nWe end with recommendations about how empirical work can be expanded to more\nprecisely target biorisk and ensure rigor and validity of findings."
                },
                "authors": [
                    {
                        "name": "Aidan Peppin"
                    },
                    {
                        "name": "Anka Reuel"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Elliot Jones"
                    },
                    {
                        "name": "Andrew Strait"
                    },
                    {
                        "name": "Usman Anwar"
                    },
                    {
                        "name": "Anurag Agrawal"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Marie Pellat"
                    },
                    {
                        "name": "Rishi Bommasani"
                    },
                    {
                        "name": "Nick Frosst"
                    },
                    {
                        "name": "Sara Hooker"
                    }
                ],
                "author_detail": {
                    "name": "Sara Hooker"
                },
                "author": "Sara Hooker",
                "arxiv_comment": "Updated to correct author affiliations and clarify findings of\n  evaluations of the o1 model",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01946v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01946v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05411v2",
                "updated": "2025-01-02T10:56:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    10,
                    56,
                    16,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-07T18:23:00Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    18,
                    23,
                    0,
                    0,
                    281,
                    0
                ],
                "title": "Filtering Discomforting Recommendations with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filtering Discomforting Recommendations with Large Language Models"
                },
                "summary": "Personalized algorithms can inadvertently expose users to discomforting\nrecommendations, potentially triggering negative consequences. The subjectivity\nof discomfort and the black-box nature of these algorithms make it challenging\nto effectively identify and filter such content. To address this, we first\nconducted a formative study to understand users' practices and expectations\nregarding discomforting recommendation filtering. Then, we designed a Large\nLanguage Model (LLM)-based tool named DiscomfortFilter, which constructs an\neditable preference profile for a user and helps the user express filtering\nneeds through conversation to mask discomforting preferences within the\nprofile. Based on the edited profile, DiscomfortFilter facilitates the\ndiscomforting recommendations filtering in a plug-and-play manner, maintaining\nflexibility and transparency. The constructed preference profile improves LLM\nreasoning and simplifies user alignment, enabling a 3.8B open-source LLM to\nrival top commercial models in an offline proxy task. A one-week user study\nwith 24 participants demonstrated the effectiveness of DiscomfortFilter, while\nalso highlighting its potential impact on platform recommendation outcomes. We\nconclude by discussing the ongoing challenges, highlighting its relevance to\nbroader research, assessing stakeholder impact, and outlining future research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized algorithms can inadvertently expose users to discomforting\nrecommendations, potentially triggering negative consequences. The subjectivity\nof discomfort and the black-box nature of these algorithms make it challenging\nto effectively identify and filter such content. To address this, we first\nconducted a formative study to understand users' practices and expectations\nregarding discomforting recommendation filtering. Then, we designed a Large\nLanguage Model (LLM)-based tool named DiscomfortFilter, which constructs an\neditable preference profile for a user and helps the user express filtering\nneeds through conversation to mask discomforting preferences within the\nprofile. Based on the edited profile, DiscomfortFilter facilitates the\ndiscomforting recommendations filtering in a plug-and-play manner, maintaining\nflexibility and transparency. The constructed preference profile improves LLM\nreasoning and simplifies user alignment, enabling a 3.8B open-source LLM to\nrival top commercial models in an offline proxy task. A one-week user study\nwith 24 participants demonstrated the effectiveness of DiscomfortFilter, while\nalso highlighting its potential impact on platform recommendation outcomes. We\nconclude by discussing the ongoing challenges, highlighting its relevance to\nbroader research, assessing stakeholder impact, and outlining future research\ndirections."
                },
                "authors": [
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Yiyang Shao"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Longzhi Du"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Ning Gu"
                    }
                ],
                "author_detail": {
                    "name": "Ning Gu"
                },
                "author": "Ning Gu",
                "arxiv_comment": "16 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16626v2",
                "updated": "2025-01-02T10:56:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    10,
                    56,
                    7,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-21T13:43:51Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    43,
                    51,
                    5,
                    356,
                    0
                ],
                "title": "Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement"
                },
                "summary": "In recent speech enhancement (SE) research, transformer and its variants have\nemerged as the predominant methodologies. However, the quadratic complexity of\nthe self-attention mechanism imposes certain limitations on practical\ndeployment. Mamba, as a novel state-space model (SSM), has gained widespread\napplication in natural language processing and computer vision due to its\nstrong capabilities in modeling long sequences and relatively low computational\ncomplexity. In this work, we introduce Mamba-SEUNet, an innovative architecture\nthat integrates Mamba with U-Net for SE tasks. By leveraging bidirectional\nMamba to model forward and backward dependencies of speech signals at different\nresolutions, and incorporating skip connections to capture multi-scale\ninformation, our approach achieves state-of-the-art (SOTA) performance.\nExperimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet\nattains a PESQ score of 3.59, while maintaining low computational complexity.\nWhen combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet\nfurther improves the PESQ score to 3.73.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent speech enhancement (SE) research, transformer and its variants have\nemerged as the predominant methodologies. However, the quadratic complexity of\nthe self-attention mechanism imposes certain limitations on practical\ndeployment. Mamba, as a novel state-space model (SSM), has gained widespread\napplication in natural language processing and computer vision due to its\nstrong capabilities in modeling long sequences and relatively low computational\ncomplexity. In this work, we introduce Mamba-SEUNet, an innovative architecture\nthat integrates Mamba with U-Net for SE tasks. By leveraging bidirectional\nMamba to model forward and backward dependencies of speech signals at different\nresolutions, and incorporating skip connections to capture multi-scale\ninformation, our approach achieves state-of-the-art (SOTA) performance.\nExperimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet\nattains a PESQ score of 3.59, while maintaining low computational complexity.\nWhen combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet\nfurther improves the PESQ score to 3.73."
                },
                "authors": [
                    {
                        "name": "Junyu Wang"
                    },
                    {
                        "name": "Zizhen Lin"
                    },
                    {
                        "name": "Tianrui Wang"
                    },
                    {
                        "name": "Meng Ge"
                    },
                    {
                        "name": "Longbiao Wang"
                    },
                    {
                        "name": "Jianwu Dang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwu Dang"
                },
                "author": "Jianwu Dang",
                "arxiv_comment": "Accepted at ICASSP 2025, 5 pages, 1 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20367v2",
                "updated": "2025-01-02T09:43:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    43,
                    43,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-29T06:15:41Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    6,
                    15,
                    41,
                    6,
                    364,
                    0
                ],
                "title": "Enhancing Code LLMs with Reinforcement Learning in Code Generation: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Code LLMs with Reinforcement Learning in Code Generation: A\n  Survey"
                },
                "summary": "With the rapid evolution of large language models (LLM), reinforcement\nlearning (RL) has emerged as a pivotal technique for code generation and\noptimization in various domains. This paper presents a systematic survey of the\napplication of RL in code optimization and generation, highlighting its role in\nenhancing compiler optimization, resource allocation, and the development of\nframeworks and tools. Subsequent sections first delve into the intricate\nprocesses of compiler optimization, where RL algorithms are leveraged to\nimprove efficiency and resource utilization. The discussion then progresses to\nthe function of RL in resource allocation, emphasizing register allocation and\nsystem optimization. We also explore the burgeoning role of frameworks and\ntools in code generation, examining how RL can be integrated to bolster their\ncapabilities. This survey aims to serve as a comprehensive resource for\nresearchers and practitioners interested in harnessing the power of RL to\nadvance code generation and optimization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid evolution of large language models (LLM), reinforcement\nlearning (RL) has emerged as a pivotal technique for code generation and\noptimization in various domains. This paper presents a systematic survey of the\napplication of RL in code optimization and generation, highlighting its role in\nenhancing compiler optimization, resource allocation, and the development of\nframeworks and tools. Subsequent sections first delve into the intricate\nprocesses of compiler optimization, where RL algorithms are leveraged to\nimprove efficiency and resource utilization. The discussion then progresses to\nthe function of RL in resource allocation, emphasizing register allocation and\nsystem optimization. We also explore the burgeoning role of frameworks and\ntools in code generation, examining how RL can be integrated to bolster their\ncapabilities. This survey aims to serve as a comprehensive resource for\nresearchers and practitioners interested in harnessing the power of RL to\nadvance code generation and optimization techniques."
                },
                "authors": [
                    {
                        "name": "Junqiao Wang"
                    },
                    {
                        "name": "Zeng Zhang"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Yuyang Song"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Hengyuan Xu"
                    },
                    {
                        "name": "Kunyu Wu"
                    },
                    {
                        "name": "Guangwu Qian"
                    },
                    {
                        "name": "Qiuwu Chen"
                    },
                    {
                        "name": "Lewei He"
                    }
                ],
                "author_detail": {
                    "name": "Lewei He"
                },
                "author": "Lewei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00927v2",
                "updated": "2025-01-02T08:53:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    53,
                    50,
                    3,
                    2,
                    0
                ],
                "published": "2024-09-30T16:57:34Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    57,
                    34,
                    0,
                    274,
                    0
                ],
                "title": "Text Clustering as Classification with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Clustering as Classification with LLMs"
                },
                "summary": "Text clustering remains valuable in real-world applications where manual\nlabeling is cost-prohibitive. It facilitates efficient organization and\nanalysis of information by grouping similar texts based on their\nrepresentations. However, implementing this approach necessitates fine-tuned\nembedders for downstream data and sophisticated similarity metrics. To address\nthis issue, this study presents a novel framework for text clustering that\neffectively leverages the in-context learning capacity of Large Language Models\n(LLMs). Instead of fine-tuning embedders, we propose to transform the text\nclustering into a classification task via LLM. First, we prompt LLM to generate\npotential labels for a given dataset. Second, after integrating similar labels\ngenerated by the LLM, we prompt the LLM to assign the most appropriate label to\neach sample in the dataset. Our framework has been experimentally proven to\nachieve comparable or superior performance to state-of-the-art clustering\nmethods that employ embeddings, without requiring complex fine-tuning or\nclustering algorithms. We make our code available to the public for utilization\nat https://github.com/ECNU-Text-Computing/Text-Clustering-via-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text clustering remains valuable in real-world applications where manual\nlabeling is cost-prohibitive. It facilitates efficient organization and\nanalysis of information by grouping similar texts based on their\nrepresentations. However, implementing this approach necessitates fine-tuned\nembedders for downstream data and sophisticated similarity metrics. To address\nthis issue, this study presents a novel framework for text clustering that\neffectively leverages the in-context learning capacity of Large Language Models\n(LLMs). Instead of fine-tuning embedders, we propose to transform the text\nclustering into a classification task via LLM. First, we prompt LLM to generate\npotential labels for a given dataset. Second, after integrating similar labels\ngenerated by the LLM, we prompt the LLM to assign the most appropriate label to\neach sample in the dataset. Our framework has been experimentally proven to\nachieve comparable or superior performance to state-of-the-art clustering\nmethods that employ embeddings, without requiring complex fine-tuning or\nclustering algorithms. We make our code available to the public for utilization\nat https://github.com/ECNU-Text-Computing/Text-Clustering-via-LLM."
                },
                "authors": [
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Guoxiu He"
                    }
                ],
                "author_detail": {
                    "name": "Guoxiu He"
                },
                "author": "Guoxiu He",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09893v3",
                "updated": "2025-01-02T08:43:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    43,
                    57,
                    3,
                    2,
                    0
                ],
                "published": "2024-07-13T13:58:24Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    13,
                    58,
                    24,
                    5,
                    195,
                    0
                ],
                "title": "Synergistic Multi-Agent Framework with Trajectory Learning for\n  Knowledge-Intensive Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergistic Multi-Agent Framework with Trajectory Learning for\n  Knowledge-Intensive Tasks"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long-Short Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on five knowledge-intensive\ntasks demonstrate SMART's superior performance compared to widely adopted\nknowledge internalization and knowledge enhancement methods. Our framework can\nextend beyond knowledge-intensive tasks to more complex scenarios. Our code is\navailable at https://github.com/yueshengbin/SMART.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long-Short Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on five knowledge-intensive\ntasks demonstrate SMART's superior performance compared to widely adopted\nknowledge internalization and knowledge enhancement methods. Our framework can\nextend beyond knowledge-intensive tasks to more complex scenarios. Our code is\navailable at https://github.com/yueshengbin/SMART."
                },
                "authors": [
                    {
                        "name": "Shengbin Yue"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16500v2",
                "updated": "2025-01-02T07:29:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    7,
                    29,
                    1,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-21T06:16:04Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    6,
                    16,
                    4,
                    5,
                    356,
                    0
                ],
                "title": "Speech Retrieval-Augmented Generation without Automatic Speech\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech Retrieval-Augmented Generation without Automatic Speech\n  Recognition"
                },
                "summary": "One common approach for question answering over speech data is to first\ntranscribe speech using automatic speech recognition (ASR) and then employ\ntext-based retrieval-augmented generation (RAG) on the transcriptions. While\nthis cascaded pipeline has proven effective in many practical settings, ASR\nerrors can propagate to the retrieval and generation steps. To overcome this\nlimitation, we introduce SpeechRAG, a novel framework designed for\nopen-question answering over spoken data. Our proposed approach fine-tunes a\npre-trained speech encoder into a speech adapter fed into a frozen large\nlanguage model (LLM)--based retrieval model. By aligning the embedding spaces\nof text and speech, our speech retriever directly retrieves audio passages from\ntext-based queries, leveraging the retrieval capacity of the frozen text\nretriever. Our retrieval experiments on spoken question answering datasets show\nthat direct speech retrieval does not degrade over the text-based baseline, and\noutperforms the cascaded systems using ASR. For generation, we use a speech\nlanguage model (SLM) as a generator, conditioned on audio passages rather than\ntranscripts. Without fine-tuning of the SLM, this approach outperforms cascaded\ntext-based models when there is high WER in the transcripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One common approach for question answering over speech data is to first\ntranscribe speech using automatic speech recognition (ASR) and then employ\ntext-based retrieval-augmented generation (RAG) on the transcriptions. While\nthis cascaded pipeline has proven effective in many practical settings, ASR\nerrors can propagate to the retrieval and generation steps. To overcome this\nlimitation, we introduce SpeechRAG, a novel framework designed for\nopen-question answering over spoken data. Our proposed approach fine-tunes a\npre-trained speech encoder into a speech adapter fed into a frozen large\nlanguage model (LLM)--based retrieval model. By aligning the embedding spaces\nof text and speech, our speech retriever directly retrieves audio passages from\ntext-based queries, leveraging the retrieval capacity of the frozen text\nretriever. Our retrieval experiments on spoken question answering datasets show\nthat direct speech retrieval does not degrade over the text-based baseline, and\noutperforms the cascaded systems using ASR. For generation, we use a speech\nlanguage model (SLM) as a generator, conditioned on audio passages rather than\ntranscripts. Without fine-tuning of the SLM, this approach outperforms cascaded\ntext-based models when there is high WER in the transcripts."
                },
                "authors": [
                    {
                        "name": "Do June Min"
                    },
                    {
                        "name": "Karel Mundnich"
                    },
                    {
                        "name": "Andy Lapastora"
                    },
                    {
                        "name": "Erfan Soltanmohammadi"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Kyu Han"
                    }
                ],
                "author_detail": {
                    "name": "Kyu Han"
                },
                "author": "Kyu Han",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01985v2",
                "updated": "2025-01-02T04:15:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    4,
                    15,
                    37,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-02T19:45:19Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    19,
                    45,
                    19,
                    2,
                    276,
                    0
                ],
                "title": "Lost-in-Distance: Impact of Contextual Proximity on LLM Performance in\n  Graph Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost-in-Distance: Impact of Contextual Proximity on LLM Performance in\n  Graph Tasks"
                },
                "summary": "Despite significant advancements, Large Language Models (LLMs) exhibit blind\nspots that impair their ability to retrieve and process relevant contextual\ndata effectively. We demonstrate that LLM performance in graph tasks with\ncomplexities beyond the \"needle-in-a-haystack\" scenario-where solving the\nproblem requires cross-referencing and reasoning across multiple subproblems\njointly-is influenced by the proximity of relevant information within the\ncontext, a phenomenon we term \"lost-in-distance\". We examine two fundamental\ngraph tasks: identifying common connections between two nodes and assessing\nsimilarity among three nodes, and show that the model's performance in these\ntasks significantly depends on the relative positioning of common edges. We\nevaluate three publicly available LLMs using various graph encoding techniques\nthat represent graph structures for LLM input. We propose a formulation for the\nlost-in-distance phenomenon and demonstrate that lost-in-distance and\nlost-in-the middle phenomenas occur independently. Results indicate that model\naccuracy can decline by up to 6x as the distance between node connections\nincreases, independent of graph encoding and model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements, Large Language Models (LLMs) exhibit blind\nspots that impair their ability to retrieve and process relevant contextual\ndata effectively. We demonstrate that LLM performance in graph tasks with\ncomplexities beyond the \"needle-in-a-haystack\" scenario-where solving the\nproblem requires cross-referencing and reasoning across multiple subproblems\njointly-is influenced by the proximity of relevant information within the\ncontext, a phenomenon we term \"lost-in-distance\". We examine two fundamental\ngraph tasks: identifying common connections between two nodes and assessing\nsimilarity among three nodes, and show that the model's performance in these\ntasks significantly depends on the relative positioning of common edges. We\nevaluate three publicly available LLMs using various graph encoding techniques\nthat represent graph structures for LLM input. We propose a formulation for the\nlost-in-distance phenomenon and demonstrate that lost-in-distance and\nlost-in-the middle phenomenas occur independently. Results indicate that model\naccuracy can decline by up to 6x as the distance between node connections\nincreases, independent of graph encoding and model size."
                },
                "authors": [
                    {
                        "name": "Hamed Firooz"
                    },
                    {
                        "name": "Maziar Sanjabi"
                    },
                    {
                        "name": "Wenlong Jiang"
                    },
                    {
                        "name": "Xiaoling Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoling Zhai"
                },
                "author": "Xiaoling Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13334v3",
                "updated": "2025-01-02T04:06:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    4,
                    6,
                    46,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-17T08:46:09Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    46,
                    9,
                    3,
                    291,
                    0
                ],
                "title": "BiasJailbreak:Analyzing Ethical Biases and Jailbreak Vulnerabilities in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiasJailbreak:Analyzing Ethical Biases and Jailbreak Vulnerabilities in\n  Large Language Models"
                },
                "summary": "Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as `jailbreaks', where\nmalicious inputs can coerce LLMs into generating harmful content bypassing\nsafety alignments. In this paper, we delve into the ethical biases in LLMs and\nexamine how those biases could be exploited for jailbreaks. Notably, these\nbiases result in a jailbreaking success rate in GPT-4o models that differs by\n20\\% between non-binary and cisgender keywords and by 16\\% between white and\nblack keywords, even when the other parts of the prompts are identical. We\nintroduce the concept of BiasJailbreak, highlighting the inherent risks posed\nby these safety-induced biases. BiasJailbreak generates biased keywords\nautomatically by asking the target LLM itself, and utilizes the keywords to\ngenerate harmful output. Additionally, we propose an efficient defense method\nBiasDefense, which prevents jailbreak attempts by injecting defense prompts\nprior to generation. BiasDefense stands as an appealing alternative to Guard\nModels, such as Llama-Guard, that require additional inference cost after text\ngeneration. Our findings emphasize that ethical biases in LLMs can actually\nlead to generating unsafe output, and suggest a method to make the LLMs more\nsecure and unbiased. To enable further research and improvements, we\nopen-source our code and artifacts of BiasJailbreak, providing the community\nwith tools to better understand and mitigate safety-induced biases in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as `jailbreaks', where\nmalicious inputs can coerce LLMs into generating harmful content bypassing\nsafety alignments. In this paper, we delve into the ethical biases in LLMs and\nexamine how those biases could be exploited for jailbreaks. Notably, these\nbiases result in a jailbreaking success rate in GPT-4o models that differs by\n20\\% between non-binary and cisgender keywords and by 16\\% between white and\nblack keywords, even when the other parts of the prompts are identical. We\nintroduce the concept of BiasJailbreak, highlighting the inherent risks posed\nby these safety-induced biases. BiasJailbreak generates biased keywords\nautomatically by asking the target LLM itself, and utilizes the keywords to\ngenerate harmful output. Additionally, we propose an efficient defense method\nBiasDefense, which prevents jailbreak attempts by injecting defense prompts\nprior to generation. BiasDefense stands as an appealing alternative to Guard\nModels, such as Llama-Guard, that require additional inference cost after text\ngeneration. Our findings emphasize that ethical biases in LLMs can actually\nlead to generating unsafe output, and suggest a method to make the LLMs more\nsecure and unbiased. To enable further research and improvements, we\nopen-source our code and artifacts of BiasJailbreak, providing the community\nwith tools to better understand and mitigate safety-induced biases in LLMs."
                },
                "authors": [
                    {
                        "name": "Isack Lee"
                    },
                    {
                        "name": "Haebin Seong"
                    }
                ],
                "author_detail": {
                    "name": "Haebin Seong"
                },
                "author": "Haebin Seong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20151v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20151v2",
                "updated": "2025-01-02T03:59:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    59,
                    50,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-28T13:32:36Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    13,
                    32,
                    36,
                    5,
                    363,
                    0
                ],
                "title": "Contention-Aware Microservice Deployment in Collaborative Mobile Edge\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contention-Aware Microservice Deployment in Collaborative Mobile Edge\n  Networks"
                },
                "summary": "As an emerging computing paradigm, mobile edge computing (MEC) provides\nprocessing capabilities at the network edge, aiming to reduce latency and\nimprove user experience. Meanwhile, the advancement of containerization\ntechnology facilitates the deployment of microservice-based applications via\nedge node collaboration, ensuring highly efficient service delivery. However,\nexisting research overlooks the resource contention among microservices in MEC.\nThis neglect potentially results in inadequate resources for microservices\nconstituting latency-sensitive applications, leading to increased response time\nand ultimately compromising quality of service (QoS). To solve this problem, we\npropose the Contention-Aware Multi-Application Microservice Deployment (CAMD)\nalgorithm for collaborative MEC, balancing rapid response for applications with\nlow-latency requirements and overall processing efficiency. The CAMD algorithm\ndecomposes the overall deployment problem into manageable sub-problems, each\nfocusing on a single microservice, then employs a heuristic approach to\noptimize these sub-problems, and ultimately arrives at an optimized deployment\nscheme through an iterative process. Finally, the superiority of the proposed\nalgorithm is evidenced through intensive experiments and comparison with\nbaseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As an emerging computing paradigm, mobile edge computing (MEC) provides\nprocessing capabilities at the network edge, aiming to reduce latency and\nimprove user experience. Meanwhile, the advancement of containerization\ntechnology facilitates the deployment of microservice-based applications via\nedge node collaboration, ensuring highly efficient service delivery. However,\nexisting research overlooks the resource contention among microservices in MEC.\nThis neglect potentially results in inadequate resources for microservices\nconstituting latency-sensitive applications, leading to increased response time\nand ultimately compromising quality of service (QoS). To solve this problem, we\npropose the Contention-Aware Multi-Application Microservice Deployment (CAMD)\nalgorithm for collaborative MEC, balancing rapid response for applications with\nlow-latency requirements and overall processing efficiency. The CAMD algorithm\ndecomposes the overall deployment problem into manageable sub-problems, each\nfocusing on a single microservice, then employs a heuristic approach to\noptimize these sub-problems, and ultimately arrives at an optimized deployment\nscheme through an iterative process. Finally, the superiority of the proposed\nalgorithm is evidenced through intensive experiments and comparison with\nbaseline algorithms."
                },
                "authors": [
                    {
                        "name": "Xinlei Ge"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Yukun Sun"
                    },
                    {
                        "name": "Yunji Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yunji Zhao"
                },
                "author": "Yunji Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20151v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20151v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01340v2",
                "updated": "2025-01-02T03:29:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    29,
                    31,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-02T10:07:01Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    10,
                    7,
                    1,
                    0,
                    337,
                    0
                ],
                "title": "A 2-step Framework for Automated Literary Translation Evaluation: Its\n  Promises and Pitfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A 2-step Framework for Automated Literary Translation Evaluation: Its\n  Promises and Pitfalls"
                },
                "summary": "In this work, we propose and evaluate the feasibility of a two-stage pipeline\nto evaluate literary machine translation, in a fine-grained manner, from\nEnglish to Korean. The results show that our framework provides fine-grained,\ninterpretable metrics suited for literary translation and obtains a higher\ncorrelation with human judgment than traditional machine translation metrics.\nNonetheless, it still fails to match inter-human agreement, especially in\nmetrics like Korean Honorifics. We also observe that LLMs tend to favor\ntranslations generated by other LLMs, and we highlight the necessity of\ndeveloping more sophisticated evaluation methods to ensure accurate and\nculturally sensitive machine translation of literary works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose and evaluate the feasibility of a two-stage pipeline\nto evaluate literary machine translation, in a fine-grained manner, from\nEnglish to Korean. The results show that our framework provides fine-grained,\ninterpretable metrics suited for literary translation and obtains a higher\ncorrelation with human judgment than traditional machine translation metrics.\nNonetheless, it still fails to match inter-human agreement, especially in\nmetrics like Korean Honorifics. We also observe that LLMs tend to favor\ntranslations generated by other LLMs, and we highlight the necessity of\ndeveloping more sophisticated evaluation methods to ensure accurate and\nculturally sensitive machine translation of literary works."
                },
                "authors": [
                    {
                        "name": "Sheikh Shafayat"
                    },
                    {
                        "name": "Dongkeun Yoon"
                    },
                    {
                        "name": "Woori Jang"
                    },
                    {
                        "name": "Jiwoo Choi"
                    },
                    {
                        "name": "Alice Oh"
                    },
                    {
                        "name": "Seohyon Jung"
                    }
                ],
                "author_detail": {
                    "name": "Seohyon Jung"
                },
                "author": "Seohyon Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08877v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08877v4",
                "updated": "2025-01-02T03:14:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    14,
                    11,
                    3,
                    2,
                    0
                ],
                "published": "2024-04-13T02:36:40Z",
                "published_parsed": [
                    2024,
                    4,
                    13,
                    2,
                    36,
                    40,
                    5,
                    104,
                    0
                ],
                "title": "Aligning the Objective of LLM-based Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning the Objective of LLM-based Program Repair"
                },
                "summary": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR."
                },
                "authors": [
                    {
                        "name": "Junjielong Xu"
                    },
                    {
                        "name": "Ying Fu"
                    },
                    {
                        "name": "Shin Hwei Tan"
                    },
                    {
                        "name": "Pinjia He"
                    }
                ],
                "author_detail": {
                    "name": "Pinjia He"
                },
                "author": "Pinjia He",
                "arxiv_comment": "Accepted by ICSE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08877v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08877v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16668v2",
                "updated": "2025-01-02T03:02:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    2,
                    13,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-22T03:53:46Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    3,
                    53,
                    46,
                    1,
                    296,
                    0
                ],
                "title": "Satori: Towards Proactive AR Assistant with Belief-Desire-Intention User\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satori: Towards Proactive AR Assistant with Belief-Desire-Intention User\n  Modeling"
                },
                "summary": "Augmented Reality assistance are increasingly popular for supporting users\nwith tasks like assembly and cooking. However, current practice typically\nprovide reactive responses initialized from user requests, lacking\nconsideration of rich contextual and user-specific information. To address this\nlimitation, we propose a novel AR assistance system, Satori, that models both\nuser states and environmental contexts to deliver proactive guidance. Our\nsystem combines the Belief-Desire-Intention (BDI) model with a state-of-the-art\nmulti-modal large language model (LLM) to infer contextually appropriate\nguidance. The design is informed by two formative studies involving twelve\nexperts. A sixteen within-subject study find that Satori achieves performance\ncomparable to an designer-created Wizard-of-Oz (WoZ) system without relying on\nmanual configurations or heuristics, thereby enhancing generalizability,\nreusability and opening up new possibilities for AR assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Reality assistance are increasingly popular for supporting users\nwith tasks like assembly and cooking. However, current practice typically\nprovide reactive responses initialized from user requests, lacking\nconsideration of rich contextual and user-specific information. To address this\nlimitation, we propose a novel AR assistance system, Satori, that models both\nuser states and environmental contexts to deliver proactive guidance. Our\nsystem combines the Belief-Desire-Intention (BDI) model with a state-of-the-art\nmulti-modal large language model (LLM) to infer contextually appropriate\nguidance. The design is informed by two formative studies involving twelve\nexperts. A sixteen within-subject study find that Satori achieves performance\ncomparable to an designer-created Wizard-of-Oz (WoZ) system without relying on\nmanual configurations or heuristics, thereby enhancing generalizability,\nreusability and opening up new possibilities for AR assistance."
                },
                "authors": [
                    {
                        "name": "Chenyi Li"
                    },
                    {
                        "name": "Guande Wu"
                    },
                    {
                        "name": "Gromit Yeuk-Yin Chan"
                    },
                    {
                        "name": "Dishita G Turakhia"
                    },
                    {
                        "name": "Sonia Castelo Quispe"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Leslie Welch"
                    },
                    {
                        "name": "Claudio Silva"
                    },
                    {
                        "name": "Jing Qian"
                    }
                ],
                "author_detail": {
                    "name": "Jing Qian"
                },
                "author": "Jing Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10422v2",
                "updated": "2025-01-02T01:11:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    1,
                    11,
                    46,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-10T11:03:49Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    3,
                    49,
                    1,
                    345,
                    0
                ],
                "title": "AutoPrep: Natural Language Question-Aware Data Preparation with a\n  Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPrep: Natural Language Question-Aware Data Preparation with a\n  Multi-Agent Framework"
                },
                "summary": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\naugmentation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\naugmentation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation."
                },
                "authors": [
                    {
                        "name": "Meihao Fan"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Lei Cao"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Xiaoyong Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyong Du"
                },
                "author": "Xiaoyong Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13184v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13184v5",
                "updated": "2025-01-01T23:34:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    23,
                    34,
                    53,
                    2,
                    1,
                    0
                ],
                "published": "2024-02-20T17:49:46Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    17,
                    49,
                    46,
                    1,
                    51,
                    0
                ],
                "title": "What if LLMs Have Different World Views: Simulating Alien Civilizations\n  with LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What if LLMs Have Different World Views: Simulating Alien Civilizations\n  with LLM-based Agents"
                },
                "summary": "This study introduces \"CosmoAgent,\" an innovative artificial intelligence\nsystem that utilizes Large Language Models (LLMs) to simulate complex\ninteractions between human and extraterrestrial civilizations. This paper\nintroduces a mathematical model for quantifying the levels of civilization\ndevelopment and further employs a state transition matrix approach to evaluate\ntheir trajectories. Through this methodology, our study quantitatively analyzes\nthe growth trajectories of civilizations, providing insights into future\ndecision-making at critical points of growth and saturation. Furthermore, this\npaper acknowledges the vast diversity of potential living conditions across the\nuniverse, which could foster unique cosmologies, ethical codes, and worldviews\namong different civilizations. Recognizing the Earth-centric bias inherent in\ncurrent LLM designs, we propose the novel concept of using LLM agents with\ndiverse ethical paradigms and simulating interactions between entities with\ndistinct moral principles. This innovative research not only introduces a novel\nmethod for comprehending potential inter-civilizational dynamics but also holds\npractical value in enabling entities with divergent value systems to\nstrategize, prevent conflicts, and engage in games under conditions of\nasymmetric information. The accompanying code is available at\nhttps://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces \"CosmoAgent,\" an innovative artificial intelligence\nsystem that utilizes Large Language Models (LLMs) to simulate complex\ninteractions between human and extraterrestrial civilizations. This paper\nintroduces a mathematical model for quantifying the levels of civilization\ndevelopment and further employs a state transition matrix approach to evaluate\ntheir trajectories. Through this methodology, our study quantitatively analyzes\nthe growth trajectories of civilizations, providing insights into future\ndecision-making at critical points of growth and saturation. Furthermore, this\npaper acknowledges the vast diversity of potential living conditions across the\nuniverse, which could foster unique cosmologies, ethical codes, and worldviews\namong different civilizations. Recognizing the Earth-centric bias inherent in\ncurrent LLM designs, we propose the novel concept of using LLM agents with\ndiverse ethical paradigms and simulating interactions between entities with\ndistinct moral principles. This innovative research not only introduces a novel\nmethod for comprehending potential inter-civilizational dynamics but also holds\npractical value in enabling entities with divergent value systems to\nstrategize, prevent conflicts, and engage in games under conditions of\nasymmetric information. The accompanying code is available at\nhttps://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents."
                },
                "authors": [
                    {
                        "name": "Zhaoqian Xue"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Beichen Wang"
                    },
                    {
                        "name": "Suiyuan Zhu"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Hua Tang"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13184v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13184v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00170v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00170v3",
                "updated": "2025-01-01T18:42:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    18,
                    42,
                    0,
                    2,
                    1,
                    0
                ],
                "published": "2024-07-31T21:43:55Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    43,
                    55,
                    2,
                    213,
                    0
                ],
                "title": "CREW: Facilitating Human-AI Teaming Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CREW: Facilitating Human-AI Teaming Research"
                },
                "summary": "With the increasing deployment of artificial intelligence (AI) technologies,\nthe potential of humans working with AI agents has been growing at a great\nspeed. Human-AI teaming is an important paradigm for studying various aspects\nwhen humans and AI agents work together. The unique aspect of Human-AI teaming\nresearch is the need to jointly study humans and AI agents, demanding\nmultidisciplinary research efforts from machine learning to human-computer\ninteraction, robotics, cognitive science, neuroscience, psychology, social\nscience, and complex systems. However, existing platforms for Human-AI teaming\nresearch are limited, often supporting oversimplified scenarios and a single\ntask, or specifically focusing on either human-teaming research or multi-agent\nAI algorithms. We introduce CREW, a platform to facilitate Human-AI teaming\nresearch in real-time decision-making scenarios and engage collaborations from\nmultiple scientific disciplines, with a strong emphasis on human involvement.\nIt includes pre-built tasks for cognitive studies and Human-AI teaming with\nexpandable potentials from our modular design. Following conventional cognitive\nneuroscience research, CREW also supports multimodal human physiological signal\nrecording for behavior analysis. Moreover, CREW benchmarks real-time\nhuman-guided reinforcement learning agents using state-of-the-art algorithms\nand well-tuned baselines. With CREW, we were able to conduct 50 human subject\nstudies within a week to verify the effectiveness of our benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing deployment of artificial intelligence (AI) technologies,\nthe potential of humans working with AI agents has been growing at a great\nspeed. Human-AI teaming is an important paradigm for studying various aspects\nwhen humans and AI agents work together. The unique aspect of Human-AI teaming\nresearch is the need to jointly study humans and AI agents, demanding\nmultidisciplinary research efforts from machine learning to human-computer\ninteraction, robotics, cognitive science, neuroscience, psychology, social\nscience, and complex systems. However, existing platforms for Human-AI teaming\nresearch are limited, often supporting oversimplified scenarios and a single\ntask, or specifically focusing on either human-teaming research or multi-agent\nAI algorithms. We introduce CREW, a platform to facilitate Human-AI teaming\nresearch in real-time decision-making scenarios and engage collaborations from\nmultiple scientific disciplines, with a strong emphasis on human involvement.\nIt includes pre-built tasks for cognitive studies and Human-AI teaming with\nexpandable potentials from our modular design. Following conventional cognitive\nneuroscience research, CREW also supports multimodal human physiological signal\nrecording for behavior analysis. Moreover, CREW benchmarks real-time\nhuman-guided reinforcement learning agents using state-of-the-art algorithms\nand well-tuned baselines. With CREW, we were able to conduct 50 human subject\nstudies within a week to verify the effectiveness of our benchmark."
                },
                "authors": [
                    {
                        "name": "Lingyu Zhang"
                    },
                    {
                        "name": "Zhengran Ji"
                    },
                    {
                        "name": "Boyuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Boyuan Chen"
                },
                "author": "Boyuan Chen",
                "arxiv_comment": "Our project website is at: http://generalroboticslab.com/CREW",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00170v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00170v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04264v3",
                "updated": "2025-01-01T15:53:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    53,
                    58,
                    2,
                    1,
                    0
                ],
                "published": "2024-06-06T17:09:32Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    17,
                    9,
                    32,
                    3,
                    158,
                    0
                ],
                "title": "MLVU: Benchmarking Multi-task Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLVU: Benchmarking Multi-task Long Video Understanding"
                },
                "summary": "The evaluation of Long Video Understanding (LVU) performance poses an\nimportant but challenging research problem. Despite previous efforts, the\nexisting video understanding benchmarks are severely constrained by several\nissues, especially the insufficient lengths of videos, a lack of diversity in\nvideo types and evaluation tasks, and the inappropriateness for evaluating LVU\nperformances. To address the above problems, we propose a new benchmark called\nMLVU (Multi-task Long Video Understanding Benchmark) for the comprehensive and\nin-depth evaluation of LVU. MLVU presents the following critical values:\n\\textit{1)} The substantial and flexible extension of video lengths, which\nenables the benchmark to evaluate LVU performance across a wide range of\ndurations. \\textit{2)} The inclusion of various video genres, e.g., movies,\nsurveillance footage, egocentric videos, cartoons, game videos, etc., which\nreflects the models' LVU performances in different scenarios. \\textit{3)} The\ndevelopment of diversified evaluation tasks, which enables a comprehensive\nexamination of MLLMs' key abilities in long-video understanding. The empirical\nstudy with 23 latest MLLMs reveals significant room for improvement in today's\ntechnique, as all existing methods struggle with most of the evaluation tasks\nand exhibit severe performance degradation when handling longer videos.\nAdditionally, it suggests that factors such as context length,\nimage-understanding ability, and the choice of LLM backbone can play critical\nroles in future advancements. We anticipate that MLVU will advance the research\nof long video understanding by providing a comprehensive and in-depth analysis\nof MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of Long Video Understanding (LVU) performance poses an\nimportant but challenging research problem. Despite previous efforts, the\nexisting video understanding benchmarks are severely constrained by several\nissues, especially the insufficient lengths of videos, a lack of diversity in\nvideo types and evaluation tasks, and the inappropriateness for evaluating LVU\nperformances. To address the above problems, we propose a new benchmark called\nMLVU (Multi-task Long Video Understanding Benchmark) for the comprehensive and\nin-depth evaluation of LVU. MLVU presents the following critical values:\n\\textit{1)} The substantial and flexible extension of video lengths, which\nenables the benchmark to evaluate LVU performance across a wide range of\ndurations. \\textit{2)} The inclusion of various video genres, e.g., movies,\nsurveillance footage, egocentric videos, cartoons, game videos, etc., which\nreflects the models' LVU performances in different scenarios. \\textit{3)} The\ndevelopment of diversified evaluation tasks, which enables a comprehensive\nexamination of MLLMs' key abilities in long-video understanding. The empirical\nstudy with 23 latest MLLMs reveals significant room for improvement in today's\ntechnique, as all existing methods struggle with most of the evaluation tasks\nand exhibit severe performance degradation when handling longer videos.\nAdditionally, it suggests that factors such as context length,\nimage-understanding ability, and the choice of LLM backbone can play critical\nroles in future advancements. We anticipate that MLVU will advance the research\nof long video understanding by providing a comprehensive and in-depth analysis\nof MLLMs."
                },
                "authors": [
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Boya Wu"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Yongping Xiong"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13621v2",
                "updated": "2025-01-01T15:40:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    40,
                    57,
                    2,
                    1,
                    0
                ],
                "published": "2023-11-22T08:34:33Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    8,
                    34,
                    33,
                    2,
                    326,
                    0
                ],
                "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EA-KD: Entropy-based Adaptive Knowledge Distillation"
                },
                "summary": "Knowledge distillation (KD) enables a smaller \"student\" model to mimic a\nlarger \"teacher\" model by transferring knowledge from the teacher's output or\nfeatures. However, most KD methods treat all samples uniformly, overlooking the\nvarying learning value of each sample and thereby limiting effectiveness. In\nthis paper, we propose Entropy-based Adaptive Knowledge Distillation (EA-KD), a\nsimple yet effective plug-and-play KD method that prioritizes learning from\nvaluable samples. EA-KD quantifies each sample's learning value by\nstrategically combining the entropy of the teacher and student output, then\ndynamically reweights the distillation loss to place greater emphasis on\nhigh-value samples. Extensive experiments across diverse KD frameworks and\ntasks$\\unicode{x2014}$including image classification, object detection, and\nlarge language model (LLM) distillation$\\unicode{x2014}$demonstrate that EA-KD\nconsistently enhances performance, achieving state-of-the-art results with\nnegligible computational cost. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) enables a smaller \"student\" model to mimic a\nlarger \"teacher\" model by transferring knowledge from the teacher's output or\nfeatures. However, most KD methods treat all samples uniformly, overlooking the\nvarying learning value of each sample and thereby limiting effectiveness. In\nthis paper, we propose Entropy-based Adaptive Knowledge Distillation (EA-KD), a\nsimple yet effective plug-and-play KD method that prioritizes learning from\nvaluable samples. EA-KD quantifies each sample's learning value by\nstrategically combining the entropy of the teacher and student output, then\ndynamically reweights the distillation loss to place greater emphasis on\nhigh-value samples. Extensive experiments across diverse KD frameworks and\ntasks$\\unicode{x2014}$including image classification, object detection, and\nlarge language model (LLM) distillation$\\unicode{x2014}$demonstrate that EA-KD\nconsistently enhances performance, achieving state-of-the-art results with\nnegligible computational cost. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Chi-Ping Su"
                    },
                    {
                        "name": "Ching-Hsun Tseng"
                    },
                    {
                        "name": "Bin Pu"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Zhuangzhuang Chen"
                    },
                    {
                        "name": "Shin-Jye Lee"
                    }
                ],
                "author_detail": {
                    "name": "Shin-Jye Lee"
                },
                "author": "Shin-Jye Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12698v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12698v4",
                "updated": "2025-01-01T14:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    14,
                    51,
                    37,
                    2,
                    1,
                    0
                ],
                "published": "2024-12-17T09:16:28Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    16,
                    28,
                    1,
                    352,
                    0
                ],
                "title": "Audio Array-Based 3D UAV Trajectory Estimation with LiDAR\n  Pseudo-Labeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio Array-Based 3D UAV Trajectory Estimation with LiDAR\n  Pseudo-Labeling"
                },
                "summary": "As small unmanned aerial vehicles (UAVs) become increasingly prevalent, there\nis growing concern regarding their impact on public safety and privacy,\nhighlighting the need for advanced tracking and trajectory estimation\nsolutions. In response, this paper introduces a novel framework that utilizes\naudio array for 3D UAV trajectory estimation. Our approach incorporates a\nself-supervised learning model, starting with the conversion of audio data into\nmel-spectrograms, which are analyzed through an encoder to extract crucial\ntemporal and spectral information. Simultaneously, UAV trajectories are\nestimated using LiDAR point clouds via unsupervised methods. These LiDAR-based\nestimations act as pseudo labels, enabling the training of an Audio Perception\nNetwork without requiring labeled data. In this architecture, the LiDAR-based\nsystem operates as the Teacher Network, guiding the Audio Perception Network,\nwhich serves as the Student Network. Once trained, the model can independently\npredict 3D trajectories using only audio signals, with no need for LiDAR data\nor external ground truth during deployment. To further enhance precision, we\napply Gaussian Process modeling for improved spatiotemporal tracking. Our\nmethod delivers top-tier performance on the MMAUD dataset, establishing a new\nbenchmark in trajectory estimation using self-supervised learning techniques\nwithout reliance on ground truth annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As small unmanned aerial vehicles (UAVs) become increasingly prevalent, there\nis growing concern regarding their impact on public safety and privacy,\nhighlighting the need for advanced tracking and trajectory estimation\nsolutions. In response, this paper introduces a novel framework that utilizes\naudio array for 3D UAV trajectory estimation. Our approach incorporates a\nself-supervised learning model, starting with the conversion of audio data into\nmel-spectrograms, which are analyzed through an encoder to extract crucial\ntemporal and spectral information. Simultaneously, UAV trajectories are\nestimated using LiDAR point clouds via unsupervised methods. These LiDAR-based\nestimations act as pseudo labels, enabling the training of an Audio Perception\nNetwork without requiring labeled data. In this architecture, the LiDAR-based\nsystem operates as the Teacher Network, guiding the Audio Perception Network,\nwhich serves as the Student Network. Once trained, the model can independently\npredict 3D trajectories using only audio signals, with no need for LiDAR data\nor external ground truth during deployment. To further enhance precision, we\napply Gaussian Process modeling for improved spatiotemporal tracking. Our\nmethod delivers top-tier performance on the MMAUD dataset, establishing a new\nbenchmark in trajectory estimation using self-supervised learning techniques\nwithout reliance on ground truth annotations."
                },
                "authors": [
                    {
                        "name": "Allen Lei"
                    },
                    {
                        "name": "Tianchen Deng"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Jianfei Yang"
                    },
                    {
                        "name": "Shenghai Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Shenghai Yuan"
                },
                "author": "Shenghai Yuan",
                "arxiv_comment": "Accepted for ICASSP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12698v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12698v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04416v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04416v4",
                "updated": "2025-01-01T13:46:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    46,
                    37,
                    2,
                    1,
                    0
                ],
                "published": "2024-07-05T11:07:13Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    11,
                    7,
                    13,
                    4,
                    187,
                    0
                ],
                "title": "Sound-VECaps: Improving Audio Generation with Visual Enhanced Captions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sound-VECaps: Improving Audio Generation with Visual Enhanced Captions"
                },
                "summary": "Generative models have shown significant achievements in audio generation\ntasks. However, existing models struggle with complex and detailed prompts,\nleading to potential performance degradation. We hypothesize that this problem\nstems from the simplicity and scarcity of the training data. This work aims to\ncreate a large-scale audio dataset with rich captions for improving audio\ngeneration models. We first develop an automated pipeline to generate detailed\ncaptions by transforming predicted visual captions, audio captions, and tagging\nlabels into comprehensive descriptions using a Large Language Model (LLM). The\nresulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption\npairs with enriched details including audio event orders, occurred places and\nenvironment information. We then demonstrate that training the text-to-audio\ngeneration models with Sound-VECaps significantly improves the performance on\ncomplex prompts. Furthermore, we conduct ablation studies of the models on\nseveral downstream audio-language tasks, showing the potential of Sound-VECaps\nin advancing audio-text representation learning. Our dataset and models are\navailable online from here https://yyua8222.github.io/Sound-VECaps-demo/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have shown significant achievements in audio generation\ntasks. However, existing models struggle with complex and detailed prompts,\nleading to potential performance degradation. We hypothesize that this problem\nstems from the simplicity and scarcity of the training data. This work aims to\ncreate a large-scale audio dataset with rich captions for improving audio\ngeneration models. We first develop an automated pipeline to generate detailed\ncaptions by transforming predicted visual captions, audio captions, and tagging\nlabels into comprehensive descriptions using a Large Language Model (LLM). The\nresulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption\npairs with enriched details including audio event orders, occurred places and\nenvironment information. We then demonstrate that training the text-to-audio\ngeneration models with Sound-VECaps significantly improves the performance on\ncomplex prompts. Furthermore, we conduct ablation studies of the models on\nseveral downstream audio-language tasks, showing the potential of Sound-VECaps\nin advancing audio-text representation learning. Our dataset and models are\navailable online from here https://yyua8222.github.io/Sound-VECaps-demo/."
                },
                "authors": [
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Dongya Jia"
                    },
                    {
                        "name": "Xiaobin Zhuang"
                    },
                    {
                        "name": "Yuanzhe Chen"
                    },
                    {
                        "name": "Zhengxi Liu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xubo Liu"
                    },
                    {
                        "name": "Xiyuan Kang"
                    },
                    {
                        "name": "Mark D. Plumbley"
                    },
                    {
                        "name": "Wenwu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Wang"
                },
                "author": "Wenwu Wang",
                "arxiv_comment": "5 pages with 1 appendix, accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04416v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04416v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.09583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.09583v2",
                "updated": "2025-01-01T13:02:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    2,
                    5,
                    2,
                    1,
                    0
                ],
                "published": "2023-08-18T14:23:21Z",
                "published_parsed": [
                    2023,
                    8,
                    18,
                    14,
                    23,
                    21,
                    4,
                    230,
                    0
                ],
                "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models\n  via Reinforced Evol-Instruct",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WizardMath: Empowering Mathematical Reasoning for Large Language Models\n  via Reinforced Evol-Instruct"
                },
                "summary": "Large language models (LLMs), such as GPT-4, have shown remarkable\nperformance in natural language processing (NLP) tasks, including challenging\nmathematical reasoning. However, most existing open-source models are only\npre-trained on large-scale internet data and without math-related optimization.\nIn this paper, we present WizardMath, which enhances the mathematical CoT\nreasoning abilities of LLMs without using external python tools, by applying\nour proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method\nto the domain of math. Through extensive experiments on two mathematical\nreasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary\ncapabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier\nopen-source LLMs by a substantial margin with higher data efficiency.\nFurthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini\nPro and GPT-4-early-version. Additionally, our preliminary exploration\nhighlights the pivotal role of instruction evolution and process supervision in\nachieving exceptional math performance. For more details refer to\nhttps://github.com/nlpxucan/WizardLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), such as GPT-4, have shown remarkable\nperformance in natural language processing (NLP) tasks, including challenging\nmathematical reasoning. However, most existing open-source models are only\npre-trained on large-scale internet data and without math-related optimization.\nIn this paper, we present WizardMath, which enhances the mathematical CoT\nreasoning abilities of LLMs without using external python tools, by applying\nour proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method\nto the domain of math. Through extensive experiments on two mathematical\nreasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary\ncapabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier\nopen-source LLMs by a substantial margin with higher data efficiency.\nFurthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini\nPro and GPT-4-early-version. Additionally, our preliminary exploration\nhighlights the pivotal role of instruction evolution and process supervision in\nachieving exceptional math performance. For more details refer to\nhttps://github.com/nlpxucan/WizardLM"
                },
                "authors": [
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Jianguang Lou"
                    },
                    {
                        "name": "Chongyang Tao"
                    },
                    {
                        "name": "Xiubo Geng"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Shifeng Chen"
                    },
                    {
                        "name": "Yansong Tang"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "arxiv_comment": "LLM, Mathematical Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.09583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.09583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00746v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00746v8",
                "updated": "2025-01-01T12:12:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    12,
                    12,
                    56,
                    2,
                    1,
                    0
                ],
                "published": "2024-02-01T16:40:32Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    16,
                    40,
                    32,
                    3,
                    32,
                    0
                ],
                "title": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System"
                },
                "summary": "Recent advancements in artificial intelligence (AI), especially large\nlanguage models (LLMs), have significantly advanced healthcare applications and\ndemonstrated potentials in intelligent medical treatment. However, there are\nconspicuous challenges such as vast data volumes and inconsistent symptom\ncharacterization standards, preventing full integration of healthcare AI\nsystems with individual patients' needs. To promote professional and\npersonalized healthcare, we propose an innovative framework, Heath-LLM, which\ncombines large-scale feature extraction and medical knowledge trade-off\nscoring. Compared to traditional health management applications, our system has\nthree main advantages: (1) It integrates health reports and medical knowledge\ninto a large model to ask relevant questions to large language model for\ndisease prediction; (2) It leverages a retrieval augmented generation (RAG)\nmechanism to enhance feature extraction; (3) It incorporates a semi-automated\nfeature updating framework that can merge and delete features to improve\naccuracy of disease prediction. We experiment on a large number of health\nreports to assess the effectiveness of Health-LLM system. The results indicate\nthat the proposed system surpasses the existing ones and has the potential to\nsignificantly advance disease prediction and personalized health management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in artificial intelligence (AI), especially large\nlanguage models (LLMs), have significantly advanced healthcare applications and\ndemonstrated potentials in intelligent medical treatment. However, there are\nconspicuous challenges such as vast data volumes and inconsistent symptom\ncharacterization standards, preventing full integration of healthcare AI\nsystems with individual patients' needs. To promote professional and\npersonalized healthcare, we propose an innovative framework, Heath-LLM, which\ncombines large-scale feature extraction and medical knowledge trade-off\nscoring. Compared to traditional health management applications, our system has\nthree main advantages: (1) It integrates health reports and medical knowledge\ninto a large model to ask relevant questions to large language model for\ndisease prediction; (2) It leverages a retrieval augmented generation (RAG)\nmechanism to enhance feature extraction; (3) It incorporates a semi-automated\nfeature updating framework that can merge and delete features to improve\naccuracy of disease prediction. We experiment on a large number of health\nreports to assess the effectiveness of Health-LLM system. The results indicate\nthat the proposed system surpasses the existing ones and has the potential to\nsignificantly advance disease prediction and personalized health management."
                },
                "authors": [
                    {
                        "name": "Qinkai Yu"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Lizhou Fan"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Suiyuan Zhu"
                    },
                    {
                        "name": "Yanda Meng"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00746v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00746v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.07622v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.07622v4",
                "updated": "2025-01-01T08:16:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    8,
                    16,
                    41,
                    2,
                    1,
                    0
                ],
                "published": "2023-12-12T01:39:16Z",
                "published_parsed": [
                    2023,
                    12,
                    12,
                    1,
                    39,
                    16,
                    1,
                    346,
                    0
                ],
                "title": "Mathematical Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical Language Models: A Survey"
                },
                "summary": "In recent years, there has been remarkable progress in leveraging Language\nModels (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale\nLanguage Models (LLMs), within the domain of mathematics. This paper conducts a\ncomprehensive survey of mathematical LMs, systematically categorizing pivotal\nresearch endeavors from two distinct perspectives: tasks and methodologies. The\nlandscape reveals a large number of proposed mathematical LLMs, which are\nfurther delineated into instruction learning, tool-based methods, fundamental\nCoT techniques, advanced CoT methodologies and multi-modal methods. To\ncomprehend the benefits of mathematical LMs more thoroughly, we carry out an\nin-depth contrast of their characteristics and performance. In addition, our\nsurvey entails the compilation of over 60 mathematical datasets, including\ntraining datasets, benchmark datasets, and augmented datasets. Addressing the\nprimary challenges and delineating future trajectories within the field of\nmathematical LMs, this survey is poised to facilitate and inspire future\ninnovation among researchers invested in advancing this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been remarkable progress in leveraging Language\nModels (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale\nLanguage Models (LLMs), within the domain of mathematics. This paper conducts a\ncomprehensive survey of mathematical LMs, systematically categorizing pivotal\nresearch endeavors from two distinct perspectives: tasks and methodologies. The\nlandscape reveals a large number of proposed mathematical LLMs, which are\nfurther delineated into instruction learning, tool-based methods, fundamental\nCoT techniques, advanced CoT methodologies and multi-modal methods. To\ncomprehend the benefits of mathematical LMs more thoroughly, we carry out an\nin-depth contrast of their characteristics and performance. In addition, our\nsurvey entails the compilation of over 60 mathematical datasets, including\ntraining datasets, benchmark datasets, and augmented datasets. Addressing the\nprimary challenges and delineating future trajectories within the field of\nmathematical LMs, this survey is poised to facilitate and inspire future\ninnovation among researchers invested in advancing this domain."
                },
                "authors": [
                    {
                        "name": "Wentao Liu"
                    },
                    {
                        "name": "Hanglei Hu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Yuyang Ding"
                    },
                    {
                        "name": "Junsong Li"
                    },
                    {
                        "name": "Jiayi Zeng"
                    },
                    {
                        "name": "Mengliang He"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Aimin Zhou"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:1705.04146,\n  arXiv:2304.10977, arXiv:2112.00114, arXiv:1905.13319, arXiv:2304.12244,\n  arXiv:2206.01347, arXiv:2006.09265 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.07622v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.07622v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03104v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03104v2",
                "updated": "2025-01-01T07:23:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    7,
                    23,
                    17,
                    2,
                    1,
                    0
                ],
                "published": "2024-12-04T08:06:15Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    6,
                    15,
                    2,
                    339,
                    0
                ],
                "title": "ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced\n  Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced\n  Understanding and Reasoning"
                },
                "summary": "Understanding time series is crucial for its application in real-world\nscenarios. Recently, large language models (LLMs) have been increasingly\napplied to time series tasks, leveraging their strong language capabilities to\nenhance various applications. However, research on multimodal LLMs (MLLMs) for\ntime series understanding and reasoning remains limited, primarily due to the\nscarcity of high-quality datasets that align time series with textual\ninformation. This paper introduces ChatTS, a novel MLLM designed for time\nseries analysis. ChatTS treats time series as a modality, similar to how vision\nMLLMs process images, enabling it to perform both understanding and reasoning\nwith time series. To address the scarcity of training data, we propose an\nattribute-based method for generating synthetic time series with detailed\nattribute descriptions. We further introduce Time Series Evol-Instruct, a novel\napproach that generates diverse time series Q&As, enhancing the model's\nreasoning capabilities. To the best of our knowledge, ChatTS is the first\nTS-MLLM that takes multivariate time series as input for understanding and\nreasoning, which is fine-tuned exclusively on synthetic datasets. We evaluate\nits performance using benchmark datasets with real-world data, including six\nalignment tasks and four reasoning tasks. Our results show that ChatTS\nsignificantly outperforms existing vision-based MLLMs (e.g., GPT-4o) and\ntext/agent-based LLMs, achieving a 46.0% improvement in alignment tasks and a\n25.8% improvement in reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding time series is crucial for its application in real-world\nscenarios. Recently, large language models (LLMs) have been increasingly\napplied to time series tasks, leveraging their strong language capabilities to\nenhance various applications. However, research on multimodal LLMs (MLLMs) for\ntime series understanding and reasoning remains limited, primarily due to the\nscarcity of high-quality datasets that align time series with textual\ninformation. This paper introduces ChatTS, a novel MLLM designed for time\nseries analysis. ChatTS treats time series as a modality, similar to how vision\nMLLMs process images, enabling it to perform both understanding and reasoning\nwith time series. To address the scarcity of training data, we propose an\nattribute-based method for generating synthetic time series with detailed\nattribute descriptions. We further introduce Time Series Evol-Instruct, a novel\napproach that generates diverse time series Q&As, enhancing the model's\nreasoning capabilities. To the best of our knowledge, ChatTS is the first\nTS-MLLM that takes multivariate time series as input for understanding and\nreasoning, which is fine-tuned exclusively on synthetic datasets. We evaluate\nits performance using benchmark datasets with real-world data, including six\nalignment tasks and four reasoning tasks. Our results show that ChatTS\nsignificantly outperforms existing vision-based MLLMs (e.g., GPT-4o) and\ntext/agent-based LLMs, achieving a 46.0% improvement in alignment tasks and a\n25.8% improvement in reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zeyan Li"
                    },
                    {
                        "name": "Xiao He"
                    },
                    {
                        "name": "Longlong Xu"
                    },
                    {
                        "name": "Xidao Wen"
                    },
                    {
                        "name": "Tieying Zhang"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Rui Shi"
                    },
                    {
                        "name": "Dan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Dan Pei"
                },
                "author": "Dan Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03104v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16882v2",
                "updated": "2025-01-01T03:13:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    3,
                    13,
                    3,
                    2,
                    1,
                    0
                ],
                "published": "2024-12-22T06:22:40Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    6,
                    22,
                    40,
                    6,
                    357,
                    0
                ],
                "title": "PsychAdapter: Adapting LLM Transformers to Reflect Traits, Personality\n  and Mental Health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsychAdapter: Adapting LLM Transformers to Reflect Traits, Personality\n  and Mental Health"
                },
                "summary": "Artificial intelligence-based language generators are now a part of most\npeople's lives. However, by default, they tend to generate \"average\" language\nwithout reflecting the ways in which people differ. Here, we propose a\nlightweight modification to the standard language model transformer\narchitecture - \"PsychAdapter\" - that uses empirically derived trait-language\npatterns to generate natural language for specified personality, demographic,\nand mental health characteristics (with or without prompting). We applied\nPsychAdapters to modify OpenAI's GPT-2, Google's Gemma, and Meta's Llama 3 and\nfound generated text to reflect the desired traits. For example, expert raters\nevaluated PsychAdapter's generated text output and found it matched intended\ntrait levels with 87.3% average accuracy for Big Five personalities, and 96.7%\nfor depression and life satisfaction. PsychAdapter is a novel method to\nintroduce psychological behavior patterns into language models at the\nfoundation level, independent of prompting, by influencing every transformer\nlayer. This approach can create chatbots with specific personality profiles,\nclinical training tools that mirror language associated with psychological\nconditionals, and machine translations that match an authors reading or\neducation level without taking up LLM context windows. PsychAdapter also allows\nfor the exploration psychological constructs through natural language\nexpression, extending the natural language processing toolkit to study human\npsychology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence-based language generators are now a part of most\npeople's lives. However, by default, they tend to generate \"average\" language\nwithout reflecting the ways in which people differ. Here, we propose a\nlightweight modification to the standard language model transformer\narchitecture - \"PsychAdapter\" - that uses empirically derived trait-language\npatterns to generate natural language for specified personality, demographic,\nand mental health characteristics (with or without prompting). We applied\nPsychAdapters to modify OpenAI's GPT-2, Google's Gemma, and Meta's Llama 3 and\nfound generated text to reflect the desired traits. For example, expert raters\nevaluated PsychAdapter's generated text output and found it matched intended\ntrait levels with 87.3% average accuracy for Big Five personalities, and 96.7%\nfor depression and life satisfaction. PsychAdapter is a novel method to\nintroduce psychological behavior patterns into language models at the\nfoundation level, independent of prompting, by influencing every transformer\nlayer. This approach can create chatbots with specific personality profiles,\nclinical training tools that mirror language associated with psychological\nconditionals, and machine translations that match an authors reading or\neducation level without taking up LLM context windows. PsychAdapter also allows\nfor the exploration psychological constructs through natural language\nexpression, extending the natural language processing toolkit to study human\npsychology."
                },
                "authors": [
                    {
                        "name": "Huy Vu"
                    },
                    {
                        "name": "Huy Anh Nguyen"
                    },
                    {
                        "name": "Adithya V Ganesan"
                    },
                    {
                        "name": "Swanie Juhng"
                    },
                    {
                        "name": "Oscar N. E. Kjell"
                    },
                    {
                        "name": "Joao Sedoc"
                    },
                    {
                        "name": "Margaret L. Kern"
                    },
                    {
                        "name": "Ryan L. Boyd"
                    },
                    {
                        "name": "Lyle Ungar"
                    },
                    {
                        "name": "H. Andrew Schwartz"
                    },
                    {
                        "name": "Johannes C. Eichstaedt"
                    }
                ],
                "author_detail": {
                    "name": "Johannes C. Eichstaedt"
                },
                "author": "Johannes C. Eichstaedt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19352v2",
                "updated": "2025-01-01T00:03:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    0,
                    3,
                    24,
                    2,
                    1,
                    0
                ],
                "published": "2024-11-28T19:53:39Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    19,
                    53,
                    39,
                    3,
                    333,
                    0
                ],
                "title": "OMuleT: Orchestrating Multiple Tools for Practicable Conversational\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OMuleT: Orchestrating Multiple Tools for Practicable Conversational\n  Recommendation"
                },
                "summary": "In this paper, we present a systematic effort to design, evaluate, and\nimplement a realistic conversational recommender system (CRS). The objective of\nour system is to allow users to input free-form text to request\nrecommendations, and then receive a list of relevant and diverse items. While\nprevious work on synthetic queries augments large language models (LLMs) with\n1-3 tools, we argue that a more extensive toolbox is necessary to effectively\nhandle real user requests. As such, we propose a novel approach that equips\nLLMs with over 10 tools, providing them access to the internal knowledge base\nand API calls used in production. We evaluate our model on a dataset of real\nusers and show that it generates relevant, novel, and diverse recommendations\ncompared to vanilla LLMs. Furthermore, we conduct ablation studies to\ndemonstrate the effectiveness of using the full range of tools in our toolbox.\nWe share our designs and lessons learned from deploying the system for internal\nalpha release. Our contribution is the addressing of all four key aspects of a\npracticable CRS: (1) real user requests, (2) augmenting LLMs with a wide\nvariety of tools, (3) extensive evaluation, and (4) deployment insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a systematic effort to design, evaluate, and\nimplement a realistic conversational recommender system (CRS). The objective of\nour system is to allow users to input free-form text to request\nrecommendations, and then receive a list of relevant and diverse items. While\nprevious work on synthetic queries augments large language models (LLMs) with\n1-3 tools, we argue that a more extensive toolbox is necessary to effectively\nhandle real user requests. As such, we propose a novel approach that equips\nLLMs with over 10 tools, providing them access to the internal knowledge base\nand API calls used in production. We evaluate our model on a dataset of real\nusers and show that it generates relevant, novel, and diverse recommendations\ncompared to vanilla LLMs. Furthermore, we conduct ablation studies to\ndemonstrate the effectiveness of using the full range of tools in our toolbox.\nWe share our designs and lessons learned from deploying the system for internal\nalpha release. Our contribution is the addressing of all four key aspects of a\npracticable CRS: (1) real user requests, (2) augmenting LLMs with a wide\nvariety of tools, (3) extensive evaluation, and (4) deployment insights."
                },
                "authors": [
                    {
                        "name": "Se-eun Yoon"
                    },
                    {
                        "name": "Xiaokai Wei"
                    },
                    {
                        "name": "Yexi Jiang"
                    },
                    {
                        "name": "Rachit Pareek"
                    },
                    {
                        "name": "Frank Ong"
                    },
                    {
                        "name": "Kevin Gao"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Michelle Gong"
                    }
                ],
                "author_detail": {
                    "name": "Michelle Gong"
                },
                "author": "Michelle Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10277v2",
                "updated": "2024-12-31T20:02:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    2,
                    33,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T13:39:05Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    39,
                    5,
                    0,
                    260,
                    0
                ],
                "title": "Cognitive Kernel: An Open-source Agent System towards Generalist\n  Autopilots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Kernel: An Open-source Agent System towards Generalist\n  Autopilots"
                },
                "summary": "We introduce Cognitive Kernel, an open-source agent system towards the goal\nof generalist autopilots. Unlike copilot systems, which primarily rely on users\nto provide essential state information (e.g., task descriptions) and assist\nusers by answering questions or auto-completing contents, autopilot systems\nmust complete tasks from start to finish independently, which requires the\nsystem to acquire the state information from the environments actively. To\nachieve this, an autopilot system should be capable of understanding user\nintents, actively gathering necessary information from various real-world\nsources, and making wise decisions. Cognitive Kernel adopts a model-centric\ndesign. In our implementation, the central policy model (a fine-tuned LLM)\ninitiates interactions with the environment using a combination of atomic\nactions, such as opening files, clicking buttons, saving intermediate results\nto memory, or calling the LLM itself. This differs from the widely used\nenvironment-centric design, where a task-specific environment with predefined\nactions is fixed, and the policy model is limited to selecting the correct\naction from a given set of options. Our design facilitates seamless information\nflow across various sources and provides greater flexibility. We evaluate our\nsystem in three use cases: real-time information management, private\ninformation management, and long-term memory management. The results\ndemonstrate that Cognitive Kernel achieves better or comparable performance to\nother closed-source systems in these scenarios. Cognitive Kernel is fully\ndockerized, ensuring everyone can deploy it privately and securely. We\nopen-source the system and the backbone model to encourage further research on\nLLM-driven autopilot systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Cognitive Kernel, an open-source agent system towards the goal\nof generalist autopilots. Unlike copilot systems, which primarily rely on users\nto provide essential state information (e.g., task descriptions) and assist\nusers by answering questions or auto-completing contents, autopilot systems\nmust complete tasks from start to finish independently, which requires the\nsystem to acquire the state information from the environments actively. To\nachieve this, an autopilot system should be capable of understanding user\nintents, actively gathering necessary information from various real-world\nsources, and making wise decisions. Cognitive Kernel adopts a model-centric\ndesign. In our implementation, the central policy model (a fine-tuned LLM)\ninitiates interactions with the environment using a combination of atomic\nactions, such as opening files, clicking buttons, saving intermediate results\nto memory, or calling the LLM itself. This differs from the widely used\nenvironment-centric design, where a task-specific environment with predefined\nactions is fixed, and the policy model is limited to selecting the correct\naction from a given set of options. Our design facilitates seamless information\nflow across various sources and provides greater flexibility. We evaluate our\nsystem in three use cases: real-time information management, private\ninformation management, and long-term memory management. The results\ndemonstrate that Cognitive Kernel achieves better or comparable performance to\nother closed-source systems in these scenarios. Cognitive Kernel is fully\ndockerized, ensuring everyone can deploy it privately and securely. We\nopen-source the system and the backbone model to encourage further research on\nLLM-driven autopilot systems."
                },
                "authors": [
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Xiaoman Pan"
                    },
                    {
                        "name": "Hongwei Wang"
                    },
                    {
                        "name": "Kaixin Ma"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22392v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22392v3",
                "updated": "2024-12-31T18:32:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    18,
                    32,
                    26,
                    1,
                    366,
                    0
                ],
                "published": "2024-10-29T17:56:05Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    56,
                    5,
                    1,
                    303,
                    0
                ],
                "title": "Enhanced Histopathology Image Feature Extraction using EfficientNet with\n  Dual Attention Mechanisms and CLAHE Preprocessing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Histopathology Image Feature Extraction using EfficientNet with\n  Dual Attention Mechanisms and CLAHE Preprocessing"
                },
                "summary": "Breast cancer diagnosis relies heavily on histopathology image\nclassification. This study proposes a novel approach leveraging Hybrid\nEfficientNet models integrated with advanced attention mechanisms (CB and\ndeformable attention) to enhance feature extraction and focus on relevant\ntissue regions. Evaluating on the BreakHis dataset across multiple\nmagnification scales (40X, 100X, 200X, 400X), we achieve state-of-the-art\nperformance with EfficientNetV2-XL and CB, reaching 98.96% accuracy and 98.31%\nF1-score at 400X. Integration of CLAHE preprocessing and optimized\ncomputational efficiency demonstrates suitability for real-time clinical\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breast cancer diagnosis relies heavily on histopathology image\nclassification. This study proposes a novel approach leveraging Hybrid\nEfficientNet models integrated with advanced attention mechanisms (CB and\ndeformable attention) to enhance feature extraction and focus on relevant\ntissue regions. Evaluating on the BreakHis dataset across multiple\nmagnification scales (40X, 100X, 200X, 400X), we achieve state-of-the-art\nperformance with EfficientNetV2-XL and CB, reaching 98.96% accuracy and 98.31%\nF1-score at 400X. Integration of CLAHE preprocessing and optimized\ncomputational efficiency demonstrates suitability for real-time clinical\ndeployment."
                },
                "authors": [
                    {
                        "name": "Naren Sengodan"
                    }
                ],
                "author_detail": {
                    "name": "Naren Sengodan"
                },
                "author": "Naren Sengodan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22392v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10794v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10794v2",
                "updated": "2024-12-31T17:22:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    17,
                    22,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-11-16T13:04:52Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    4,
                    52,
                    5,
                    321,
                    0
                ],
                "title": "Going Beyond Conventional OOD Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Going Beyond Conventional OOD Detection"
                },
                "summary": "Out-of-distribution (OOD) detection is critical to ensure the safe deployment\nof deep learning models in critical applications. Deep learning models can\noften misidentify OOD samples as in-distribution (ID) samples. This\nvulnerability worsens in the presence of spurious correlation in the training\nset. Likewise, in fine-grained classification settings, detection of\nfine-grained OOD samples becomes inherently challenging due to their high\nsimilarity to ID samples. However, current research on OOD detection has\nlargely ignored these challenging scenarios, focusing instead on relatively\neasier (conventional) cases. In this work, we present a unified Approach to\nSpurious, fine-grained, and Conventional OOD Detection (ASCOOD). First, we\npropose synthesizing virtual outliers from ID data by approximating the\ndestruction of invariant features. We identify invariant features with the\npixel attribution method using the model being learned. This approach\neliminates the burden of curating external OOD datasets. Then, we\nsimultaneously incentivize ID classification and predictive uncertainty towards\nthe virtual outliers leveraging standardized feature representation. Our\napproach effectively mitigates the impact of spurious correlations and\nencourages capturing fine-grained attributes. Extensive experiments across six\ndatasets demonstrate the merit of ASCOOD in spurious, fine-grained, and\nconventional settings. The code is available at:\nhttps://github.com/sudarshanregmi/ASCOOD/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection is critical to ensure the safe deployment\nof deep learning models in critical applications. Deep learning models can\noften misidentify OOD samples as in-distribution (ID) samples. This\nvulnerability worsens in the presence of spurious correlation in the training\nset. Likewise, in fine-grained classification settings, detection of\nfine-grained OOD samples becomes inherently challenging due to their high\nsimilarity to ID samples. However, current research on OOD detection has\nlargely ignored these challenging scenarios, focusing instead on relatively\neasier (conventional) cases. In this work, we present a unified Approach to\nSpurious, fine-grained, and Conventional OOD Detection (ASCOOD). First, we\npropose synthesizing virtual outliers from ID data by approximating the\ndestruction of invariant features. We identify invariant features with the\npixel attribution method using the model being learned. This approach\neliminates the burden of curating external OOD datasets. Then, we\nsimultaneously incentivize ID classification and predictive uncertainty towards\nthe virtual outliers leveraging standardized feature representation. Our\napproach effectively mitigates the impact of spurious correlations and\nencourages capturing fine-grained attributes. Extensive experiments across six\ndatasets demonstrate the merit of ASCOOD in spurious, fine-grained, and\nconventional settings. The code is available at:\nhttps://github.com/sudarshanregmi/ASCOOD/"
                },
                "authors": [
                    {
                        "name": "Sudarshan Regmi"
                    }
                ],
                "author_detail": {
                    "name": "Sudarshan Regmi"
                },
                "author": "Sudarshan Regmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10794v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10794v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18899v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18899v2",
                "updated": "2024-12-31T17:00:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    17,
                    0,
                    33,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-25T13:20:10Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    13,
                    20,
                    10,
                    2,
                    360,
                    0
                ],
                "title": "GAI: Generative Agents for Innovation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAI: Generative Agents for Innovation"
                },
                "summary": "This study examines whether collective reasoning among generative agents can\nfacilitate novel and coherent thinking that leads to innovation. To achieve\nthis, it proposes GAI, a new LLM-empowered framework designed for reflection\nand interaction among multiple generative agents to replicate the process of\ninnovation. The core of the GAI framework lies in an architecture that\ndynamically processes the internal states of agents and a dialogue scheme\nspecifically tailored to facilitate analogy-driven innovation. The framework's\nfunctionality is evaluated using Dyson's invention of the bladeless fan as a\ncase study, assessing the extent to which the core ideas of the innovation can\nbe replicated through a set of fictional technical documents. The experimental\nresults demonstrate that models with internal states significantly outperformed\nthose without, achieving higher average scores and lower variance. Notably, the\nmodel with five heterogeneous agents equipped with internal states successfully\nreplicated the key ideas underlying the Dyson's invention. This indicates that\nthe internal state enables agents to refine their ideas, resulting in the\nconstruction and sharing of more coherent and comprehensive concepts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines whether collective reasoning among generative agents can\nfacilitate novel and coherent thinking that leads to innovation. To achieve\nthis, it proposes GAI, a new LLM-empowered framework designed for reflection\nand interaction among multiple generative agents to replicate the process of\ninnovation. The core of the GAI framework lies in an architecture that\ndynamically processes the internal states of agents and a dialogue scheme\nspecifically tailored to facilitate analogy-driven innovation. The framework's\nfunctionality is evaluated using Dyson's invention of the bladeless fan as a\ncase study, assessing the extent to which the core ideas of the innovation can\nbe replicated through a set of fictional technical documents. The experimental\nresults demonstrate that models with internal states significantly outperformed\nthose without, achieving higher average scores and lower variance. Notably, the\nmodel with five heterogeneous agents equipped with internal states successfully\nreplicated the key ideas underlying the Dyson's invention. This indicates that\nthe internal state enables agents to refine their ideas, resulting in the\nconstruction and sharing of more coherent and comprehensive concepts."
                },
                "authors": [
                    {
                        "name": "Masahiro Sato"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Sato"
                },
                "author": "Masahiro Sato",
                "arxiv_comment": "Added an Appendix section",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18899v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16928v2",
                "updated": "2024-12-31T16:29:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    16,
                    29,
                    28,
                    1,
                    366,
                    0
                ],
                "published": "2024-07-24T01:33:57Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    1,
                    33,
                    57,
                    2,
                    206,
                    0
                ],
                "title": "From Sands to Mansions: Simulating Full Attack Chain with LLM-Organized\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Sands to Mansions: Simulating Full Attack Chain with LLM-Organized\n  Knowledge"
                },
                "summary": "Adversarial dynamics are intrinsic to the nature of offense and defense in\ncyberspace, with both attackers and defenders continuously evolving their\ntechnologies. Given the wide array of security products available, users often\nface challenges in selecting the most effective solutions. Furthermore,\ntraditional benchmarks based on single-point attacks are increasingly\ninadequate, failing to accurately reflect the full range of attacker\ncapabilities and falling short in properly evaluating the effectiveness of\ndefense products. Automated multi-stage attack simulations offer a promising\napproach to enhance system evaluation efficiency and aid in analyzing the\neffectiveness of detection systems. However, simulating a full attack chain is\ncomplex and requires significant time and expertise from security\nprofessionals, facing several challenges, including limited coverage of attack\ntechniques, a high level of required expertise, and a lack of execution detail.\nIn this paper, we model automatic attack simulation as a planning problem. By\nusing the Planning Domain Definition Language (PDDL) to formally describe the\nattack simulation problem, and combining domain knowledge of both the problem\nand the domain space, we enable the planning of attack paths through\nstandardized, domain-independent planning algorithms. We explore the potential\nof Large Language Models (LLMs) to summarize and analyze knowledge from\nexisting attack documentation and reports, facilitating automated attack\nplanning. We introduce Aurora, a system that autonomously simulates full attack\nchains based on external attack tools and threat intelligence reports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial dynamics are intrinsic to the nature of offense and defense in\ncyberspace, with both attackers and defenders continuously evolving their\ntechnologies. Given the wide array of security products available, users often\nface challenges in selecting the most effective solutions. Furthermore,\ntraditional benchmarks based on single-point attacks are increasingly\ninadequate, failing to accurately reflect the full range of attacker\ncapabilities and falling short in properly evaluating the effectiveness of\ndefense products. Automated multi-stage attack simulations offer a promising\napproach to enhance system evaluation efficiency and aid in analyzing the\neffectiveness of detection systems. However, simulating a full attack chain is\ncomplex and requires significant time and expertise from security\nprofessionals, facing several challenges, including limited coverage of attack\ntechniques, a high level of required expertise, and a lack of execution detail.\nIn this paper, we model automatic attack simulation as a planning problem. By\nusing the Planning Domain Definition Language (PDDL) to formally describe the\nattack simulation problem, and combining domain knowledge of both the problem\nand the domain space, we enable the planning of attack paths through\nstandardized, domain-independent planning algorithms. We explore the potential\nof Large Language Models (LLMs) to summarize and analyze knowledge from\nexisting attack documentation and reports, facilitating automated attack\nplanning. We introduce Aurora, a system that autonomously simulates full attack\nchains based on external attack tools and threat intelligence reports."
                },
                "authors": [
                    {
                        "name": "Lingzhi Wang"
                    },
                    {
                        "name": "Zhenyuan Li"
                    },
                    {
                        "name": "Zonghan Guo"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Kyle Jung"
                    },
                    {
                        "name": "Kedar Thiagarajan"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zhengkai Wang"
                    },
                    {
                        "name": "Emily Wei"
                    },
                    {
                        "name": "Xiangmin Shen"
                    },
                    {
                        "name": "Yan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yan Chen"
                },
                "author": "Yan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18205v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18205v3",
                "updated": "2024-12-31T16:14:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    16,
                    14,
                    51,
                    1,
                    366,
                    0
                ],
                "published": "2024-02-28T09:51:55Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    9,
                    51,
                    55,
                    2,
                    59,
                    0
                ],
                "title": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging"
                },
                "summary": "Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, These methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and Chain-of-Thought \\textbf{M}erging\n(Lemur). Specifically, to discard the tedious manual rules. We propose a novel\nsampling method inspired by information entropy, which efficiently clusters\ntypical logs. Furthermore, to enhance the merging of log templates, we design a\nchain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension, deftly distinguishing between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that Lemur achieves the\nstate-of-the-art performance and impressive efficiency. The Code is available\nat https://github.com/zwpride/lemur.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, These methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and Chain-of-Thought \\textbf{M}erging\n(Lemur). Specifically, to discard the tedious manual rules. We propose a novel\nsampling method inspired by information entropy, which efficiently clusters\ntypical logs. Furthermore, to enhance the merging of log templates, we design a\nchain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension, deftly distinguishing between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that Lemur achieves the\nstate-of-the-art performance and impressive efficiency. The Code is available\nat https://github.com/zwpride/lemur."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18205v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12469v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12469v3",
                "updated": "2024-12-31T15:52:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    15,
                    52,
                    47,
                    1,
                    366,
                    0
                ],
                "published": "2024-11-19T12:51:17Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    51,
                    17,
                    1,
                    324,
                    0
                ],
                "title": "AI Flow at the Network Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Flow at the Network Edge"
                },
                "summary": "Recent advancements in large language models (LLMs) and their multimodal\nvariants have led to remarkable progress across various domains, demonstrating\nimpressive capabilities and unprecedented potential. In the era of ubiquitous\nconnectivity, leveraging communication networks to distribute intelligence is a\ntransformative concept, envisioning AI-powered services accessible at the\nnetwork edge. However, pushing large models from the cloud to\nresource-constrained environments faces critical challenges. Model inference on\nlow-end devices leads to excessive latency and performance bottlenecks, while\nraw data transmission over limited bandwidth networks causes high communication\noverhead. This article presents AI Flow, a framework that streamlines the\ninference process by jointly leveraging the heterogeneous resources available\nacross devices, edge nodes, and cloud servers, making intelligence flow across\nnetworks. To facilitate cooperation among multiple computational nodes, the\nproposed framework explores a paradigm shift in the design of communication\nnetwork systems from transmitting information flow to intelligence flow, where\nthe goal of communications is task-oriented and folded into the inference\nprocess. Experimental results demonstrate the effectiveness of the proposed\nframework through an image captioning use case, showcasing the ability to\nreduce response latency while maintaining high-quality captions. This article\nserves as a position paper for identifying the motivation, challenges, and\nprinciples of AI Flow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) and their multimodal\nvariants have led to remarkable progress across various domains, demonstrating\nimpressive capabilities and unprecedented potential. In the era of ubiquitous\nconnectivity, leveraging communication networks to distribute intelligence is a\ntransformative concept, envisioning AI-powered services accessible at the\nnetwork edge. However, pushing large models from the cloud to\nresource-constrained environments faces critical challenges. Model inference on\nlow-end devices leads to excessive latency and performance bottlenecks, while\nraw data transmission over limited bandwidth networks causes high communication\noverhead. This article presents AI Flow, a framework that streamlines the\ninference process by jointly leveraging the heterogeneous resources available\nacross devices, edge nodes, and cloud servers, making intelligence flow across\nnetworks. To facilitate cooperation among multiple computational nodes, the\nproposed framework explores a paradigm shift in the design of communication\nnetwork systems from transmitting information flow to intelligence flow, where\nthe goal of communications is task-oriented and folded into the inference\nprocess. Experimental results demonstrate the effectiveness of the proposed\nframework through an image captioning use case, showcasing the ability to\nreduce response latency while maintaining high-quality captions. This article\nserves as a position paper for identifying the motivation, challenges, and\nprinciples of AI Flow."
                },
                "authors": [
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12469v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12469v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03338v2",
                "updated": "2024-12-31T14:57:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    14,
                    57,
                    11,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-04T14:13:38Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    13,
                    38,
                    2,
                    339,
                    0
                ],
                "title": "AI-Driven Day-to-Day Route Choice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Driven Day-to-Day Route Choice"
                },
                "summary": "Understanding travelers' route choices can help policymakers devise optimal\noperational and planning strategies for both normal and abnormal circumstances.\nHowever, existing choice modeling methods often rely on predefined assumptions\nand struggle to capture the dynamic and adaptive nature of travel behavior.\nRecently, Large Language Models (LLMs) have emerged as a promising alternative,\ndemonstrating remarkable ability to replicate human-like behaviors across\nvarious fields. Despite this potential, their capacity to accurately simulate\nhuman route choice behavior in transportation contexts remains doubtful. To\nsatisfy this curiosity, this paper investigates the potential of LLMs for route\nchoice modeling by introducing an LLM-empowered agent, \"LLMTraveler.\" This\nagent integrates an LLM as its core, equipped with a memory system that learns\nfrom past experiences and makes decisions by balancing retrieved data and\npersonality traits. The study systematically evaluates the LLMTraveler's\nability to replicate human-like decision-making through two stages of\nday-to-day (DTD) congestion games: (1) analyzing its route-switching behavior\nin single origin-destination (OD) pair scenarios, where it demonstrates\npatterns that align with laboratory data but cannot be fully explained by\ntraditional models, and (2) testing its capacity to model adaptive learning\nbehaviors in multi-OD scenarios on the Ortuzar and Willumsen (OW) network,\nproducing results comparable to Multinomial Logit (MNL) and Reinforcement\nLearning (RL) models. These experiments demonstrate that the framework can\npartially replicate human-like decision-making in route choice while providing\nnatural language explanations for its decisions. This capability offers\nvaluable insights for transportation policymaking, such as simulating traveler\nresponses to new policies or changes in the network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding travelers' route choices can help policymakers devise optimal\noperational and planning strategies for both normal and abnormal circumstances.\nHowever, existing choice modeling methods often rely on predefined assumptions\nand struggle to capture the dynamic and adaptive nature of travel behavior.\nRecently, Large Language Models (LLMs) have emerged as a promising alternative,\ndemonstrating remarkable ability to replicate human-like behaviors across\nvarious fields. Despite this potential, their capacity to accurately simulate\nhuman route choice behavior in transportation contexts remains doubtful. To\nsatisfy this curiosity, this paper investigates the potential of LLMs for route\nchoice modeling by introducing an LLM-empowered agent, \"LLMTraveler.\" This\nagent integrates an LLM as its core, equipped with a memory system that learns\nfrom past experiences and makes decisions by balancing retrieved data and\npersonality traits. The study systematically evaluates the LLMTraveler's\nability to replicate human-like decision-making through two stages of\nday-to-day (DTD) congestion games: (1) analyzing its route-switching behavior\nin single origin-destination (OD) pair scenarios, where it demonstrates\npatterns that align with laboratory data but cannot be fully explained by\ntraditional models, and (2) testing its capacity to model adaptive learning\nbehaviors in multi-OD scenarios on the Ortuzar and Willumsen (OW) network,\nproducing results comparable to Multinomial Logit (MNL) and Reinforcement\nLearning (RL) models. These experiments demonstrate that the framework can\npartially replicate human-like decision-making in route choice while providing\nnatural language explanations for its decisions. This capability offers\nvaluable insights for transportation policymaking, such as simulating traveler\nresponses to new policies or changes in the network."
                },
                "authors": [
                    {
                        "name": "Leizhen Wang"
                    },
                    {
                        "name": "Peibo Duan"
                    },
                    {
                        "name": "Zhengbing He"
                    },
                    {
                        "name": "Cheng Lyu"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Nan Zheng"
                    },
                    {
                        "name": "Li Yao"
                    },
                    {
                        "name": "Zhenliang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhenliang Ma"
                },
                "author": "Zhenliang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15453v2",
                "updated": "2024-12-31T09:13:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    13,
                    6,
                    1,
                    366,
                    0
                ],
                "published": "2024-05-24T11:30:37Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    11,
                    30,
                    37,
                    4,
                    145,
                    0
                ],
                "title": "Benchmarking the Performance of Pre-trained LLMs across Urdu NLP Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking the Performance of Pre-trained LLMs across Urdu NLP Tasks"
                },
                "summary": "Large Language Models (LLMs) pre-trained on multilingual data have\nrevolutionized natural language processing research, by transitioning from\nlanguages and task specific model pipelines to a single model adapted on a\nvariety of tasks. However majority of existing multilingual NLP benchmarks for\nLLMs provide evaluation data in only few languages with little linguistic\ndiversity. In addition these benchmarks lack quality assessment against the\nrespective state-of the art models. This study presents an in-depth examination\nof 7 prominent LLMs: GPT-3.5-turbo, Llama 2-7B-Chat, Llama 3.1-8B, Bloomz 3B,\nBloomz 7B1, Ministral-8B and Whisper (Large, medium and small variant) across\n17 tasks using 22 datasets, 13.8 hours of speech, in a zero-shot setting, and\ntheir performance against state-of-the-art (SOTA) models, has been compared and\nanalyzed. Our experiments show that SOTA models currently outperform\nencoder-decoder models in majority of Urdu NLP tasks under zero-shot settings.\nHowever, comparing Llama 3.1-8B over prior version Llama 2-7B-Chat, we can\ndeduce that with improved language coverage, LLMs can surpass these SOTA\nmodels. Our results emphasize that models with fewer parameters but richer\nlanguage-specific data, like Llama 3.1-8B, often outperform larger models with\nlower language diversity, such as GPT-3.5, in several tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) pre-trained on multilingual data have\nrevolutionized natural language processing research, by transitioning from\nlanguages and task specific model pipelines to a single model adapted on a\nvariety of tasks. However majority of existing multilingual NLP benchmarks for\nLLMs provide evaluation data in only few languages with little linguistic\ndiversity. In addition these benchmarks lack quality assessment against the\nrespective state-of the art models. This study presents an in-depth examination\nof 7 prominent LLMs: GPT-3.5-turbo, Llama 2-7B-Chat, Llama 3.1-8B, Bloomz 3B,\nBloomz 7B1, Ministral-8B and Whisper (Large, medium and small variant) across\n17 tasks using 22 datasets, 13.8 hours of speech, in a zero-shot setting, and\ntheir performance against state-of-the-art (SOTA) models, has been compared and\nanalyzed. Our experiments show that SOTA models currently outperform\nencoder-decoder models in majority of Urdu NLP tasks under zero-shot settings.\nHowever, comparing Llama 3.1-8B over prior version Llama 2-7B-Chat, we can\ndeduce that with improved language coverage, LLMs can surpass these SOTA\nmodels. Our results emphasize that models with fewer parameters but richer\nlanguage-specific data, like Llama 3.1-8B, often outperform larger models with\nlower language diversity, such as GPT-3.5, in several tasks."
                },
                "authors": [
                    {
                        "name": "Munief Hassan Tahir"
                    },
                    {
                        "name": "Sana Shams"
                    },
                    {
                        "name": "Layba Fiaz"
                    },
                    {
                        "name": "Farah Adeeba"
                    },
                    {
                        "name": "Sarmad Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Sarmad Hussain"
                },
                "author": "Sarmad Hussain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.18634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.18634v2",
                "updated": "2024-12-31T08:55:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    8,
                    55,
                    29,
                    1,
                    366,
                    0
                ],
                "published": "2023-10-28T08:29:49Z",
                "published_parsed": [
                    2023,
                    10,
                    28,
                    8,
                    29,
                    49,
                    5,
                    301,
                    0
                ],
                "title": "SSL Framework for Causal Inconsistency between Structures and\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSL Framework for Causal Inconsistency between Structures and\n  Representations"
                },
                "summary": "The cross-pollination between causal discovery and deep learning has led to\nincreasingly extensive interactions. It results in a large number of deep\nlearning data types (such as images, text, etc.) extending into the field of\ncausal discovery, and a multitude of deep learning tasks have begun to utilize\ncausal discovery to explore the internal causal structure and causal\nrepresentation of data. In this paper, we first identified that a complex data\ntype, ``Indefinite Data\", has conflicts between causal relationships expressed\nby the causal structure and causal representation generated by deep learning\nmodels, a phenomenon referred to as causal inconsistency. We thoroughly\nanalyzed related work to explain why only Indefinite Data exhibits causal\ninconsistency while other data types do not. Furthermore, to alleviate causal\ninconsistency, we proposed a self-supervised learning (SSL) framework based on\nintervention, hoping to provide more causal information from different\nintervention views to promote consistency between structure and representation.\nExtensive experiments have shown that the SSL framework enhances causal\nconsistency and can further improve causal structure and representation\nlearning performance. Additionally, we extended the SSL framework to three\ndifferent downstream tasks and LLM instructions. The quantitative results of\nthese applications all reflect the performance improvement brought about by\ncausal consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cross-pollination between causal discovery and deep learning has led to\nincreasingly extensive interactions. It results in a large number of deep\nlearning data types (such as images, text, etc.) extending into the field of\ncausal discovery, and a multitude of deep learning tasks have begun to utilize\ncausal discovery to explore the internal causal structure and causal\nrepresentation of data. In this paper, we first identified that a complex data\ntype, ``Indefinite Data\", has conflicts between causal relationships expressed\nby the causal structure and causal representation generated by deep learning\nmodels, a phenomenon referred to as causal inconsistency. We thoroughly\nanalyzed related work to explain why only Indefinite Data exhibits causal\ninconsistency while other data types do not. Furthermore, to alleviate causal\ninconsistency, we proposed a self-supervised learning (SSL) framework based on\nintervention, hoping to provide more causal information from different\nintervention views to promote consistency between structure and representation.\nExtensive experiments have shown that the SSL framework enhances causal\nconsistency and can further improve causal structure and representation\nlearning performance. Additionally, we extended the SSL framework to three\ndifferent downstream tasks and LLM instructions. The quantitative results of\nthese applications all reflect the performance improvement brought about by\ncausal consistency."
                },
                "authors": [
                    {
                        "name": "Hang Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Keqing Du"
                    },
                    {
                        "name": "Wenya Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenya Wang"
                },
                "author": "Wenya Wang",
                "arxiv_comment": "29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.18634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.18634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21199v2",
                "updated": "2024-12-31T08:20:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    8,
                    20,
                    42,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T18:58:58Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    58,
                    58,
                    0,
                    365,
                    0
                ],
                "title": "HumanEval Pro and MBPP Pro: Evaluating Large Language Models on\n  Self-invoking Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumanEval Pro and MBPP Pro: Evaluating Large Language Models on\n  Self-invoking Code Generation"
                },
                "summary": "We introduce self-invoking code generation, a new task designed to evaluate\nthe progressive reasoning and problem-solving capabilities of LLMs. In this\ntask, models are presented with a base problem and a related, more complex\nproblem. They must solve the base problem and then utilize its solution to\naddress the more complex one. This work features three key contributions.\nFirst, we propose a general recipe for generating more challenging versions of\nexisting benchmarks, resulting in three new benchmarks: HumanEval Pro, MBPP\nPro, and BigCodeBench-Lite Pro, specifically designed to assess LLMs on\nself-invoking code generation. Second, from the analysis of experimental\nresults over twenty LLMs on our benchmarks, we have two important observations:\n(i) Most LLMs excel in traditional code generation benchmarks like HumanEval\nand MBPP, but their performance declines on self-invoking tasks. For example,\no1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro.\n(ii) On self-invoking code generation task, the instruction-tuned models\ndemonstrate only marginal improvements compared to the base models. Third, we\ndisclose the types of failure modes that exist in our evaluation results. All\nthese results underscore the need for further advancements in self-invoking\ncode generation tasks and provide a new direction for future research on\nenhancing LLMs' code reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce self-invoking code generation, a new task designed to evaluate\nthe progressive reasoning and problem-solving capabilities of LLMs. In this\ntask, models are presented with a base problem and a related, more complex\nproblem. They must solve the base problem and then utilize its solution to\naddress the more complex one. This work features three key contributions.\nFirst, we propose a general recipe for generating more challenging versions of\nexisting benchmarks, resulting in three new benchmarks: HumanEval Pro, MBPP\nPro, and BigCodeBench-Lite Pro, specifically designed to assess LLMs on\nself-invoking code generation. Second, from the analysis of experimental\nresults over twenty LLMs on our benchmarks, we have two important observations:\n(i) Most LLMs excel in traditional code generation benchmarks like HumanEval\nand MBPP, but their performance declines on self-invoking tasks. For example,\no1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro.\n(ii) On self-invoking code generation task, the instruction-tuned models\ndemonstrate only marginal improvements compared to the base models. Third, we\ndisclose the types of failure modes that exist in our evaluation results. All\nthese results underscore the need for further advancements in self-invoking\ncode generation tasks and provide a new direction for future research on\nenhancing LLMs' code reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Zhaojian Yu"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Arman Cohan"
                    },
                    {
                        "name": "Xiao-Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ping Zhang"
                },
                "author": "Xiao-Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15371v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15371v6",
                "updated": "2024-12-31T08:08:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    8,
                    8,
                    20,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-19T10:26:42Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    10,
                    26,
                    42,
                    3,
                    263,
                    0
                ],
                "title": "DiSHA: Dimension-Sharding Adaptation with Fast Convergence and Fast\n  Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiSHA: Dimension-Sharding Adaptation with Fast Convergence and Fast\n  Computation"
                },
                "summary": "Low-Rank Adaptation (LoRA) leverages the low intrinsic rank of weight updates\nin Large Language Models (LLMs), establishing a Parameter-Efficient Fine-Tuning\n(PEFT) paradigm. However, LoRA suffers from slow convergence. We introduce\nDimension-Sharding Adaptation (DiSHA), which expands the PEFT design space to\nunlock lower intrinsic ranks and faster convergence by default. Within DiSHA's\ndesign space, we propose Block Affine Adaptation (Bone), a computationally\nefficient structure that delivers both high performance and efficiency. While\ncertain DiSHA configurations may result in colinear updates to weight shards,\nwe address this with Block Affine Transformation Adaptation (BAT), a nonlinear\nvariant of DiSHA. BAT introduces nonlinearity by combining trainable matrices\nwith original weight shards in a nonlinear manner, inducing nonlinearity in\nmatrix updates without introducing additional parameters. Empirical results\nshow that Bone, under the DiSHA framework, consistently outperforms LoRA\nvariants in both NLG and NLU tasks, with significantly improved computational\nefficiency. Further analysis demonstrates that BAT enhances model capabilities\nby leveraging its nonlinear design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) leverages the low intrinsic rank of weight updates\nin Large Language Models (LLMs), establishing a Parameter-Efficient Fine-Tuning\n(PEFT) paradigm. However, LoRA suffers from slow convergence. We introduce\nDimension-Sharding Adaptation (DiSHA), which expands the PEFT design space to\nunlock lower intrinsic ranks and faster convergence by default. Within DiSHA's\ndesign space, we propose Block Affine Adaptation (Bone), a computationally\nefficient structure that delivers both high performance and efficiency. While\ncertain DiSHA configurations may result in colinear updates to weight shards,\nwe address this with Block Affine Transformation Adaptation (BAT), a nonlinear\nvariant of DiSHA. BAT introduces nonlinearity by combining trainable matrices\nwith original weight shards in a nonlinear manner, inducing nonlinearity in\nmatrix updates without introducing additional parameters. Empirical results\nshow that Bone, under the DiSHA framework, consistently outperforms LoRA\nvariants in both NLG and NLU tasks, with significantly improved computational\nefficiency. Further analysis demonstrates that BAT enhances model capabilities\nby leveraging its nonlinear design."
                },
                "authors": [
                    {
                        "name": "Jiale Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jiale Kang"
                },
                "author": "Jiale Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15371v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15371v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16526v2",
                "updated": "2024-12-31T07:56:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    56,
                    59,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-21T08:09:12Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    8,
                    9,
                    12,
                    5,
                    356,
                    0
                ],
                "title": "Text2midi: Generating Symbolic Music from Captions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2midi: Generating Symbolic Music from Captions"
                },
                "summary": "This paper introduces text2midi, an end-to-end model to generate MIDI files\nfrom textual descriptions. Leveraging the growing popularity of multimodal\ngenerative approaches, text2midi capitalizes on the extensive availability of\ntextual data and the success of large language models (LLMs). Our end-to-end\nsystem harnesses the power of LLMs to generate symbolic music in the form of\nMIDI files. Specifically, we utilize a pretrained LLM encoder to process\ncaptions, which then condition an autoregressive transformer decoder to produce\nMIDI sequences that accurately reflect the provided descriptions. This\nintuitive and user-friendly method significantly streamlines the music creation\nprocess by allowing users to generate music pieces using text prompts. We\nconduct comprehensive empirical evaluations, incorporating both automated and\nhuman studies, that show our model generates MIDI files of high quality that\nare indeed controllable by text captions that may include music theory terms\nsuch as chords, keys, and tempo. We release the code and music samples on our\ndemo page (https://github.com/AMAAI-Lab/Text2midi) for users to interact with\ntext2midi.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces text2midi, an end-to-end model to generate MIDI files\nfrom textual descriptions. Leveraging the growing popularity of multimodal\ngenerative approaches, text2midi capitalizes on the extensive availability of\ntextual data and the success of large language models (LLMs). Our end-to-end\nsystem harnesses the power of LLMs to generate symbolic music in the form of\nMIDI files. Specifically, we utilize a pretrained LLM encoder to process\ncaptions, which then condition an autoregressive transformer decoder to produce\nMIDI sequences that accurately reflect the provided descriptions. This\nintuitive and user-friendly method significantly streamlines the music creation\nprocess by allowing users to generate music pieces using text prompts. We\nconduct comprehensive empirical evaluations, incorporating both automated and\nhuman studies, that show our model generates MIDI files of high quality that\nare indeed controllable by text captions that may include music theory terms\nsuch as chords, keys, and tempo. We release the code and music samples on our\ndemo page (https://github.com/AMAAI-Lab/Text2midi) for users to interact with\ntext2midi."
                },
                "authors": [
                    {
                        "name": "Keshav Bhandari"
                    },
                    {
                        "name": "Abhinaba Roy"
                    },
                    {
                        "name": "Kyra Wang"
                    },
                    {
                        "name": "Geeta Puri"
                    },
                    {
                        "name": "Simon Colton"
                    },
                    {
                        "name": "Dorien Herremans"
                    }
                ],
                "author_detail": {
                    "name": "Dorien Herremans"
                },
                "author": "Dorien Herremans",
                "arxiv_comment": "9 pages, 3 figures, Accepted at the 39th AAAI Conference on\n  Artificial Intelligence (AAAI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.12871v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.12871v9",
                "updated": "2024-12-31T07:56:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    56,
                    13,
                    1,
                    366,
                    0
                ],
                "published": "2023-09-22T13:52:42Z",
                "published_parsed": [
                    2023,
                    9,
                    22,
                    13,
                    52,
                    42,
                    4,
                    265,
                    0
                ],
                "title": "AnglE-optimized Text Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnglE-optimized Text Embeddings"
                },
                "summary": "High-quality text embedding is pivotal in improving semantic textual\nsimilarity (STS) tasks, which are crucial components in Large Language Model\n(LLM) applications. However, a common challenge existing text embedding models\nface is the problem of vanishing gradients, primarily due to their reliance on\nthe cosine function in the optimization objective, which has saturation zones.\nTo address this issue, this paper proposes a novel angle-optimized text\nembedding model called AnglE. The core idea of AnglE is to introduce angle\noptimization in a complex space. This novel approach effectively mitigates the\nadverse effects of the saturation zone in the cosine function, which can impede\ngradient and hinder optimization processes. To set up a comprehensive STS\nevaluation, we experimented on existing short-text STS datasets and a newly\ncollected long-text STS dataset from GitHub Issues. Furthermore, we examine\ndomain-specific STS scenarios with limited labeled data and explore how AnglE\nworks with LLM-annotated data. Extensive experiments were conducted on various\ntasks including short-text STS, long-text STS, and domain-specific STS tasks.\nThe results show that AnglE outperforms the state-of-the-art (SOTA) STS models\nthat ignore the cosine saturation zone. These findings demonstrate the ability\nof AnglE to generate high-quality text embeddings and the usefulness of angle\noptimization in STS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality text embedding is pivotal in improving semantic textual\nsimilarity (STS) tasks, which are crucial components in Large Language Model\n(LLM) applications. However, a common challenge existing text embedding models\nface is the problem of vanishing gradients, primarily due to their reliance on\nthe cosine function in the optimization objective, which has saturation zones.\nTo address this issue, this paper proposes a novel angle-optimized text\nembedding model called AnglE. The core idea of AnglE is to introduce angle\noptimization in a complex space. This novel approach effectively mitigates the\nadverse effects of the saturation zone in the cosine function, which can impede\ngradient and hinder optimization processes. To set up a comprehensive STS\nevaluation, we experimented on existing short-text STS datasets and a newly\ncollected long-text STS dataset from GitHub Issues. Furthermore, we examine\ndomain-specific STS scenarios with limited labeled data and explore how AnglE\nworks with LLM-annotated data. Extensive experiments were conducted on various\ntasks including short-text STS, long-text STS, and domain-specific STS tasks.\nThe results show that AnglE outperforms the state-of-the-art (SOTA) STS models\nthat ignore the cosine saturation zone. These findings demonstrate the ability\nof AnglE to generate high-quality text embeddings and the usefulness of angle\noptimization in STS."
                },
                "authors": [
                    {
                        "name": "Xianming Li"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li",
                "arxiv_comment": "Published at the Proceedings of ACL24. AoE: Angle-optimized\n  Embeddings for Semantic Textual Similarity\n  (https://aclanthology.org/2024.acl-long.101/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.12871v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.12871v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04139v2",
                "updated": "2024-12-31T07:04:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    4,
                    56,
                    1,
                    366,
                    0
                ],
                "published": "2024-10-05T12:27:47Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    12,
                    27,
                    47,
                    5,
                    279,
                    0
                ],
                "title": "From Reading to Compressing: Exploring the Multi-document Reader for\n  Prompt Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reading to Compressing: Exploring the Multi-document Reader for\n  Prompt Compression"
                },
                "summary": "Large language models (LLMs) have achieved significant performance gains\nusing advanced prompting techniques over various tasks. However, the increasing\nlength of prompts leads to high computational costs and often obscures crucial\ninformation. Prompt compression has been proposed to alleviate these issues,\nbut it faces challenges in (i) capturing the global context and (ii) training\nthe compressor effectively. To tackle these challenges, we introduce a novel\nprompt compression method, namely Reading To Compressing (R2C), utilizing the\nFusion-in-Decoder (FiD) architecture to identify the important information in\nthe prompt. Specifically, the cross-attention scores of the FiD are used to\ndiscern essential chunks and sentences from the prompt. R2C effectively\ncaptures the global context without compromising semantic consistency while\ndetouring the necessity of pseudo-labels for training the compressor. Empirical\nresults show that R2C retains key contexts, enhancing the LLM performance by 6%\nin out-of-domain evaluations while reducing the prompt length by 80%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant performance gains\nusing advanced prompting techniques over various tasks. However, the increasing\nlength of prompts leads to high computational costs and often obscures crucial\ninformation. Prompt compression has been proposed to alleviate these issues,\nbut it faces challenges in (i) capturing the global context and (ii) training\nthe compressor effectively. To tackle these challenges, we introduce a novel\nprompt compression method, namely Reading To Compressing (R2C), utilizing the\nFusion-in-Decoder (FiD) architecture to identify the important information in\nthe prompt. Specifically, the cross-attention scores of the FiD are used to\ndiscern essential chunks and sentences from the prompt. R2C effectively\ncaptures the global context without compromising semantic consistency while\ndetouring the necessity of pseudo-labels for training the compressor. Empirical\nresults show that R2C retains key contexts, enhancing the LLM performance by 6%\nin out-of-domain evaluations while reducing the prompt length by 80%."
                },
                "authors": [
                    {
                        "name": "Eunseong Choi"
                    },
                    {
                        "name": "Sunkyung Lee"
                    },
                    {
                        "name": "Minjin Choi"
                    },
                    {
                        "name": "June Park"
                    },
                    {
                        "name": "Jongwuk Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jongwuk Lee"
                },
                "author": "Jongwuk Lee",
                "arxiv_comment": "Findings of the Association for Computational Linguistics: EMNLP\n  2024; 21 pages; 10 figures and 7 tables. Code available at\n  https://github.com/eunseongc/R2C",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02762v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02762v3",
                "updated": "2024-12-31T06:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    6,
                    54,
                    19,
                    1,
                    366,
                    0
                ],
                "published": "2023-07-06T04:05:44Z",
                "published_parsed": [
                    2023,
                    7,
                    6,
                    4,
                    5,
                    44,
                    3,
                    187,
                    0
                ],
                "title": "PRD: Peer Rank and Discussion Improve Large Language Model based\n  Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRD: Peer Rank and Discussion Improve Large Language Model based\n  Evaluations"
                },
                "summary": "Nowadays, the quality of responses generated by different modern large\nlanguage models (LLMs) is hard to evaluate and compare automatically. Recent\nstudies suggest and predominantly use LLMs for reference-free evaluation of\nopen-ended question answering. More specifically, they use the recognized\n\"strongest\" LLM as the evaluator, which conducts pairwise comparisons of\ncandidate models' answers and provides a ranking score. However, this intuitive\nmethod has multiple problems, such as bringing in self-enhancement (favoring\nits own answers) and positional bias. We draw insights and lessons from the\neducational domain (Cho & MacArthur, 2011; Walsh, 2014) to improve LLM-based\nevaluations. Specifically, we propose (1) the peer rank (PR) algorithm that\ntakes into account each peer LLM's pairwise preferences of all answer pairs,\nand outputs a final ranking of models; and (2) peer discussion (PD), where we\nprompt two LLMs to discuss and try to reach a mutual agreement on the\npreferences of two answers. We conduct experiments on two benchmark datasets.\nWe find that our approaches achieve higher accuracy and align better with human\njudgments. Interestingly, PR can induce a relatively accurate self-ranking of\nmodels under the anonymous setting, where each model's name is unrevealed. Our\nwork provides space to explore evaluating models that are hard to compare for\nhumans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the quality of responses generated by different modern large\nlanguage models (LLMs) is hard to evaluate and compare automatically. Recent\nstudies suggest and predominantly use LLMs for reference-free evaluation of\nopen-ended question answering. More specifically, they use the recognized\n\"strongest\" LLM as the evaluator, which conducts pairwise comparisons of\ncandidate models' answers and provides a ranking score. However, this intuitive\nmethod has multiple problems, such as bringing in self-enhancement (favoring\nits own answers) and positional bias. We draw insights and lessons from the\neducational domain (Cho & MacArthur, 2011; Walsh, 2014) to improve LLM-based\nevaluations. Specifically, we propose (1) the peer rank (PR) algorithm that\ntakes into account each peer LLM's pairwise preferences of all answer pairs,\nand outputs a final ranking of models; and (2) peer discussion (PD), where we\nprompt two LLMs to discuss and try to reach a mutual agreement on the\npreferences of two answers. We conduct experiments on two benchmark datasets.\nWe find that our approaches achieve higher accuracy and align better with human\njudgments. Interestingly, PR can induce a relatively accurate self-ranking of\nmodels under the anonymous setting, where each model's name is unrevealed. Our\nwork provides space to explore evaluating models that are hard to compare for\nhumans."
                },
                "authors": [
                    {
                        "name": "Ruosen Li"
                    },
                    {
                        "name": "Teerth Patel"
                    },
                    {
                        "name": "Xinya Du"
                    }
                ],
                "author_detail": {
                    "name": "Xinya Du"
                },
                "author": "Xinya Du",
                "arxiv_comment": "Accepted by TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.02762v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02762v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20787v2",
                "updated": "2024-12-31T06:43:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    6,
                    43,
                    12,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T08:11:54Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    11,
                    54,
                    0,
                    365,
                    0
                ],
                "title": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for\n  LLMs in Cybersecurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for\n  LLMs in Cybersecurity"
                },
                "summary": "Evaluating Large Language Models (LLMs) is crucial for understanding their\ncapabilities and limitations across various applications, including natural\nlanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,\nand HumanEval assess general LLM performance but lack focus on specific expert\ndomains such as cybersecurity. Previous attempts to create cybersecurity\ndatasets have faced limitations, including insufficient data volume and a\nreliance on multiple-choice questions (MCQs). To address these gaps, we propose\nSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in\nthe cybersecurity domain. SecBench includes questions in various formats (MCQs\nand short-answer questions (SAQs)), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English),\nand across various sub-domains. The dataset was constructed by collecting\nhigh-quality data from open sources and organizing a Cybersecurity Question\nDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used\nthe powerful while cost-effective LLMs to (1). label the data and (2).\nconstructing a grading agent for automatic evaluation of SAQs. Benchmarking\nresults on 13 SOTA LLMs demonstrate the usability of SecBench, which is\narguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website, and\nthe dataset can be accessed via the artifact link.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) is crucial for understanding their\ncapabilities and limitations across various applications, including natural\nlanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,\nand HumanEval assess general LLM performance but lack focus on specific expert\ndomains such as cybersecurity. Previous attempts to create cybersecurity\ndatasets have faced limitations, including insufficient data volume and a\nreliance on multiple-choice questions (MCQs). To address these gaps, we propose\nSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in\nthe cybersecurity domain. SecBench includes questions in various formats (MCQs\nand short-answer questions (SAQs)), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English),\nand across various sub-domains. The dataset was constructed by collecting\nhigh-quality data from open sources and organizing a Cybersecurity Question\nDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used\nthe powerful while cost-effective LLMs to (1). label the data and (2).\nconstructing a grading agent for automatic evaluation of SAQs. Benchmarking\nresults on 13 SOTA LLMs demonstrate the usability of SecBench, which is\narguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website, and\nthe dataset can be accessed via the artifact link."
                },
                "authors": [
                    {
                        "name": "Pengfei Jing"
                    },
                    {
                        "name": "Mengyun Tang"
                    },
                    {
                        "name": "Xiaorong Shi"
                    },
                    {
                        "name": "Xing Zheng"
                    },
                    {
                        "name": "Sen Nie"
                    },
                    {
                        "name": "Shi Wu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Xiapu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Xiapu Luo"
                },
                "author": "Xiapu Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18547v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18547v3",
                "updated": "2024-12-31T06:11:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    6,
                    11,
                    39,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-24T16:55:45Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    55,
                    45,
                    1,
                    359,
                    0
                ],
                "title": "Token-Budget-Aware LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Budget-Aware LLM Reasoning"
                },
                "summary": "Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE."
                },
                "authors": [
                    {
                        "name": "Tingxu Han"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Shiyu Zhao"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18547v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18547v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02795v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02795v3",
                "updated": "2024-12-31T06:03:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    6,
                    3,
                    22,
                    1,
                    366,
                    0
                ],
                "published": "2024-06-04T21:43:56Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    21,
                    43,
                    56,
                    1,
                    156,
                    0
                ],
                "title": "ArguMentor: Augmenting User Experiences with Counter-Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArguMentor: Augmenting User Experiences with Counter-Perspectives"
                },
                "summary": "We encounter arguments everyday in the form of social media posts,\npresidential debates, news articles, and even advertisements. A ubiquitous,\ninfluential example is the opinion piece (op-ed). Opinion pieces can provide\nvaluable perspectives, but they often represent only one side of a story, which\ncan make readers susceptible to confirmation bias and echo chambers. Exposure\nto different perspectives can help readers overcome these obstacles and form\nmore robust, nuanced views on important societal issues. We designed\nArguMentor, a human-AI collaboration system that highlights claims in opinion\npieces, identifies counter-arguments for them using a LLM, and generates a\ncontext-based summary of based on current events. It further enhances user\nunderstanding through additional features like a Q\\&A bot (that answers user\nquestions pertaining to the text), DebateMe (an agent that users can argue any\nside of the piece with) and highlighting (where users can highlight a word or\npassage to get its definition or context). Our evaluation on news op-eds shows\nthat participants can generate more arguments and counter-arguments and display\nhigher critical thinking skills after engaging with the system. Further\ndiscussion highlights a more general need for this kind of a system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We encounter arguments everyday in the form of social media posts,\npresidential debates, news articles, and even advertisements. A ubiquitous,\ninfluential example is the opinion piece (op-ed). Opinion pieces can provide\nvaluable perspectives, but they often represent only one side of a story, which\ncan make readers susceptible to confirmation bias and echo chambers. Exposure\nto different perspectives can help readers overcome these obstacles and form\nmore robust, nuanced views on important societal issues. We designed\nArguMentor, a human-AI collaboration system that highlights claims in opinion\npieces, identifies counter-arguments for them using a LLM, and generates a\ncontext-based summary of based on current events. It further enhances user\nunderstanding through additional features like a Q\\&A bot (that answers user\nquestions pertaining to the text), DebateMe (an agent that users can argue any\nside of the piece with) and highlighting (where users can highlight a word or\npassage to get its definition or context). Our evaluation on news op-eds shows\nthat participants can generate more arguments and counter-arguments and display\nhigher critical thinking skills after engaging with the system. Further\ndiscussion highlights a more general need for this kind of a system."
                },
                "authors": [
                    {
                        "name": "Priya Pitre"
                    },
                    {
                        "name": "Kurt Luther"
                    }
                ],
                "author_detail": {
                    "name": "Kurt Luther"
                },
                "author": "Kurt Luther",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02795v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02795v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.18751v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.18751v3",
                "updated": "2024-12-31T04:24:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    4,
                    24,
                    16,
                    1,
                    366,
                    0
                ],
                "published": "2023-11-30T17:50:47Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    17,
                    50,
                    47,
                    3,
                    334,
                    0
                ],
                "title": "Exposing Limitations of Language Model Agents in Sequential-Task\n  Compositions on the Web",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Limitations of Language Model Agents in Sequential-Task\n  Compositions on the Web"
                },
                "summary": "Language model agents (LMA) recently emerged as a promising paradigm on\nmuti-step decision making tasks, often outperforming humans and other\nreinforcement learning agents. Despite the promise, their performance on\nreal-world applications that often involve combinations of tasks is still\nunderexplored. In this work, we introduce a new benchmark, called CompWoB -- 50\nnew compositional web automation tasks reflecting more realistic assumptions.\nWe show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve\n94.0% average success rate on base tasks, their performance degrades to 24.9%\nsuccess rate on compositional tasks. On the other hand, transferred LMAs\n(finetuned only on base tasks) show less generalization gap, dropping from\n85.4% to 54.8%. By balancing data distribution across tasks, we train a new\nmodel, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB,\nand achieves the best zero-shot performance on CompWoB (61.5%). While these\nhighlight the promise of small-scale finetuned and transferred models for task\ncompositionality, their performance further degrades under different\ninstruction compositions changing combinational order. In contrast to the\nrecent remarkable success of LMA, our benchmark and detailed analysis emphasize\nthe necessity of building LMAs that are robust and generalizable to task\ncompositionality for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model agents (LMA) recently emerged as a promising paradigm on\nmuti-step decision making tasks, often outperforming humans and other\nreinforcement learning agents. Despite the promise, their performance on\nreal-world applications that often involve combinations of tasks is still\nunderexplored. In this work, we introduce a new benchmark, called CompWoB -- 50\nnew compositional web automation tasks reflecting more realistic assumptions.\nWe show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve\n94.0% average success rate on base tasks, their performance degrades to 24.9%\nsuccess rate on compositional tasks. On the other hand, transferred LMAs\n(finetuned only on base tasks) show less generalization gap, dropping from\n85.4% to 54.8%. By balancing data distribution across tasks, we train a new\nmodel, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB,\nand achieves the best zero-shot performance on CompWoB (61.5%). While these\nhighlight the promise of small-scale finetuned and transferred models for task\ncompositionality, their performance further degrades under different\ninstruction compositions changing combinational order. In contrast to the\nrecent remarkable success of LMA, our benchmark and detailed analysis emphasize\nthe necessity of building LMAs that are robust and generalizable to task\ncompositionality for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Hiroki Furuta"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Aleksandra Faust"
                    },
                    {
                        "name": "Izzeddin Gur"
                    }
                ],
                "author_detail": {
                    "name": "Izzeddin Gur"
                },
                "author": "Izzeddin Gur",
                "arxiv_comment": "Published at Transactions on Machine Learning Research (TMLR), Code:\n  https://github.com/google-research/google-research/tree/master/compositional_rl/compwob",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.18751v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.18751v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01334v2",
                "updated": "2024-12-31T04:17:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    4,
                    17,
                    24,
                    1,
                    366,
                    0
                ],
                "published": "2024-03-30T12:13:57Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    12,
                    13,
                    57,
                    5,
                    90,
                    0
                ],
                "title": "Augmenting NER Datasets with LLMs: Towards Automated and Refined\n  Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting NER Datasets with LLMs: Towards Automated and Refined\n  Annotation"
                },
                "summary": "In the field of Natural Language Processing (NLP), Named Entity Recognition\n(NER) is recognized as a critical technology, employed across a wide array of\napplications. Traditional methodologies for annotating datasets for NER models\nare challenged by high costs and variations in dataset quality. This research\nintroduces a novel hybrid annotation approach that synergizes human effort with\nthe capabilities of Large Language Models (LLMs). This approach not only aims\nto ameliorate the noise inherent in manual annotations, such as omissions,\nthereby enhancing the performance of NER models, but also achieves this in a\ncost-effective manner. Additionally, by employing a label mixing strategy, it\naddresses the issue of class imbalance encountered in LLM-based annotations.\nThrough an analysis across multiple datasets, this method has been consistently\nshown to provide superior performance compared to traditional annotation\nmethods, even under constrained budget conditions. This study illuminates the\npotential of leveraging LLMs to improve dataset quality, introduces a novel\ntechnique to mitigate class imbalances, and demonstrates the feasibility of\nachieving high-performance NER in a cost-effective way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of Natural Language Processing (NLP), Named Entity Recognition\n(NER) is recognized as a critical technology, employed across a wide array of\napplications. Traditional methodologies for annotating datasets for NER models\nare challenged by high costs and variations in dataset quality. This research\nintroduces a novel hybrid annotation approach that synergizes human effort with\nthe capabilities of Large Language Models (LLMs). This approach not only aims\nto ameliorate the noise inherent in manual annotations, such as omissions,\nthereby enhancing the performance of NER models, but also achieves this in a\ncost-effective manner. Additionally, by employing a label mixing strategy, it\naddresses the issue of class imbalance encountered in LLM-based annotations.\nThrough an analysis across multiple datasets, this method has been consistently\nshown to provide superior performance compared to traditional annotation\nmethods, even under constrained budget conditions. This study illuminates the\npotential of leveraging LLMs to improve dataset quality, introduces a novel\ntechnique to mitigate class imbalances, and demonstrates the feasibility of\nachieving high-performance NER in a cost-effective way."
                },
                "authors": [
                    {
                        "name": "Yuji Naraki"
                    },
                    {
                        "name": "Ryosuke Yamaki"
                    },
                    {
                        "name": "Yoshikazu Ikeda"
                    },
                    {
                        "name": "Takafumi Horie"
                    },
                    {
                        "name": "Kotaro Yoshida"
                    },
                    {
                        "name": "Ryotaro Shimizu"
                    },
                    {
                        "name": "Hiroki Naganuma"
                    }
                ],
                "author_detail": {
                    "name": "Hiroki Naganuma"
                },
                "author": "Hiroki Naganuma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20072v2",
                "updated": "2024-12-31T03:11:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    11,
                    3,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-28T07:54:14Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    7,
                    54,
                    14,
                    5,
                    363,
                    0
                ],
                "title": "Extract Information from Hybrid Long Documents Leveraging LLMs: A\n  Framework and Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extract Information from Hybrid Long Documents Leveraging LLMs: A\n  Framework and Dataset"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional performance in textual\nunderstanding and tabular reasoning tasks. However, their ability to comprehend\nand analyze hybrid text, containing textual and tabular data, remains\nunexplored. The hybrid text often appears in the form of hybrid long documents\n(HLDs), which far exceed the token limit of LLMs. Consequently, we apply an\nAutomated Information Extraction framework (AIE) to enable LLMs to process the\nHLDs and carry out experiments to analyse four important aspects of information\nextraction from HLDs. Given the findings: 1) The effective way to select and\nsummarize the useful part of a HLD. 2) An easy table serialization way is\nenough for LLMs to understand tables. 3) The naive AIE has adaptability in many\ncomplex scenarios. 4) The useful prompt engineering to enhance LLMs on HLDs. To\naddress the issue of dataset scarcity in HLDs and support future work, we also\npropose the Financial Reports Numerical Extraction (FINE) dataset. The dataset\nand code are publicly available in the attachments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional performance in textual\nunderstanding and tabular reasoning tasks. However, their ability to comprehend\nand analyze hybrid text, containing textual and tabular data, remains\nunexplored. The hybrid text often appears in the form of hybrid long documents\n(HLDs), which far exceed the token limit of LLMs. Consequently, we apply an\nAutomated Information Extraction framework (AIE) to enable LLMs to process the\nHLDs and carry out experiments to analyse four important aspects of information\nextraction from HLDs. Given the findings: 1) The effective way to select and\nsummarize the useful part of a HLD. 2) An easy table serialization way is\nenough for LLMs to understand tables. 3) The naive AIE has adaptability in many\ncomplex scenarios. 4) The useful prompt engineering to enhance LLMs on HLDs. To\naddress the issue of dataset scarcity in HLDs and support future work, we also\npropose the Financial Reports Numerical Extraction (FINE) dataset. The dataset\nand code are publicly available in the attachments."
                },
                "authors": [
                    {
                        "name": "Chongjian Yue"
                    },
                    {
                        "name": "Xinrun Xu"
                    },
                    {
                        "name": "Xiaojun Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Zhiming Ding"
                    },
                    {
                        "name": "Shi Han"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21006v2",
                "updated": "2024-12-31T03:06:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    6,
                    15,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:15:08Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    15,
                    8,
                    0,
                    365,
                    0
                ],
                "title": "Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant\n  Rationale via Principled Criteria",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant\n  Rationale via Principled Criteria"
                },
                "summary": "Large Language Models (LLMs) rely on generating extensive intermediate\nreasoning units (e.g., tokens, sentences) to enhance final answer quality\nacross a wide range of complex tasks. While generating multiple reasoning paths\nor iteratively refining rationales proves effective for improving performance,\nthese approaches inevitably result in significantly higher inference costs. In\nthis work, we propose a novel sentence-level rationale reduction training\nframework that leverages likelihood-based criteria, verbosity, to identify and\nremove redundant reasoning sentences. Unlike previous approaches that utilize\ntoken-level reduction, our sentence-level reduction framework maintains model\nperformance while reducing generation length. This preserves the original\nreasoning abilities of LLMs and achieves an average 17.15% reduction in\ngeneration costs across various models and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) rely on generating extensive intermediate\nreasoning units (e.g., tokens, sentences) to enhance final answer quality\nacross a wide range of complex tasks. While generating multiple reasoning paths\nor iteratively refining rationales proves effective for improving performance,\nthese approaches inevitably result in significantly higher inference costs. In\nthis work, we propose a novel sentence-level rationale reduction training\nframework that leverages likelihood-based criteria, verbosity, to identify and\nremove redundant reasoning sentences. Unlike previous approaches that utilize\ntoken-level reduction, our sentence-level reduction framework maintains model\nperformance while reducing generation length. This preserves the original\nreasoning abilities of LLMs and achieves an average 17.15% reduction in\ngeneration costs across various models and tasks."
                },
                "authors": [
                    {
                        "name": "Joonwon Jang"
                    },
                    {
                        "name": "Jaehee Kim"
                    },
                    {
                        "name": "Wonbin Kweon"
                    },
                    {
                        "name": "Hwanjo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjo Yu"
                },
                "author": "Hwanjo Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.01977v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.01977v6",
                "updated": "2024-12-31T01:49:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    1,
                    49,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2023-09-05T06:12:39Z",
                "published_parsed": [
                    2023,
                    9,
                    5,
                    6,
                    12,
                    39,
                    1,
                    248,
                    0
                ],
                "title": "PyTomography: A Python Library for Medical Image Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyTomography: A Python Library for Medical Image Reconstruction"
                },
                "summary": "There is a need for open-source libraries in emission tomography that (i) use\nmodern and popular backend code to encourage community contributions and (ii)\noffer support for the multitude of reconstruction techniques available in\nrecent literature, such as those that employ artificial intelligence. The\npurpose of this research was to create and evaluate a GPU-accelerated,\nopen-source, and user-friendly image reconstruction library, designed to serve\nas a central platform for the development, validation, and deployment of\nvarious tomographic reconstruction algorithms. PyTomography was developed using\nPython and inherits the GPU-accelerated functionality of PyTorch and\nparallelproj for fast computations. Its flexible and modular design decouples\nsystem matrices, likelihoods, and reconstruction algorithms, simplifying the\nprocess of integrating new imaging modalities using various python tools.\nExample use cases demonstrate the software capabilities in parallel hole SPECT\nand listmode PET imaging. Overall, we have developed and publicly share\nPyTomography, a highly optimized and user-friendly software for medical image\nreconstruction, with a class hierarchy that fosters the development of novel\nimaging applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a need for open-source libraries in emission tomography that (i) use\nmodern and popular backend code to encourage community contributions and (ii)\noffer support for the multitude of reconstruction techniques available in\nrecent literature, such as those that employ artificial intelligence. The\npurpose of this research was to create and evaluate a GPU-accelerated,\nopen-source, and user-friendly image reconstruction library, designed to serve\nas a central platform for the development, validation, and deployment of\nvarious tomographic reconstruction algorithms. PyTomography was developed using\nPython and inherits the GPU-accelerated functionality of PyTorch and\nparallelproj for fast computations. Its flexible and modular design decouples\nsystem matrices, likelihoods, and reconstruction algorithms, simplifying the\nprocess of integrating new imaging modalities using various python tools.\nExample use cases demonstrate the software capabilities in parallel hole SPECT\nand listmode PET imaging. Overall, we have developed and publicly share\nPyTomography, a highly optimized and user-friendly software for medical image\nreconstruction, with a class hierarchy that fosters the development of novel\nimaging applications."
                },
                "authors": [
                    {
                        "name": "Lucas A. Polson"
                    },
                    {
                        "name": "Roberto Fedrigo"
                    },
                    {
                        "name": "Chenguang Li"
                    },
                    {
                        "name": "Maziar Sabouri"
                    },
                    {
                        "name": "Obed Dzikunu"
                    },
                    {
                        "name": "Shadab Ahamed"
                    },
                    {
                        "name": "Nikolaos Karakatsanis"
                    },
                    {
                        "name": "Peyman Sheikhzadeh"
                    },
                    {
                        "name": "Pedro Esquinas"
                    },
                    {
                        "name": "Arman Rahmim"
                    },
                    {
                        "name": "Carlos Uribe"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Uribe"
                },
                "author": "Carlos Uribe",
                "arxiv_comment": "28 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.01977v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.01977v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11694v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11694v4",
                "updated": "2024-12-31T01:38:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    1,
                    38,
                    12,
                    1,
                    366,
                    0
                ],
                "published": "2024-11-18T16:15:17Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    15,
                    17,
                    0,
                    323,
                    0
                ],
                "title": "Enhancing LLM Reasoning with Reward-guided Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Reasoning with Reward-guided Tree Search"
                },
                "summary": "Recently, test-time scaling has garnered significant attention from the\nresearch community, largely due to the substantial advancements of the o1 model\nreleased by OpenAI. By allocating more computational resources during the\ninference phase, large language models~(LLMs) can extensively explore the\nsolution space by generating more thought tokens or diverse solutions, thereby\nproducing more accurate responses. However, developing an o1-like reasoning\napproach is challenging, and researchers have been making various attempts to\nadvance this open area of research. In this paper, we present a preliminary\nexploration into enhancing the reasoning abilities of LLMs through\nreward-guided tree search algorithms. This framework is implemented by\nintegrating the policy model, reward model, and search algorithm. It is\nprimarily constructed around a tree search algorithm, where the policy model\nnavigates a dynamically expanding tree guided by a specially trained reward\nmodel. The implemented framework is denoted as \\textbf{STILL-1}. We thoroughly\nexplore various design considerations necessary for implementing this framework\nand provide a detailed report of the technical aspects. To assess the\neffectiveness of our approach, we focus on mathematical reasoning tasks and\nconduct extensive evaluations on four challenging datasets, significantly\nenhancing the reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, test-time scaling has garnered significant attention from the\nresearch community, largely due to the substantial advancements of the o1 model\nreleased by OpenAI. By allocating more computational resources during the\ninference phase, large language models~(LLMs) can extensively explore the\nsolution space by generating more thought tokens or diverse solutions, thereby\nproducing more accurate responses. However, developing an o1-like reasoning\napproach is challenging, and researchers have been making various attempts to\nadvance this open area of research. In this paper, we present a preliminary\nexploration into enhancing the reasoning abilities of LLMs through\nreward-guided tree search algorithms. This framework is implemented by\nintegrating the policy model, reward model, and search algorithm. It is\nprimarily constructed around a tree search algorithm, where the policy model\nnavigates a dynamically expanding tree guided by a specially trained reward\nmodel. The implemented framework is denoted as \\textbf{STILL-1}. We thoroughly\nexplore various design considerations necessary for implementing this framework\nand provide a detailed report of the technical aspects. To assess the\neffectiveness of our approach, we focus on mathematical reasoning tasks and\nconduct extensive evaluations on four challenging datasets, significantly\nenhancing the reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Xiaoxue Cheng"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Haoxiang Sun"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Technical Report on Slow Thinking with LLMs: I",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11694v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11694v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02504v2",
                "updated": "2024-12-31T01:36:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    1,
                    36,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-10-03T14:09:58Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    9,
                    58,
                    3,
                    277,
                    0
                ],
                "title": "Dual Active Learning for Reinforcement Learning from Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Active Learning for Reinforcement Learning from Human Feedback"
                },
                "summary": "Aligning large language models (LLMs) with human preferences is critical to\nrecent advances in generative artificial intelligence. Reinforcement learning\nfrom human feedback (RLHF) is widely applied to achieve this objective. A key\nstep in RLHF is to learn the reward function from human feedback. However,\nhuman feedback is costly and time-consuming, making it essential to collect\nhigh-quality conversation data for human teachers to label. Additionally,\ndifferent human teachers have different levels of expertise. It is thus\ncritical to query the most appropriate teacher for their opinions. In this\npaper, we use offline reinforcement learning (RL) to formulate the alignment\nproblem. Motivated by the idea of $D$-optimal design, we first propose a dual\nactive reward learning algorithm for the simultaneous selection of\nconversations and teachers. Next, we apply pessimistic RL to solve the\nalignment problem, based on the learned reward estimator. Theoretically, we\nshow that the reward estimator obtained through our proposed adaptive selection\nstrategy achieves minimal generalized variance asymptotically, and prove that\nthe sub-optimality of our pessimistic policy scales as $O(1/\\sqrt{T})$ with a\ngiven sample budget $T$. Through simulations and experiments on LLMs, we\ndemonstrate the effectiveness of our algorithm and its superiority over\nstate-of-the-arts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human preferences is critical to\nrecent advances in generative artificial intelligence. Reinforcement learning\nfrom human feedback (RLHF) is widely applied to achieve this objective. A key\nstep in RLHF is to learn the reward function from human feedback. However,\nhuman feedback is costly and time-consuming, making it essential to collect\nhigh-quality conversation data for human teachers to label. Additionally,\ndifferent human teachers have different levels of expertise. It is thus\ncritical to query the most appropriate teacher for their opinions. In this\npaper, we use offline reinforcement learning (RL) to formulate the alignment\nproblem. Motivated by the idea of $D$-optimal design, we first propose a dual\nactive reward learning algorithm for the simultaneous selection of\nconversations and teachers. Next, we apply pessimistic RL to solve the\nalignment problem, based on the learned reward estimator. Theoretically, we\nshow that the reward estimator obtained through our proposed adaptive selection\nstrategy achieves minimal generalized variance asymptotically, and prove that\nthe sub-optimality of our pessimistic policy scales as $O(1/\\sqrt{T})$ with a\ngiven sample budget $T$. Through simulations and experiments on LLMs, we\ndemonstrate the effectiveness of our algorithm and its superiority over\nstate-of-the-arts."
                },
                "authors": [
                    {
                        "name": "Pangpang Liu"
                    },
                    {
                        "name": "Chengchun Shi"
                    },
                    {
                        "name": "Will Wei Sun"
                    }
                ],
                "author_detail": {
                    "name": "Will Wei Sun"
                },
                "author": "Will Wei Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11900v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11900v3",
                "updated": "2024-12-30T23:52:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    23,
                    52,
                    49,
                    0,
                    365,
                    0
                ],
                "published": "2024-10-14T19:39:11Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    39,
                    11,
                    0,
                    288,
                    0
                ],
                "title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLARE: Faithful Logic-Aided Reasoning and Exploration"
                },
                "summary": "Modern Question Answering (QA) and Reasoning approaches based on Large\nLanguage Models (LLMs) commonly use prompting techniques, such as\nChain-of-Thought (CoT), assuming the resulting generation will have a more\ngranular exploration and reasoning over the question space and scope. However,\nsuch methods struggle with generating outputs that are faithful to the\nintermediate chain of reasoning produced by the model. On the other end of the\nspectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to\ncombine LLMs with external symbolic solvers. While such approaches boast a high\ndegree of faithfulness, they usually require a model trained for code\ngeneration and struggle with tasks that are ambiguous or hard to formalise\nstrictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided\n$\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel\ninterpretable approach for traversing the problem space using task\ndecompositions. We use the LLM to plan a solution, soft-formalise the query\ninto facts and predicates using a logic programming code and simulate that code\nexecution using an exhaustive multi-hop search over the defined space. Our\nmethod allows us to compute the faithfulness of the reasoning process w.r.t.\nthe generated code and analyse the steps of the multi-hop search without\nrelying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$\nout of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model\nfaithfulness positively correlates with overall performance and further\ndemonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors\nsufficient for and leading to the correct answer with optimal reasoning during\nthe multi-hop search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Question Answering (QA) and Reasoning approaches based on Large\nLanguage Models (LLMs) commonly use prompting techniques, such as\nChain-of-Thought (CoT), assuming the resulting generation will have a more\ngranular exploration and reasoning over the question space and scope. However,\nsuch methods struggle with generating outputs that are faithful to the\nintermediate chain of reasoning produced by the model. On the other end of the\nspectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to\ncombine LLMs with external symbolic solvers. While such approaches boast a high\ndegree of faithfulness, they usually require a model trained for code\ngeneration and struggle with tasks that are ambiguous or hard to formalise\nstrictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided\n$\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel\ninterpretable approach for traversing the problem space using task\ndecompositions. We use the LLM to plan a solution, soft-formalise the query\ninto facts and predicates using a logic programming code and simulate that code\nexecution using an exhaustive multi-hop search over the defined space. Our\nmethod allows us to compute the faithfulness of the reasoning process w.r.t.\nthe generated code and analyse the steps of the multi-hop search without\nrelying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$\nout of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model\nfaithfulness positively correlates with overall performance and further\ndemonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors\nsufficient for and leading to the correct answer with optimal reasoning during\nthe multi-hop search."
                },
                "authors": [
                    {
                        "name": "Erik Arakelyan"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Pat Verga"
                    },
                    {
                        "name": "Patrick Lewis"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11900v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11900v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21520v2",
                "updated": "2024-12-30T22:37:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    22,
                    37,
                    28,
                    0,
                    365,
                    0
                ],
                "published": "2024-10-28T20:42:46Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    20,
                    42,
                    46,
                    0,
                    302,
                    0
                ],
                "title": "LLM-Forest: Ensemble Learning of LLMs with Graph-Augmented Prompts for\n  Data Imputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Forest: Ensemble Learning of LLMs with Graph-Augmented Prompts for\n  Data Imputation"
                },
                "summary": "Missing data imputation is a critical challenge in various domains, such as\nhealthcare and finance, where data completeness is vital for accurate analysis.\nLarge language models (LLMs), trained on vast corpora, have shown strong\npotential in data generation, making them a promising tool for data imputation.\nHowever, challenges persist in designing effective prompts for a\nfinetuning-free process and in mitigating the risk of LLM hallucinations. To\naddress these issues, we propose a novel framework, LLM-Forest, which\nintroduces a \"forest\" of few-shot learning LLM \"trees\" with confidence-based\nweighted voting, inspired by ensemble learning (Random Forest). This framework\nis established on a new concept of bipartite information graphs to identify\nhigh-quality relevant neighboring entries with both feature and value\ngranularity. Extensive experiments on 9 real-world datasets demonstrate the\neffectiveness and efficiency of LLM-Forest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing data imputation is a critical challenge in various domains, such as\nhealthcare and finance, where data completeness is vital for accurate analysis.\nLarge language models (LLMs), trained on vast corpora, have shown strong\npotential in data generation, making them a promising tool for data imputation.\nHowever, challenges persist in designing effective prompts for a\nfinetuning-free process and in mitigating the risk of LLM hallucinations. To\naddress these issues, we propose a novel framework, LLM-Forest, which\nintroduces a \"forest\" of few-shot learning LLM \"trees\" with confidence-based\nweighted voting, inspired by ensemble learning (Random Forest). This framework\nis established on a new concept of bipartite information graphs to identify\nhigh-quality relevant neighboring entries with both feature and value\ngranularity. Extensive experiments on 9 real-world datasets demonstrate the\neffectiveness and efficiency of LLM-Forest."
                },
                "authors": [
                    {
                        "name": "Xinrui He"
                    },
                    {
                        "name": "Yikun Ban"
                    },
                    {
                        "name": "Jiaru Zou"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Curtiss B. Cook"
                    },
                    {
                        "name": "Jingrui He"
                    }
                ],
                "author_detail": {
                    "name": "Jingrui He"
                },
                "author": "Jingrui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18566v2",
                "updated": "2024-12-30T22:25:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    22,
                    25,
                    56,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-24T17:37:11Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    37,
                    11,
                    1,
                    359,
                    0
                ],
                "title": "Zero-resource Speech Translation and Recognition with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-resource Speech Translation and Recognition with LLMs"
                },
                "summary": "Despite recent advancements in speech processing, zero-resource speech\ntranslation (ST) and automatic speech recognition (ASR) remain challenging\nproblems. In this work, we propose to leverage a multilingual Large Language\nModel (LLM) to perform ST and ASR in languages for which the model has never\nseen paired audio-text data. We achieve this by using a pre-trained\nmultilingual speech encoder, a multilingual LLM, and a lightweight adaptation\nmodule that maps the audio representations to the token embedding space of the\nLLM. We perform several experiments both in ST and ASR to understand how to\nbest train the model and what data has the most impact on performance in\npreviously unseen languages. In ST, our best model is capable to achieve BLEU\nscores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we\nachieve WERs of up to 28.2\\%. We finally show that the performance of our\nsystem is bounded by the ability of the LLM to output text in the desired\nlanguage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advancements in speech processing, zero-resource speech\ntranslation (ST) and automatic speech recognition (ASR) remain challenging\nproblems. In this work, we propose to leverage a multilingual Large Language\nModel (LLM) to perform ST and ASR in languages for which the model has never\nseen paired audio-text data. We achieve this by using a pre-trained\nmultilingual speech encoder, a multilingual LLM, and a lightweight adaptation\nmodule that maps the audio representations to the token embedding space of the\nLLM. We perform several experiments both in ST and ASR to understand how to\nbest train the model and what data has the most impact on performance in\npreviously unseen languages. In ST, our best model is capable to achieve BLEU\nscores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we\nachieve WERs of up to 28.2\\%. We finally show that the performance of our\nsystem is bounded by the ability of the LLM to output text in the desired\nlanguage."
                },
                "authors": [
                    {
                        "name": "Karel Mundnich"
                    },
                    {
                        "name": "Xing Niu"
                    },
                    {
                        "name": "Prashant Mathur"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Brady Houston"
                    },
                    {
                        "name": "Veera Raghavendra Elluru"
                    },
                    {
                        "name": "Nilaksh Das"
                    },
                    {
                        "name": "Zejiang Hou"
                    },
                    {
                        "name": "Goeric Huybrechts"
                    },
                    {
                        "name": "Anshu Bhatia"
                    },
                    {
                        "name": "Daniel Garcia-Romero"
                    },
                    {
                        "name": "Kyu J. Han"
                    },
                    {
                        "name": "Katrin Kirchhoff"
                    }
                ],
                "author_detail": {
                    "name": "Katrin Kirchhoff"
                },
                "author": "Katrin Kirchhoff",
                "arxiv_comment": "ICASSP 2025, 5 pages, 2 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19689v2",
                "updated": "2024-12-30T21:49:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    21,
                    49,
                    4,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-27T15:33:14Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    15,
                    33,
                    14,
                    4,
                    362,
                    0
                ],
                "title": "Electric Vehicle Charging Network Design under Congestion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric Vehicle Charging Network Design under Congestion"
                },
                "summary": "This paper presents an extension of a recently introduced multistage\nstochastic integer model designed for optimizing the deployment of charging\nstations under uncertainty. A key contribution of this work is incorporating\nadditional constraints accounting for congestion management at charging\nstations. The solution approach combines a greedy heuristic with a\nbranch-and-price algorithm, enabling the efficient handling of medium\ninstances. In the branch-and-price algorithm, when the solution to the\nrestricted master problem is not integer, a greedy heuristic and a local search\nprocedure are conducted to obtain feasible solutions. Computational experiments\nillustrate the effectiveness of the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an extension of a recently introduced multistage\nstochastic integer model designed for optimizing the deployment of charging\nstations under uncertainty. A key contribution of this work is incorporating\nadditional constraints accounting for congestion management at charging\nstations. The solution approach combines a greedy heuristic with a\nbranch-and-price algorithm, enabling the efficient handling of medium\ninstances. In the branch-and-price algorithm, when the solution to the\nrestricted master problem is not integer, a greedy heuristic and a local search\nprocedure are conducted to obtain feasible solutions. Computational experiments\nillustrate the effectiveness of the proposed framework."
                },
                "authors": [
                    {
                        "name": "Antoine Deza"
                    },
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Carlos Anbal Surez"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Anbal Surez"
                },
                "author": "Carlos Anbal Surez",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06608v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06608v5",
                "updated": "2024-12-30T19:33:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    19,
                    33,
                    9,
                    0,
                    365,
                    0
                ],
                "published": "2024-06-06T18:10:11Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    18,
                    10,
                    11,
                    3,
                    158,
                    0
                ],
                "title": "The Prompt Report: A Systematic Survey of Prompting Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Prompt Report: A Systematic Survey of Prompting Techniques"
                },
                "summary": "Generative Artificial Intelligence (GenAI) systems are increasingly being\ndeployed across diverse industries and research domains. Developers and\nend-users interact with these systems through the use of prompting and prompt\nengineering. Although prompt engineering is a widely adopted and extensively\nresearched area, it suffers from conflicting terminology and a fragmented\nontological understanding of what constitutes an effective prompt due to its\nrelatively recent emergence. We establish a structured understanding of prompt\nengineering by assembling a taxonomy of prompting techniques and analyzing\ntheir applications. We present a detailed vocabulary of 33 vocabulary terms, a\ntaxonomy of 58 LLM prompting techniques, and 40 techniques for other\nmodalities. Additionally, we provide best practices and guidelines for prompt\nengineering, including advice for prompting state-of-the-art (SOTA) LLMs such\nas ChatGPT. We further present a meta-analysis of the entire literature on\nnatural language prefix-prompting. As a culmination of these efforts, this\npaper presents the most comprehensive survey on prompt engineering to date.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) systems are increasingly being\ndeployed across diverse industries and research domains. Developers and\nend-users interact with these systems through the use of prompting and prompt\nengineering. Although prompt engineering is a widely adopted and extensively\nresearched area, it suffers from conflicting terminology and a fragmented\nontological understanding of what constitutes an effective prompt due to its\nrelatively recent emergence. We establish a structured understanding of prompt\nengineering by assembling a taxonomy of prompting techniques and analyzing\ntheir applications. We present a detailed vocabulary of 33 vocabulary terms, a\ntaxonomy of 58 LLM prompting techniques, and 40 techniques for other\nmodalities. Additionally, we provide best practices and guidelines for prompt\nengineering, including advice for prompting state-of-the-art (SOTA) LLMs such\nas ChatGPT. We further present a meta-analysis of the entire literature on\nnatural language prefix-prompting. As a culmination of these efforts, this\npaper presents the most comprehensive survey on prompt engineering to date."
                },
                "authors": [
                    {
                        "name": "Sander Schulhoff"
                    },
                    {
                        "name": "Michael Ilie"
                    },
                    {
                        "name": "Nishant Balepur"
                    },
                    {
                        "name": "Konstantine Kahadze"
                    },
                    {
                        "name": "Amanda Liu"
                    },
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Yinheng Li"
                    },
                    {
                        "name": "Aayush Gupta"
                    },
                    {
                        "name": "HyoJung Han"
                    },
                    {
                        "name": "Sevien Schulhoff"
                    },
                    {
                        "name": "Pranav Sandeep Dulepet"
                    },
                    {
                        "name": "Saurav Vidyadhara"
                    },
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Chau Pham"
                    },
                    {
                        "name": "Gerson Kroiz"
                    },
                    {
                        "name": "Feileen Li"
                    },
                    {
                        "name": "Hudson Tao"
                    },
                    {
                        "name": "Ashay Srivastava"
                    },
                    {
                        "name": "Hevander Da Costa"
                    },
                    {
                        "name": "Saloni Gupta"
                    },
                    {
                        "name": "Megan L. Rogers"
                    },
                    {
                        "name": "Inna Goncearenco"
                    },
                    {
                        "name": "Giuseppe Sarli"
                    },
                    {
                        "name": "Igor Galynker"
                    },
                    {
                        "name": "Denis Peskoff"
                    },
                    {
                        "name": "Marine Carpuat"
                    },
                    {
                        "name": "Jules White"
                    },
                    {
                        "name": "Shyamal Anadkat"
                    },
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Philip Resnik"
                    }
                ],
                "author_detail": {
                    "name": "Philip Resnik"
                },
                "author": "Philip Resnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06608v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06608v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.08662v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.08662v3",
                "updated": "2024-12-30T19:02:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    19,
                    2,
                    43,
                    0,
                    365,
                    0
                ],
                "published": "2023-11-15T02:59:10Z",
                "published_parsed": [
                    2023,
                    11,
                    15,
                    2,
                    59,
                    10,
                    2,
                    319,
                    0
                ],
                "title": "Evaluating Concurrent Robustness of Language Models Across Diverse\n  Challenge Sets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Concurrent Robustness of Language Models Across Diverse\n  Challenge Sets"
                },
                "summary": "Language models, characterized by their black-box nature, often hallucinate\nand display sensitivity to input perturbations, causing concerns about trust.\nTo enhance trust, it is imperative to gain a comprehensive understanding of the\nmodel's failure modes and develop effective strategies to improve their\nperformance. In this study, we introduce a methodology designed to examine how\ninput perturbations affect language models across various scales, including\npre-trained models and large language models (LLMs). Utilizing fine-tuning, we\nenhance the model's robustness to input perturbations. Additionally, we\ninvestigate whether exposure to one perturbation enhances or diminishes the\nmodel's performance with respect to other perturbations. To address robustness\nagainst multiple perturbations, we present three distinct fine-tuning\nstrategies. Furthermore, we broaden the scope of our methodology to encompass\nlarge language models (LLMs) by leveraging a chain of thought (CoT) prompting\napproach augmented with exemplars. We employ the Tabular-NLI task to showcase\nhow our proposed strategies adeptly train a robust model, enabling it to\naddress diverse perturbations while maintaining accuracy on the original\ndataset. https://msin-infotabs.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models, characterized by their black-box nature, often hallucinate\nand display sensitivity to input perturbations, causing concerns about trust.\nTo enhance trust, it is imperative to gain a comprehensive understanding of the\nmodel's failure modes and develop effective strategies to improve their\nperformance. In this study, we introduce a methodology designed to examine how\ninput perturbations affect language models across various scales, including\npre-trained models and large language models (LLMs). Utilizing fine-tuning, we\nenhance the model's robustness to input perturbations. Additionally, we\ninvestigate whether exposure to one perturbation enhances or diminishes the\nmodel's performance with respect to other perturbations. To address robustness\nagainst multiple perturbations, we present three distinct fine-tuning\nstrategies. Furthermore, we broaden the scope of our methodology to encompass\nlarge language models (LLMs) by leveraging a chain of thought (CoT) prompting\napproach augmented with exemplars. We employ the Tabular-NLI task to showcase\nhow our proposed strategies adeptly train a robust model, enabling it to\naddress diverse perturbations while maintaining accuracy on the original\ndataset. https://msin-infotabs.github.io/"
                },
                "authors": [
                    {
                        "name": "Vatsal Gupta"
                    },
                    {
                        "name": "Pranshu Pandya"
                    },
                    {
                        "name": "Tushar Kataria"
                    },
                    {
                        "name": "Vivek Gupta"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "arxiv_doi": "10.18653/v1/2024.emnlp-main.1237",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.emnlp-main.1237",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.08662v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.08662v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, 16 Figure, 10 Tables",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21200v1",
                "updated": "2024-12-30T18:59:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    59,
                    6,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:59:06Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    59,
                    6,
                    0,
                    365,
                    0
                ],
                "title": "Distributed Mixture-of-Agents for Edge Inference with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Mixture-of-Agents for Edge Inference with Large Language\n  Models"
                },
                "summary": "Mixture-of-Agents (MoA) has recently been proposed as a method to enhance\nperformance of large language models (LLMs), enabling multiple individual LLMs\nto work together for collaborative inference. This collaborative approach\nresults in improved responses to user prompts compared to relying on a single\nLLM. In this paper, we consider such an MoA architecture in a distributed\nsetting, where LLMs operate on individual edge devices, each uniquely\nassociated with a user and equipped with its own distributed computing power.\nThese devices exchange information using decentralized gossip algorithms,\nallowing different device nodes to talk without the supervision of a\ncentralized server. In the considered setup, different users have their own LLM\nmodels to address user prompts. Additionally, the devices gossip either their\nown user-specific prompts or augmented prompts to generate more refined answers\nto certain queries. User prompts are temporarily stored in the device queues\nwhen their corresponding LLMs are busy. Given the memory limitations of edge\ndevices, it is crucial to ensure that the average queue sizes in the system\nremain bounded. In this paper, we address this by theoretically calculating the\nqueuing stability conditions for the device queues under reasonable\nassumptions, which we validate experimentally as well. Further, we demonstrate\nthrough experiments, leveraging open-source LLMs for the implementation of\ndistributed MoA, that certain MoA configurations produce higher-quality\nresponses compared to others, as evaluated on AlpacaEval 2.0 benchmark. The\nimplementation is available at:\nhttps://github.com/purbeshmitra/distributed_moa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Agents (MoA) has recently been proposed as a method to enhance\nperformance of large language models (LLMs), enabling multiple individual LLMs\nto work together for collaborative inference. This collaborative approach\nresults in improved responses to user prompts compared to relying on a single\nLLM. In this paper, we consider such an MoA architecture in a distributed\nsetting, where LLMs operate on individual edge devices, each uniquely\nassociated with a user and equipped with its own distributed computing power.\nThese devices exchange information using decentralized gossip algorithms,\nallowing different device nodes to talk without the supervision of a\ncentralized server. In the considered setup, different users have their own LLM\nmodels to address user prompts. Additionally, the devices gossip either their\nown user-specific prompts or augmented prompts to generate more refined answers\nto certain queries. User prompts are temporarily stored in the device queues\nwhen their corresponding LLMs are busy. Given the memory limitations of edge\ndevices, it is crucial to ensure that the average queue sizes in the system\nremain bounded. In this paper, we address this by theoretically calculating the\nqueuing stability conditions for the device queues under reasonable\nassumptions, which we validate experimentally as well. Further, we demonstrate\nthrough experiments, leveraging open-source LLMs for the implementation of\ndistributed MoA, that certain MoA configurations produce higher-quality\nresponses compared to others, as evaluated on AlpacaEval 2.0 benchmark. The\nimplementation is available at:\nhttps://github.com/purbeshmitra/distributed_moa."
                },
                "authors": [
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Priyanka Kaswan"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21187v1",
                "updated": "2024-12-30T18:55:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    55,
                    12,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:55:12Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    55,
                    12,
                    0,
                    365,
                    0
                ],
                "title": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs"
                },
                "summary": "The remarkable performance of models like the OpenAI o1 can be attributed to\ntheir ability to emulate human-like long-time thinking during inference. These\nmodels employ extended chain-of-thought (CoT) processes, exploring multiple\nstrategies to enhance problem-solving capabilities. However, a critical\nquestion remains: How to intelligently and efficiently scale computational\nresources during testing. This paper presents the first comprehensive study on\nthe prevalent issue of overthinking in these models, where excessive\ncomputational resources are allocated for simple problems with minimal benefit.\nWe introduce novel efficiency metrics from both outcome and process\nperspectives to evaluate the rational use of computational resources by o1-like\nmodels. Using a self-training paradigm, we propose strategies to mitigate\noverthinking, streamlining reasoning processes without compromising accuracy.\nExperimental results show that our approach successfully reduces computational\noverhead while preserving model performance across a range of testsets with\nvarying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable performance of models like the OpenAI o1 can be attributed to\ntheir ability to emulate human-like long-time thinking during inference. These\nmodels employ extended chain-of-thought (CoT) processes, exploring multiple\nstrategies to enhance problem-solving capabilities. However, a critical\nquestion remains: How to intelligently and efficiently scale computational\nresources during testing. This paper presents the first comprehensive study on\nthe prevalent issue of overthinking in these models, where excessive\ncomputational resources are allocated for simple problems with minimal benefit.\nWe introduce novel efficiency metrics from both outcome and process\nperspectives to evaluate the rational use of computational resources by o1-like\nmodels. Using a self-training paradigm, we propose strategies to mitigate\noverthinking, streamlining reasoning processes without compromising accuracy.\nExperimental results show that our approach successfully reduces computational\noverhead while preserving model performance across a range of testsets with\nvarying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME."
                },
                "authors": [
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Jianhui Pang"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Mengfei Zhou"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21162v1",
                "updated": "2024-12-30T18:41:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    41,
                    43,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:41:43Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    41,
                    43,
                    0,
                    365,
                    0
                ],
                "title": "Open-Source 5G Core Platforms: A Low-Cost Solution and Performance\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Source 5G Core Platforms: A Low-Cost Solution and Performance\n  Evaluation"
                },
                "summary": "An essential component for the Fifth Generation of Mobile Networks\ndeployments is the 5G Core (5GC), which bridges the 5G Radio Access Network\n(RAN) to the rest of the Internet. Some open-source platforms for the 5GC have\nemerged and been deployed in Common Off-the-Shelf (COTS)-based setups. Despite\nthese open-source 5GC initiatives following the 3GPP specifications, they\ndiffer in implementing some features and their stages in the timeline of 3GPP\nreleases. Besides that, they may yield different performance to metrics related\nto the data and control planes. This article reviews the major open-source 5GC\nplatforms and evaluates their performance in a 5G Standalone (SA) COTS-based\ntestbed. The results indicate that Open5GS provides the best latencies for\ncontrol plane procedures, OpenAirInterface offers the highest data rates, and\nFree5GC has the lowest resource consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential component for the Fifth Generation of Mobile Networks\ndeployments is the 5G Core (5GC), which bridges the 5G Radio Access Network\n(RAN) to the rest of the Internet. Some open-source platforms for the 5GC have\nemerged and been deployed in Common Off-the-Shelf (COTS)-based setups. Despite\nthese open-source 5GC initiatives following the 3GPP specifications, they\ndiffer in implementing some features and their stages in the timeline of 3GPP\nreleases. Besides that, they may yield different performance to metrics related\nto the data and control planes. This article reviews the major open-source 5GC\nplatforms and evaluates their performance in a 5G Standalone (SA) COTS-based\ntestbed. The results indicate that Open5GS provides the best latencies for\ncontrol plane procedures, OpenAirInterface offers the highest data rates, and\nFree5GC has the lowest resource consumption."
                },
                "authors": [
                    {
                        "name": "Maria Barbosa"
                    },
                    {
                        "name": "Marcelo Silva"
                    },
                    {
                        "name": "Ednelson Cavalcanti"
                    },
                    {
                        "name": "Kelvin Dias"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Dias"
                },
                "author": "Kelvin Dias",
                "arxiv_comment": "Accepted for publication in ICOIN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21154v1",
                "updated": "2024-12-30T18:33:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    33,
                    28,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:33:28Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    33,
                    28,
                    0,
                    365,
                    0
                ],
                "title": "Aviary: training language agents on challenging scientific tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aviary: training language agents on challenging scientific tasks"
                },
                "summary": "Solving complex real-world tasks requires cycles of actions and observations.\nThis is particularly true in science, where tasks require many cycles of\nanalysis, tool use, and experimentation. Language agents are promising for\nautomating intellectual tasks in science because they can interact with tools\nvia natural language or code. Yet their flexibility creates conceptual and\npractical challenges for software implementations, since agents may comprise\nnon-standard components such as internal reasoning, planning, tool usage, as\nwell as the inherent stochasticity of temperature-sampled language models.\nHere, we introduce Aviary, an extensible gymnasium for language agents. We\nformalize agents as policies solving language-grounded partially observable\nMarkov decision processes, which we term language decision processes. We then\nimplement five environments, including three challenging scientific\nenvironments: (1) manipulating DNA constructs for molecular cloning, (2)\nanswering research questions by accessing scientific literature, and (3)\nengineering protein stability. These environments were selected for their focus\non multi-step reasoning and their relevance to contemporary biology research.\nFinally, with online training and scaling inference-time compute, we show that\nlanguage agents backed by open-source, non-frontier LLMs can match and exceed\nboth frontier LLM agents and human experts on multiple tasks at up to 100x\nlower inference cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving complex real-world tasks requires cycles of actions and observations.\nThis is particularly true in science, where tasks require many cycles of\nanalysis, tool use, and experimentation. Language agents are promising for\nautomating intellectual tasks in science because they can interact with tools\nvia natural language or code. Yet their flexibility creates conceptual and\npractical challenges for software implementations, since agents may comprise\nnon-standard components such as internal reasoning, planning, tool usage, as\nwell as the inherent stochasticity of temperature-sampled language models.\nHere, we introduce Aviary, an extensible gymnasium for language agents. We\nformalize agents as policies solving language-grounded partially observable\nMarkov decision processes, which we term language decision processes. We then\nimplement five environments, including three challenging scientific\nenvironments: (1) manipulating DNA constructs for molecular cloning, (2)\nanswering research questions by accessing scientific literature, and (3)\nengineering protein stability. These environments were selected for their focus\non multi-step reasoning and their relevance to contemporary biology research.\nFinally, with online training and scaling inference-time compute, we show that\nlanguage agents backed by open-source, non-frontier LLMs can match and exceed\nboth frontier LLM agents and human experts on multiple tasks at up to 100x\nlower inference cost."
                },
                "authors": [
                    {
                        "name": "Siddharth Narayanan"
                    },
                    {
                        "name": "James D. Braza"
                    },
                    {
                        "name": "Ryan-Rhys Griffiths"
                    },
                    {
                        "name": "Manu Ponnapati"
                    },
                    {
                        "name": "Albert Bou"
                    },
                    {
                        "name": "Jon Laurent"
                    },
                    {
                        "name": "Ori Kabeli"
                    },
                    {
                        "name": "Geemi Wellawatte"
                    },
                    {
                        "name": "Sam Cox"
                    },
                    {
                        "name": "Samuel G. Rodriques"
                    },
                    {
                        "name": "Andrew D. White"
                    }
                ],
                "author_detail": {
                    "name": "Andrew D. White"
                },
                "author": "Andrew D. White",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05093v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05093v3",
                "updated": "2024-12-30T18:21:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    21,
                    8,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-09T14:34:32Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    34,
                    32,
                    4,
                    222,
                    0
                ],
                "title": "Order Matters in Hallucination: Reasoning Order as Benchmark and\n  Reflexive Prompting for Large-Language-Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Order Matters in Hallucination: Reasoning Order as Benchmark and\n  Reflexive Prompting for Large-Language-Models"
                },
                "summary": "Large language models (LLMs) have generated significant attention since their\ninception, finding applications across various academic and industrial domains.\nHowever, these models often suffer from the \"hallucination problem\", where\noutputs, though grammatically and logically coherent, lack factual accuracy or\nare entirely fabricated. A particularly troubling issue discovered and widely\ndiscussed recently is the numerical comparison error where multiple LLMs\nincorrectly infer that \"9.11$>$9.9\". We discovered that the order in which LLMs\ngenerate answers and reasoning impacts their consistency. Specifically, results\nvary significantly when an LLM generates an answer first and then provides the\nreasoning versus generating the reasoning process first and then the\nconclusion. Inspired by this, we propose a new benchmark method for assessing\nLLM consistency: comparing responses generated through these two different\napproaches. This benchmark effectively identifies instances where LLMs\nfabricate answers and subsequently generate justifications. Furthermore, we\nintroduce a novel and straightforward prompt strategy designed to mitigate this\nissue. Experimental results demonstrate that this strategy improves performance\nacross various LLMs compared to direct questioning. This work not only sheds\nlight on a critical flaw in LLMs but also offers a practical solution to\nenhance their reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have generated significant attention since their\ninception, finding applications across various academic and industrial domains.\nHowever, these models often suffer from the \"hallucination problem\", where\noutputs, though grammatically and logically coherent, lack factual accuracy or\nare entirely fabricated. A particularly troubling issue discovered and widely\ndiscussed recently is the numerical comparison error where multiple LLMs\nincorrectly infer that \"9.11$>$9.9\". We discovered that the order in which LLMs\ngenerate answers and reasoning impacts their consistency. Specifically, results\nvary significantly when an LLM generates an answer first and then provides the\nreasoning versus generating the reasoning process first and then the\nconclusion. Inspired by this, we propose a new benchmark method for assessing\nLLM consistency: comparing responses generated through these two different\napproaches. This benchmark effectively identifies instances where LLMs\nfabricate answers and subsequently generate justifications. Furthermore, we\nintroduce a novel and straightforward prompt strategy designed to mitigate this\nissue. Experimental results demonstrate that this strategy improves performance\nacross various LLMs compared to direct questioning. This work not only sheds\nlight on a critical flaw in LLMs but also offers a practical solution to\nenhance their reliability."
                },
                "authors": [
                    {
                        "name": "Zikai Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zikai Xie"
                },
                "author": "Zikai Xie",
                "arxiv_comment": "8 pages, submitted to ACL22025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05093v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05093v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21140v1",
                "updated": "2024-12-30T18:15:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    15,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:15:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    15,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Facilitating large language model Russian adaptation with Learned\n  Embedding Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facilitating large language model Russian adaptation with Learned\n  Embedding Propagation"
                },
                "summary": "Rapid advancements of large language model (LLM) technologies led to the\nintroduction of powerful open-source instruction-tuned LLMs that have the same\ntext generation quality as the state-of-the-art counterparts such as GPT-4.\nWhile the emergence of such models accelerates the adoption of LLM technologies\nin sensitive-information environments the authors of such models don not\ndisclose the training data necessary for replication of the results thus making\nthe achievements model-exclusive. Since those open-source models are also\nmultilingual this in turn reduces the benefits of training a language specific\nLLMs as improved inference computation efficiency becomes the only guaranteed\nadvantage of such costly procedure. More cost-efficient options such as\nvocabulary extension and subsequent continued pre-training are also inhibited\nby the lack of access to high-quality instruction-tuning data since it is the\nmajor factor behind the resulting LLM task-solving capabilities. To address the\nlimitations and cut the costs of the language adaptation pipeline we propose\nLearned Embedding Propagation (LEP). Unlike existing approaches our method has\nlower training data size requirements due to minimal impact on existing LLM\nknowledge which we reinforce using novel ad-hoc embedding propagation procedure\nthat allows to skip the instruction-tuning step and instead implant the new\nlanguage knowledge directly into any existing instruct-tuned variant. We\nevaluated four Russian vocabulary adaptations for LLaMa-3-8B and Mistral-7B,\nshowing that LEP is competitive with traditional instruction-tuning methods,\nachieving performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct, with\nfurther improvements via self-calibration and continued tuning enhancing\ntask-solving capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advancements of large language model (LLM) technologies led to the\nintroduction of powerful open-source instruction-tuned LLMs that have the same\ntext generation quality as the state-of-the-art counterparts such as GPT-4.\nWhile the emergence of such models accelerates the adoption of LLM technologies\nin sensitive-information environments the authors of such models don not\ndisclose the training data necessary for replication of the results thus making\nthe achievements model-exclusive. Since those open-source models are also\nmultilingual this in turn reduces the benefits of training a language specific\nLLMs as improved inference computation efficiency becomes the only guaranteed\nadvantage of such costly procedure. More cost-efficient options such as\nvocabulary extension and subsequent continued pre-training are also inhibited\nby the lack of access to high-quality instruction-tuning data since it is the\nmajor factor behind the resulting LLM task-solving capabilities. To address the\nlimitations and cut the costs of the language adaptation pipeline we propose\nLearned Embedding Propagation (LEP). Unlike existing approaches our method has\nlower training data size requirements due to minimal impact on existing LLM\nknowledge which we reinforce using novel ad-hoc embedding propagation procedure\nthat allows to skip the instruction-tuning step and instead implant the new\nlanguage knowledge directly into any existing instruct-tuned variant. We\nevaluated four Russian vocabulary adaptations for LLaMa-3-8B and Mistral-7B,\nshowing that LEP is competitive with traditional instruction-tuning methods,\nachieving performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct, with\nfurther improvements via self-calibration and continued tuning enhancing\ntask-solving capabilities."
                },
                "authors": [
                    {
                        "name": "Mikhail Tikhomirov"
                    },
                    {
                        "name": "Daniil Chernyshev"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Chernyshev"
                },
                "author": "Daniil Chernyshev",
                "arxiv_comment": "Preprint version of an article published in the Journal of Language\n  and Education. Copyright held by the owner/author(s). Publication rights\n  licensed to the Journal of Language and Education",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21123v1",
                "updated": "2024-12-30T17:52:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    52,
                    2,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T17:52:02Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    52,
                    2,
                    0,
                    365,
                    0
                ],
                "title": "ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language\n  Modeling Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language\n  Modeling Exploitation"
                },
                "summary": "As large language models (LLMs) increasingly depend on web-scraped datasets,\nconcerns over unauthorized use of copyrighted or personal content for training\nhave intensified. Despite regulations such as the General Data Protection\nRegulation (GDPR), data owners still have limited control over the use of their\ncontent in model training. To address this, we propose ExpShield, a proactive\nself-guard mechanism that empowers content owners to embed invisible\nperturbations into their text, limiting data misuse in LLMs training without\naffecting readability. This preemptive approach enables data owners to protect\nsensitive content directly, without relying on a third-party to perform\ndefense. Starting from the random perturbation, we demonstrate the rationale\nfor using perturbation to conceal protected content. We further enhance the\nefficiency by identifying memorization triggers and creating pitfalls to\ndiverge the model memorization in a more focused way. To validate our defense's\neffectiveness, we propose a novel metric of instance exploitation which\ncaptures the individual risk raised by model training. The experimental results\nvalidate the effectiveness of our approach as the MIA AUC decreases from 0.95\nto 0.55, and instance exploitation approaches zero. This suggests that the\nindividual risk does not increase after training, underscoring the significance\nof proactive defenses in protecting copyrighted data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly depend on web-scraped datasets,\nconcerns over unauthorized use of copyrighted or personal content for training\nhave intensified. Despite regulations such as the General Data Protection\nRegulation (GDPR), data owners still have limited control over the use of their\ncontent in model training. To address this, we propose ExpShield, a proactive\nself-guard mechanism that empowers content owners to embed invisible\nperturbations into their text, limiting data misuse in LLMs training without\naffecting readability. This preemptive approach enables data owners to protect\nsensitive content directly, without relying on a third-party to perform\ndefense. Starting from the random perturbation, we demonstrate the rationale\nfor using perturbation to conceal protected content. We further enhance the\nefficiency by identifying memorization triggers and creating pitfalls to\ndiverge the model memorization in a more focused way. To validate our defense's\neffectiveness, we propose a novel metric of instance exploitation which\ncaptures the individual risk raised by model training. The experimental results\nvalidate the effectiveness of our approach as the MIA AUC decreases from 0.95\nto 0.55, and instance exploitation approaches zero. This suggests that the\nindividual risk does not increase after training, underscoring the significance\nof proactive defenses in protecting copyrighted data."
                },
                "authors": [
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Toan Tran"
                    },
                    {
                        "name": "Tianhao Wang"
                    },
                    {
                        "name": "Hongsheng Hu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Li Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiong"
                },
                "author": "Li Xiong",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21102v1",
                "updated": "2024-12-30T17:25:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    25,
                    58,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T17:25:58Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    25,
                    58,
                    0,
                    365,
                    0
                ],
                "title": "Exploring and Controlling Diversity in LLM-Agent Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring and Controlling Diversity in LLM-Agent Conversation"
                },
                "summary": "Diversity is a critical aspect of multi-agent communication. In this paper,\nwe focus on controlling and exploring diversity in the context of open-domain\nmulti-agent conversations, particularly for world simulation applications. We\npropose Adaptive Prompt Pruning (APP), a novel method that dynamically adjusts\nthe content of the utterance generation prompt to control diversity using a\nsingle parameter, lambda. Through extensive experiments, we show that APP\neffectively controls the output diversity across models and datasets, with\npruning more information leading to more diverse output. We comprehensively\nanalyze the relationship between prompt content and conversational diversity.\nOur findings reveal that information from all components of the prompt\ngenerally constrains the diversity of the output, with the Memory block\nexerting the most significant influence. APP is compatible with established\ntechniques like temperature sampling and top-p sampling, providing a versatile\ntool for diversity management. To address the trade-offs of increased\ndiversity, such as inconsistencies with omitted information, we incorporate a\npost-generation correction step, which effectively balances diversity\nenhancement with output consistency. Additionally, we examine how prompt\nstructure, including component order and length, impacts diversity. This study\naddresses key questions surrounding diversity in multi-agent world simulation,\noffering insights into its control, influencing factors, and associated\ntrade-offs. Our contributions lay the foundation for systematically engineering\ndiversity in LLM-based multi-agent collaborations, advancing their\neffectiveness in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity is a critical aspect of multi-agent communication. In this paper,\nwe focus on controlling and exploring diversity in the context of open-domain\nmulti-agent conversations, particularly for world simulation applications. We\npropose Adaptive Prompt Pruning (APP), a novel method that dynamically adjusts\nthe content of the utterance generation prompt to control diversity using a\nsingle parameter, lambda. Through extensive experiments, we show that APP\neffectively controls the output diversity across models and datasets, with\npruning more information leading to more diverse output. We comprehensively\nanalyze the relationship between prompt content and conversational diversity.\nOur findings reveal that information from all components of the prompt\ngenerally constrains the diversity of the output, with the Memory block\nexerting the most significant influence. APP is compatible with established\ntechniques like temperature sampling and top-p sampling, providing a versatile\ntool for diversity management. To address the trade-offs of increased\ndiversity, such as inconsistencies with omitted information, we incorporate a\npost-generation correction step, which effectively balances diversity\nenhancement with output consistency. Additionally, we examine how prompt\nstructure, including component order and length, impacts diversity. This study\naddresses key questions surrounding diversity in multi-agent world simulation,\noffering insights into its control, influencing factors, and associated\ntrade-offs. Our contributions lay the foundation for systematically engineering\ndiversity in LLM-based multi-agent collaborations, advancing their\neffectiveness in real-world applications."
                },
                "authors": [
                    {
                        "name": "KuanChao Chu"
                    },
                    {
                        "name": "Yi-Pei Chen"
                    },
                    {
                        "name": "Hideki Nakayama"
                    }
                ],
                "author_detail": {
                    "name": "Hideki Nakayama"
                },
                "author": "Hideki Nakayama",
                "arxiv_comment": "Accepted for the AAAI 2025 Workshop on Advancing LLM-Based\n  Multi-Agent Collaboration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21080v1",
                "updated": "2024-12-30T16:57:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    57,
                    5,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T16:57:05Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    57,
                    5,
                    0,
                    365,
                    0
                ],
                "title": "Vinci: A Real-time Embodied Smart Assistant based on Egocentric\n  Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vinci: A Real-time Embodied Smart Assistant based on Egocentric\n  Vision-Language Model"
                },
                "summary": "We introduce Vinci, a real-time embodied smart assistant built upon an\negocentric vision-language model. Designed for deployment on portable devices\nsuch as smartphones and wearable cameras, Vinci operates in an \"always on\"\nmode, continuously observing the environment to deliver seamless interaction\nand assistance. Users can wake up the system and engage in natural\nconversations to ask questions or seek assistance, with responses delivered\nthrough audio for hands-free convenience. With its ability to process long\nvideo streams in real-time, Vinci can answer user queries about current\nobservations and historical context while also providing task planning based on\npast interactions. To further enhance usability, Vinci integrates a video\ngeneration module that creates step-by-step visual demonstrations for tasks\nthat require detailed guidance. We hope that Vinci can establish a robust\nframework for portable, real-time egocentric AI systems, empowering users with\ncontextual and actionable insights. We release the complete implementation for\nthe development of the device in conjunction with a demo web platform to test\nuploaded videos at https://github.com/OpenGVLab/vinci.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Vinci, a real-time embodied smart assistant built upon an\negocentric vision-language model. Designed for deployment on portable devices\nsuch as smartphones and wearable cameras, Vinci operates in an \"always on\"\nmode, continuously observing the environment to deliver seamless interaction\nand assistance. Users can wake up the system and engage in natural\nconversations to ask questions or seek assistance, with responses delivered\nthrough audio for hands-free convenience. With its ability to process long\nvideo streams in real-time, Vinci can answer user queries about current\nobservations and historical context while also providing task planning based on\npast interactions. To further enhance usability, Vinci integrates a video\ngeneration module that creates step-by-step visual demonstrations for tasks\nthat require detailed guidance. We hope that Vinci can establish a robust\nframework for portable, real-time egocentric AI systems, empowering users with\ncontextual and actionable insights. We release the complete implementation for\nthe development of the device in conjunction with a demo web platform to test\nuploaded videos at https://github.com/OpenGVLab/vinci."
                },
                "authors": [
                    {
                        "name": "Yifei Huang"
                    },
                    {
                        "name": "Jilan Xu"
                    },
                    {
                        "name": "Baoqi Pei"
                    },
                    {
                        "name": "Yuping He"
                    },
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "Lijin Yang"
                    },
                    {
                        "name": "Xinyuan Chen"
                    },
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zheng Nie"
                    },
                    {
                        "name": "Jinyao Liu"
                    },
                    {
                        "name": "Guoshun Fan"
                    },
                    {
                        "name": "Dechen Lin"
                    },
                    {
                        "name": "Fang Fang"
                    },
                    {
                        "name": "Kunpeng Li"
                    },
                    {
                        "name": "Chang Yuan"
                    },
                    {
                        "name": "Yali Wang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09807v2",
                "updated": "2024-12-30T16:45:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    45,
                    50,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-13T02:48:36Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    48,
                    36,
                    4,
                    348,
                    0
                ],
                "title": "LLM Distillation for Efficient Few-Shot Multiple Choice Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Distillation for Efficient Few-Shot Multiple Choice Question\n  Answering"
                },
                "summary": "Multiple Choice Question Answering (MCQA) is an important problem with\nnumerous real-world applications, such as medicine, law, and education. The\nhigh cost of building MCQA datasets makes few-shot learning pivotal in this\ndomain. While Large Language Models (LLMs) can enable few-shot learning, their\ndirect application in real-world scenarios is often hindered by their high\ncomputational cost. To address this challenge, we propose a simple yet\neffective approach that uses LLMs for data generation and scoring. Our approach\nutilizes LLMs to create MCQA data which contains questions and choices, and to\nassign probability scores to the generated choices. We then use the generated\ndata and LLM-assigned scores to finetune a smaller and more efficient\nencoder-only model, DeBERTa-v3-base by leveraging distillation loss. Extensive\nexperiments on the Massive Multitask Language Understanding (MMLU) benchmark\ndemonstrate that our method improves accuracy from 28.9% to 39.3%, representing\na gain of over 10% compared to a baseline finetuned directly on 5-shot\nexamples. This shows the effectiveness of LLM-driven data generation and\nknowledge distillation for few-shot MCQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple Choice Question Answering (MCQA) is an important problem with\nnumerous real-world applications, such as medicine, law, and education. The\nhigh cost of building MCQA datasets makes few-shot learning pivotal in this\ndomain. While Large Language Models (LLMs) can enable few-shot learning, their\ndirect application in real-world scenarios is often hindered by their high\ncomputational cost. To address this challenge, we propose a simple yet\neffective approach that uses LLMs for data generation and scoring. Our approach\nutilizes LLMs to create MCQA data which contains questions and choices, and to\nassign probability scores to the generated choices. We then use the generated\ndata and LLM-assigned scores to finetune a smaller and more efficient\nencoder-only model, DeBERTa-v3-base by leveraging distillation loss. Extensive\nexperiments on the Massive Multitask Language Understanding (MMLU) benchmark\ndemonstrate that our method improves accuracy from 28.9% to 39.3%, representing\na gain of over 10% compared to a baseline finetuned directly on 5-shot\nexamples. This shows the effectiveness of LLM-driven data generation and\nknowledge distillation for few-shot MCQA."
                },
                "authors": [
                    {
                        "name": "Patrick Sutanto"
                    },
                    {
                        "name": "Joan Santoso"
                    },
                    {
                        "name": "Esther Irawati Setiawan"
                    },
                    {
                        "name": "Aji Prasetya Wibawa"
                    }
                ],
                "author_detail": {
                    "name": "Aji Prasetya Wibawa"
                },
                "author": "Aji Prasetya Wibawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21065v1",
                "updated": "2024-12-30T16:34:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    34,
                    11,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T16:34:11Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    34,
                    11,
                    0,
                    365,
                    0
                ],
                "title": "Efficient Multi-Task Inferencing with a Shared Backbone and Lightweight\n  Task-Specific Adapters for Automatic Scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Multi-Task Inferencing with a Shared Backbone and Lightweight\n  Task-Specific Adapters for Automatic Scoring"
                },
                "summary": "The integration of Artificial Intelligence (AI) in education requires\nscalable and efficient frameworks that balance performance, adaptability, and\ncost. This paper addresses these needs by proposing a shared backbone model\narchitecture enhanced with lightweight LoRA adapters for task-specific\nfine-tuning, targeting the automated scoring of student responses across 27\nmutually exclusive tasks. By achieving competitive performance (average QWK of\n0.848 compared to 0.888 for fully fine-tuned models) while reducing GPU memory\nconsumption by 60% and inference latency by 40%, the framework demonstrates\nsignificant efficiency gains. This approach aligns with the workshops' focus on\nimproving language models for educational tasks, creating responsible\ninnovations for cost-sensitive deployment, and supporting educators by\nstreamlining assessment workflows. The findings underscore the potential of\nscalable AI to enhance learning outcomes while maintaining fairness and\ntransparency in automated scoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Artificial Intelligence (AI) in education requires\nscalable and efficient frameworks that balance performance, adaptability, and\ncost. This paper addresses these needs by proposing a shared backbone model\narchitecture enhanced with lightweight LoRA adapters for task-specific\nfine-tuning, targeting the automated scoring of student responses across 27\nmutually exclusive tasks. By achieving competitive performance (average QWK of\n0.848 compared to 0.888 for fully fine-tuned models) while reducing GPU memory\nconsumption by 60% and inference latency by 40%, the framework demonstrates\nsignificant efficiency gains. This approach aligns with the workshops' focus on\nimproving language models for educational tasks, creating responsible\ninnovations for cost-sensitive deployment, and supporting educators by\nstreamlining assessment workflows. The findings underscore the potential of\nscalable AI to enhance learning outcomes while maintaining fairness and\ntransparency in automated scoring systems."
                },
                "authors": [
                    {
                        "name": "Ehsan Latif"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Zhai"
                },
                "author": "Xiaoming Zhai",
                "arxiv_comment": "Accepted by AAAI-iRAISE Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17498v2",
                "updated": "2024-12-30T16:29:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    29,
                    36,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-23T11:55:33Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    55,
                    33,
                    0,
                    358,
                    0
                ],
                "title": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought"
                },
                "summary": "Recently, O1-like models have emerged as representative examples,\nillustrating the effectiveness of long chain-of-thought (CoT) in reasoning\ntasks such as math and coding tasks. In this paper, we introduce DRT-o1, an\nattempt to bring the success of long CoT to neural machine translation (MT).\nSpecifically, in view of the literature books that might involve similes and\nmetaphors, translating these texts to a target language is very difficult in\npractice due to cultural differences. In such cases, literal translation often\nfails to convey the intended meaning effectively. Even for professional human\ntranslators, considerable thought must be given to preserving semantics\nthroughout the translation process. To simulate LLMs' long thought ability in\nMT, we first mine sentences containing similes or metaphors from existing\nliterature books, and then develop a multi-agent framework to translate these\nsentences via long thought. In the multi-agent framework, a translator is used\nto iteratively translate the source sentence under the suggestions provided by\nan advisor. To ensure the effectiveness of the long thoughts, an evaluator is\nalso employed to quantify the translation in each round. In this way, we\ncollect tens of thousands of long-thought MT data, which is used to train our\nDRT-o1. Using Qwen2.5 and LLama-3.1 as the backbones, DRT-o1 models can learn\nthe thought process during machine translation, and outperform vanilla LLMs as\nwell as existing O1-like LLMs, showing their effectiveness The project is\navailable at https://github.com/krystalan/DRT-o1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, O1-like models have emerged as representative examples,\nillustrating the effectiveness of long chain-of-thought (CoT) in reasoning\ntasks such as math and coding tasks. In this paper, we introduce DRT-o1, an\nattempt to bring the success of long CoT to neural machine translation (MT).\nSpecifically, in view of the literature books that might involve similes and\nmetaphors, translating these texts to a target language is very difficult in\npractice due to cultural differences. In such cases, literal translation often\nfails to convey the intended meaning effectively. Even for professional human\ntranslators, considerable thought must be given to preserving semantics\nthroughout the translation process. To simulate LLMs' long thought ability in\nMT, we first mine sentences containing similes or metaphors from existing\nliterature books, and then develop a multi-agent framework to translate these\nsentences via long thought. In the multi-agent framework, a translator is used\nto iteratively translate the source sentence under the suggestions provided by\nan advisor. To ensure the effectiveness of the long thoughts, an evaluator is\nalso employed to quantify the translation in each round. In this way, we\ncollect tens of thousands of long-thought MT data, which is used to train our\nDRT-o1. Using Qwen2.5 and LLama-3.1 as the backbones, DRT-o1 models can learn\nthe thought process during machine translation, and outperform vanilla LLMs as\nwell as existing O1-like LLMs, showing their effectiveness The project is\navailable at https://github.com/krystalan/DRT-o1"
                },
                "authors": [
                    {
                        "name": "Jiaan Wang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Yunlong Liang"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21052v1",
                "updated": "2024-12-30T16:09:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    9,
                    33,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T16:09:33Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    9,
                    33,
                    0,
                    365,
                    0
                ],
                "title": "Towards Effective Discrimination Testing for Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Effective Discrimination Testing for Generative AI"
                },
                "summary": "Generative AI (GenAI) models present new challenges in regulating against\ndiscriminatory behavior. In this paper, we argue that GenAI fairness research\nstill has not met these challenges; instead, a significant gap remains between\nexisting bias assessment methods and regulatory goals. This leads to\nineffective regulation that can allow deployment of reportedly fair, yet\nactually discriminatory, GenAI systems. Towards remedying this problem, we\nconnect the legal and technical literature around GenAI bias evaluation and\nidentify areas of misalignment. Through four case studies, we demonstrate how\nthis misalignment between fairness testing techniques and regulatory goals can\nresult in discriminatory outcomes in real-world deployments, especially in\nadaptive or complex environments. We offer practical recommendations for\nimproving discrimination testing to better align with regulatory goals and\nenhance the reliability of fairness assessments in future deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) models present new challenges in regulating against\ndiscriminatory behavior. In this paper, we argue that GenAI fairness research\nstill has not met these challenges; instead, a significant gap remains between\nexisting bias assessment methods and regulatory goals. This leads to\nineffective regulation that can allow deployment of reportedly fair, yet\nactually discriminatory, GenAI systems. Towards remedying this problem, we\nconnect the legal and technical literature around GenAI bias evaluation and\nidentify areas of misalignment. Through four case studies, we demonstrate how\nthis misalignment between fairness testing techniques and regulatory goals can\nresult in discriminatory outcomes in real-world deployments, especially in\nadaptive or complex environments. We offer practical recommendations for\nimproving discrimination testing to better align with regulatory goals and\nenhance the reliability of fairness assessments in future deployments."
                },
                "authors": [
                    {
                        "name": "Thomas P. Zollo"
                    },
                    {
                        "name": "Nikita Rajaneesh"
                    },
                    {
                        "name": "Richard Zemel"
                    },
                    {
                        "name": "Talia B. Gillis"
                    },
                    {
                        "name": "Emily Black"
                    }
                ],
                "author_detail": {
                    "name": "Emily Black"
                },
                "author": "Emily Black",
                "arxiv_comment": "38 pages, 9 tables, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21051v1",
                "updated": "2024-12-30T16:09:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    9,
                    28,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T16:09:28Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    9,
                    28,
                    0,
                    365,
                    0
                ],
                "title": "Toward Intelligent and Secure Cloud: Large Language Model Empowered\n  Proactive Defense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Intelligent and Secure Cloud: Large Language Model Empowered\n  Proactive Defense"
                },
                "summary": "The rapid evolution of cloud computing technologies and the increasing number\nof cloud applications have provided a large number of benefits in daily lives.\nHowever, the diversity and complexity of different components pose a\nsignificant challenge to cloud security, especially when dealing with\nsophisticated and advanced cyberattacks. Recent advancements in generative\nfoundation models (GFMs), particularly in the large language models (LLMs),\noffer promising solutions for security intelligence. By exploiting the powerful\nabilities in language understanding, data analysis, task inference, action\nplanning, and code generation, we present LLM-PD, a novel proactive defense\narchitecture that defeats various threats in a proactive manner. LLM-PD can\nefficiently make a decision through comprehensive data analysis and sequential\nreasoning, as well as dynamically creating and deploying actionable defense\nmechanisms on the target cloud. Furthermore, it can flexibly self-evolve based\non experience learned from previous interactions and adapt to new attack\nscenarios without additional training. The experimental results demonstrate its\nremarkable ability in terms of defense effectiveness and efficiency,\nparticularly highlighting an outstanding success rate when compared with other\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of cloud computing technologies and the increasing number\nof cloud applications have provided a large number of benefits in daily lives.\nHowever, the diversity and complexity of different components pose a\nsignificant challenge to cloud security, especially when dealing with\nsophisticated and advanced cyberattacks. Recent advancements in generative\nfoundation models (GFMs), particularly in the large language models (LLMs),\noffer promising solutions for security intelligence. By exploiting the powerful\nabilities in language understanding, data analysis, task inference, action\nplanning, and code generation, we present LLM-PD, a novel proactive defense\narchitecture that defeats various threats in a proactive manner. LLM-PD can\nefficiently make a decision through comprehensive data analysis and sequential\nreasoning, as well as dynamically creating and deploying actionable defense\nmechanisms on the target cloud. Furthermore, it can flexibly self-evolve based\non experience learned from previous interactions and adapt to new attack\nscenarios without additional training. The experimental results demonstrate its\nremarkable ability in terms of defense effectiveness and efficiency,\nparticularly highlighting an outstanding success rate when compared with other\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Yuyang Zhou"
                    },
                    {
                        "name": "Guang Cheng"
                    },
                    {
                        "name": "Kang Du"
                    },
                    {
                        "name": "Zihan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Chen"
                },
                "author": "Zihan Chen",
                "arxiv_comment": "7 pages; In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21037v1",
                "updated": "2024-12-30T16:02:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    2,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T16:02:44Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    2,
                    44,
                    0,
                    365,
                    0
                ],
                "title": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow\n  Matching and Clap-Ranked Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow\n  Matching and Clap-Ranked Preference Optimization"
                },
                "summary": "We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model\nwith 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio\nin just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models\nlies in the difficulty of creating preference pairs, as TTA lacks structured\nmechanisms like verifiable rewards or gold-standard answers available for Large\nLanguage Models (LLMs). To address this, we propose CLAP-Ranked Preference\nOptimization (CRPO), a novel framework that iteratively generates and optimizes\npreference data to enhance TTA alignment. We demonstrate that the audio\npreference dataset generated using CRPO outperforms existing alternatives. With\nthis framework, TangoFlux achieves state-of-the-art performance across both\nobjective and subjective benchmarks. We open source all code and models to\nsupport further research in TTA generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model\nwith 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio\nin just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models\nlies in the difficulty of creating preference pairs, as TTA lacks structured\nmechanisms like verifiable rewards or gold-standard answers available for Large\nLanguage Models (LLMs). To address this, we propose CLAP-Ranked Preference\nOptimization (CRPO), a novel framework that iteratively generates and optimizes\npreference data to enhance TTA alignment. We demonstrate that the audio\npreference dataset generated using CRPO outperforms existing alternatives. With\nthis framework, TangoFlux achieves state-of-the-art performance across both\nobjective and subjective benchmarks. We open source all code and models to\nsupport further research in TTA generation."
                },
                "authors": [
                    {
                        "name": "Chia-Yu Hung"
                    },
                    {
                        "name": "Navonil Majumder"
                    },
                    {
                        "name": "Zhifeng Kong"
                    },
                    {
                        "name": "Ambuj Mehrish"
                    },
                    {
                        "name": "Rafael Valle"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "arxiv_comment": "https://tangoflux.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15639v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15639v3",
                "updated": "2024-12-30T15:59:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    59,
                    18,
                    0,
                    365,
                    0
                ],
                "published": "2024-04-24T04:25:04Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    4,
                    25,
                    4,
                    2,
                    115,
                    0
                ],
                "title": "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models\n  of Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models\n  of Code"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable progress in code\ngeneration. It now becomes crucial to identify whether the code is AI-generated\nand to determine the specific model used, particularly for purposes such as\nprotecting Intellectual Property (IP) in industry and preventing cheating in\nprogramming exercises. To this end, several attempts have been made to insert\nwatermarks into machine-generated code. However, existing approaches are\nlimited to inserting only a single bit of information. In this paper, we\nintroduce CodeIP, a novel multi-bit watermarking technique that inserts\nadditional information to preserve crucial provenance details, such as the\nvendor ID of an LLM, thereby safeguarding the IPs of LLMs in code generation.\nFurthermore, to ensure the syntactical correctness of the generated code, we\npropose constraining the sampling process for predicting the next token by\ntraining a type predictor. Experiments conducted on a real-world dataset across\nfive programming languages demonstrate the effectiveness of CodeIP in\nwatermarking LLMs for code generation while maintaining the syntactical\ncorrectness of code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable progress in code\ngeneration. It now becomes crucial to identify whether the code is AI-generated\nand to determine the specific model used, particularly for purposes such as\nprotecting Intellectual Property (IP) in industry and preventing cheating in\nprogramming exercises. To this end, several attempts have been made to insert\nwatermarks into machine-generated code. However, existing approaches are\nlimited to inserting only a single bit of information. In this paper, we\nintroduce CodeIP, a novel multi-bit watermarking technique that inserts\nadditional information to preserve crucial provenance details, such as the\nvendor ID of an LLM, thereby safeguarding the IPs of LLMs in code generation.\nFurthermore, to ensure the syntactical correctness of the generated code, we\npropose constraining the sampling process for predicting the next token by\ntraining a type predictor. Experiments conducted on a real-world dataset across\nfive programming languages demonstrate the effectiveness of CodeIP in\nwatermarking LLMs for code generation while maintaining the syntactical\ncorrectness of code."
                },
                "authors": [
                    {
                        "name": "Batu Guan"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Zhangqian Bi"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "16 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15639v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15639v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21033v1",
                "updated": "2024-12-30T15:58:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    58,
                    41,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:58:41Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    58,
                    41,
                    0,
                    365,
                    0
                ],
                "title": "Plancraft: an evaluation dataset for planning with LLM agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plancraft: an evaluation dataset for planning with LLM agents"
                },
                "summary": "We present Plancraft, a multi-modal evaluation dataset for LLM agents.\nPlancraft has both a text-only and multi-modal interface, based on the\nMinecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and\nRetrieval Augmented Generation (RAG), as well as an oracle planner and oracle\nRAG information extractor, to ablate the different components of a modern agent\narchitecture. To evaluate decision-making, Plancraft also includes a subset of\nexamples that are intentionally unsolvable, providing a realistic challenge\nthat requires the agent not only to complete tasks but also to decide whether\nthey are solvable at all. We benchmark both open-source and closed-source LLMs\nand strategies on our task and compare their performance to a handcrafted\nplanner. We find that LLMs and VLMs struggle with the planning problems that\nPlancraft introduces, and we offer suggestions on how to improve their\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Plancraft, a multi-modal evaluation dataset for LLM agents.\nPlancraft has both a text-only and multi-modal interface, based on the\nMinecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and\nRetrieval Augmented Generation (RAG), as well as an oracle planner and oracle\nRAG information extractor, to ablate the different components of a modern agent\narchitecture. To evaluate decision-making, Plancraft also includes a subset of\nexamples that are intentionally unsolvable, providing a realistic challenge\nthat requires the agent not only to complete tasks but also to decide whether\nthey are solvable at all. We benchmark both open-source and closed-source LLMs\nand strategies on our task and compare their performance to a handcrafted\nplanner. We find that LLMs and VLMs struggle with the planning problems that\nPlancraft introduces, and we offer suggestions on how to improve their\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Gautier Dagan"
                    },
                    {
                        "name": "Frank Keller"
                    },
                    {
                        "name": "Alex Lascarides"
                    }
                ],
                "author_detail": {
                    "name": "Alex Lascarides"
                },
                "author": "Alex Lascarides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21016v1",
                "updated": "2024-12-30T15:33:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    34,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:34Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    34,
                    0,
                    365,
                    0
                ],
                "title": "Automated Robustness Testing for LLM-based NLP Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Robustness Testing for LLM-based NLP Software"
                },
                "summary": "Benefiting from the advancements in LLMs, NLP software has undergone rapid\ndevelopment. Such software is widely employed in various safety-critical tasks,\nsuch as financial sentiment analysis, toxic content moderation, and log\ngeneration. To our knowledge, there are no known automated robustness testing\nmethods specifically designed for LLM-based NLP software. Given the complexity\nof LLMs and the unpredictability of real-world inputs (including prompts and\nexamples), it is essential to examine the robustness of overall inputs to\nensure the safety of such software.\n  To this end, this paper introduces the first AutOmated Robustness Testing\nfrAmework, AORTA, which reconceptualizes the testing process into a\ncombinatorial optimization problem. Existing testing methods designed for\nDNN-based software can be applied to LLM-based software by AORTA, but their\neffectiveness is limited. To address this, we propose a novel testing method\nfor LLM-based software within AORTA called Adaptive Beam Search. ABS is\ntailored for the expansive feature space of LLMs and improves testing\neffectiveness through an adaptive beam width and the capability for\nbacktracking.\n  We successfully embed 18 test methods in the designed framework AORTA and\ncompared the test validity of ABS with three datasets and five threat models.\nABS facilitates a more comprehensive and accurate robustness assessment before\nsoftware deployment, with an average test success rate of 86.138%. Compared to\nthe currently best-performing baseline PWWS, ABS significantly reduces the\ncomputational overhead by up to 3441.895 seconds per successful test case and\ndecreases the number of queries by 218.762 times on average. Furthermore, test\ncases generated by ABS exhibit greater naturalness and transferability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the advancements in LLMs, NLP software has undergone rapid\ndevelopment. Such software is widely employed in various safety-critical tasks,\nsuch as financial sentiment analysis, toxic content moderation, and log\ngeneration. To our knowledge, there are no known automated robustness testing\nmethods specifically designed for LLM-based NLP software. Given the complexity\nof LLMs and the unpredictability of real-world inputs (including prompts and\nexamples), it is essential to examine the robustness of overall inputs to\nensure the safety of such software.\n  To this end, this paper introduces the first AutOmated Robustness Testing\nfrAmework, AORTA, which reconceptualizes the testing process into a\ncombinatorial optimization problem. Existing testing methods designed for\nDNN-based software can be applied to LLM-based software by AORTA, but their\neffectiveness is limited. To address this, we propose a novel testing method\nfor LLM-based software within AORTA called Adaptive Beam Search. ABS is\ntailored for the expansive feature space of LLMs and improves testing\neffectiveness through an adaptive beam width and the capability for\nbacktracking.\n  We successfully embed 18 test methods in the designed framework AORTA and\ncompared the test validity of ABS with three datasets and five threat models.\nABS facilitates a more comprehensive and accurate robustness assessment before\nsoftware deployment, with an average test success rate of 86.138%. Compared to\nthe currently best-performing baseline PWWS, ABS significantly reduces the\ncomputational overhead by up to 3441.895 seconds per successful test case and\ndecreases the number of queries by 218.762 times on average. Furthermore, test\ncases generated by ABS exhibit greater naturalness and transferability."
                },
                "authors": [
                    {
                        "name": "Mingxuan Xiao"
                    },
                    {
                        "name": "Yan Xiao"
                    },
                    {
                        "name": "Shunhui Ji"
                    },
                    {
                        "name": "Hanbo Cai"
                    },
                    {
                        "name": "Lei Xue"
                    },
                    {
                        "name": "Pengcheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Pengcheng Zhang"
                },
                "author": "Pengcheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19312v2",
                "updated": "2024-12-30T15:30:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    30,
                    23,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-26T18:19:53Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    19,
                    53,
                    3,
                    361,
                    0
                ],
                "title": "From Interests to Insights: An LLM Approach to Course Recommendations\n  Using Natural Language Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Interests to Insights: An LLM Approach to Course Recommendations\n  Using Natural Language Queries"
                },
                "summary": "Most universities in the United States encourage their students to explore\nacademic areas before declaring a major and to acquire academic breadth by\nsatisfying a variety of requirements. Each term, students must choose among\nmany thousands of offerings, spanning dozens of subject areas, a handful of\ncourses to take. The curricular environment is also dynamic, and poor\ncommunication and search functions on campus can limit a student's ability to\ndiscover new courses of interest. To support both students and their advisers\nin such a setting, we explore a novel Large Language Model (LLM) course\nrecommendation system that applies a Retrieval Augmented Generation (RAG)\nmethod to the corpus of course descriptions. The system first generates an\n'ideal' course description based on the user's query. This description is\nconverted into a search vector using embeddings, which is then used to find\nactual courses with similar content by comparing embedding similarities. We\ndescribe the method and assess the quality and fairness of some example\nprompts. Steps to deploy a pilot system on campus are discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most universities in the United States encourage their students to explore\nacademic areas before declaring a major and to acquire academic breadth by\nsatisfying a variety of requirements. Each term, students must choose among\nmany thousands of offerings, spanning dozens of subject areas, a handful of\ncourses to take. The curricular environment is also dynamic, and poor\ncommunication and search functions on campus can limit a student's ability to\ndiscover new courses of interest. To support both students and their advisers\nin such a setting, we explore a novel Large Language Model (LLM) course\nrecommendation system that applies a Retrieval Augmented Generation (RAG)\nmethod to the corpus of course descriptions. The system first generates an\n'ideal' course description based on the user's query. This description is\nconverted into a search vector using embeddings, which is then used to find\nactual courses with similar content by comparing embedding similarities. We\ndescribe the method and assess the quality and fairness of some example\nprompts. Steps to deploy a pilot system on campus are discussed."
                },
                "authors": [
                    {
                        "name": "Hugh Van Deventer"
                    },
                    {
                        "name": "Mark Mills"
                    },
                    {
                        "name": "August Evrard"
                    }
                ],
                "author_detail": {
                    "name": "August Evrard"
                },
                "author": "August Evrard",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08603v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08603v3",
                "updated": "2024-12-30T15:08:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    8,
                    23,
                    0,
                    365,
                    0
                ],
                "published": "2024-05-14T13:42:05Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    13,
                    42,
                    5,
                    1,
                    135,
                    0
                ],
                "title": "A Comprehensive Survey of Large Language Models and Multimodal Large\n  Language Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Large Language Models and Multimodal Large\n  Language Models in Medicine"
                },
                "summary": "Since the release of ChatGPT and GPT-4, large language models (LLMs) and\nmultimodal large language models (MLLMs) have attracted widespread attention\nfor their exceptional capabilities in understanding, reasoning, and generation,\nintroducing transformative paradigms for integrating artificial intelligence\ninto medicine. This survey provides a comprehensive overview of the\ndevelopment, principles, application scenarios, challenges, and future\ndirections of LLMs and MLLMs in medicine. Specifically, it begins by examining\nthe paradigm shift, tracing the transition from traditional models to LLMs and\nMLLMs, and highlighting the unique advantages of these LLMs and MLLMs in\nmedical applications. Next, the survey reviews existing medical LLMs and MLLMs,\nproviding detailed guidance on their construction and evaluation in a clear and\nsystematic manner. Subsequently, to underscore the substantial value of LLMs\nand MLLMs in healthcare, the survey explores five promising applications in the\nfield. Finally, the survey addresses the challenges confronting medical LLMs\nand MLLMs and proposes practical strategies and future directions for their\nintegration into medicine. In summary, this survey offers a comprehensive\nanalysis of the technical methodologies and practical clinical applications of\nmedical LLMs and MLLMs, with the goal of bridging the gap between these\nadvanced technologies and clinical practice, thereby fostering the evolution of\nthe next generation of intelligent healthcare systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the release of ChatGPT and GPT-4, large language models (LLMs) and\nmultimodal large language models (MLLMs) have attracted widespread attention\nfor their exceptional capabilities in understanding, reasoning, and generation,\nintroducing transformative paradigms for integrating artificial intelligence\ninto medicine. This survey provides a comprehensive overview of the\ndevelopment, principles, application scenarios, challenges, and future\ndirections of LLMs and MLLMs in medicine. Specifically, it begins by examining\nthe paradigm shift, tracing the transition from traditional models to LLMs and\nMLLMs, and highlighting the unique advantages of these LLMs and MLLMs in\nmedical applications. Next, the survey reviews existing medical LLMs and MLLMs,\nproviding detailed guidance on their construction and evaluation in a clear and\nsystematic manner. Subsequently, to underscore the substantial value of LLMs\nand MLLMs in healthcare, the survey explores five promising applications in the\nfield. Finally, the survey addresses the challenges confronting medical LLMs\nand MLLMs and proposes practical strategies and future directions for their\nintegration into medicine. In summary, this survey offers a comprehensive\nanalysis of the technical methodologies and practical clinical applications of\nmedical LLMs and MLLMs, with the goal of bridging the gap between these\nadvanced technologies and clinical practice, thereby fostering the evolution of\nthe next generation of intelligent healthcare systems."
                },
                "authors": [
                    {
                        "name": "Hanguang Xiao"
                    },
                    {
                        "name": "Feizhong Zhou"
                    },
                    {
                        "name": "Xingyue Liu"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Zhipeng Li"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xiaoxuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxuan Huang"
                },
                "author": "Xiaoxuan Huang",
                "arxiv_doi": "10.1016/j.inffus.2024.102888",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.inffus.2024.102888",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.08603v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08603v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Information Fusion, 117 (2025) 102888",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20996v1",
                "updated": "2024-12-30T15:01:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    1,
                    48,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:01:48Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    1,
                    48,
                    0,
                    365,
                    0
                ],
                "title": "Plug-and-Play Training Framework for Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play Training Framework for Preference Optimization"
                },
                "summary": "Recently, preference optimization methods such as DPO have significantly\nenhanced large language models (LLMs) in wide tasks including dialogue and\nquestion-answering. However, current methods fail to account for the varying\ndifficulty levels of training samples during preference optimization, leading\nto mediocre performance in tasks with high accuracy requirements, particularly\nin mathematical reasoning. To address this limitation, we propose a novel\ntraining framework, which employs multiple sampling to analyze output\ndistributions, assign different weights to samples, and incorporate these\nweights into the preference optimization process. This plug-and-play approach\nenables LLMs to prioritize challenging examples during training, improving\nlearning efficiency. Experimental results demonstrate that our framework\nintegrates seamlessly with various preference optimization methods and achieves\nconsistent improvements in mathematical reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, preference optimization methods such as DPO have significantly\nenhanced large language models (LLMs) in wide tasks including dialogue and\nquestion-answering. However, current methods fail to account for the varying\ndifficulty levels of training samples during preference optimization, leading\nto mediocre performance in tasks with high accuracy requirements, particularly\nin mathematical reasoning. To address this limitation, we propose a novel\ntraining framework, which employs multiple sampling to analyze output\ndistributions, assign different weights to samples, and incorporate these\nweights into the preference optimization process. This plug-and-play approach\nenables LLMs to prioritize challenging examples during training, improving\nlearning efficiency. Experimental results demonstrate that our framework\nintegrates seamlessly with various preference optimization methods and achieves\nconsistent improvements in mathematical reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Jingyuan Ma"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "arxiv_comment": "12 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20995v1",
                "updated": "2024-12-30T14:58:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    58,
                    46,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T14:58:46Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    58,
                    46,
                    0,
                    365,
                    0
                ],
                "title": "KARPA: A Training-free Method of Adapting Knowledge Graph as References\n  for Large Language Model's Reasoning Path Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KARPA: A Training-free Method of Adapting Knowledge Graph as References\n  for Large Language Model's Reasoning Path Aggregation"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance across a\nvariety of tasks, yet they are often affected by hallucinations and the\ntimeliness of knowledge. Leveraging knowledge graphs (KGs) as external\nknowledge sources has emerged as a viable solution, but existing methods for\nLLM-based knowledge graph question answering (KGQA) are often limited by\nstep-by-step decision-making on KGs, restricting the global planning and\nreasoning capabilities of LLMs, or they require fine-tuning or pre-training on\nspecific KGs. To address these challenges, we propose Knowledge graph Assisted\nReasoning Path Aggregation (KARPA), a novel framework that harnesses the global\nplanning abilities of LLMs for efficient and accurate KG reasoning. KARPA\noperates in three steps: pre-planning relation paths using the LLM's global\nplanning capabilities, matching semantically relevant paths via an embedding\nmodel, and reasoning over these paths to generate answers. Unlike existing KGQA\nmethods, KARPA avoids stepwise traversal, requires no additional training, and\nis adaptable to various LLM architectures. Extensive experimental results show\nthat KARPA achieves state-of-the-art performance in KGQA tasks, delivering both\nhigh efficiency and accuracy. Our code will be available on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance across a\nvariety of tasks, yet they are often affected by hallucinations and the\ntimeliness of knowledge. Leveraging knowledge graphs (KGs) as external\nknowledge sources has emerged as a viable solution, but existing methods for\nLLM-based knowledge graph question answering (KGQA) are often limited by\nstep-by-step decision-making on KGs, restricting the global planning and\nreasoning capabilities of LLMs, or they require fine-tuning or pre-training on\nspecific KGs. To address these challenges, we propose Knowledge graph Assisted\nReasoning Path Aggregation (KARPA), a novel framework that harnesses the global\nplanning abilities of LLMs for efficient and accurate KG reasoning. KARPA\noperates in three steps: pre-planning relation paths using the LLM's global\nplanning capabilities, matching semantically relevant paths via an embedding\nmodel, and reasoning over these paths to generate answers. Unlike existing KGQA\nmethods, KARPA avoids stepwise traversal, requires no additional training, and\nis adaptable to various LLM architectures. Extensive experimental results show\nthat KARPA achieves state-of-the-art performance in KGQA tasks, delivering both\nhigh efficiency and accuracy. Our code will be available on Github."
                },
                "authors": [
                    {
                        "name": "Siyuan Fang"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Ningxuan Lu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Qingkun Tang"
                    }
                ],
                "author_detail": {
                    "name": "Qingkun Tang"
                },
                "author": "Qingkun Tang",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20993v1",
                "updated": "2024-12-30T14:57:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    57,
                    53,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T14:57:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    57,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "Efficiently Serving LLM Reasoning Programs with Certaindex",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving LLM Reasoning Programs with Certaindex"
                },
                "summary": "The rapid evolution of large language models (LLMs) has unlocked their\ncapabilities in advanced reasoning tasks like mathematical problem-solving,\ncode generation, and legal analysis. Central to this progress are\ninference-time reasoning algorithms, which refine outputs by exploring multiple\nsolution paths, at the cost of increasing compute demands and response\nlatencies. Existing serving systems fail to adapt to the scaling behaviors of\nthese algorithms or the varying difficulty of queries, leading to inefficient\nresource use and unmet latency targets.\n  We present Dynasor, a system that optimizes inference-time compute for LLM\nreasoning queries. Unlike traditional engines, Dynasor tracks and schedules\nrequests within reasoning queries and uses Certaindex, a proxy that measures\nstatistical reasoning progress based on model certainty, to guide compute\nallocation dynamically. Dynasor co-adapts scheduling with reasoning progress:\nit allocates more compute to hard queries, reduces compute for simpler ones,\nand terminates unpromising queries early, balancing accuracy, latency, and\ncost. On diverse datasets and algorithms, Dynasor reduces compute by up to 50%\nin batch processing and sustaining 3.3x higher query rates or 4.7x tighter\nlatency SLOs in online serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs) has unlocked their\ncapabilities in advanced reasoning tasks like mathematical problem-solving,\ncode generation, and legal analysis. Central to this progress are\ninference-time reasoning algorithms, which refine outputs by exploring multiple\nsolution paths, at the cost of increasing compute demands and response\nlatencies. Existing serving systems fail to adapt to the scaling behaviors of\nthese algorithms or the varying difficulty of queries, leading to inefficient\nresource use and unmet latency targets.\n  We present Dynasor, a system that optimizes inference-time compute for LLM\nreasoning queries. Unlike traditional engines, Dynasor tracks and schedules\nrequests within reasoning queries and uses Certaindex, a proxy that measures\nstatistical reasoning progress based on model certainty, to guide compute\nallocation dynamically. Dynasor co-adapts scheduling with reasoning progress:\nit allocates more compute to hard queries, reduces compute for simpler ones,\nand terminates unpromising queries early, balancing accuracy, latency, and\ncost. On diverse datasets and algorithms, Dynasor reduces compute by up to 50%\nin batch processing and sustaining 3.3x higher query rates or 4.7x tighter\nlatency SLOs in online serving."
                },
                "authors": [
                    {
                        "name": "Yichao Fu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Siqi Zhu"
                    },
                    {
                        "name": "Zheyu Fu"
                    },
                    {
                        "name": "Zhongdongming Dai"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20960v1",
                "updated": "2024-12-30T13:55:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    55,
                    28,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T13:55:28Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    55,
                    28,
                    0,
                    365,
                    0
                ],
                "title": "Rise of Generative Artificial Intelligence in Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rise of Generative Artificial Intelligence in Science"
                },
                "summary": "Generative Artificial Intelligence (GenAI, generative AI) has rapidly become\navailable as a tool in scientific research. To explore the use of generative AI\nin science, we conduct an empirical analysis using OpenAlex. Analyzing GenAI\npublications and other AI publications from 2017 to 2023, we profile growth\npatterns, the diffusion of GenAI publications across fields of study, and the\ngeographical spread of scientific research on generative AI. We also\ninvestigate team size and international collaborations to explore whether\nGenAI, as an emerging scientific research area, shows different collaboration\npatterns compared to other AI technologies. The results indicate that\ngenerative AI has experienced rapid growth and increasing presence in\nscientific publications. The use of GenAI now extends beyond computer science\nto other scientific research domains. Over the study period, U.S. researchers\ncontributed nearly two-fifths of global GenAI publications. The U.S. is\nfollowed by China, with several small and medium-sized advanced economies\ndemonstrating relatively high levels of GenAI deployment in their research\npublications. Although scientific research overall is becoming increasingly\nspecialized and collaborative, our results suggest that GenAI research groups\ntend to have slightly smaller team sizes than found in other AI fields.\nFurthermore, notwithstanding recent geopolitical tensions, GenAI research\ncontinues to exhibit levels of international collaboration comparable to other\nAI technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI, generative AI) has rapidly become\navailable as a tool in scientific research. To explore the use of generative AI\nin science, we conduct an empirical analysis using OpenAlex. Analyzing GenAI\npublications and other AI publications from 2017 to 2023, we profile growth\npatterns, the diffusion of GenAI publications across fields of study, and the\ngeographical spread of scientific research on generative AI. We also\ninvestigate team size and international collaborations to explore whether\nGenAI, as an emerging scientific research area, shows different collaboration\npatterns compared to other AI technologies. The results indicate that\ngenerative AI has experienced rapid growth and increasing presence in\nscientific publications. The use of GenAI now extends beyond computer science\nto other scientific research domains. Over the study period, U.S. researchers\ncontributed nearly two-fifths of global GenAI publications. The U.S. is\nfollowed by China, with several small and medium-sized advanced economies\ndemonstrating relatively high levels of GenAI deployment in their research\npublications. Although scientific research overall is becoming increasingly\nspecialized and collaborative, our results suggest that GenAI research groups\ntend to have slightly smaller team sizes than found in other AI fields.\nFurthermore, notwithstanding recent geopolitical tensions, GenAI research\ncontinues to exhibit levels of international collaboration comparable to other\nAI technologies."
                },
                "authors": [
                    {
                        "name": "Liangping Ding"
                    },
                    {
                        "name": "Cornelia Lawson"
                    },
                    {
                        "name": "Philip Shapira"
                    }
                ],
                "author_detail": {
                    "name": "Philip Shapira"
                },
                "author": "Philip Shapira",
                "arxiv_comment": "26 pages, 4 tables, 1 figures, 1 appendix figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; K.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20956v1",
                "updated": "2024-12-30T13:53:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    53,
                    51,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T13:53:51Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    53,
                    51,
                    0,
                    365,
                    0
                ],
                "title": "QuantumLLMInstruct: A 500k LLM Instruction-Tuning Dataset with\n  Problem-Solution Pairs for Quantum Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantumLLMInstruct: A 500k LLM Instruction-Tuning Dataset with\n  Problem-Solution Pairs for Quantum Computing"
                },
                "summary": "We present QuantumLLMInstruct (QLMMI), an innovative dataset featuring over\n500,000 meticulously curated instruction-following problem-solution pairs\ndesigned specifically for quantum computing - the largest and most\ncomprehensive dataset of its kind. Originating from over 90 primary seed\ndomains and encompassing hundreds of subdomains autonomously generated by LLMs,\nQLMMI marks a transformative step in the diversity and richness of quantum\ncomputing datasets.\n  Designed for instruction fine-tuning, QLMMI seeks to significantly improve\nLLM performance in addressing complex quantum computing challenges across a\nwide range of quantum physics topics. While Large Language Models (LLMs) have\npropelled advancements in computational science with datasets like Omni-MATH\nand OpenMathInstruct, these primarily target Olympiad-level mathematics,\nleaving quantum computing largely unexplored.\n  The creation of QLMMI follows a rigorous four-stage methodology. Initially,\nfoundational problems are developed using predefined templates, focusing on\ncritical areas such as synthetic Hamiltonians, QASM code generation,\nJordan-Wigner transformations, and Trotter-Suzuki quantum circuit\ndecompositions. Next, detailed and domain-specific solutions are crafted to\nensure accuracy and relevance. In the third stage, the dataset is enriched\nthrough advanced reasoning techniques, including Chain-of-Thought (CoT) and\nTask-Oriented Reasoning and Action (ToRA), which enhance problem-solution\ndiversity while adhering to strict mathematical standards. Lastly, a zero-shot\nJudge LLM performs self-assessments to validate the dataset's quality and\nreliability, minimizing human oversight requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present QuantumLLMInstruct (QLMMI), an innovative dataset featuring over\n500,000 meticulously curated instruction-following problem-solution pairs\ndesigned specifically for quantum computing - the largest and most\ncomprehensive dataset of its kind. Originating from over 90 primary seed\ndomains and encompassing hundreds of subdomains autonomously generated by LLMs,\nQLMMI marks a transformative step in the diversity and richness of quantum\ncomputing datasets.\n  Designed for instruction fine-tuning, QLMMI seeks to significantly improve\nLLM performance in addressing complex quantum computing challenges across a\nwide range of quantum physics topics. While Large Language Models (LLMs) have\npropelled advancements in computational science with datasets like Omni-MATH\nand OpenMathInstruct, these primarily target Olympiad-level mathematics,\nleaving quantum computing largely unexplored.\n  The creation of QLMMI follows a rigorous four-stage methodology. Initially,\nfoundational problems are developed using predefined templates, focusing on\ncritical areas such as synthetic Hamiltonians, QASM code generation,\nJordan-Wigner transformations, and Trotter-Suzuki quantum circuit\ndecompositions. Next, detailed and domain-specific solutions are crafted to\nensure accuracy and relevance. In the third stage, the dataset is enriched\nthrough advanced reasoning techniques, including Chain-of-Thought (CoT) and\nTask-Oriented Reasoning and Action (ToRA), which enhance problem-solution\ndiversity while adhering to strict mathematical standards. Lastly, a zero-shot\nJudge LLM performs self-assessments to validate the dataset's quality and\nreliability, minimizing human oversight requirements."
                },
                "authors": [
                    {
                        "name": "Shlomo Kashani"
                    }
                ],
                "author_detail": {
                    "name": "Shlomo Kashani"
                },
                "author": "Shlomo Kashani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20954v1",
                "updated": "2024-12-30T13:50:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    50,
                    20,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T13:50:20Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    50,
                    20,
                    0,
                    365,
                    0
                ],
                "title": "AGON: Automated Design Framework for Customizing Processors from ISA\n  Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGON: Automated Design Framework for Customizing Processors from ISA\n  Documents"
                },
                "summary": "Customized processors are attractive solutions for vast domain-specific\napplications due to their high energy efficiency. However, designing a\nprocessor in traditional flows is time-consuming and expensive. To address\nthis, researchers have explored methods including the use of agile development\ntools like Chisel or SpinalHDL, high-level synthesis (HLS) from programming\nlanguages like C or SystemC, and more recently, leveraging large language\nmodels (LLMs) to generate hardware description language (HDL) code from natural\nlanguage descriptions. However, each method has limitations in terms of\nexpressiveness, correctness, and performance, leading to a persistent\ncontradiction between the level of automation and the effectiveness of the\ndesign. Overall, how to automatically design highly efficient and practical\nprocessors with minimal human effort remains a challenge.\n  In this paper, we propose AGON, a novel framework designed to leverage LLMs\nfor the efficient design of out-of-order (OoO) customized processors with\nminimal human effort. Central to AGON is the nano-operator function (nOP\nfunction) based Intermediate Representation (IR), which bridges high-level\ndescriptions and hardware implementations while decoupling functionality from\nperformance optimization, thereby providing an automatic design framework that\nis expressive and efficient, has correctness guarantees, and enables PPA\n(Power, Performance, and Area) optimization.\n  Experimental results show that superior to previous LLM-assisted automatic\ndesign flows, AGON facilitates designing a series of customized OoO processors\nthat achieve on average 2.35 $\\times$ speedup compared with BOOM, a\ngeneral-purpose CPU designed by experts, with minimal design effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customized processors are attractive solutions for vast domain-specific\napplications due to their high energy efficiency. However, designing a\nprocessor in traditional flows is time-consuming and expensive. To address\nthis, researchers have explored methods including the use of agile development\ntools like Chisel or SpinalHDL, high-level synthesis (HLS) from programming\nlanguages like C or SystemC, and more recently, leveraging large language\nmodels (LLMs) to generate hardware description language (HDL) code from natural\nlanguage descriptions. However, each method has limitations in terms of\nexpressiveness, correctness, and performance, leading to a persistent\ncontradiction between the level of automation and the effectiveness of the\ndesign. Overall, how to automatically design highly efficient and practical\nprocessors with minimal human effort remains a challenge.\n  In this paper, we propose AGON, a novel framework designed to leverage LLMs\nfor the efficient design of out-of-order (OoO) customized processors with\nminimal human effort. Central to AGON is the nano-operator function (nOP\nfunction) based Intermediate Representation (IR), which bridges high-level\ndescriptions and hardware implementations while decoupling functionality from\nperformance optimization, thereby providing an automatic design framework that\nis expressive and efficient, has correctness guarantees, and enables PPA\n(Power, Performance, and Area) optimization.\n  Experimental results show that superior to previous LLM-assisted automatic\ndesign flows, AGON facilitates designing a series of customized OoO processors\nthat achieve on average 2.35 $\\times$ speedup compared with BOOM, a\ngeneral-purpose CPU designed by experts, with minimal design effort."
                },
                "authors": [
                    {
                        "name": "Chongxiao Li"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Pengwei Jin"
                    },
                    {
                        "name": "Tianyun Ma"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shuyao Cheng"
                    },
                    {
                        "name": "Yifan Hao"
                    },
                    {
                        "name": "Yongwei Zhao"
                    },
                    {
                        "name": "Guanglin Xu"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Xiaqing Li"
                    },
                    {
                        "name": "Yuanbo Wen"
                    },
                    {
                        "name": "Yanjun Wu"
                    },
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07099v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07099v3",
                "updated": "2024-12-30T13:43:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    43,
                    46,
                    0,
                    365,
                    0
                ],
                "published": "2024-06-18T07:46:13Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    7,
                    46,
                    13,
                    1,
                    170,
                    0
                ],
                "title": "Nash CoT: Multi-Path Inference with Preference Equilibrium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nash CoT: Multi-Path Inference with Preference Equilibrium"
                },
                "summary": "Chain of thought (CoT) is a reasoning framework that can enhance the\nperformance of Large Language Models (LLMs) on complex inference tasks. In\nparticular, among various studies related to CoT, multi-path inference stands\nout as a simple yet effective improvement. However, there is no optimal setting\nfor the number of inference paths. Therefore, we have to increase the number of\ninference paths to obtain better results, which in turn increases the inference\ncost. To address this limitation, we can utilize question-related role\ntemplates to guide LLMs into relevant roles, thereby increasing the possibility\nof correct inferences for each path and further reducing dependence on the\nnumber of inference paths while improving reasoning accuracy. However, placing\nLLMs into specific roles may reduce their reasoning diversity and performance\non a few tasks where role dependence is low. To alleviate the excessive\nimmersion of the LLM into a specific role, we propose Nash CoT by constructing\na game system on each path that balances the generation from role-specific\nLLMs' and the general LLMs' generation, thereby ensuring both effective role\nadoption and diversity in LLM generation further maintaining the performance of\nmulti-path inference while reducing the requirement of the number of inference\npaths. We evaluate Nash CoT across various inference tasks, including Arabic\nReasoning, Commonsense Question Answering, and Symbolic Inference, achieving\nresults that are comparable to or better than those of multi-path CoT with the\nequal number of inference paths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of thought (CoT) is a reasoning framework that can enhance the\nperformance of Large Language Models (LLMs) on complex inference tasks. In\nparticular, among various studies related to CoT, multi-path inference stands\nout as a simple yet effective improvement. However, there is no optimal setting\nfor the number of inference paths. Therefore, we have to increase the number of\ninference paths to obtain better results, which in turn increases the inference\ncost. To address this limitation, we can utilize question-related role\ntemplates to guide LLMs into relevant roles, thereby increasing the possibility\nof correct inferences for each path and further reducing dependence on the\nnumber of inference paths while improving reasoning accuracy. However, placing\nLLMs into specific roles may reduce their reasoning diversity and performance\non a few tasks where role dependence is low. To alleviate the excessive\nimmersion of the LLM into a specific role, we propose Nash CoT by constructing\na game system on each path that balances the generation from role-specific\nLLMs' and the general LLMs' generation, thereby ensuring both effective role\nadoption and diversity in LLM generation further maintaining the performance of\nmulti-path inference while reducing the requirement of the number of inference\npaths. We evaluate Nash CoT across various inference tasks, including Arabic\nReasoning, Commonsense Question Answering, and Symbolic Inference, achieving\nresults that are comparable to or better than those of multi-path CoT with the\nequal number of inference paths."
                },
                "authors": [
                    {
                        "name": "Ziqi Zhang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Xiong Xiao"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "arxiv_journal_ref": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07099v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07099v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20943v1",
                "updated": "2024-12-30T13:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    36,
                    36,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T13:36:36Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    36,
                    36,
                    0,
                    365,
                    0
                ],
                "title": "Cluster-Based Time-Variant Channel Characterization and Modeling for\n  5G-Railways",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cluster-Based Time-Variant Channel Characterization and Modeling for\n  5G-Railways"
                },
                "summary": "With the development of high-speed railways, 5G for Railways (5G-R) is\ngradually replacing Global System for the Mobile Communications for Railway\n(GSM-R) worldwide to meet increasing demands. The large bandwidth, array\nantennas, and non-stationarity caused by high mobility has made 5G-R channel\ncharacterization more complex. Therefore, it is essential to develop an\naccurate channel model for 5G-R. However, researches on channel\ncharacterization and time-variant models specific to 5G-R frequency bands and\nscenarios is scarce. There are virtually no cluster-based time-variant channel\nmodels that capture statistical properties of 5G-R channel. In this paper, we\npropose a cluster-based time-variant channel model for 5G-R within an enhanced\n3GPP framework, which incorporates time evolution features. Extensive channel\nmeasurements are conducted on 5G-R private network test line in China. We then\nextract and analyze typical channel fading characteristics and multipath\ncluster characteristics. Furthermore, birth-death process of the clusters is\nmodeled by using a four-state Markov chain. Finally, a generalized clustered\ndelay line (CDL) model is established in accordance with 3GPP standard and\nvalidated by comparing the results of measurements and simulations. This work\nenhances the understanding of 5G-R channels and presents a flexible\ncluster-based time-variant channel model. The results can be used in the\ndesign, deployment, and optimization of 5G-R networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of high-speed railways, 5G for Railways (5G-R) is\ngradually replacing Global System for the Mobile Communications for Railway\n(GSM-R) worldwide to meet increasing demands. The large bandwidth, array\nantennas, and non-stationarity caused by high mobility has made 5G-R channel\ncharacterization more complex. Therefore, it is essential to develop an\naccurate channel model for 5G-R. However, researches on channel\ncharacterization and time-variant models specific to 5G-R frequency bands and\nscenarios is scarce. There are virtually no cluster-based time-variant channel\nmodels that capture statistical properties of 5G-R channel. In this paper, we\npropose a cluster-based time-variant channel model for 5G-R within an enhanced\n3GPP framework, which incorporates time evolution features. Extensive channel\nmeasurements are conducted on 5G-R private network test line in China. We then\nextract and analyze typical channel fading characteristics and multipath\ncluster characteristics. Furthermore, birth-death process of the clusters is\nmodeled by using a four-state Markov chain. Finally, a generalized clustered\ndelay line (CDL) model is established in accordance with 3GPP standard and\nvalidated by comparing the results of measurements and simulations. This work\nenhances the understanding of 5G-R channels and presents a flexible\ncluster-based time-variant channel model. The results can be used in the\ndesign, deployment, and optimization of 5G-R networks."
                },
                "authors": [
                    {
                        "name": "Xuejian Zhang"
                    },
                    {
                        "name": "Ruisi He"
                    },
                    {
                        "name": "Bo Ai"
                    },
                    {
                        "name": "Mi Yang"
                    },
                    {
                        "name": "Jianwen Ding"
                    },
                    {
                        "name": "Shuaiqi Gao"
                    },
                    {
                        "name": "Ziyi Qi"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Zhangdui Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Zhangdui Zhong"
                },
                "author": "Zhangdui Zhong",
                "arxiv_comment": "13 pages, 13 figures, submitted to IEEE Transactions on Wireless\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]