[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v3",
                "updated": "2024-08-21T02:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    2,
                    32,
                    43,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Büchner"
                    },
                    {
                        "name": "Leonardo Agudo Jácome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jácome"
                },
                "author": "Leonardo Agudo Jácome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adrià Armejach"
                    },
                    {
                        "name": "Miquel Moretó"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moretó"
                },
                "author": "Miquel Moretó",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16672v1",
                "updated": "2024-07-23T17:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T17:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "title": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications"
                },
                "summary": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications."
                },
                "authors": [
                    {
                        "name": "Sahan Liyanaarachchi"
                    },
                    {
                        "name": "Stavros Mitrolaris"
                    },
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16303v1",
                "updated": "2024-07-23T08:58:06Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:58:06Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "title": "Hidden Web Caches Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Web Caches Discovery"
                },
                "summary": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers."
                },
                "authors": [
                    {
                        "name": "Matteo Golinelli"
                    },
                    {
                        "name": "Bruno Crispo"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Crispo"
                },
                "author": "Bruno Crispo",
                "arxiv_doi": "10.1145/3678890.3678931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678890.3678931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive Version of Record was published in The 27th\n  International Symposium on Research in Attacks, Intrusions and Defenses (RAID\n  2024), September 30-October 02, 2024, Padua, Italy,\n  https://doi.org/10.1145/3678890.3678931",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v1",
                "updated": "2024-07-23T08:55:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16286v1",
                "updated": "2024-07-23T08:40:27Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:40:27Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "title": "A deeper look at depth pruning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A deeper look at depth pruning of LLMs"
                },
                "summary": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Thomas Breuel"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15309v1",
                "updated": "2024-07-22T14:37:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T14:37:58Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15581v1",
                "updated": "2024-07-22T12:17:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T12:17:01Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "title": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores"
                },
                "summary": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget."
                },
                "authors": [
                    {
                        "name": "Giorgos Xanthakis"
                    },
                    {
                        "name": "Antonios Katsarakis"
                    },
                    {
                        "name": "Giorgos Saloustros"
                    },
                    {
                        "name": "Angelos Bilas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Bilas"
                },
                "author": "Angelos Bilas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.11055v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.11055v5",
                "updated": "2024-07-22T10:02:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    10,
                    2,
                    57,
                    0,
                    204,
                    0
                ],
                "published": "2022-12-21T14:59:23Z",
                "published_parsed": [
                    2022,
                    12,
                    21,
                    14,
                    59,
                    23,
                    2,
                    355,
                    0
                ],
                "title": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi"
                },
                "summary": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities."
                },
                "authors": [
                    {
                        "name": "Daniel Hausmann"
                    },
                    {
                        "name": "Lutz Schröder"
                    }
                ],
                "author_detail": {
                    "name": "Lutz Schröder"
                },
                "author": "Lutz Schröder",
                "arxiv_doi": "10.46298/lmcs-20(3:9)2024",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.46298/lmcs-20(3:9)2024",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2212.11055v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.11055v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Logical Methods in Computer Science, Volume 20, Issue 3 (July 23,\n  2024) lmcs:10532",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B70, 03B44",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.15240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15240v1",
                "updated": "2024-08-27T17:57:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    57,
                    45,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:57:45Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    57,
                    45,
                    1,
                    240,
                    0
                ],
                "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Verifiers: Reward Modeling as Next-Token Prediction"
                },
                "summary": "Verifiers or reward models are often used to enhance the reasoning\nperformance of large language models (LLMs). A common approach is the Best-of-N\nmethod, where N candidate solutions generated by the LLM are ranked by a\nverifier, and the best one is selected. While LLM-based verifiers are typically\ntrained as discriminative classifiers to score solutions, they do not utilize\nthe text generation capabilities of pretrained LLMs. To overcome this\nlimitation, we instead propose training verifiers using the ubiquitous\nnext-token prediction objective, jointly on verification and solution\ngeneration. Compared to standard verifiers, such generative verifiers (GenRM)\ncan benefit from several advantages of LLMs: they integrate seamlessly with\ninstruction tuning, enable chain-of-thought reasoning, and can utilize\nadditional inference-time compute via majority voting for better verification.\nWe demonstrate that when using Gemma-based verifiers on algorithmic and\ngrade-school math reasoning tasks, GenRM outperforms discriminative verifiers\nand LLM-as-a-Judge, showing a 16-64% improvement in the percentage of problems\nsolved with Best-of-N. Furthermore, we show that GenRM scales favorably across\ndataset size, model capacity, and inference-time compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifiers or reward models are often used to enhance the reasoning\nperformance of large language models (LLMs). A common approach is the Best-of-N\nmethod, where N candidate solutions generated by the LLM are ranked by a\nverifier, and the best one is selected. While LLM-based verifiers are typically\ntrained as discriminative classifiers to score solutions, they do not utilize\nthe text generation capabilities of pretrained LLMs. To overcome this\nlimitation, we instead propose training verifiers using the ubiquitous\nnext-token prediction objective, jointly on verification and solution\ngeneration. Compared to standard verifiers, such generative verifiers (GenRM)\ncan benefit from several advantages of LLMs: they integrate seamlessly with\ninstruction tuning, enable chain-of-thought reasoning, and can utilize\nadditional inference-time compute via majority voting for better verification.\nWe demonstrate that when using Gemma-based verifiers on algorithmic and\ngrade-school math reasoning tasks, GenRM outperforms discriminative verifiers\nand LLM-as-a-Judge, showing a 16-64% improvement in the percentage of problems\nsolved with Best-of-N. Furthermore, we show that GenRM scales favorably across\ndataset size, model capacity, and inference-time compute."
                },
                "authors": [
                    {
                        "name": "Lunjun Zhang"
                    },
                    {
                        "name": "Arian Hosseini"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Aviral Kumar"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Agarwal"
                },
                "author": "Rishabh Agarwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16553v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16553v7",
                "updated": "2024-08-27T17:57:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    57,
                    7,
                    1,
                    240,
                    0
                ],
                "published": "2024-01-29T20:44:10Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    20,
                    44,
                    10,
                    0,
                    29,
                    0
                ],
                "title": "SelectLLM: Can LLMs Select Important Instructions to Annotate?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Can LLMs Select Important Instructions to Annotate?"
                },
                "summary": "Instruction tuning benefits from large and diverse datasets; however,\ncreating such datasets involves a high cost of human labeling. While synthetic\ndatasets generated by large language models (LLMs) have partly solved this\nissue, they often contain low-quality data. One effective solution is\nselectively annotating unlabelled instructions, especially given the relative\nease of acquiring unlabeled instructions or texts from various sources.\nHowever, how to select unlabelled instructions is not well-explored, especially\nin the context of LLMs. Therefore, we introduce SelectLLM, an alternative\nframework that leverages the capabilities of LLMs to select unlabeled\ninstructions more effectively. Specifically, SelectLLM consists of two key\nsteps: Coreset-based clustering of unlabelled instructions for enlarging\ndiversity and prompting of LLM to identify the most beneficial instructions\nwithin each cluster. We evaluate SelectLLM on AlpacaEval2 and MT-Bench,\ndemonstrating its ability to outperform state-of-the-art methods like\nAlpagasus. In addition, we compare the performance and compatibility of\nSelectLLM with various LLMs, such as ChatGPT, LLaMA-3.1-70B, and Gemma-2-27b.\nSelectLLM's adaptability and robustness are further evidenced by its ability to\nmaintain high performance across both human and synthetic datasets. All code\nand data are publicly available (https://github.com/minnesotanlp/select-llm).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning benefits from large and diverse datasets; however,\ncreating such datasets involves a high cost of human labeling. While synthetic\ndatasets generated by large language models (LLMs) have partly solved this\nissue, they often contain low-quality data. One effective solution is\nselectively annotating unlabelled instructions, especially given the relative\nease of acquiring unlabeled instructions or texts from various sources.\nHowever, how to select unlabelled instructions is not well-explored, especially\nin the context of LLMs. Therefore, we introduce SelectLLM, an alternative\nframework that leverages the capabilities of LLMs to select unlabeled\ninstructions more effectively. Specifically, SelectLLM consists of two key\nsteps: Coreset-based clustering of unlabelled instructions for enlarging\ndiversity and prompting of LLM to identify the most beneficial instructions\nwithin each cluster. We evaluate SelectLLM on AlpacaEval2 and MT-Bench,\ndemonstrating its ability to outperform state-of-the-art methods like\nAlpagasus. In addition, we compare the performance and compatibility of\nSelectLLM with various LLMs, such as ChatGPT, LLaMA-3.1-70B, and Gemma-2-27b.\nSelectLLM's adaptability and robustness are further evidenced by its ability to\nmaintain high performance across both human and synthetic datasets. All code\nand data are publicly available (https://github.com/minnesotanlp/select-llm)."
                },
                "authors": [
                    {
                        "name": "Ritik Sachin Parkar"
                    },
                    {
                        "name": "Jaehyung Kim"
                    },
                    {
                        "name": "Jong Inn Park"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "arxiv_comment": "First Authors: Ritik Sachin Parkar and Jaehyung Kim | Second Author:\n  Jong Inn Park | PI: Dongyeop Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16553v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16553v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15237v1",
                "updated": "2024-08-27T17:56:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    56,
                    11,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:56:11Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    56,
                    11,
                    1,
                    240,
                    0
                ],
                "title": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models"
                },
                "summary": "Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model."
                },
                "authors": [
                    {
                        "name": "Junxiong Wang"
                    },
                    {
                        "name": "Daniele Paliotta"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Alexander M. Rush"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "Code is open-sourced at https://github.com/jxiw/MambaInLlama",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15231v1",
                "updated": "2024-08-27T17:48:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    48,
                    29,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:48:29Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    48,
                    29,
                    1,
                    240,
                    0
                ],
                "title": "DCT-CryptoNets: Scaling Private Inference in the Frequency Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCT-CryptoNets: Scaling Private Inference in the Frequency Domain"
                },
                "summary": "The convergence of fully homomorphic encryption (FHE) and machine learning\noffers unprecedented opportunities for private inference of sensitive data. FHE\nenables computation directly on encrypted data, safeguarding the entire machine\nlearning pipeline, including data and model confidentiality. However, existing\nFHE-based implementations for deep neural networks face significant challenges\nin computational cost, latency, and scalability, limiting their practical\ndeployment. This paper introduces DCT-CryptoNets, a novel approach that\nleverages frequency-domain learning to tackle these issues. Our method operates\ndirectly in the frequency domain, utilizing the discrete cosine transform (DCT)\ncommonly employed in JPEG compression. This approach is inherently compatible\nwith remote computing services, where images are usually transmitted and stored\nin compressed formats. DCT-CryptoNets reduces the computational burden of\nhomomorphic operations by focusing on perceptually relevant low-frequency\ncomponents. This is demonstrated by substantial latency reduction of up to\n5.3$\\times$ compared to prior work on image classification tasks, including a\nnovel demonstration of ImageNet inference within 2.5 hours, down from 12.5\nhours compared to prior work on equivalent compute resources. Moreover,\nDCT-CryptoNets improves the reliability of encrypted accuracy by reducing\nvariability (e.g., from $\\pm$2.5\\% to $\\pm$1.0\\% on ImageNet). This study\ndemonstrates a promising avenue for achieving efficient and practical\nprivacy-preserving deep learning on high resolution images seen in real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The convergence of fully homomorphic encryption (FHE) and machine learning\noffers unprecedented opportunities for private inference of sensitive data. FHE\nenables computation directly on encrypted data, safeguarding the entire machine\nlearning pipeline, including data and model confidentiality. However, existing\nFHE-based implementations for deep neural networks face significant challenges\nin computational cost, latency, and scalability, limiting their practical\ndeployment. This paper introduces DCT-CryptoNets, a novel approach that\nleverages frequency-domain learning to tackle these issues. Our method operates\ndirectly in the frequency domain, utilizing the discrete cosine transform (DCT)\ncommonly employed in JPEG compression. This approach is inherently compatible\nwith remote computing services, where images are usually transmitted and stored\nin compressed formats. DCT-CryptoNets reduces the computational burden of\nhomomorphic operations by focusing on perceptually relevant low-frequency\ncomponents. This is demonstrated by substantial latency reduction of up to\n5.3$\\times$ compared to prior work on image classification tasks, including a\nnovel demonstration of ImageNet inference within 2.5 hours, down from 12.5\nhours compared to prior work on equivalent compute resources. Moreover,\nDCT-CryptoNets improves the reliability of encrypted accuracy by reducing\nvariability (e.g., from $\\pm$2.5\\% to $\\pm$1.0\\% on ImageNet). This study\ndemonstrates a promising avenue for achieving efficient and practical\nprivacy-preserving deep learning on high resolution images seen in real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Arjun Roy"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "Under Review; 10 pages content, 3 pages appendix, 4 figures, 8\n  tables; Code TBD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15221v1",
                "updated": "2024-08-27T17:33:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    33,
                    30,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:33:30Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    33,
                    30,
                    1,
                    240,
                    0
                ],
                "title": "LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet"
                },
                "summary": "Recent large language model (LLM) defenses have greatly improved models'\nability to refuse harmful queries, even when adversarially attacked. However,\nLLM defenses are primarily evaluated against automated adversarial attacks in a\nsingle turn of conversation, an insufficient threat model for real-world\nmalicious use. We demonstrate that multi-turn human jailbreaks uncover\nsignificant vulnerabilities, exceeding 70% attack success rate (ASR) on\nHarmBench against defenses that report single-digit ASRs with automated\nsingle-turn attacks. Human jailbreaks also reveal vulnerabilities in machine\nunlearning defenses, successfully recovering dual-use biosecurity knowledge\nfrom unlearned models. We compile these results into Multi-Turn Human\nJailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks.\nWe publicly release MHJ alongside a compendium of jailbreak tactics developed\nacross dozens of commercial red teaming engagements, supporting research\ntowards stronger LLM defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model (LLM) defenses have greatly improved models'\nability to refuse harmful queries, even when adversarially attacked. However,\nLLM defenses are primarily evaluated against automated adversarial attacks in a\nsingle turn of conversation, an insufficient threat model for real-world\nmalicious use. We demonstrate that multi-turn human jailbreaks uncover\nsignificant vulnerabilities, exceeding 70% attack success rate (ASR) on\nHarmBench against defenses that report single-digit ASRs with automated\nsingle-turn attacks. Human jailbreaks also reveal vulnerabilities in machine\nunlearning defenses, successfully recovering dual-use biosecurity knowledge\nfrom unlearned models. We compile these results into Multi-Turn Human\nJailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks.\nWe publicly release MHJ alongside a compendium of jailbreak tactics developed\nacross dozens of commercial red teaming engagements, supporting research\ntowards stronger LLM defenses."
                },
                "authors": [
                    {
                        "name": "Nathaniel Li"
                    },
                    {
                        "name": "Ziwen Han"
                    },
                    {
                        "name": "Ian Steneker"
                    },
                    {
                        "name": "Willow Primack"
                    },
                    {
                        "name": "Riley Goodside"
                    },
                    {
                        "name": "Hugh Zhang"
                    },
                    {
                        "name": "Zifan Wang"
                    },
                    {
                        "name": "Cristina Menghini"
                    },
                    {
                        "name": "Summer Yue"
                    }
                ],
                "author_detail": {
                    "name": "Summer Yue"
                },
                "author": "Summer Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.13643v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.13643v2",
                "updated": "2024-08-27T17:32:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    32,
                    39,
                    1,
                    240,
                    0
                ],
                "published": "2022-09-27T19:16:26Z",
                "published_parsed": [
                    2022,
                    9,
                    27,
                    19,
                    16,
                    26,
                    1,
                    270,
                    0
                ],
                "title": "MPC-Pipe: an Efficient Pipeline Scheme for Secure Multi-party Machine\n  Learning Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPC-Pipe: an Efficient Pipeline Scheme for Secure Multi-party Machine\n  Learning Inference"
                },
                "summary": "Multi-party computing (MPC) has been gaining popularity as a secure computing\nmodel over the past few years. However, prior works have demonstrated that MPC\nprotocols still pay substantial performance penalties compared to plaintext,\nparticularly when applied to ML algorithms. The overhead is due to added\ncomputation and communication costs. Prior studies, as well as our own\nanalysis, found that most MPC protocols today sequentially perform\ncommunication and computation. The participating parties must compute on their\nshares first and then perform data communication to allow the distribution of\nnew secret shares before proceeding to the next computation step. In this work,\nwe show that serialization is unnecessary, particularly in the context of ML\ncomputations (both in Convolutional neural networks and in Transformer-based\nmodels). We demonstrate that it is possible to carefully orchestrate the\ncomputation and communication steps to overlap.\n  We propose MPC-Pipe, an efficient MPC system for both training and inference\nof ML workloads, which pipelines computations and communications in an MPC\nprotocol during the online phase. MPC-Pipe proposes three pipeline schemes to\noptimize the online phase of ML in the semi-honest majority adversary setting.\nWe implement MPC-Pipe by augmenting a modified version of CrypTen, which\nseparates online and offline phases. We evaluate the end-to-end system\nperformance benefits of the online phase of MPC using deep neural networks\n(VGG16, ResNet50) and Transformers using different network settings. We show\nthat MPC-Pipe can improve the throughput and latency of ML workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-party computing (MPC) has been gaining popularity as a secure computing\nmodel over the past few years. However, prior works have demonstrated that MPC\nprotocols still pay substantial performance penalties compared to plaintext,\nparticularly when applied to ML algorithms. The overhead is due to added\ncomputation and communication costs. Prior studies, as well as our own\nanalysis, found that most MPC protocols today sequentially perform\ncommunication and computation. The participating parties must compute on their\nshares first and then perform data communication to allow the distribution of\nnew secret shares before proceeding to the next computation step. In this work,\nwe show that serialization is unnecessary, particularly in the context of ML\ncomputations (both in Convolutional neural networks and in Transformer-based\nmodels). We demonstrate that it is possible to carefully orchestrate the\ncomputation and communication steps to overlap.\n  We propose MPC-Pipe, an efficient MPC system for both training and inference\nof ML workloads, which pipelines computations and communications in an MPC\nprotocol during the online phase. MPC-Pipe proposes three pipeline schemes to\noptimize the online phase of ML in the semi-honest majority adversary setting.\nWe implement MPC-Pipe by augmenting a modified version of CrypTen, which\nseparates online and offline phases. We evaluate the end-to-end system\nperformance benefits of the online phase of MPC using deep neural networks\n(VGG16, ResNet50) and Transformers using different network settings. We show\nthat MPC-Pipe can improve the throughput and latency of ML workloads."
                },
                "authors": [
                    {
                        "name": "Yongqin Wang"
                    },
                    {
                        "name": "Rachit Rajat"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "arxiv_comment": "To be appeared in ASPLOS'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.13643v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.13643v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15207v1",
                "updated": "2024-08-27T17:14:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    14,
                    21,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:14:21Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    14,
                    21,
                    1,
                    240,
                    0
                ],
                "title": "Investigating Coverage Criteria in Large Language Models: An In-Depth\n  Study Through Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Coverage Criteria in Large Language Models: An In-Depth\n  Study Through Jailbreak Attacks"
                },
                "summary": "The swift advancement of large language models (LLMs) has profoundly shaped\nthe landscape of artificial intelligence; however, their deployment in\nsensitive domains raises grave concerns, particularly due to their\nsusceptibility to malicious exploitation. This situation underscores the\ninsufficiencies in pre-deployment testing, highlighting the urgent need for\nmore rigorous and comprehensive evaluation methods. This study presents a\ncomprehensive empirical analysis assessing the efficacy of conventional\ncoverage criteria in identifying these vulnerabilities, with a particular\nemphasis on the pressing issue of jailbreak attacks. Our investigation begins\nwith a clustering analysis of the hidden states in LLMs, demonstrating that\nintrinsic characteristics of these states can distinctly differentiate between\nvarious types of queries. Subsequently, we assess the performance of these\ncriteria across three critical dimensions: criterion level, layer level, and\ntoken level. Our findings uncover significant disparities in neuron activation\npatterns between the processing of normal and jailbreak queries, thereby\ncorroborating the clustering results. Leveraging these findings, we propose an\ninnovative approach for the real-time detection of jailbreak attacks by\nutilizing neural activation features. Our classifier demonstrates remarkable\naccuracy, averaging 96.33% in identifying jailbreak queries, including those\nthat could lead to adversarial attacks. The importance of our research lies in\nits comprehensive approach to addressing the intricate challenges of LLM\nsecurity. By enabling instantaneous detection from the model's first token\noutput, our method holds promise for future systems integrating LLMs, offering\nrobust real-time detection capabilities. This study advances our understanding\nof LLM security testing, and lays a critical foundation for the development of\nmore resilient AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The swift advancement of large language models (LLMs) has profoundly shaped\nthe landscape of artificial intelligence; however, their deployment in\nsensitive domains raises grave concerns, particularly due to their\nsusceptibility to malicious exploitation. This situation underscores the\ninsufficiencies in pre-deployment testing, highlighting the urgent need for\nmore rigorous and comprehensive evaluation methods. This study presents a\ncomprehensive empirical analysis assessing the efficacy of conventional\ncoverage criteria in identifying these vulnerabilities, with a particular\nemphasis on the pressing issue of jailbreak attacks. Our investigation begins\nwith a clustering analysis of the hidden states in LLMs, demonstrating that\nintrinsic characteristics of these states can distinctly differentiate between\nvarious types of queries. Subsequently, we assess the performance of these\ncriteria across three critical dimensions: criterion level, layer level, and\ntoken level. Our findings uncover significant disparities in neuron activation\npatterns between the processing of normal and jailbreak queries, thereby\ncorroborating the clustering results. Leveraging these findings, we propose an\ninnovative approach for the real-time detection of jailbreak attacks by\nutilizing neural activation features. Our classifier demonstrates remarkable\naccuracy, averaging 96.33% in identifying jailbreak queries, including those\nthat could lead to adversarial attacks. The importance of our research lies in\nits comprehensive approach to addressing the intricate challenges of LLM\nsecurity. By enabling instantaneous detection from the model's first token\noutput, our method holds promise for future systems integrating LLMs, offering\nrobust real-time detection capabilities. This study advances our understanding\nof LLM security testing, and lays a critical foundation for the development of\nmore resilient AI systems."
                },
                "authors": [
                    {
                        "name": "Shide Zhou"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15204v1",
                "updated": "2024-08-27T17:03:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    3,
                    18,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:03:18Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    3,
                    18,
                    1,
                    240,
                    0
                ],
                "title": "Can Unconfident LLM Annotations Be Used for Confident Conclusions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Unconfident LLM Annotations Be Used for Confident Conclusions?"
                },
                "summary": "Large language models (LLMs) have shown high agreement with human raters\nacross a variety of tasks, demonstrating potential to ease the challenges of\nhuman data collection. In computational social science (CSS), researchers are\nincreasingly leveraging LLM annotations to complement slow and expensive human\nannotations. Still, guidelines for collecting and using LLM annotations,\nwithout compromising the validity of downstream conclusions, remain limited. We\nintroduce Confidence-Driven Inference: a method that combines LLM annotations\nand LLM confidence indicators to strategically select which human annotations\nshould be collected, with the goal of producing accurate statistical estimates\nand provably valid confidence intervals while reducing the number of human\nannotations needed. Our approach comes with safeguards against LLM annotations\nof poor quality, guaranteeing that the conclusions will be both valid and no\nless accurate than if we only relied on human annotations. We demonstrate the\neffectiveness of Confidence-Driven Inference over baselines in statistical\nestimation tasks across three CSS settings--text politeness, stance, and\nbias--reducing the needed number of human annotations by over 25% in each.\nAlthough we use CSS settings for demonstration, Confidence-Driven Inference can\nbe used to estimate most standard quantities across a broad range of NLP\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown high agreement with human raters\nacross a variety of tasks, demonstrating potential to ease the challenges of\nhuman data collection. In computational social science (CSS), researchers are\nincreasingly leveraging LLM annotations to complement slow and expensive human\nannotations. Still, guidelines for collecting and using LLM annotations,\nwithout compromising the validity of downstream conclusions, remain limited. We\nintroduce Confidence-Driven Inference: a method that combines LLM annotations\nand LLM confidence indicators to strategically select which human annotations\nshould be collected, with the goal of producing accurate statistical estimates\nand provably valid confidence intervals while reducing the number of human\nannotations needed. Our approach comes with safeguards against LLM annotations\nof poor quality, guaranteeing that the conclusions will be both valid and no\nless accurate than if we only relied on human annotations. We demonstrate the\neffectiveness of Confidence-Driven Inference over baselines in statistical\nestimation tasks across three CSS settings--text politeness, stance, and\nbias--reducing the needed number of human annotations by over 25% in each.\nAlthough we use CSS settings for demonstration, Confidence-Driven Inference can\nbe used to estimate most standard quantities across a broad range of NLP\nproblems."
                },
                "authors": [
                    {
                        "name": "Kristina Gligorić"
                    },
                    {
                        "name": "Tijana Zrnic"
                    },
                    {
                        "name": "Cinoo Lee"
                    },
                    {
                        "name": "Emmanuel J. Candès"
                    },
                    {
                        "name": "Dan Jurafsky"
                    }
                ],
                "author_detail": {
                    "name": "Dan Jurafsky"
                },
                "author": "Dan Jurafsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15192v1",
                "updated": "2024-08-27T16:51:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    51,
                    51,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T16:51:51Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    51,
                    51,
                    1,
                    240,
                    0
                ],
                "title": "Simultaneously Constraining the Neutron Star Equation of State and Mass\n  Distribution through Multimessenger Observations and Nuclear Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously Constraining the Neutron Star Equation of State and Mass\n  Distribution through Multimessenger Observations and Nuclear Benchmarks"
                },
                "summary": "With ongoing advancements in nuclear theory and experimentation, together\nwith a growing body of neutron star (NS) observations, a wealth of information\non the equation of state (EOS) for matter at extreme densities has become\naccessible. Here, we utilize a hybrid EOS formulation that combines an\nempirical parameterization centered around the nuclear saturation density with\na generic three-segment piecewise polytrope model at higher densities. We\nincorporate data derived from chiral effective field theory ($\\chi$EFT),\nperturbative quantum chromodynamics (pQCD), and from experiments such as\nPREX-II and CREX. Furthermore, we examine the influence of a total of 129 NS\nmass measurements up to April 2023, as well as simultaneous mass and radius\nmeasurements derived from the X-ray emission from surface hot spots on NSs.\nAdditionally, we consider constraints on tidal properties inferred from the\ngravitational waves emitted by coalescing NS binaries. To integrate this\nextensive and varied array of constraints, we utilize a hierarchical Bayesian\nstatistical framework to simultaneously deduce the EOS and the distribution of\nNS masses. We find that incorporating data from $\\chi$EFT significantly\ntightens the constraints on the EOS of NSs near or below the nuclear saturation\ndensity. However, constraints derived from pQCD computations and nuclear\nexperiments such as PREX-II and CREX have minimal impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With ongoing advancements in nuclear theory and experimentation, together\nwith a growing body of neutron star (NS) observations, a wealth of information\non the equation of state (EOS) for matter at extreme densities has become\naccessible. Here, we utilize a hybrid EOS formulation that combines an\nempirical parameterization centered around the nuclear saturation density with\na generic three-segment piecewise polytrope model at higher densities. We\nincorporate data derived from chiral effective field theory ($\\chi$EFT),\nperturbative quantum chromodynamics (pQCD), and from experiments such as\nPREX-II and CREX. Furthermore, we examine the influence of a total of 129 NS\nmass measurements up to April 2023, as well as simultaneous mass and radius\nmeasurements derived from the X-ray emission from surface hot spots on NSs.\nAdditionally, we consider constraints on tidal properties inferred from the\ngravitational waves emitted by coalescing NS binaries. To integrate this\nextensive and varied array of constraints, we utilize a hierarchical Bayesian\nstatistical framework to simultaneously deduce the EOS and the distribution of\nNS masses. We find that incorporating data from $\\chi$EFT significantly\ntightens the constraints on the EOS of NSs near or below the nuclear saturation\ndensity. However, constraints derived from pQCD computations and nuclear\nexperiments such as PREX-II and CREX have minimal impact."
                },
                "authors": [
                    {
                        "name": "Bhaskar Biswas"
                    },
                    {
                        "name": "Stephan Rosswog"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Rosswog"
                },
                "author": "Stephan Rosswog",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12040v3",
                "updated": "2024-08-27T16:25:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    25,
                    47,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-01T17:59:55Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    17,
                    59,
                    55,
                    0,
                    183,
                    0
                ],
                "title": "Comprehensive Performance Evaluation of YOLOv10, YOLOv9 and YOLOv8 on\n  Detecting and Counting Fruitlet in Complex Orchard Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive Performance Evaluation of YOLOv10, YOLOv9 and YOLOv8 on\n  Detecting and Counting Fruitlet in Complex Orchard Environments"
                },
                "summary": "This study performed an extensive evaluation of the performances of all\nconfigurations of YOLOv8, YOLOv9, and YOLOv10 object detection algorithms for\nfruitlet (of green fruit) detection in commercial orchards. Additionally, this\nresearch performed and validated in-field counting of fruitlets using an iPhone\nand machine vision sensors in 5 different apple varieties (Scifresh, Scilate,\nHoneycrisp, Cosmic crisp & Golden delicious). This comprehensive investigation\nof total 17 different configurations (5 for YOLOv8, 6 for YOLOv9 and 6 for\nYOLOv10) revealed that YOLOv9 outperforms YOLOv10 and YOLOv8 in terms of\nmAP@50, while YOLOv10x outperformed all 17 configurations tested in terms of\nprecision and recall. Specifically, YOLOv9 Gelan-e achieved the highest mAP@50\nof 0.935, outperforming YOLOv10n's 0.921 and YOLOv8s's 0.924. In terms of\nprecision, YOLOv10x achieved the highest precision of 0.908, indicating\nsuperior object identification accuracy compared to other configurations tested\n(e.g. YOLOv9 Gelan-c with a precision of 0.903 and YOLOv8m with 0.897. In terms\nof recall, YOLOv10s achieved the highest in its series (0.872), while YOLOv9\nGelan m performed the best among YOLOv9 configurations (0.899), and YOLOv8n\nperformed the best among the YOLOv8 configurations (0.883). Meanwhile, three\nconfigurations of YOLOv10: YOLOv10b, YOLOv10l, and YOLOv10x achieved superior\npost-processing speeds of 1.5 milliseconds, outperforming all other\nconfigurations within the YOLOv9 and YOLOv8 families. Specifically, YOLOv9\nGelan-e recorded a post-processing speed of 1.9 milliseconds, and YOLOv8m\nachieved 2.1 milliseconds. Furthermore, YOLOv8n exhibited the highest inference\nspeed among all configurations tested, achieving a processing time of 4.1\nmilliseconds while YOLOv9 Gelan-t and YOLOv10n also demonstrated comparatively\nslower inference speeds of 9.3 ms and 5.5 ms, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study performed an extensive evaluation of the performances of all\nconfigurations of YOLOv8, YOLOv9, and YOLOv10 object detection algorithms for\nfruitlet (of green fruit) detection in commercial orchards. Additionally, this\nresearch performed and validated in-field counting of fruitlets using an iPhone\nand machine vision sensors in 5 different apple varieties (Scifresh, Scilate,\nHoneycrisp, Cosmic crisp & Golden delicious). This comprehensive investigation\nof total 17 different configurations (5 for YOLOv8, 6 for YOLOv9 and 6 for\nYOLOv10) revealed that YOLOv9 outperforms YOLOv10 and YOLOv8 in terms of\nmAP@50, while YOLOv10x outperformed all 17 configurations tested in terms of\nprecision and recall. Specifically, YOLOv9 Gelan-e achieved the highest mAP@50\nof 0.935, outperforming YOLOv10n's 0.921 and YOLOv8s's 0.924. In terms of\nprecision, YOLOv10x achieved the highest precision of 0.908, indicating\nsuperior object identification accuracy compared to other configurations tested\n(e.g. YOLOv9 Gelan-c with a precision of 0.903 and YOLOv8m with 0.897. In terms\nof recall, YOLOv10s achieved the highest in its series (0.872), while YOLOv9\nGelan m performed the best among YOLOv9 configurations (0.899), and YOLOv8n\nperformed the best among the YOLOv8 configurations (0.883). Meanwhile, three\nconfigurations of YOLOv10: YOLOv10b, YOLOv10l, and YOLOv10x achieved superior\npost-processing speeds of 1.5 milliseconds, outperforming all other\nconfigurations within the YOLOv9 and YOLOv8 families. Specifically, YOLOv9\nGelan-e recorded a post-processing speed of 1.9 milliseconds, and YOLOv8m\nachieved 2.1 milliseconds. Furthermore, YOLOv8n exhibited the highest inference\nspeed among all configurations tested, achieving a processing time of 4.1\nmilliseconds while YOLOv9 Gelan-t and YOLOv10n also demonstrated comparatively\nslower inference speeds of 9.3 ms and 5.5 ms, respectively."
                },
                "authors": [
                    {
                        "name": "Ranjan Sapkota"
                    },
                    {
                        "name": "Zhichao Meng"
                    },
                    {
                        "name": "Martin Churuvija"
                    },
                    {
                        "name": "Xiaoqiang Du"
                    },
                    {
                        "name": "Zenghong Ma"
                    },
                    {
                        "name": "Manoj Karkee"
                    }
                ],
                "author_detail": {
                    "name": "Manoj Karkee"
                },
                "author": "Manoj Karkee",
                "arxiv_comment": "14 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15172v1",
                "updated": "2024-08-27T16:10:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    10,
                    21,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T16:10:21Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    10,
                    21,
                    1,
                    240,
                    0
                ],
                "title": "X-Reflect: Cross-Reflection Prompting for Multimodal Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Reflect: Cross-Reflection Prompting for Multimodal Recommendation"
                },
                "summary": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) have been\nshown to enhance the effectiveness of enriching item descriptions, thereby\nimproving the accuracy of recommendation systems. However, most existing\napproaches either rely on text-only prompting or employ basic multimodal\nstrategies that do not fully exploit the complementary information available\nfrom both textual and visual modalities. This paper introduces a novel\nframework, Cross-Reflection Prompting, termed X-Reflect, designed to address\nthese limitations by prompting LMMs to explicitly identify and reconcile\nsupportive and conflicting information between text and images. By capturing\nnuanced insights from both modalities, this approach generates more\ncomprehensive and contextually richer item representations. Extensive\nexperiments conducted on two widely used benchmarks demonstrate that our method\noutperforms existing prompting baselines in downstream recommendation accuracy.\nAdditionally, we evaluate the generalizability of our framework across\ndifferent LMM backbones and the robustness of the prompting strategies,\noffering insights for optimization. This work underscores the importance of\nintegrating multimodal information and presents a novel solution for improving\nitem understanding in multimodal recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) have been\nshown to enhance the effectiveness of enriching item descriptions, thereby\nimproving the accuracy of recommendation systems. However, most existing\napproaches either rely on text-only prompting or employ basic multimodal\nstrategies that do not fully exploit the complementary information available\nfrom both textual and visual modalities. This paper introduces a novel\nframework, Cross-Reflection Prompting, termed X-Reflect, designed to address\nthese limitations by prompting LMMs to explicitly identify and reconcile\nsupportive and conflicting information between text and images. By capturing\nnuanced insights from both modalities, this approach generates more\ncomprehensive and contextually richer item representations. Extensive\nexperiments conducted on two widely used benchmarks demonstrate that our method\noutperforms existing prompting baselines in downstream recommendation accuracy.\nAdditionally, we evaluate the generalizability of our framework across\ndifferent LMM backbones and the robustness of the prompting strategies,\noffering insights for optimization. This work underscores the importance of\nintegrating multimodal information and presents a novel solution for improving\nitem understanding in multimodal recommendation systems."
                },
                "authors": [
                    {
                        "name": "Hanjia Lyu"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Md Mehrab Tanjim"
                    },
                    {
                        "name": "Stefano Petrangeli"
                    },
                    {
                        "name": "Somdeb Sarkhel"
                    },
                    {
                        "name": "Jiebo Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jiebo Luo"
                },
                "author": "Jiebo Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15171v1",
                "updated": "2024-08-27T16:09:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    9,
                    56,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T16:09:56Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    9,
                    56,
                    1,
                    240,
                    0
                ],
                "title": "Measuring text summarization factuality using atomic facts entailment\n  metrics in the context of retrieval augmented generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring text summarization factuality using atomic facts entailment\n  metrics in the context of retrieval augmented generation"
                },
                "summary": "The use of large language models (LLMs) has significantly increased since the\nintroduction of ChatGPT in 2022, demonstrating their value across various\napplications. However, a major challenge for enterprise and commercial adoption\nof LLMs is their tendency to generate inaccurate information, a phenomenon\nknown as \"hallucination.\" This project proposes a method for estimating the\nfactuality of a summary generated by LLMs when compared to a source text. Our\napproach utilizes Naive Bayes classification to assess the accuracy of the\ncontent produced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) has significantly increased since the\nintroduction of ChatGPT in 2022, demonstrating their value across various\napplications. However, a major challenge for enterprise and commercial adoption\nof LLMs is their tendency to generate inaccurate information, a phenomenon\nknown as \"hallucination.\" This project proposes a method for estimating the\nfactuality of a summary generated by LLMs when compared to a source text. Our\napproach utilizes Naive Bayes classification to assess the accuracy of the\ncontent produced."
                },
                "authors": [
                    {
                        "name": "N. E. Kriman"
                    }
                ],
                "author_detail": {
                    "name": "N. E. Kriman"
                },
                "author": "N. E. Kriman",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16947v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16947v2",
                "updated": "2024-08-27T16:08:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    8,
                    52,
                    1,
                    240,
                    0
                ],
                "published": "2024-04-25T18:00:37Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    18,
                    0,
                    37,
                    3,
                    116,
                    0
                ],
                "title": "Fuzzing MLIR Compilers with Custom Mutation Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fuzzing MLIR Compilers with Custom Mutation Synthesis"
                },
                "summary": "Compiler technologies in deep learning and domain-specific hardware\nacceleration are increasingly adopting extensible compiler frameworks such as\nMulti-Level Intermediate Representation (MLIR) to facilitate more efficient\ndevelopment. With MLIR, compiler developers can easily define their own custom\nIRs in the form of MLIR dialects. However, the diversity and rapid evolution of\nsuch custom IRs make it impractical to manually write a custom test generator\nfor each dialect. To address this problem, we design a new test generator\ncalled SYNTHFUZZ that combines grammar-based fuzzing with custom mutation\nsynthesis. The key essence of SYNTHFUZZ is two fold: (1) It automatically\ninfers parameterized context-dependent custom mutations from existing test\ncases. (2) It then concretizes the mutation's content depending on the target\ncontext and reduces the chance of inserting invalid edits by performing\nk-ancestor and pre(post)fix matching. SYNTHFUZZ obviates the need to manually\ndefine custom mutation operators for each dialect. We compare SYNTHFUZZ to\nthree baselines: Grammarinator, MLIRSmith, and NeuRI. We conduct this\ncomprehensive comparison on four different MLIR projects. Each project defines\na new set of MLIR dialects where manually writing a custom test generator would\ntake weeks of effort. Our evaluation shows that SYNTHFUZZ on average improves\nMLIR dialect pair coverage by 1.75 times, which increases branch coverage by\n1.22 times. Further, we show that our context dependent custom mutation\nincreases the proportion of valid tests by up to 1.11 times, indicating that\nSYNTHFUZZ correctly concretizes its parameterized mutations with respect to the\ntarget context. Parameterization of the mutations reduces the fraction of tests\nviolating the base MLIR constraints by 0.57 times, increasing the time spent\nfuzzing dialect-specific code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compiler technologies in deep learning and domain-specific hardware\nacceleration are increasingly adopting extensible compiler frameworks such as\nMulti-Level Intermediate Representation (MLIR) to facilitate more efficient\ndevelopment. With MLIR, compiler developers can easily define their own custom\nIRs in the form of MLIR dialects. However, the diversity and rapid evolution of\nsuch custom IRs make it impractical to manually write a custom test generator\nfor each dialect. To address this problem, we design a new test generator\ncalled SYNTHFUZZ that combines grammar-based fuzzing with custom mutation\nsynthesis. The key essence of SYNTHFUZZ is two fold: (1) It automatically\ninfers parameterized context-dependent custom mutations from existing test\ncases. (2) It then concretizes the mutation's content depending on the target\ncontext and reduces the chance of inserting invalid edits by performing\nk-ancestor and pre(post)fix matching. SYNTHFUZZ obviates the need to manually\ndefine custom mutation operators for each dialect. We compare SYNTHFUZZ to\nthree baselines: Grammarinator, MLIRSmith, and NeuRI. We conduct this\ncomprehensive comparison on four different MLIR projects. Each project defines\na new set of MLIR dialects where manually writing a custom test generator would\ntake weeks of effort. Our evaluation shows that SYNTHFUZZ on average improves\nMLIR dialect pair coverage by 1.75 times, which increases branch coverage by\n1.22 times. Further, we show that our context dependent custom mutation\nincreases the proportion of valid tests by up to 1.11 times, indicating that\nSYNTHFUZZ correctly concretizes its parameterized mutations with respect to the\ntarget context. Parameterization of the mutations reduces the fraction of tests\nviolating the base MLIR constraints by 0.57 times, increasing the time spent\nfuzzing dialect-specific code."
                },
                "authors": [
                    {
                        "name": "Ben Limpanukorn"
                    },
                    {
                        "name": "Jiyuan Wang"
                    },
                    {
                        "name": "Hong Jin Kang"
                    },
                    {
                        "name": "Eric Zitong Zhou"
                    },
                    {
                        "name": "Miryung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Miryung Kim"
                },
                "author": "Miryung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16947v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16947v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13993v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13993v3",
                "updated": "2024-08-27T15:56:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    56,
                    33,
                    1,
                    240,
                    0
                ],
                "published": "2024-04-22T08:59:35Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    8,
                    59,
                    35,
                    0,
                    113,
                    0
                ],
                "title": "Zero-Shot Character Identification and Speaker Prediction in Comics via\n  Iterative Multimodal Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Character Identification and Speaker Prediction in Comics via\n  Iterative Multimodal Fusion"
                },
                "summary": "Recognizing characters and predicting speakers of dialogue are critical for\ncomic processing tasks, such as voice generation or translation. However,\nbecause characters vary by comic title, supervised learning approaches like\ntraining character classifiers which require specific annotations for each\ncomic title are infeasible. This motivates us to propose a novel zero-shot\napproach, allowing machines to identify characters and predict speaker names\nbased solely on unannotated comic images. In spite of their importance in\nreal-world applications, these task have largely remained unexplored due to\nchallenges in story comprehension and multimodal integration. Recent large\nlanguage models (LLMs) have shown great capability for text understanding and\nreasoning, while their application to multimodal content analysis is still an\nopen problem. To address this problem, we propose an iterative multimodal\nframework, the first to employ multimodal information for both character\nidentification and speaker prediction tasks. Our experiments demonstrate the\neffectiveness of the proposed framework, establishing a robust baseline for\nthese tasks. Furthermore, since our method requires no training data or\nannotations, it can be used as-is on any comic series.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing characters and predicting speakers of dialogue are critical for\ncomic processing tasks, such as voice generation or translation. However,\nbecause characters vary by comic title, supervised learning approaches like\ntraining character classifiers which require specific annotations for each\ncomic title are infeasible. This motivates us to propose a novel zero-shot\napproach, allowing machines to identify characters and predict speaker names\nbased solely on unannotated comic images. In spite of their importance in\nreal-world applications, these task have largely remained unexplored due to\nchallenges in story comprehension and multimodal integration. Recent large\nlanguage models (LLMs) have shown great capability for text understanding and\nreasoning, while their application to multimodal content analysis is still an\nopen problem. To address this problem, we propose an iterative multimodal\nframework, the first to employ multimodal information for both character\nidentification and speaker prediction tasks. Our experiments demonstrate the\neffectiveness of the proposed framework, establishing a robust baseline for\nthese tasks. Furthermore, since our method requires no training data or\nannotations, it can be used as-is on any comic series."
                },
                "authors": [
                    {
                        "name": "Yingxuan Li"
                    },
                    {
                        "name": "Ryota Hinami"
                    },
                    {
                        "name": "Kiyoharu Aizawa"
                    },
                    {
                        "name": "Yusuke Matsui"
                    }
                ],
                "author_detail": {
                    "name": "Yusuke Matsui"
                },
                "author": "Yusuke Matsui",
                "arxiv_comment": "Accepted to ACM Multimedia 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13993v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13993v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15149v1",
                "updated": "2024-08-27T15:41:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    41,
                    58,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T15:41:58Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    41,
                    58,
                    1,
                    240,
                    0
                ],
                "title": "LLOT: application of Laplacian Linear Optimal Transport in spatial\n  transcriptome reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLOT: application of Laplacian Linear Optimal Transport in spatial\n  transcriptome reconstruction"
                },
                "summary": "Single-cell RNA sequencing (scRNA-seq) allows transcriptional profiling, and\ncell-type annotation of individual cells. However, sample preparation in\ntypical scRNA-seq experiments often homogenizes the samples, thus spatial\nlocations of individual cells are often lost. Although spatial transcriptomic\ntechniques, such as in situ hybridization (ISH) or Slide-seq, can be used to\nmeasure gene expression in specific locations in samples, it remains a\nchallenge to measure or infer expression level for every gene at a single-cell\nresolution in every location in tissues. Existing computational methods show\npromise in reconstructing these missing data by integrating scRNA-seq data with\nspatial expression data such as those obtained from spatial transcriptomics.\nHere we describe Laplacian Linear Optimal Transport (LLOT), an interpretable\nmethod to integrate single-cell and spatial transcriptomics data to reconstruct\nmissing information at a whole-genome and single-cell resolution. LLOT\niteratively corrects platform effects and employs Laplacian Optimal Transport\nto decompose each spot in spatial transcriptomics data into a spatially-smooth\nprobabilistic mixture of single cells. We benchmarked LLOT against several\nother methods on datasets of Drosophila embryo, mouse cerebellum and synthetic\ndatasets generated by scDesign3 in the paper, and another three datasets in the\nsupplementary. The results showed that LLOT consistently outperformed others in\nreconstructing spatial expressions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-cell RNA sequencing (scRNA-seq) allows transcriptional profiling, and\ncell-type annotation of individual cells. However, sample preparation in\ntypical scRNA-seq experiments often homogenizes the samples, thus spatial\nlocations of individual cells are often lost. Although spatial transcriptomic\ntechniques, such as in situ hybridization (ISH) or Slide-seq, can be used to\nmeasure gene expression in specific locations in samples, it remains a\nchallenge to measure or infer expression level for every gene at a single-cell\nresolution in every location in tissues. Existing computational methods show\npromise in reconstructing these missing data by integrating scRNA-seq data with\nspatial expression data such as those obtained from spatial transcriptomics.\nHere we describe Laplacian Linear Optimal Transport (LLOT), an interpretable\nmethod to integrate single-cell and spatial transcriptomics data to reconstruct\nmissing information at a whole-genome and single-cell resolution. LLOT\niteratively corrects platform effects and employs Laplacian Optimal Transport\nto decompose each spot in spatial transcriptomics data into a spatially-smooth\nprobabilistic mixture of single cells. We benchmarked LLOT against several\nother methods on datasets of Drosophila embryo, mouse cerebellum and synthetic\ndatasets generated by scDesign3 in the paper, and another three datasets in the\nsupplementary. The results showed that LLOT consistently outperformed others in\nreconstructing spatial expressions."
                },
                "authors": [
                    {
                        "name": "Junhao Zhu"
                    },
                    {
                        "name": "Kevin Zhang"
                    },
                    {
                        "name": "Dehan Kong"
                    },
                    {
                        "name": "Zhaolei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaolei Zhang"
                },
                "author": "Zhaolei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15138v1",
                "updated": "2024-08-27T15:23:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    23,
                    9,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T15:23:09Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    23,
                    9,
                    1,
                    240,
                    0
                ],
                "title": "How transformers learn structured data: insights from hierarchical\n  filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How transformers learn structured data: insights from hierarchical\n  filtering"
                },
                "summary": "We introduce a hierarchical filtering procedure for generative models of\nsequences on trees, enabling control over the range of positional correlations\nin the data. Leveraging this controlled setting, we provide evidence that\nvanilla encoder-only transformer architectures can implement the optimal Belief\nPropagation algorithm on both root classification and masked language modeling\ntasks. Correlations at larger distances corresponding to increasing layers of\nthe hierarchy are sequentially included as the network is trained. We analyze\nhow the transformer layers succeed by focusing on attention maps from models\ntrained with varying degrees of filtering. These attention maps show clear\nevidence for iterative hierarchical reconstruction of correlations, and we can\nrelate these observations to a plausible implementation of the exact inference\nalgorithm for the network sizes considered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a hierarchical filtering procedure for generative models of\nsequences on trees, enabling control over the range of positional correlations\nin the data. Leveraging this controlled setting, we provide evidence that\nvanilla encoder-only transformer architectures can implement the optimal Belief\nPropagation algorithm on both root classification and masked language modeling\ntasks. Correlations at larger distances corresponding to increasing layers of\nthe hierarchy are sequentially included as the network is trained. We analyze\nhow the transformer layers succeed by focusing on attention maps from models\ntrained with varying degrees of filtering. These attention maps show clear\nevidence for iterative hierarchical reconstruction of correlations, and we can\nrelate these observations to a plausible implementation of the exact inference\nalgorithm for the network sizes considered."
                },
                "authors": [
                    {
                        "name": "Jerome Garnier-Brun"
                    },
                    {
                        "name": "Marc Mézard"
                    },
                    {
                        "name": "Emanuele Moscato"
                    },
                    {
                        "name": "Luca Saglietti"
                    }
                ],
                "author_detail": {
                    "name": "Luca Saglietti"
                },
                "author": "Luca Saglietti",
                "arxiv_comment": "18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15137v1",
                "updated": "2024-08-27T15:19:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    19,
                    54,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T15:19:54Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    19,
                    54,
                    1,
                    240,
                    0
                ],
                "title": "Warm Jupiters around M-dwarfs are great opportunities for extensive\n  chemical, cloud and haze characterisation with JWST",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warm Jupiters around M-dwarfs are great opportunities for extensive\n  chemical, cloud and haze characterisation with JWST"
                },
                "summary": "The population of short-period giant exoplanets around M-dwarf stars is\nslowly rising. These planets present an extraordinary opportunity for\natmospheric characterisation and defy our current understanding of planetary\nformation. Furthermore, clouds and hazes are ubiquitous in warm exoplanets but\ntheir behaviour is still poorly understood. We study the case of a standard\nwarm Jupiter around a M-dwarf star to show the opportunity of this exoplanet\npopulation for atmospheric characterisation. We aim to derive the cloud, haze,\nand chemical budget of such planets using JWST. We leverage a 3D Global Climate\nModel, the generic PCM, to simulate the cloudy and cloud-free atmosphere of\nwarm Jupiters around a M-dwarf. We then post-process our simulations to produce\nspectral phase curves and transit spectra as would be seen with JWST.We show\nthat using the amplitude and offset of the spectral phase curves, we can\ndirectly infer the presence of clouds and hazes in the atmosphere of such giant\nplanets. Chemical characterisation of multiple species is possible with an\nunprecedented signal-to-noise ratio, using the transit spectrum in one single\nvisit. In such atmospheres, NH3 could be detected for the first time in a giant\nexoplanet. We make the case that these planets are key to understanding the\ncloud and haze budget in warm giants. Finally, such planets are targets of\ngreat interest for Ariel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The population of short-period giant exoplanets around M-dwarf stars is\nslowly rising. These planets present an extraordinary opportunity for\natmospheric characterisation and defy our current understanding of planetary\nformation. Furthermore, clouds and hazes are ubiquitous in warm exoplanets but\ntheir behaviour is still poorly understood. We study the case of a standard\nwarm Jupiter around a M-dwarf star to show the opportunity of this exoplanet\npopulation for atmospheric characterisation. We aim to derive the cloud, haze,\nand chemical budget of such planets using JWST. We leverage a 3D Global Climate\nModel, the generic PCM, to simulate the cloudy and cloud-free atmosphere of\nwarm Jupiters around a M-dwarf. We then post-process our simulations to produce\nspectral phase curves and transit spectra as would be seen with JWST.We show\nthat using the amplitude and offset of the spectral phase curves, we can\ndirectly infer the presence of clouds and hazes in the atmosphere of such giant\nplanets. Chemical characterisation of multiple species is possible with an\nunprecedented signal-to-noise ratio, using the transit spectrum in one single\nvisit. In such atmospheres, NH3 could be detected for the first time in a giant\nexoplanet. We make the case that these planets are key to understanding the\ncloud and haze budget in warm giants. Finally, such planets are targets of\ngreat interest for Ariel."
                },
                "authors": [
                    {
                        "name": "Lucas Teinturier"
                    },
                    {
                        "name": "Elsa Ducrot"
                    },
                    {
                        "name": "Benjamin Charnay"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Charnay"
                },
                "author": "Benjamin Charnay",
                "arxiv_comment": "Accepted for publication in Astronomy & Astrophysics. 7 pages, 4\n  figures and 6 figures in appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15136v1",
                "updated": "2024-08-27T15:19:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    19,
                    7,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T15:19:07Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    19,
                    7,
                    1,
                    240,
                    0
                ],
                "title": "Low-Budget Simulation-Based Inference with Bayesian Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Budget Simulation-Based Inference with Bayesian Neural Networks"
                },
                "summary": "Simulation-based inference methods have been shown to be inaccurate in the\ndata-poor regime, when training simulations are limited or expensive. Under\nthese circumstances, the inference network is particularly prone to\noverfitting, and using it without accounting for the computational uncertainty\narising from the lack of identifiability of the network weights can lead to\nunreliable results. To address this issue, we propose using Bayesian neural\nnetworks in low-budget simulation-based inference, thereby explicitly\naccounting for the computational uncertainty of the posterior approximation. We\ndesign a family of Bayesian neural network priors that are tailored for\ninference and show that they lead to well-calibrated posteriors on tested\nbenchmarks, even when as few as $O(10)$ simulations are available. This opens\nup the possibility of performing reliable simulation-based inference using very\nexpensive simulators, as we demonstrate on a problem from the field of\ncosmology where single simulations are computationally expensive. We show that\nBayesian neural networks produce informative and well-calibrated posterior\nestimates with only a few hundred simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference methods have been shown to be inaccurate in the\ndata-poor regime, when training simulations are limited or expensive. Under\nthese circumstances, the inference network is particularly prone to\noverfitting, and using it without accounting for the computational uncertainty\narising from the lack of identifiability of the network weights can lead to\nunreliable results. To address this issue, we propose using Bayesian neural\nnetworks in low-budget simulation-based inference, thereby explicitly\naccounting for the computational uncertainty of the posterior approximation. We\ndesign a family of Bayesian neural network priors that are tailored for\ninference and show that they lead to well-calibrated posteriors on tested\nbenchmarks, even when as few as $O(10)$ simulations are available. This opens\nup the possibility of performing reliable simulation-based inference using very\nexpensive simulators, as we demonstrate on a problem from the field of\ncosmology where single simulations are computationally expensive. We show that\nBayesian neural networks produce informative and well-calibrated posterior\nestimates with only a few hundred simulations."
                },
                "authors": [
                    {
                        "name": "Arnaud Delaunoy"
                    },
                    {
                        "name": "Maxence de la Brassinne Bonardeaux"
                    },
                    {
                        "name": "Siddharth Mishra-Sharma"
                    },
                    {
                        "name": "Gilles Louppe"
                    }
                ],
                "author_detail": {
                    "name": "Gilles Louppe"
                },
                "author": "Gilles Louppe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07531v2",
                "updated": "2024-08-27T15:16:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    16,
                    6,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-14T13:03:41Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    3,
                    41,
                    2,
                    227,
                    0
                ],
                "title": "Development of a Large Language Model-based Multi-Agent Clinical\n  Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based\n  Triage and Treatment Planning in Emergency Departments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a Large Language Model-based Multi-Agent Clinical\n  Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based\n  Triage and Treatment Planning in Emergency Departments"
                },
                "summary": "Emergency department (ED) overcrowding and the complexity of rapid\ndecision-making in critical care settings pose significant challenges to\nhealthcare systems worldwide. While clinical decision support systems (CDSS)\nhave shown promise, the integration of large language models (LLMs) offers new\npossibilities for enhancing triage accuracy and clinical decision-making. This\nstudy presents an LLM-driven CDSS designed to assist ED physicians and nurses\nin patient triage, treatment planning, and overall emergency care management.\n  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM,\norchestrated by CrewAI and Langchain. The system comprises four AI agents\nemulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED\nCoordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for\ntriage assessment and integrates with the RxNorm API for medication management.\n  The model was evaluated using the Asclepius dataset, with performance\nassessed by a clinical emergency medicine specialist. The CDSS demonstrated\nhigh accuracy in triage decision-making compared to the baseline of a\nsingle-agent system. Furthermore, the system exhibited strong performance in\ncritical areas, including primary diagnosis, critical findings identification,\ndisposition decision-making, treatment planning, and resource allocation.\n  Our multi-agent CDSS demonstrates significant potential for supporting\ncomprehensive emergency care management. By leveraging state-of-the-art AI\ntechnologies, this system offers a scalable and adaptable tool that could\nenhance emergency medical care delivery, potentially alleviating ED\novercrowding and improving patient outcomes. This work contributes to the\ngrowing field of AI applications in emergency medicine and offers a promising\ndirection for future research and clinical implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency department (ED) overcrowding and the complexity of rapid\ndecision-making in critical care settings pose significant challenges to\nhealthcare systems worldwide. While clinical decision support systems (CDSS)\nhave shown promise, the integration of large language models (LLMs) offers new\npossibilities for enhancing triage accuracy and clinical decision-making. This\nstudy presents an LLM-driven CDSS designed to assist ED physicians and nurses\nin patient triage, treatment planning, and overall emergency care management.\n  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM,\norchestrated by CrewAI and Langchain. The system comprises four AI agents\nemulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED\nCoordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for\ntriage assessment and integrates with the RxNorm API for medication management.\n  The model was evaluated using the Asclepius dataset, with performance\nassessed by a clinical emergency medicine specialist. The CDSS demonstrated\nhigh accuracy in triage decision-making compared to the baseline of a\nsingle-agent system. Furthermore, the system exhibited strong performance in\ncritical areas, including primary diagnosis, critical findings identification,\ndisposition decision-making, treatment planning, and resource allocation.\n  Our multi-agent CDSS demonstrates significant potential for supporting\ncomprehensive emergency care management. By leveraging state-of-the-art AI\ntechnologies, this system offers a scalable and adaptable tool that could\nenhance emergency medical care delivery, potentially alleviating ED\novercrowding and improving patient outcomes. This work contributes to the\ngrowing field of AI applications in emergency medicine and offers a promising\ndirection for future research and clinical implementation."
                },
                "authors": [
                    {
                        "name": "Seungjun Han"
                    },
                    {
                        "name": "Wongyung Choi"
                    }
                ],
                "author_detail": {
                    "name": "Wongyung Choi"
                },
                "author": "Wongyung Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15133v1",
                "updated": "2024-08-27T15:13:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    13,
                    6,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T15:13:06Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    13,
                    6,
                    1,
                    240,
                    0
                ],
                "title": "Using LLMs for Explaining Sets of Counterfactual Examples to Final Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs for Explaining Sets of Counterfactual Examples to Final Users"
                },
                "summary": "Causality is vital for understanding true cause-and-effect relationships\nbetween variables within predictive models, rather than relying on mere\ncorrelations, making it highly relevant in the field of Explainable AI. In an\nautomated decision-making scenario, causal inference methods can analyze the\nunderlying data-generation process, enabling explanations of a model's decision\nby manipulating features and creating counterfactual examples. These\ncounterfactuals explore hypothetical scenarios where a minimal number of\nfactors are altered, providing end-users with valuable information on how to\nchange their situation. However, interpreting a set of multiple counterfactuals\ncan be challenging for end-users who are not used to analyzing raw data\nrecords. In our work, we propose a novel multi-step pipeline that uses\ncounterfactuals to generate natural language explanations of actions that will\nlead to a change in outcome in classifiers of tabular data using LLMs. This\npipeline is designed to guide the LLM through smaller tasks that mimic human\nreasoning when explaining a decision based on counterfactual cases. We\nconducted various experiments using a public dataset and proposed a method of\nclosed-loop evaluation to assess the coherence of the final explanation with\nthe counterfactuals, as well as the quality of the content. Results are\npromising, although further experiments with other datasets and human\nevaluations should be carried out.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causality is vital for understanding true cause-and-effect relationships\nbetween variables within predictive models, rather than relying on mere\ncorrelations, making it highly relevant in the field of Explainable AI. In an\nautomated decision-making scenario, causal inference methods can analyze the\nunderlying data-generation process, enabling explanations of a model's decision\nby manipulating features and creating counterfactual examples. These\ncounterfactuals explore hypothetical scenarios where a minimal number of\nfactors are altered, providing end-users with valuable information on how to\nchange their situation. However, interpreting a set of multiple counterfactuals\ncan be challenging for end-users who are not used to analyzing raw data\nrecords. In our work, we propose a novel multi-step pipeline that uses\ncounterfactuals to generate natural language explanations of actions that will\nlead to a change in outcome in classifiers of tabular data using LLMs. This\npipeline is designed to guide the LLM through smaller tasks that mimic human\nreasoning when explaining a decision based on counterfactual cases. We\nconducted various experiments using a public dataset and proposed a method of\nclosed-loop evaluation to assess the coherence of the final explanation with\nthe counterfactuals, as well as the quality of the content. Results are\npromising, although further experiments with other datasets and human\nevaluations should be carried out."
                },
                "authors": [
                    {
                        "name": "Arturo Fredes"
                    },
                    {
                        "name": "Jordi Vitria"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Vitria"
                },
                "author": "Jordi Vitria",
                "arxiv_comment": "Presented as a poster in the 2nd Workshop on Causal Inference and\n  Machine Learning in Practice at KDD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00283v2",
                "updated": "2024-08-27T15:09:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    9,
                    4,
                    1,
                    240,
                    0
                ],
                "published": "2024-03-01T04:57:19Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    4,
                    57,
                    19,
                    4,
                    61,
                    0
                ],
                "title": "Risk Twin: Real-time Risk Visualization and Control for Structural\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk Twin: Real-time Risk Visualization and Control for Structural\n  Systems"
                },
                "summary": "Digital twinning in structural engineering is a rapidly evolving technology\nthat aims to eliminate the gap between physical systems and their digital\nmodels through real-time sensing, visualization, and control techniques.\nAlthough Digital Twins can offer dynamic insights into physical systems, their\naccuracy is inevitably compromised by uncertainties in sensing, modeling,\nsimulation, and control. This paper proposes a specialized Digital Twin\nformulation, named Risk Twin, designed for real-time risk visualization and\nrisk-informed control of structural systems. Integrating structural reliability\nand Bayesian inference methods with Digital Twinning techniques, Risk Twin can\nanalyze and visualize the reliability indices for structural components in\nreal-time. To facilitate real-time inference and reliability updating, a\nsimulation-free scheme is proposed. This scheme leverages precomputed\nquantities prepared during an offline phase for rapid inference in the online\nphase. Proof-of-concept numerical and real-world Risk Twins are constructed to\nshowcase the proposed concepts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital twinning in structural engineering is a rapidly evolving technology\nthat aims to eliminate the gap between physical systems and their digital\nmodels through real-time sensing, visualization, and control techniques.\nAlthough Digital Twins can offer dynamic insights into physical systems, their\naccuracy is inevitably compromised by uncertainties in sensing, modeling,\nsimulation, and control. This paper proposes a specialized Digital Twin\nformulation, named Risk Twin, designed for real-time risk visualization and\nrisk-informed control of structural systems. Integrating structural reliability\nand Bayesian inference methods with Digital Twinning techniques, Risk Twin can\nanalyze and visualize the reliability indices for structural components in\nreal-time. To facilitate real-time inference and reliability updating, a\nsimulation-free scheme is proposed. This scheme leverages precomputed\nquantities prepared during an offline phase for rapid inference in the online\nphase. Proof-of-concept numerical and real-world Risk Twins are constructed to\nshowcase the proposed concepts."
                },
                "authors": [
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Ziqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ziqi Wang"
                },
                "author": "Ziqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15128v1",
                "updated": "2024-08-27T15:08:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    8,
                    6,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T15:08:06Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    8,
                    6,
                    1,
                    240,
                    0
                ],
                "title": "Evaluating the Energy Consumption of Machine Learning: Systematic\n  Literature Review and Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Energy Consumption of Machine Learning: Systematic\n  Literature Review and Experiments"
                },
                "summary": "Monitoring, understanding, and optimizing the energy consumption of Machine\nLearning (ML) are various reasons why it is necessary to evaluate the energy\nusage of ML. However, there exists no universal tool that can answer this\nquestion for all use cases, and there may even be disagreement on how to\nevaluate energy consumption for a specific use case. Tools and methods are\nbased on different approaches, each with their own advantages and drawbacks,\nand they need to be mapped out and explained in order to select the most\nsuitable one for a given situation. We address this challenge through two\napproaches. First, we conduct a systematic literature review of all tools and\nmethods that permit to evaluate the energy consumption of ML (both at training\nand at inference), irrespective of whether they were originally designed for\nmachine learning or general software. Second, we develop and use an\nexperimental protocol to compare a selection of these tools and methods. The\ncomparison is both qualitative and quantitative on a range of ML tasks of\ndifferent nature (vision, language) and computational complexity. The\nsystematic literature review serves as a comprehensive guide for understanding\nthe array of tools and methods used in evaluating energy consumption of ML, for\nvarious use cases going from basic energy monitoring to consumption\noptimization. Two open-source repositories are provided for further\nexploration. The first one contains tools that can be used to replicate this\nwork or extend the current review. The second repository houses the\nexperimental protocol, allowing users to augment the protocol with new ML\ncomputing tasks and additional energy evaluation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring, understanding, and optimizing the energy consumption of Machine\nLearning (ML) are various reasons why it is necessary to evaluate the energy\nusage of ML. However, there exists no universal tool that can answer this\nquestion for all use cases, and there may even be disagreement on how to\nevaluate energy consumption for a specific use case. Tools and methods are\nbased on different approaches, each with their own advantages and drawbacks,\nand they need to be mapped out and explained in order to select the most\nsuitable one for a given situation. We address this challenge through two\napproaches. First, we conduct a systematic literature review of all tools and\nmethods that permit to evaluate the energy consumption of ML (both at training\nand at inference), irrespective of whether they were originally designed for\nmachine learning or general software. Second, we develop and use an\nexperimental protocol to compare a selection of these tools and methods. The\ncomparison is both qualitative and quantitative on a range of ML tasks of\ndifferent nature (vision, language) and computational complexity. The\nsystematic literature review serves as a comprehensive guide for understanding\nthe array of tools and methods used in evaluating energy consumption of ML, for\nvarious use cases going from basic energy monitoring to consumption\noptimization. Two open-source repositories are provided for further\nexploration. The first one contains tools that can be used to replicate this\nwork or extend the current review. The second repository houses the\nexperimental protocol, allowing users to augment the protocol with new ML\ncomputing tasks and additional energy evaluation tools."
                },
                "authors": [
                    {
                        "name": "Charlotte Rodriguez"
                    },
                    {
                        "name": "Laura Degioanni"
                    },
                    {
                        "name": "Laetitia Kameni"
                    },
                    {
                        "name": "Richard Vidal"
                    },
                    {
                        "name": "Giovanni Neglia"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Neglia"
                },
                "author": "Giovanni Neglia",
                "arxiv_comment": "52 pages,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15126v1",
                "updated": "2024-08-27T15:07:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    7,
                    27,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T15:07:27Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    7,
                    27,
                    1,
                    240,
                    0
                ],
                "title": "Force-Guided Bridge Matching for Full-Atom Time-Coarsened Dynamics of\n  Peptides",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Force-Guided Bridge Matching for Full-Atom Time-Coarsened Dynamics of\n  Peptides"
                },
                "summary": "Molecular Dynamics (MD) simulations are irreplaceable and ubiquitous in\nfields of materials science, chemistry, pharmacology just to name a few.\nConventional MD simulations are plagued by numerical stability as well as long\nequilibration time issues, which limits broader applications of MD simulations.\nRecently, a surge of deep learning approaches have been devised for\ntime-coarsened dynamics, which learns the state transition mechanism over much\nlarger time scales to overcome these limitations. However, only a few methods\ntarget the underlying Boltzmann distribution by resampling techniques, where\nproposals are rarely accepted as new states with low efficiency. In this work,\nwe propose a force-guided bridge matching model, FBM, a novel framework that\nfirst incorporates physical priors into bridge matching for full-atom\ntime-coarsened dynamics. With the guidance of our well-designed intermediate\nforce field, FBM is feasible to target the Boltzmann-like distribution by\ndirect inference without extra steps. Experiments on small peptides verify our\nsuperiority in terms of comprehensive metrics and demonstrate transferability\nto unseen peptide systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular Dynamics (MD) simulations are irreplaceable and ubiquitous in\nfields of materials science, chemistry, pharmacology just to name a few.\nConventional MD simulations are plagued by numerical stability as well as long\nequilibration time issues, which limits broader applications of MD simulations.\nRecently, a surge of deep learning approaches have been devised for\ntime-coarsened dynamics, which learns the state transition mechanism over much\nlarger time scales to overcome these limitations. However, only a few methods\ntarget the underlying Boltzmann distribution by resampling techniques, where\nproposals are rarely accepted as new states with low efficiency. In this work,\nwe propose a force-guided bridge matching model, FBM, a novel framework that\nfirst incorporates physical priors into bridge matching for full-atom\ntime-coarsened dynamics. With the guidance of our well-designed intermediate\nforce field, FBM is feasible to target the Boltzmann-like distribution by\ndirect inference without extra steps. Experiments on small peptides verify our\nsuperiority in terms of comprehensive metrics and demonstrate transferability\nto unseen peptide systems."
                },
                "authors": [
                    {
                        "name": "Ziyang Yu"
                    },
                    {
                        "name": "Wenbing Huang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13960v2",
                "updated": "2024-08-27T15:06:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    6,
                    17,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-25T23:48:11Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    23,
                    48,
                    11,
                    6,
                    238,
                    0
                ],
                "title": "Time Series Analysis for Education: Methods, Applications, and Future\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Analysis for Education: Methods, Applications, and Future\n  Directions"
                },
                "summary": "Recent advancements in the collection and analysis of sequential educational\ndata have brought time series analysis to a pivotal position in educational\nresearch, highlighting its essential role in facilitating data-driven\ndecision-making. However, there is a lack of comprehensive summaries that\nconsolidate these advancements. To the best of our knowledge, this paper is the\nfirst to provide a comprehensive review of time series analysis techniques\nspecifically within the educational context. We begin by exploring the\nlandscape of educational data analytics, categorizing various data sources and\ntypes relevant to education. We then review four prominent time series\nmethods-forecasting, classification, clustering, and anomaly\ndetection-illustrating their specific application points in educational\nsettings. Subsequently, we present a range of educational scenarios and\napplications, focusing on how these methods are employed to address diverse\neducational tasks, which highlights the practical integration of multiple time\nseries methods to solve complex educational problems. Finally, we conclude with\na discussion on future directions, including personalized learning analytics,\nmultimodal data fusion, and the role of large language models (LLMs) in\neducational time series. The contributions of this paper include a detailed\ntaxonomy of educational data, a synthesis of time series techniques with\nspecific educational applications, and a forward-looking perspective on\nemerging trends and future research opportunities in educational analysis. The\nrelated papers and resources are available and regularly updated at the project\npage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the collection and analysis of sequential educational\ndata have brought time series analysis to a pivotal position in educational\nresearch, highlighting its essential role in facilitating data-driven\ndecision-making. However, there is a lack of comprehensive summaries that\nconsolidate these advancements. To the best of our knowledge, this paper is the\nfirst to provide a comprehensive review of time series analysis techniques\nspecifically within the educational context. We begin by exploring the\nlandscape of educational data analytics, categorizing various data sources and\ntypes relevant to education. We then review four prominent time series\nmethods-forecasting, classification, clustering, and anomaly\ndetection-illustrating their specific application points in educational\nsettings. Subsequently, we present a range of educational scenarios and\napplications, focusing on how these methods are employed to address diverse\neducational tasks, which highlights the practical integration of multiple time\nseries methods to solve complex educational problems. Finally, we conclude with\na discussion on future directions, including personalized learning analytics,\nmultimodal data fusion, and the role of large language models (LLMs) in\neducational time series. The contributions of this paper include a detailed\ntaxonomy of educational data, a synthesis of time series techniques with\nspecific educational applications, and a forward-looking perspective on\nemerging trends and future research opportunities in educational analysis. The\nrelated papers and resources are available and regularly updated at the project\npage."
                },
                "authors": [
                    {
                        "name": "Shengzhong Mao"
                    },
                    {
                        "name": "Chaoli Zhang"
                    },
                    {
                        "name": "Yichi Song"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Xiao-Jun Zeng"
                    },
                    {
                        "name": "Zenglin Xu"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "24 pages, 3 figures, 6 tables, project page: see\n  https://github.com/ai-for-edu/time-series-analysis-for-education",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15119v2",
                "updated": "2024-08-28T09:11:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    11,
                    55,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-27T14:58:13Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    58,
                    13,
                    1,
                    240,
                    0
                ],
                "title": "Urdu Digital Text Word Optical Character Recognition Using Permuted Auto\n  Regressive Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urdu Digital Text Word Optical Character Recognition Using Permuted Auto\n  Regressive Sequence Modeling"
                },
                "summary": "This research paper presents a novel word-level Optical Character Recognition\n(OCR) model developed specifically for digital Urdu text. The model utilizes\ntransformer-based architectures and attention mechanisms to address the unique\nchallenges of recognizing Urdu script, which includes handling a diverse range\nof text styles, fonts, and variations. Trained on a comprehensive dataset of\napproximately 160,000 Urdu text images, the model incorporates a permuted\nautoregressive sequence (PARSeq) architecture. This design enables\ncontext-aware inference and iterative refinement by leveraging bidirectional\ncontext information, significantly enhancing its ability to accurately\nrecognize Urdu characters. The model achieves a character error rate (CER) of\n0.178, highlighting its effectiveness and precision in real-world applications.\nHowever, the model has some limitations, such as difficulties with blurred\nimages, non-horizontal orientations, and the presence of trailing punctuation\nmarks, which can introduce noise into the recognition process. Addressing these\nchallenges will be a key focus of future work. Future research will aim to\nfurther refine the model through advanced data augmentation techniques,\noptimization of hyperparameters, and the integration of context-aware language\nmodels, ultimately enhancing the model's performance and robustness in Urdu\ntext recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research paper presents a novel word-level Optical Character Recognition\n(OCR) model developed specifically for digital Urdu text. The model utilizes\ntransformer-based architectures and attention mechanisms to address the unique\nchallenges of recognizing Urdu script, which includes handling a diverse range\nof text styles, fonts, and variations. Trained on a comprehensive dataset of\napproximately 160,000 Urdu text images, the model incorporates a permuted\nautoregressive sequence (PARSeq) architecture. This design enables\ncontext-aware inference and iterative refinement by leveraging bidirectional\ncontext information, significantly enhancing its ability to accurately\nrecognize Urdu characters. The model achieves a character error rate (CER) of\n0.178, highlighting its effectiveness and precision in real-world applications.\nHowever, the model has some limitations, such as difficulties with blurred\nimages, non-horizontal orientations, and the presence of trailing punctuation\nmarks, which can introduce noise into the recognition process. Addressing these\nchallenges will be a key focus of future work. Future research will aim to\nfurther refine the model through advanced data augmentation techniques,\noptimization of hyperparameters, and the integration of context-aware language\nmodels, ultimately enhancing the model's performance and robustness in Urdu\ntext recognition."
                },
                "authors": [
                    {
                        "name": "Ahmed Mustafa"
                    },
                    {
                        "name": "Muhammad Tahir Rafique"
                    },
                    {
                        "name": "Muhammad Ijlal Baig"
                    },
                    {
                        "name": "Hasan Sajid"
                    },
                    {
                        "name": "Muhammad Jawad Khan"
                    },
                    {
                        "name": "Karam Dad Kallu"
                    }
                ],
                "author_detail": {
                    "name": "Karam Dad Kallu"
                },
                "author": "Karam Dad Kallu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15117v1",
                "updated": "2024-08-27T14:56:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    56,
                    50,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T14:56:50Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    56,
                    50,
                    1,
                    240,
                    0
                ],
                "title": "Inferring ghost cities on the globe in newly developed urban areas based\n  on urban vitality with multi-source data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring ghost cities on the globe in newly developed urban areas based\n  on urban vitality with multi-source data"
                },
                "summary": "Due to rapid urbanization over the past 20 years, many newly developed areas\nhave lagged in socio-economic maturity, creating an imbalance with older cities\nand leading to the rise of \"ghost cities.\" However, due to the complexity of\nsocio-economic factors, no global studies have measured this phenomenon. We\npropose a unified framework based on urban vitality theory and multi-source\ndata, validated by various data sources. We derived 8841 natural cities\nglobally with an area over 5 square kiloxmeters and divided each into new urban\nareas (developed after 2005) and old urban areas (developed before 2005). Urban\nvitality was gauged using the density of road networks, points of interest\n(POIs), and population density with 1 km resolution across morphological,\nfunctional, and social dimensions. By comparing urban vitality in new and old\nurban areas, we quantify the ghost cities index (GCI) globally using the theory\nof urban vitality for the first time. The results reveal that the vitality of\nnew urban areas is 7.69% that of old ones. The top 5% (442) of cities were\ndesignated as ghost cities, a finding mirrored by news media and other\nresearch. This study sheds light on strategies for sustainable global\nurbanization, crucial for the United Nations' Sustainable Development Goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to rapid urbanization over the past 20 years, many newly developed areas\nhave lagged in socio-economic maturity, creating an imbalance with older cities\nand leading to the rise of \"ghost cities.\" However, due to the complexity of\nsocio-economic factors, no global studies have measured this phenomenon. We\npropose a unified framework based on urban vitality theory and multi-source\ndata, validated by various data sources. We derived 8841 natural cities\nglobally with an area over 5 square kiloxmeters and divided each into new urban\nareas (developed after 2005) and old urban areas (developed before 2005). Urban\nvitality was gauged using the density of road networks, points of interest\n(POIs), and population density with 1 km resolution across morphological,\nfunctional, and social dimensions. By comparing urban vitality in new and old\nurban areas, we quantify the ghost cities index (GCI) globally using the theory\nof urban vitality for the first time. The results reveal that the vitality of\nnew urban areas is 7.69% that of old ones. The top 5% (442) of cities were\ndesignated as ghost cities, a finding mirrored by news media and other\nresearch. This study sheds light on strategies for sustainable global\nurbanization, crucial for the United Nations' Sustainable Development Goals."
                },
                "authors": [
                    {
                        "name": "Yecheng Zhang"
                    },
                    {
                        "name": "Tangqi Tu"
                    },
                    {
                        "name": "Ying long"
                    }
                ],
                "author_detail": {
                    "name": "Ying long"
                },
                "author": "Ying long",
                "arxiv_comment": "28 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15116v1",
                "updated": "2024-08-27T14:55:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    55,
                    15,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T14:55:15Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    55,
                    15,
                    1,
                    240,
                    0
                ],
                "title": "Evaluating Stability of Unreflective Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Stability of Unreflective Alignment"
                },
                "summary": "Many theoretical obstacles to AI alignment are consequences of reflective\nstability - the problem of designing alignment mechanisms that the AI would not\ndisable if given the option. However, problems stemming from reflective\nstability are not obviously present in current LLMs, leading to disagreement\nover whether they will need to be solved to enable safe delegation of cognitive\nlabor. In this paper, we propose Counterfactual Priority Change (CPC)\ndestabilization as a mechanism by which reflective stability problems may arise\nin future LLMs. We describe two risk factors for CPC-destabilization: 1)\nCPC-based stepping back and 2) preference instability. We develop preliminary\nevaluations for each of these risk factors, and apply them to frontier LLMs.\nOur findings indicate that in current LLMs, increased scale and capability are\nassociated with increases in both CPC-based stepping back and preference\ninstability, suggesting that CPC-destabilization may cause reflective stability\nproblems in future LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many theoretical obstacles to AI alignment are consequences of reflective\nstability - the problem of designing alignment mechanisms that the AI would not\ndisable if given the option. However, problems stemming from reflective\nstability are not obviously present in current LLMs, leading to disagreement\nover whether they will need to be solved to enable safe delegation of cognitive\nlabor. In this paper, we propose Counterfactual Priority Change (CPC)\ndestabilization as a mechanism by which reflective stability problems may arise\nin future LLMs. We describe two risk factors for CPC-destabilization: 1)\nCPC-based stepping back and 2) preference instability. We develop preliminary\nevaluations for each of these risk factors, and apply them to frontier LLMs.\nOur findings indicate that in current LLMs, increased scale and capability are\nassociated with increases in both CPC-based stepping back and preference\ninstability, suggesting that CPC-destabilization may cause reflective stability\nproblems in future LLMs."
                },
                "authors": [
                    {
                        "name": "James Lucassen"
                    },
                    {
                        "name": "Mark Henry"
                    },
                    {
                        "name": "Philippa Wright"
                    },
                    {
                        "name": "Owen Yeung"
                    }
                ],
                "author_detail": {
                    "name": "Owen Yeung"
                },
                "author": "Owen Yeung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14286v2",
                "updated": "2024-08-27T14:54:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    54,
                    3,
                    1,
                    240,
                    0
                ],
                "published": "2024-04-22T15:37:08Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    15,
                    37,
                    8,
                    0,
                    113,
                    0
                ],
                "title": "Evidence for eccentricity in the population of binary black holes\n  observed by LIGO-Virgo-KAGRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence for eccentricity in the population of binary black holes\n  observed by LIGO-Virgo-KAGRA"
                },
                "summary": "Binary black holes (BBHs) in eccentric orbits produce distinct modulations\nthe emitted gravitational waves (GWs). The measurement of orbital eccentricity\ncan provide robust evidence for dynamical binary formation channels. We analyze\n57 GW events from the first, second and third observing runs of the\nLIGO-Virgo-KAGRA (LVK) Collaboration using a multipolar aligned-spin\ninspiral-merger-ringdown waveform model with two eccentric parameters:\neccentricity and relativistic anomaly. This is made computationally feasible\nwith the machine-learning code DINGO which accelerates inference by 2-3 orders\nof magnitude compared to traditional inference. First, we find eccentric\naligned-spin versus quasi-circular aligned-spin $\\log_{10}$ Bayes factors of\n1.84 to 4.75 (depending on the glitch mitigation) for GW200129, 3.0 for\nGW190701 and 1.77 for GW200208_22. We measure $e_{\\text{gw}, 10Hz}$ to be\n$0.27_{-0.12}^{+0.10}$ to $0.17_{-0.13}^{+0.14}$ for GW200129,\n$0.35_{-0.11}^{+0.32}$ for GW190701 and $0.35_{-0.21}^{+0.18}$ for GW200208_22.\nSecond, we find $\\log_{10}$ Bayes factors between the eccentric aligned-spin\nversus quasi-circular precessing-spin hypothesis between 1.43 and 4.92 for\nGW200129, 2.61 for GW190701 and 1.23 for GW200208_22. Third, our analysis does\nnot show evidence for eccentricity in GW190521, which has an eccentric\naligned-spin against quasi-circular aligned-spin $\\log_{10}$ Bayes factor of\n0.04. Fourth, we estimate that if we neglect the spin-precession and use an\nastrophysical prior, the probability of one out of the 57 events being\neccentric is greater than 99.5% or $(100 - 8.4 \\times 10^{-4})$% (depending on\nthe glitch mitigation). Fifth, we study the impact on parameter estimation when\nneglecting either eccentricity or higher modes in eccentric models. These\nresults underscore the importance of including eccentric parameters in the\ncharacterization of BBHs for GW detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary black holes (BBHs) in eccentric orbits produce distinct modulations\nthe emitted gravitational waves (GWs). The measurement of orbital eccentricity\ncan provide robust evidence for dynamical binary formation channels. We analyze\n57 GW events from the first, second and third observing runs of the\nLIGO-Virgo-KAGRA (LVK) Collaboration using a multipolar aligned-spin\ninspiral-merger-ringdown waveform model with two eccentric parameters:\neccentricity and relativistic anomaly. This is made computationally feasible\nwith the machine-learning code DINGO which accelerates inference by 2-3 orders\nof magnitude compared to traditional inference. First, we find eccentric\naligned-spin versus quasi-circular aligned-spin $\\log_{10}$ Bayes factors of\n1.84 to 4.75 (depending on the glitch mitigation) for GW200129, 3.0 for\nGW190701 and 1.77 for GW200208_22. We measure $e_{\\text{gw}, 10Hz}$ to be\n$0.27_{-0.12}^{+0.10}$ to $0.17_{-0.13}^{+0.14}$ for GW200129,\n$0.35_{-0.11}^{+0.32}$ for GW190701 and $0.35_{-0.21}^{+0.18}$ for GW200208_22.\nSecond, we find $\\log_{10}$ Bayes factors between the eccentric aligned-spin\nversus quasi-circular precessing-spin hypothesis between 1.43 and 4.92 for\nGW200129, 2.61 for GW190701 and 1.23 for GW200208_22. Third, our analysis does\nnot show evidence for eccentricity in GW190521, which has an eccentric\naligned-spin against quasi-circular aligned-spin $\\log_{10}$ Bayes factor of\n0.04. Fourth, we estimate that if we neglect the spin-precession and use an\nastrophysical prior, the probability of one out of the 57 events being\neccentric is greater than 99.5% or $(100 - 8.4 \\times 10^{-4})$% (depending on\nthe glitch mitigation). Fifth, we study the impact on parameter estimation when\nneglecting either eccentricity or higher modes in eccentric models. These\nresults underscore the importance of including eccentric parameters in the\ncharacterization of BBHs for GW detectors."
                },
                "authors": [
                    {
                        "name": "Nihar Gupte"
                    },
                    {
                        "name": "Antoni Ramos-Buades"
                    },
                    {
                        "name": "Alessandra Buonanno"
                    },
                    {
                        "name": "Jonathan Gair"
                    },
                    {
                        "name": "M. Coleman Miller"
                    },
                    {
                        "name": "Maximilian Dax"
                    },
                    {
                        "name": "Stephen R. Green"
                    },
                    {
                        "name": "Michael Pürrer"
                    },
                    {
                        "name": "Jonas Wildberger"
                    },
                    {
                        "name": "Jakob Macke"
                    },
                    {
                        "name": "Isobel M. Romero-Shaw"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Schölkopf"
                },
                "author": "Bernhard Schölkopf",
                "arxiv_comment": "36 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15095v1",
                "updated": "2024-08-27T14:26:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    26,
                    21,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T14:26:21Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    26,
                    21,
                    1,
                    240,
                    0
                ],
                "title": "An Eddington Limited Accretion Disk Wind in the narrow line Seyfert 1,\n  PG 1448+273",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eddington Limited Accretion Disk Wind in the narrow line Seyfert 1,\n  PG 1448+273"
                },
                "summary": "PG 1448+273 is a luminous, nearby ($z=0.0645$), narrow line Seyfert 1 galaxy,\nwhich likely accretes close to the Eddington limit. Previous X-ray observations\nof PG 1448 with XMM-Newton in 2017 and NuSTAR in 2022 revealed the presence of\nan ultra fast outflow, as seen through its blueshifted iron K absorption\nprofile, where the outflow velocity appeared to vary in the range $0.1-0.3c$.\nIn this work, new X-ray observations of PG 1448 are presented, in the form of\nfour simultaneous XMM-Newton and NuSTAR observations performed in July and\nAugust 2023. The X-ray spectra appeared at a similar flux in each observation,\nmaking it possible to analyze the mean 2023 X-ray spectrum at high signal to\nnoise. A broad ($\\sigma=1$ keV) and highly blue-shifted ($E=9.8\\pm0.4$ keV)\niron K absorption profile is revealed in the mean spectrum. The profile can be\nmodeled by a fast, geometrically thick accretion disk wind, which reveals a\nmaximum terminal velocity of $v_{\\infty}=-0.43\\pm0.03c$, one of the fastest\nknown winds in a nearby AGN. As a result, the inferred mass outflow rate of the\nwind may reach a significant fraction of the Eddington accretion rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PG 1448+273 is a luminous, nearby ($z=0.0645$), narrow line Seyfert 1 galaxy,\nwhich likely accretes close to the Eddington limit. Previous X-ray observations\nof PG 1448 with XMM-Newton in 2017 and NuSTAR in 2022 revealed the presence of\nan ultra fast outflow, as seen through its blueshifted iron K absorption\nprofile, where the outflow velocity appeared to vary in the range $0.1-0.3c$.\nIn this work, new X-ray observations of PG 1448 are presented, in the form of\nfour simultaneous XMM-Newton and NuSTAR observations performed in July and\nAugust 2023. The X-ray spectra appeared at a similar flux in each observation,\nmaking it possible to analyze the mean 2023 X-ray spectrum at high signal to\nnoise. A broad ($\\sigma=1$ keV) and highly blue-shifted ($E=9.8\\pm0.4$ keV)\niron K absorption profile is revealed in the mean spectrum. The profile can be\nmodeled by a fast, geometrically thick accretion disk wind, which reveals a\nmaximum terminal velocity of $v_{\\infty}=-0.43\\pm0.03c$, one of the fastest\nknown winds in a nearby AGN. As a result, the inferred mass outflow rate of the\nwind may reach a significant fraction of the Eddington accretion rate."
                },
                "authors": [
                    {
                        "name": "J. N. Reeves"
                    },
                    {
                        "name": "V. Braito"
                    },
                    {
                        "name": "A. Luminari"
                    },
                    {
                        "name": "D. Porquet"
                    },
                    {
                        "name": "M. Laurenti"
                    },
                    {
                        "name": "G. Matzeu"
                    },
                    {
                        "name": "A. Lobban"
                    },
                    {
                        "name": "S. Hagen"
                    }
                ],
                "author_detail": {
                    "name": "S. Hagen"
                },
                "author": "S. Hagen",
                "arxiv_comment": "15 pages, accepted for publication in the Astrophysical Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15094v1",
                "updated": "2024-08-27T14:25:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    25,
                    42,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T14:25:42Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    25,
                    42,
                    1,
                    240,
                    0
                ],
                "title": "Constrained Diffusion Models via Dual Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constrained Diffusion Models via Dual Training"
                },
                "summary": "Diffusion models have attained prominence for their ability to synthesize a\nprobability distribution for a given dataset via a diffusion process, enabling\nthe generation of new data points with high fidelity. However, diffusion\nprocesses are prone to generating biased data based on the training dataset. To\naddress this issue, we develop constrained diffusion models by imposing\ndiffusion constraints based on desired distributions that are informed by\nrequirements. Specifically, we cast the training of diffusion models under\nrequirements as a constrained distribution optimization problem that aims to\nreduce the distribution difference between original and generated data while\nobeying constraints on the distribution of generated data. We show that our\nconstrained diffusion models generate new data from a mixture data distribution\nthat achieves the optimal trade-off among objective and constraints. To train\nconstrained diffusion models, we develop a dual training algorithm and\ncharacterize the optimality of the trained constrained diffusion model. We\nempirically demonstrate the effectiveness of our constrained models in two\nconstrained generation tasks: (i) we consider a dataset with one or more\nunderrepresented classes where we train the model with constraints to ensure\nfairly sampling from all classes during inference; (ii) we fine-tune a\npre-trained diffusion model to sample from a new dataset while avoiding\noverfitting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have attained prominence for their ability to synthesize a\nprobability distribution for a given dataset via a diffusion process, enabling\nthe generation of new data points with high fidelity. However, diffusion\nprocesses are prone to generating biased data based on the training dataset. To\naddress this issue, we develop constrained diffusion models by imposing\ndiffusion constraints based on desired distributions that are informed by\nrequirements. Specifically, we cast the training of diffusion models under\nrequirements as a constrained distribution optimization problem that aims to\nreduce the distribution difference between original and generated data while\nobeying constraints on the distribution of generated data. We show that our\nconstrained diffusion models generate new data from a mixture data distribution\nthat achieves the optimal trade-off among objective and constraints. To train\nconstrained diffusion models, we develop a dual training algorithm and\ncharacterize the optimality of the trained constrained diffusion model. We\nempirically demonstrate the effectiveness of our constrained models in two\nconstrained generation tasks: (i) we consider a dataset with one or more\nunderrepresented classes where we train the model with constraints to ensure\nfairly sampling from all classes during inference; (ii) we fine-tune a\npre-trained diffusion model to sample from a new dataset while avoiding\noverfitting."
                },
                "authors": [
                    {
                        "name": "Shervin Khalafi"
                    },
                    {
                        "name": "Dongsheng Ding"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Ribeiro"
                },
                "author": "Alejandro Ribeiro",
                "arxiv_comment": "41 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15091v1",
                "updated": "2024-08-27T14:22:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    22,
                    2,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T14:22:02Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    22,
                    2,
                    1,
                    240,
                    0
                ],
                "title": "Relation Also Knows: Rethinking the Recall and Editing of Factual\n  Associations in Auto-Regressive Transformer Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relation Also Knows: Rethinking the Recall and Editing of Factual\n  Associations in Auto-Regressive Transformer Language Models"
                },
                "summary": "The storage and recall of factual associations in auto-regressive transformer\nlanguage models (LMs) have drawn a great deal of attention, inspiring knowledge\nediting by directly modifying the located model weights. Most editing works\nachieve knowledge editing under the guidance of existing interpretations of\nknowledge recall that mainly focus on subject knowledge. However, these\ninterpretations are seriously flawed, neglecting relation information and\nleading to the over-generalizing problem for editing. In this work, we discover\na novel relation-focused perspective to interpret the knowledge recall of\ntransformer LMs during inference and apply it on knowledge editing to avoid\nover-generalizing. Experimental results on the dataset supplemented with a new\nR-Specificity criterion demonstrate that our editing approach significantly\nalleviates over-generalizing while remaining competitive on other criteria,\nbreaking the domination of subject-focused editing for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The storage and recall of factual associations in auto-regressive transformer\nlanguage models (LMs) have drawn a great deal of attention, inspiring knowledge\nediting by directly modifying the located model weights. Most editing works\nachieve knowledge editing under the guidance of existing interpretations of\nknowledge recall that mainly focus on subject knowledge. However, these\ninterpretations are seriously flawed, neglecting relation information and\nleading to the over-generalizing problem for editing. In this work, we discover\na novel relation-focused perspective to interpret the knowledge recall of\ntransformer LMs during inference and apply it on knowledge editing to avoid\nover-generalizing. Experimental results on the dataset supplemented with a new\nR-Specificity criterion demonstrate that our editing approach significantly\nalleviates over-generalizing while remaining competitive on other criteria,\nbreaking the domination of subject-focused editing for future research."
                },
                "authors": [
                    {
                        "name": "Xiyu Liu"
                    },
                    {
                        "name": "Zhengxiao Liu"
                    },
                    {
                        "name": "Naibin Gu"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wanli Ma"
                    },
                    {
                        "name": "Ji Xiang"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.05195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.05195v2",
                "updated": "2024-08-27T14:20:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    20,
                    57,
                    1,
                    240,
                    0
                ],
                "published": "2023-11-09T08:19:34Z",
                "published_parsed": [
                    2023,
                    11,
                    9,
                    8,
                    19,
                    34,
                    3,
                    313,
                    0
                ],
                "title": "PRODIGy: a PROfile-based DIalogue Generation dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRODIGy: a PROfile-based DIalogue Generation dataset"
                },
                "summary": "Providing dialogue agents with a profile representation can improve their\nconsistency and coherence, leading to better conversations. However, current\nprofile-based dialogue datasets for training such agents contain either\nexplicit profile representations that are simple and dialogue-specific, or\nimplicit representations that are difficult to collect. In this work, we\npropose a unified framework in which we bring together both standard and more\nsophisticated profile representations by creating a new resource where each\ndialogue is aligned with all possible speaker representations such as\ncommunication style, biographies, and personality. This framework allows to\ntest several baselines built using generative language models with several\nprofile configurations. The automatic evaluation shows that profile-based\nmodels have better generalisation capabilities than models trained on dialogues\nonly, both in-domain and cross-domain settings. These results are consistent\nfor fine-tuned models and instruction-based LLMs. Additionally, human\nevaluation demonstrates a clear preference for generations consistent with both\nprofile and context. Finally, to account for possible privacy concerns, all\nexperiments are done under two configurations: inter-character and\nintra-character. In the former, the LM stores the information about the\ncharacter in its internal representation, while in the latter, the LM does not\nretain any personal information but uses it only at inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing dialogue agents with a profile representation can improve their\nconsistency and coherence, leading to better conversations. However, current\nprofile-based dialogue datasets for training such agents contain either\nexplicit profile representations that are simple and dialogue-specific, or\nimplicit representations that are difficult to collect. In this work, we\npropose a unified framework in which we bring together both standard and more\nsophisticated profile representations by creating a new resource where each\ndialogue is aligned with all possible speaker representations such as\ncommunication style, biographies, and personality. This framework allows to\ntest several baselines built using generative language models with several\nprofile configurations. The automatic evaluation shows that profile-based\nmodels have better generalisation capabilities than models trained on dialogues\nonly, both in-domain and cross-domain settings. These results are consistent\nfor fine-tuned models and instruction-based LLMs. Additionally, human\nevaluation demonstrates a clear preference for generations consistent with both\nprofile and context. Finally, to account for possible privacy concerns, all\nexperiments are done under two configurations: inter-character and\nintra-character. In the former, the LM stores the information about the\ncharacter in its internal representation, while in the latter, the LM does not\nretain any personal information but uses it only at inference time."
                },
                "authors": [
                    {
                        "name": "Daniela Occhipinti"
                    },
                    {
                        "name": "Serra Sinem Tekiroglu"
                    },
                    {
                        "name": "Marco Guerini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Guerini"
                },
                "author": "Marco Guerini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.05195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.05195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.01870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.01870v2",
                "updated": "2024-08-27T14:16:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    16,
                    6,
                    1,
                    240,
                    0
                ],
                "published": "2022-09-05T10:06:03Z",
                "published_parsed": [
                    2022,
                    9,
                    5,
                    10,
                    6,
                    3,
                    0,
                    248,
                    0
                ],
                "title": "Unsupervised Domain Adaptation via Style-Aware Self-intermediate Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Domain Adaptation via Style-Aware Self-intermediate Domain"
                },
                "summary": "Unsupervised domain adaptation (UDA) has attracted considerable attention,\nwhich transfers knowledge from a label-rich source domain to a related but\nunlabeled target domain. Reducing inter-domain differences has always been a\ncrucial factor to improve performance in UDA, especially for tasks where there\nis a large gap between source and target domains. To this end, we propose a\nnovel style-aware feature fusion method (SAFF) to bridge the large domain gap\nand transfer knowledge while alleviating the loss of class-discriminative\ninformation. Inspired by the human transitive inference and learning ability, a\nnovel style-aware self-intermediate domain (SSID) is investigated to link two\nseemingly unrelated concepts through a series of intermediate auxiliary\nsynthesized concepts. Specifically, we propose a novel learning strategy of\nSSID, which selects samples from both source and target domains as anchors, and\nthen randomly fuses the object and style features of these anchors to generate\nlabeled and style-rich intermediate auxiliary features for knowledge transfer.\nMoreover, we design an external memory bank to store and update specified\nlabeled features to obtain stable class features and class-wise style features.\nBased on the proposed memory bank, the intra- and inter-domain loss functions\nare designed to improve the class recognition ability and feature\ncompatibility, respectively. Meanwhile, we simulate the rich latent feature\nspace of SSID by infinite sampling and the convergence of the loss function by\nmathematical theory. Finally, we conduct comprehensive experiments on commonly\nused domain adaptive benchmarks to evaluate the proposed SAFF, and the\nexperimental results show that the proposed SAFF can be easily combined with\ndifferent backbone networks and obtain better performance as a plug-in-plug-out\nmodule.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised domain adaptation (UDA) has attracted considerable attention,\nwhich transfers knowledge from a label-rich source domain to a related but\nunlabeled target domain. Reducing inter-domain differences has always been a\ncrucial factor to improve performance in UDA, especially for tasks where there\nis a large gap between source and target domains. To this end, we propose a\nnovel style-aware feature fusion method (SAFF) to bridge the large domain gap\nand transfer knowledge while alleviating the loss of class-discriminative\ninformation. Inspired by the human transitive inference and learning ability, a\nnovel style-aware self-intermediate domain (SSID) is investigated to link two\nseemingly unrelated concepts through a series of intermediate auxiliary\nsynthesized concepts. Specifically, we propose a novel learning strategy of\nSSID, which selects samples from both source and target domains as anchors, and\nthen randomly fuses the object and style features of these anchors to generate\nlabeled and style-rich intermediate auxiliary features for knowledge transfer.\nMoreover, we design an external memory bank to store and update specified\nlabeled features to obtain stable class features and class-wise style features.\nBased on the proposed memory bank, the intra- and inter-domain loss functions\nare designed to improve the class recognition ability and feature\ncompatibility, respectively. Meanwhile, we simulate the rich latent feature\nspace of SSID by infinite sampling and the convergence of the loss function by\nmathematical theory. Finally, we conduct comprehensive experiments on commonly\nused domain adaptive benchmarks to evaluate the proposed SAFF, and the\nexperimental results show that the proposed SAFF can be easily combined with\ndifferent backbone networks and obtain better performance as a plug-in-plug-out\nmodule."
                },
                "authors": [
                    {
                        "name": "Lianyu Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Daoqiang Zhang"
                    },
                    {
                        "name": "Huazhu Fu"
                    }
                ],
                "author_detail": {
                    "name": "Huazhu Fu"
                },
                "author": "Huazhu Fu",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.01870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.01870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14340v2",
                "updated": "2024-08-27T14:09:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    9,
                    44,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-26T15:13:14Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    13,
                    14,
                    0,
                    239,
                    0
                ],
                "title": "Foundation Models for Music: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models for Music: A Survey"
                },
                "summary": "In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm."
                },
                "authors": [
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Anders Øland"
                    },
                    {
                        "name": "Anton Ragni"
                    },
                    {
                        "name": "Bleiz MacSen Del Sette"
                    },
                    {
                        "name": "Charalampos Saitis"
                    },
                    {
                        "name": "Chris Donahue"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Christos Plachouras"
                    },
                    {
                        "name": "Emmanouil Benetos"
                    },
                    {
                        "name": "Elio Quinton"
                    },
                    {
                        "name": "Elona Shatri"
                    },
                    {
                        "name": "Fabio Morreale"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "György Fazekas"
                    },
                    {
                        "name": "Gus Xia"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Ilaria Manco"
                    },
                    {
                        "name": "Jiawen Huang"
                    },
                    {
                        "name": "Julien Guinot"
                    },
                    {
                        "name": "Liwei Lin"
                    },
                    {
                        "name": "Luca Marinelli"
                    },
                    {
                        "name": "Max W. Y. Lam"
                    },
                    {
                        "name": "Megha Sharma"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Roger B. Dannenberg"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Shangda Wu"
                    },
                    {
                        "name": "Shih-Lun Wu"
                    },
                    {
                        "name": "Shuqi Dai"
                    },
                    {
                        "name": "Shun Lei"
                    },
                    {
                        "name": "Shiyin Kang"
                    },
                    {
                        "name": "Simon Dixon"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xingjian Du"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Zeyue Tian"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Zhizheng Wu"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Ziyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ziyu Wang"
                },
                "author": "Ziyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15079v1",
                "updated": "2024-08-27T14:08:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    8,
                    23,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T14:08:23Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    8,
                    23,
                    1,
                    240,
                    0
                ],
                "title": "BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and\n  Deduplication by Introducing a Competitive Large Language Model Baseline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and\n  Deduplication by Introducing a Competitive Large Language Model Baseline"
                },
                "summary": "The general capabilities of Large Language Models (LLM) highly rely on the\ncomposition and selection on extensive pretraining datasets, treated as\ncommercial secrets by several institutions. To mitigate this issue, we\nopen-source the details of a universally applicable data processing pipeline\nand validate its effectiveness and potential by introducing a competitive LLM\nbaseline. Specifically, the data processing pipeline consists of broad\ncollection to scale up and reweighting to improve quality. We then pretrain a\n7B model BaichuanSEED with 3T tokens processed by our pipeline without any\ndeliberate downstream task-related optimization, followed by an easy but\neffective supervised fine-tuning stage. BaichuanSEED demonstrates consistency\nand predictability throughout training and achieves comparable performance on\ncomprehensive benchmarks with several commercial advanced large language\nmodels, such as Qwen1.5 and Llama3. We also conduct several heuristic\nexperiments to discuss the potential for further optimization of downstream\ntasks, such as mathematics and coding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The general capabilities of Large Language Models (LLM) highly rely on the\ncomposition and selection on extensive pretraining datasets, treated as\ncommercial secrets by several institutions. To mitigate this issue, we\nopen-source the details of a universally applicable data processing pipeline\nand validate its effectiveness and potential by introducing a competitive LLM\nbaseline. Specifically, the data processing pipeline consists of broad\ncollection to scale up and reweighting to improve quality. We then pretrain a\n7B model BaichuanSEED with 3T tokens processed by our pipeline without any\ndeliberate downstream task-related optimization, followed by an easy but\neffective supervised fine-tuning stage. BaichuanSEED demonstrates consistency\nand predictability throughout training and achieves comparable performance on\ncomprehensive benchmarks with several commercial advanced large language\nmodels, such as Qwen1.5 and Llama3. We also conduct several heuristic\nexperiments to discuss the potential for further optimization of downstream\ntasks, such as mathematics and coding."
                },
                "authors": [
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Yiding Sun"
                    },
                    {
                        "name": "Shusen Zhang"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Yanjun Shen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Haoze Sun"
                    },
                    {
                        "name": "Tianpeng Li"
                    },
                    {
                        "name": "Mingan Lin"
                    },
                    {
                        "name": "Jianhua Xu"
                    },
                    {
                        "name": "Yufan Zhang"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Lei Su"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Jiaxin Mao"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "19 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13161v2",
                "updated": "2024-08-27T14:05:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    5,
                    57,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-23T15:34:33Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    34,
                    33,
                    4,
                    236,
                    0
                ],
                "title": "Say No to Freeloader: Protecting Intellectual Property of Your Deep\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Say No to Freeloader: Protecting Intellectual Property of Your Deep\n  Model"
                },
                "summary": "Model intellectual property (IP) protection has attracted growing attention\nas science and technology advancements stem from human intellectual labor and\ncomputational expenses. Ensuring IP safety for trainers and owners is of utmost\nimportance, particularly in domains where ownership verification and\napplicability authorization are required. A notable approach to safeguarding\nmodel IP involves proactively preventing the use of well-trained models of\nauthorized domains from unauthorized domains. In this paper, we introduce a\nnovel Compact Un-transferable Pyramid Isolation Domain (CUPI-Domain) which\nserves as a barrier against illegal transfers from authorized to unauthorized\ndomains. Drawing inspiration from human transitive inference and learning\nabilities, the CUPI-Domain is designed to obstruct cross-domain transfers by\nemphasizing the distinctive style features of the authorized domain. This\nemphasis leads to failure in recognizing irrelevant private style features on\nunauthorized domains. To this end, we propose novel CUPI-Domain generators,\nwhich select features from both authorized and CUPI-Domain as anchors. Then, we\nfuse the style features and semantic features of these anchors to generate\nlabeled and style-rich CUPI-Domain. Additionally, we design external\nDomain-Information Memory Banks (DIMB) for storing and updating labeled pyramid\nfeatures to obtain stable domain class features and domain class-wise style\nfeatures. Based on the proposed whole method, the novel style and\ndiscriminative loss functions are designed to effectively enhance the\ndistinction in style and discriminative features between authorized and\nunauthorized domains, respectively. Moreover, we provide two solutions for\nutilizing CUPI-Domain based on whether the unauthorized domain is known:\ntarget-specified CUPI-Domain and target-free CUPI-Domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model intellectual property (IP) protection has attracted growing attention\nas science and technology advancements stem from human intellectual labor and\ncomputational expenses. Ensuring IP safety for trainers and owners is of utmost\nimportance, particularly in domains where ownership verification and\napplicability authorization are required. A notable approach to safeguarding\nmodel IP involves proactively preventing the use of well-trained models of\nauthorized domains from unauthorized domains. In this paper, we introduce a\nnovel Compact Un-transferable Pyramid Isolation Domain (CUPI-Domain) which\nserves as a barrier against illegal transfers from authorized to unauthorized\ndomains. Drawing inspiration from human transitive inference and learning\nabilities, the CUPI-Domain is designed to obstruct cross-domain transfers by\nemphasizing the distinctive style features of the authorized domain. This\nemphasis leads to failure in recognizing irrelevant private style features on\nunauthorized domains. To this end, we propose novel CUPI-Domain generators,\nwhich select features from both authorized and CUPI-Domain as anchors. Then, we\nfuse the style features and semantic features of these anchors to generate\nlabeled and style-rich CUPI-Domain. Additionally, we design external\nDomain-Information Memory Banks (DIMB) for storing and updating labeled pyramid\nfeatures to obtain stable domain class features and domain class-wise style\nfeatures. Based on the proposed whole method, the novel style and\ndiscriminative loss functions are designed to effectively enhance the\ndistinction in style and discriminative features between authorized and\nunauthorized domains, respectively. Moreover, we provide two solutions for\nutilizing CUPI-Domain based on whether the unauthorized domain is known:\ntarget-specified CUPI-Domain and target-free CUPI-Domain."
                },
                "authors": [
                    {
                        "name": "Lianyu Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Huazhu Fu"
                    },
                    {
                        "name": "Daoqiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Daoqiang Zhang"
                },
                "author": "Daoqiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06425v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06425v5",
                "updated": "2024-08-27T14:03:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    3,
                    15,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-12T18:04:59Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    18,
                    4,
                    59,
                    0,
                    225,
                    0
                ],
                "title": "Bayesian Learning in a Nonlinear Multiscale State-Space Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Learning in a Nonlinear Multiscale State-Space Model"
                },
                "summary": "The ubiquity of multiscale interactions in complex systems is\nwell-recognized, with development and heredity serving as a prime example of\nhow processes at different temporal scales influence one another. This work\nintroduces a novel multiscale state-space model to explore the dynamic\ninterplay between systems interacting across different time scales, with\nfeedback between each scale. We propose a Bayesian learning framework to\nestimate unknown states by learning the unknown process noise covariances\nwithin this multiscale model. We develop a Particle Gibbs with Ancestor\nSampling (PGAS) algorithm for inference and demonstrate through simulations the\nefficacy of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ubiquity of multiscale interactions in complex systems is\nwell-recognized, with development and heredity serving as a prime example of\nhow processes at different temporal scales influence one another. This work\nintroduces a novel multiscale state-space model to explore the dynamic\ninterplay between systems interacting across different time scales, with\nfeedback between each scale. We propose a Bayesian learning framework to\nestimate unknown states by learning the unknown process noise covariances\nwithin this multiscale model. We develop a Particle Gibbs with Ancestor\nSampling (PGAS) algorithm for inference and demonstrate through simulations the\nefficacy of our approach."
                },
                "authors": [
                    {
                        "name": "Nayely Vélez-Cruz"
                    },
                    {
                        "name": "Manfred D. Laubichler"
                    }
                ],
                "author_detail": {
                    "name": "Manfred D. Laubichler"
                },
                "author": "Manfred D. Laubichler",
                "arxiv_comment": "Corrected a typo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06425v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06425v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15066v1",
                "updated": "2024-08-27T13:50:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    50,
                    37,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T13:50:37Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    50,
                    37,
                    1,
                    240,
                    0
                ],
                "title": "Constraining Participation: Affordances of Feedback Features in\n  Interfaces to Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining Participation: Affordances of Feedback Features in\n  Interfaces to Large Language Models"
                },
                "summary": "Large language models (LLMs) are now accessible to anyone with a computer, a\nweb browser, and an internet connection via browser-based interfaces, shifting\nthe dynamics of participation in AI development. This paper examines the\naffordances of interactive feedback features in ChatGPT's interface, analysing\nhow they shape user input and participation in LLM iteration. Drawing on a\nsurvey of ChatGPT users and applying the mechanisms and conditions framework of\naffordances, we demonstrate that these features encourage simple, frequent, and\nperformance-focused feedback while discouraging collective input and\ndiscussions among users. We argue that this feedback format significantly\nconstrains user participation, reinforcing power imbalances between users, the\npublic, and companies developing LLMs. Our analysis contributes to the growing\nbody of literature on participatory AI by critically examining the limitations\nof existing feedback processes and proposing directions for their redesign. To\nenable more meaningful public participation in AI development, we advocate for\na shift away from processes focused on aligning model outputs with specific\nuser preferences. Instead, we emphasise the need for processes that facilitate\ndialogue between companies and diverse 'publics' about the purpose and\napplications of LLMs. This approach requires attention to the ongoing work of\ninfrastructuring - creating and sustaining the social, technical, and\ninstitutional structures necessary to address matters of concern to groups\nimpacted by AI development and deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are now accessible to anyone with a computer, a\nweb browser, and an internet connection via browser-based interfaces, shifting\nthe dynamics of participation in AI development. This paper examines the\naffordances of interactive feedback features in ChatGPT's interface, analysing\nhow they shape user input and participation in LLM iteration. Drawing on a\nsurvey of ChatGPT users and applying the mechanisms and conditions framework of\naffordances, we demonstrate that these features encourage simple, frequent, and\nperformance-focused feedback while discouraging collective input and\ndiscussions among users. We argue that this feedback format significantly\nconstrains user participation, reinforcing power imbalances between users, the\npublic, and companies developing LLMs. Our analysis contributes to the growing\nbody of literature on participatory AI by critically examining the limitations\nof existing feedback processes and proposing directions for their redesign. To\nenable more meaningful public participation in AI development, we advocate for\na shift away from processes focused on aligning model outputs with specific\nuser preferences. Instead, we emphasise the need for processes that facilitate\ndialogue between companies and diverse 'publics' about the purpose and\napplications of LLMs. This approach requires attention to the ongoing work of\ninfrastructuring - creating and sustaining the social, technical, and\ninstitutional structures necessary to address matters of concern to groups\nimpacted by AI development and deployment."
                },
                "authors": [
                    {
                        "name": "Ned Cooper"
                    },
                    {
                        "name": "Alexandra Zafiroglu"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Zafiroglu"
                },
                "author": "Alexandra Zafiroglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15061v1",
                "updated": "2024-08-27T13:45:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    45,
                    24,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T13:45:24Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    45,
                    24,
                    1,
                    240,
                    0
                ],
                "title": "Diagnosing overdispersion in longitudinal analyses with grouped nominal\n  polytomous data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnosing overdispersion in longitudinal analyses with grouped nominal\n  polytomous data"
                },
                "summary": "Experiments in Agricultural Sciences often involve the analysis of\nlongitudinal nominal polytomous variables, both in individual and grouped\nstructures. Marginal and mixed-effects models are two common approaches. The\ndistributional assumptions induce specific mean-variance relationships,\nhowever, in many instances, the observed variability is greater than assumed by\nthe model. This characterizes overdispersion, whose identification is crucial\nfor choosing an appropriate modeling framework to make inferences reliable. We\npropose an initial exploration of constructing a longitudinal multinomial\ndispersion index as a descriptive and diagnostic tool. This index is calculated\nas the ratio between the observed and assumed variances. The performance of\nthis index was evaluated through a simulation study, employing statistical\ntechniques to assess its initial performance in different scenarios. We\nidentified that as the index approaches one, it is more likely that this\ncorresponds to a high degree of overdispersion. Conversely, values closer to\nzero indicate a low degree of overdispersion. As a case study, we present an\napplication in animal science, in which the behaviour of pigs (grouped in\nstalls) is evaluated, considering three response categories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experiments in Agricultural Sciences often involve the analysis of\nlongitudinal nominal polytomous variables, both in individual and grouped\nstructures. Marginal and mixed-effects models are two common approaches. The\ndistributional assumptions induce specific mean-variance relationships,\nhowever, in many instances, the observed variability is greater than assumed by\nthe model. This characterizes overdispersion, whose identification is crucial\nfor choosing an appropriate modeling framework to make inferences reliable. We\npropose an initial exploration of constructing a longitudinal multinomial\ndispersion index as a descriptive and diagnostic tool. This index is calculated\nas the ratio between the observed and assumed variances. The performance of\nthis index was evaluated through a simulation study, employing statistical\ntechniques to assess its initial performance in different scenarios. We\nidentified that as the index approaches one, it is more likely that this\ncorresponds to a high degree of overdispersion. Conversely, values closer to\nzero indicate a low degree of overdispersion. As a case study, we present an\napplication in animal science, in which the behaviour of pigs (grouped in\nstalls) is evaluated, considering three response categories."
                },
                "authors": [
                    {
                        "name": "Maria Letícia Salvador"
                    },
                    {
                        "name": "Gabriel Rodrigues Palma"
                    },
                    {
                        "name": "Rafael de Andrade Moral"
                    },
                    {
                        "name": "Idemauro Antonio Rodrigues de Lara"
                    }
                ],
                "author_detail": {
                    "name": "Idemauro Antonio Rodrigues de Lara"
                },
                "author": "Idemauro Antonio Rodrigues de Lara",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14745v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14745v3",
                "updated": "2024-08-27T13:36:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    36,
                    12,
                    1,
                    240,
                    0
                ],
                "published": "2024-04-23T04:54:32Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    4,
                    54,
                    32,
                    1,
                    114,
                    0
                ],
                "title": "TAAT: Think and Act from Arbitrary Texts in Text2Motion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAAT: Think and Act from Arbitrary Texts in Text2Motion"
                },
                "summary": "Text to Motion aims to generate human motions from texts. Existing settings\nassume that texts include action labels, which limits flexibility in practical\nscenarios. This paper extends this task with a more realistic assumption that\nthe texts are arbitrary. Specifically, in our setting, arbitrary texts include\nexisting action texts composed of action labels and introduce scene texts\nwithout explicit action labels. To address this practical issue, we extend the\naction texts in the HUMANML3D dataset by incorporating additional scene texts,\nthereby creating a new dataset, HUMANML3D++. Concurrently, we propose a simple\nframework that extracts action representations from arbitrary texts using a\nLarge Language Model (LLM) and subsequently generates motions. Furthermore, we\nenhance the existing evaluation methodologies to address their inadequacies.\nExtensive experiments are conducted under different application scenarios to\nvalidate the effectiveness of the proposed framework on existing and proposed\ndatasets. The results indicate that Text to Motion in this realistic setting is\nvery challenging, fostering new research in this practical direction. Our\ndataset and code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text to Motion aims to generate human motions from texts. Existing settings\nassume that texts include action labels, which limits flexibility in practical\nscenarios. This paper extends this task with a more realistic assumption that\nthe texts are arbitrary. Specifically, in our setting, arbitrary texts include\nexisting action texts composed of action labels and introduce scene texts\nwithout explicit action labels. To address this practical issue, we extend the\naction texts in the HUMANML3D dataset by incorporating additional scene texts,\nthereby creating a new dataset, HUMANML3D++. Concurrently, we propose a simple\nframework that extracts action representations from arbitrary texts using a\nLarge Language Model (LLM) and subsequently generates motions. Furthermore, we\nenhance the existing evaluation methodologies to address their inadequacies.\nExtensive experiments are conducted under different application scenarios to\nvalidate the effectiveness of the proposed framework on existing and proposed\ndatasets. The results indicate that Text to Motion in this realistic setting is\nvery challenging, fostering new research in this practical direction. Our\ndataset and code will be released."
                },
                "authors": [
                    {
                        "name": "Runqi Wang"
                    },
                    {
                        "name": "Caoyuan Ma"
                    },
                    {
                        "name": "Guopeng Li"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang",
                "arxiv_comment": "Updated errors in author information",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14745v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14745v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15055v1",
                "updated": "2024-08-27T13:32:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    32,
                    31,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T13:32:31Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    32,
                    31,
                    1,
                    240,
                    0
                ],
                "title": "Causal Rule Forest: Toward Interpretable and Precise Treatment Effect\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Rule Forest: Toward Interpretable and Precise Treatment Effect\n  Estimation"
                },
                "summary": "Understanding and inferencing Heterogeneous Treatment Effects (HTE) and\nConditional Average Treatment Effects (CATE) are vital for developing\npersonalized treatment recommendations. Many state-of-the-art approaches\nachieve inspiring performance in estimating HTE on benchmark datasets or\nsimulation studies. However, the indirect predicting manner and complex model\narchitecture reduce the interpretability of these approaches. To mitigate the\ngap between predictive performance and heterogeneity interpretability, we\nintroduce the Causal Rule Forest (CRF), a novel approach to learning hidden\npatterns from data and transforming the patterns into interpretable multi-level\nBoolean rules. By training the other interpretable causal inference models with\ndata representation learned by CRF, we can reduce the predictive errors of\nthese models in estimating HTE and CATE, while keeping their interpretability\nfor identifying subgroups that a treatment is more effective. Our experiments\nunderscore the potential of CRF to advance personalized interventions and\npolicies, paving the way for future research to enhance its scalability and\napplication across complex causal inference challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and inferencing Heterogeneous Treatment Effects (HTE) and\nConditional Average Treatment Effects (CATE) are vital for developing\npersonalized treatment recommendations. Many state-of-the-art approaches\nachieve inspiring performance in estimating HTE on benchmark datasets or\nsimulation studies. However, the indirect predicting manner and complex model\narchitecture reduce the interpretability of these approaches. To mitigate the\ngap between predictive performance and heterogeneity interpretability, we\nintroduce the Causal Rule Forest (CRF), a novel approach to learning hidden\npatterns from data and transforming the patterns into interpretable multi-level\nBoolean rules. By training the other interpretable causal inference models with\ndata representation learned by CRF, we can reduce the predictive errors of\nthese models in estimating HTE and CATE, while keeping their interpretability\nfor identifying subgroups that a treatment is more effective. Our experiments\nunderscore the potential of CRF to advance personalized interventions and\npolicies, paving the way for future research to enhance its scalability and\napplication across complex causal inference challenges."
                },
                "authors": [
                    {
                        "name": "Chan Hsu"
                    },
                    {
                        "name": "Jun-Ting Wu"
                    },
                    {
                        "name": "Yihuang Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yihuang Kang"
                },
                "author": "Yihuang Kang",
                "arxiv_comment": "The 25th IEEE International Conference on Information Reuse and\n  Integration for Data Science (IRI 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15050v1",
                "updated": "2024-08-27T13:19:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    19,
                    32,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T13:19:32Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    19,
                    32,
                    1,
                    240,
                    0
                ],
                "title": "Self-supervised Topic Taxonomy Discovery in the Box Embedding Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised Topic Taxonomy Discovery in the Box Embedding Space"
                },
                "summary": "Topic taxonomy discovery aims at uncovering topics of different abstraction\nlevels and constructing hierarchical relations between them. Unfortunately,\nmost of prior work can hardly model semantic scopes of words and topics by\nholding the Euclidean embedding space assumption. What's worse, they infer\nasymmetric hierarchical relations by symmetric distances between topic\nembeddings. As a result, existing methods suffer from problems of low-quality\ntopics at high abstraction levels and inaccurate hierarchical relations. To\nalleviate these problems, this paper develops a Box embedding-based Topic Model\n(BoxTM) that maps words and topics into the box embedding space, where the\nasymmetric metric is defined to properly infer hierarchical relations among\ntopics. Additionally, our BoxTM explicitly infers upper-level topics based on\ncorrelation between specific topics through recursive clustering on topic\nboxes. Finally, extensive experiments validate high-quality of the topic\ntaxonomy learned by BoxTM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic taxonomy discovery aims at uncovering topics of different abstraction\nlevels and constructing hierarchical relations between them. Unfortunately,\nmost of prior work can hardly model semantic scopes of words and topics by\nholding the Euclidean embedding space assumption. What's worse, they infer\nasymmetric hierarchical relations by symmetric distances between topic\nembeddings. As a result, existing methods suffer from problems of low-quality\ntopics at high abstraction levels and inaccurate hierarchical relations. To\nalleviate these problems, this paper develops a Box embedding-based Topic Model\n(BoxTM) that maps words and topics into the box embedding space, where the\nasymmetric metric is defined to properly infer hierarchical relations among\ntopics. Additionally, our BoxTM explicitly infers upper-level topics based on\ncorrelation between specific topics through recursive clustering on topic\nboxes. Finally, extensive experiments validate high-quality of the topic\ntaxonomy learned by BoxTM."
                },
                "authors": [
                    {
                        "name": "Yuyin Lu"
                    },
                    {
                        "name": "Hegang Chen"
                    },
                    {
                        "name": "Pengbo Mao"
                    },
                    {
                        "name": "Yanghui Rao"
                    },
                    {
                        "name": "Haoran Xie"
                    },
                    {
                        "name": "Fu Lee Wang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "to be published in TACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05520v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05520v3",
                "updated": "2024-08-27T13:17:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    17,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-04-08T13:41:32Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    13,
                    41,
                    32,
                    0,
                    99,
                    0
                ],
                "title": "The Fact Selection Problem in LLM-Based Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fact Selection Problem in LLM-Based Program Repair"
                },
                "summary": "Recent research has shown that incorporating bug-related facts, such as stack\ntraces and GitHub issues, into prompts enhances the bug-fixing capabilities of\nlarge language models (LLMs). Considering the ever-increasing context window of\nthese models, a critical question arises: what and how many facts should be\nincluded in prompts to maximise the chance of correctly fixing bugs? To answer\nthis question, we conducted a large-scale study, employing over 19K prompts\nfeaturing various combinations of seven diverse facts to rectify 314 bugs from\nopen-source Python projects within the BugsInPy benchmark. Our findings\nrevealed that each fact, ranging from simple syntactic details like code\ncontext to semantic information previously unexplored in the context of LLMs\nsuch as angelic values, is beneficial. Specifically, each fact aids in fixing\nsome bugs that would remain unresolved or only be fixed with a low success rate\nwithout it. Importantly, we discovered that the effectiveness of program repair\nprompts is non-monotonic over the number of used facts; using too many facts\nleads to subpar outcomes. These insights led us to define the fact selection\nproblem: determining the optimal set of facts for inclusion in a prompt to\nmaximise LLM's performance on a given task instance. We found that there is no\none-size-fits-all set of facts for bug repair. Therefore, we developed a basic\nstatistical model, named Maniple, which selects facts specific to a given bug\nto include in the prompt. This model significantly surpasses the performance of\nthe best generic fact set. To underscore the significance of the fact selection\nproblem, we benchmarked Maniple against the state-of-the-art zero-shot,\nnon-conversational LLM-based bug repair methods. On our testing dataset of 157\nbugs, Maniple repairs 88 bugs, 17% above the best configuration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that incorporating bug-related facts, such as stack\ntraces and GitHub issues, into prompts enhances the bug-fixing capabilities of\nlarge language models (LLMs). Considering the ever-increasing context window of\nthese models, a critical question arises: what and how many facts should be\nincluded in prompts to maximise the chance of correctly fixing bugs? To answer\nthis question, we conducted a large-scale study, employing over 19K prompts\nfeaturing various combinations of seven diverse facts to rectify 314 bugs from\nopen-source Python projects within the BugsInPy benchmark. Our findings\nrevealed that each fact, ranging from simple syntactic details like code\ncontext to semantic information previously unexplored in the context of LLMs\nsuch as angelic values, is beneficial. Specifically, each fact aids in fixing\nsome bugs that would remain unresolved or only be fixed with a low success rate\nwithout it. Importantly, we discovered that the effectiveness of program repair\nprompts is non-monotonic over the number of used facts; using too many facts\nleads to subpar outcomes. These insights led us to define the fact selection\nproblem: determining the optimal set of facts for inclusion in a prompt to\nmaximise LLM's performance on a given task instance. We found that there is no\none-size-fits-all set of facts for bug repair. Therefore, we developed a basic\nstatistical model, named Maniple, which selects facts specific to a given bug\nto include in the prompt. This model significantly surpasses the performance of\nthe best generic fact set. To underscore the significance of the fact selection\nproblem, we benchmarked Maniple against the state-of-the-art zero-shot,\nnon-conversational LLM-based bug repair methods. On our testing dataset of 157\nbugs, Maniple repairs 88 bugs, 17% above the best configuration."
                },
                "authors": [
                    {
                        "name": "Nikhil Parasaram"
                    },
                    {
                        "name": "Huijie Yan"
                    },
                    {
                        "name": "Boyu Yang"
                    },
                    {
                        "name": "Zineb Flahy"
                    },
                    {
                        "name": "Abriele Qudsi"
                    },
                    {
                        "name": "Damian Ziaber"
                    },
                    {
                        "name": "Earl Barr"
                    },
                    {
                        "name": "Sergey Mechtaev"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Mechtaev"
                },
                "author": "Sergey Mechtaev",
                "arxiv_comment": "Code, scripts and data necessary to reproduce this work are available\n  at https://github.com/PyRepair/maniple",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05520v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05520v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15045v2",
                "updated": "2024-08-28T08:32:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    32,
                    44,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-27T13:13:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    13,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "DocLayLLM: An Efficient and Effective Multi-modal Extension of Large\n  Language Models for Text-rich Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocLayLLM: An Efficient and Effective Multi-modal Extension of Large\n  Language Models for Text-rich Document Understanding"
                },
                "summary": "Text-rich document understanding (TDU) refers to analyzing and comprehending\ndocuments containing substantial textual content. With the rapid evolution of\nlarge language models (LLMs), they have been widely leveraged for TDU due to\ntheir remarkable versatility and generalization. In this paper, we introduce\nDocLayLLM, an efficient and effective multi-modal extension of LLMs\nspecifically designed for TDU. By integrating visual patch tokens and 2D\npositional tokens into LLMs and encoding the document content using the LLMs\nthemselves, we fully take advantage of the document comprehension capability of\nLLMs and enhance their perception of OCR information. We have also deeply\nconsidered the role of the chain-of-thought (CoT) and innovatively proposed the\ntechniques of CoT Pre-training and CoT Annealing. Our DocLayLLM can achieve\nremarkable performances with lightweight training settings, showcasing its\nefficiency and effectiveness. Experimental results demonstrate that our\nDocLayLLM surpasses existing OCR-dependent methods and also outperforms\nOCR-free competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-rich document understanding (TDU) refers to analyzing and comprehending\ndocuments containing substantial textual content. With the rapid evolution of\nlarge language models (LLMs), they have been widely leveraged for TDU due to\ntheir remarkable versatility and generalization. In this paper, we introduce\nDocLayLLM, an efficient and effective multi-modal extension of LLMs\nspecifically designed for TDU. By integrating visual patch tokens and 2D\npositional tokens into LLMs and encoding the document content using the LLMs\nthemselves, we fully take advantage of the document comprehension capability of\nLLMs and enhance their perception of OCR information. We have also deeply\nconsidered the role of the chain-of-thought (CoT) and innovatively proposed the\ntechniques of CoT Pre-training and CoT Annealing. Our DocLayLLM can achieve\nremarkable performances with lightweight training settings, showcasing its\nefficiency and effectiveness. Experimental results demonstrate that our\nDocLayLLM surpasses existing OCR-dependent methods and also outperforms\nOCR-free competitors."
                },
                "authors": [
                    {
                        "name": "Wenhui Liao"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Hongliang Li"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Lianwen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Lianwen Jin"
                },
                "author": "Lianwen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15040v2",
                "updated": "2024-08-28T03:56:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    56,
                    37,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-27T13:10:05Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    10,
                    5,
                    1,
                    240,
                    0
                ],
                "title": "A Survey of Large Language Models for European Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Models for European Languages"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention due to their\nhigh performance on a wide range of natural language tasks since the release of\nChatGPT. The LLMs learn to understand and generate language by training\nbillions of model parameters on vast volumes of text data. Despite being a\nrelatively new field, LLM research is rapidly advancing in various directions.\nIn this paper, we present an overview of LLM families, including LLaMA, PaLM,\nGPT, and MoE, and the methods developed to create and enhance LLMs for official\nEuropean Union (EU) languages. We provide a comprehensive summary of common\nmonolingual and multilingual datasets used for pretraining large language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention due to their\nhigh performance on a wide range of natural language tasks since the release of\nChatGPT. The LLMs learn to understand and generate language by training\nbillions of model parameters on vast volumes of text data. Despite being a\nrelatively new field, LLM research is rapidly advancing in various directions.\nIn this paper, we present an overview of LLM families, including LLaMA, PaLM,\nGPT, and MoE, and the methods developed to create and enhance LLMs for official\nEuropean Union (EU) languages. We provide a comprehensive summary of common\nmonolingual and multilingual datasets used for pretraining large language\nmodels."
                },
                "authors": [
                    {
                        "name": "Wazir Ali"
                    },
                    {
                        "name": "Sampo Pyysalo"
                    }
                ],
                "author_detail": {
                    "name": "Sampo Pyysalo"
                },
                "author": "Sampo Pyysalo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15037v1",
                "updated": "2024-08-27T13:07:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    7,
                    7,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T13:07:07Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    7,
                    7,
                    1,
                    240,
                    0
                ],
                "title": "Evidence-Enhanced Triplet Generation Framework for Hallucination\n  Alleviation in Generative Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence-Enhanced Triplet Generation Framework for Hallucination\n  Alleviation in Generative Question Answering"
                },
                "summary": "To address the hallucination in generative question answering (GQA) where the\nanswer can not be derived from the document, we propose a novel\nevidence-enhanced triplet generation framework, EATQA, encouraging the model to\npredict all the combinations of (Question, Evidence, Answer) triplet by\nflipping the source pair and the target label to understand their logical\nrelationships, i.e., predict Answer(A), Question(Q), and Evidence(E) given a\nQE, EA, and QA pairs, respectively. Furthermore, we bridge the distribution gap\nto distill the knowledge from evidence in inference stage. Our framework\nensures the model to learn the logical relation between query, evidence and\nanswer, which simultaneously improves the evidence generation and query\nanswering. In this paper, we apply EATQA to LLama and it outperforms other\nLLMs-based methods and hallucination mitigation approaches on two challenging\nGQA benchmarks. Further analysis shows that our method not only keeps prior\nknowledge within LLM, but also mitigates hallucination and generates faithful\nanswers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the hallucination in generative question answering (GQA) where the\nanswer can not be derived from the document, we propose a novel\nevidence-enhanced triplet generation framework, EATQA, encouraging the model to\npredict all the combinations of (Question, Evidence, Answer) triplet by\nflipping the source pair and the target label to understand their logical\nrelationships, i.e., predict Answer(A), Question(Q), and Evidence(E) given a\nQE, EA, and QA pairs, respectively. Furthermore, we bridge the distribution gap\nto distill the knowledge from evidence in inference stage. Our framework\nensures the model to learn the logical relation between query, evidence and\nanswer, which simultaneously improves the evidence generation and query\nanswering. In this paper, we apply EATQA to LLama and it outperforms other\nLLMs-based methods and hallucination mitigation approaches on two challenging\nGQA benchmarks. Further analysis shows that our method not only keeps prior\nknowledge within LLM, but also mitigates hallucination and generates faithful\nanswers."
                },
                "authors": [
                    {
                        "name": "Haowei Du"
                    },
                    {
                        "name": "Huishuai Zhang"
                    },
                    {
                        "name": "Dongyan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongyan Zhao"
                },
                "author": "Dongyan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15002v1",
                "updated": "2024-08-27T12:34:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    12,
                    34,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T12:34:41Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    12,
                    34,
                    41,
                    1,
                    240,
                    0
                ],
                "title": "Knowledge Discovery in Optical Music Recognition: Enhancing Information\n  Retrieval with Instance Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Discovery in Optical Music Recognition: Enhancing Information\n  Retrieval with Instance Segmentation"
                },
                "summary": "Optical Music Recognition (OMR) automates the transcription of musical\nnotation from images into machine-readable formats like MusicXML, MEI, or MIDI,\nsignificantly reducing the costs and time of manual transcription. This study\nexplores knowledge discovery in OMR by applying instance segmentation using\nMask R-CNN to enhance the detection and delineation of musical symbols in sheet\nmusic. Unlike Optical Character Recognition (OCR), OMR must handle the\nintricate semantics of Common Western Music Notation (CWMN), where symbol\nmeanings depend on shape, position, and context. Our approach leverages\ninstance segmentation to manage the density and overlap of musical symbols,\nfacilitating more precise information retrieval from music scores. Evaluations\non the DoReMi and MUSCIMA++ datasets demonstrate substantial improvements, with\nour method achieving a mean Average Precision (mAP) of up to 59.70\\% in dense\nsymbol environments, achieving comparable results to object detection.\nFurthermore, using traditional computer vision techniques, we add a parallel\nstep for staff detection to infer the pitch for the recognised symbols. This\nstudy emphasises the role of pixel-wise segmentation in advancing accurate\nmusic symbol recognition, contributing to knowledge discovery in OMR. Our\nfindings indicate that instance segmentation provides more precise\nrepresentations of musical symbols, particularly in densely populated scores,\nadvancing OMR technology. We make our implementation, pre-processing scripts,\ntrained models, and evaluation results publicly available to support further\nresearch and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical Music Recognition (OMR) automates the transcription of musical\nnotation from images into machine-readable formats like MusicXML, MEI, or MIDI,\nsignificantly reducing the costs and time of manual transcription. This study\nexplores knowledge discovery in OMR by applying instance segmentation using\nMask R-CNN to enhance the detection and delineation of musical symbols in sheet\nmusic. Unlike Optical Character Recognition (OCR), OMR must handle the\nintricate semantics of Common Western Music Notation (CWMN), where symbol\nmeanings depend on shape, position, and context. Our approach leverages\ninstance segmentation to manage the density and overlap of musical symbols,\nfacilitating more precise information retrieval from music scores. Evaluations\non the DoReMi and MUSCIMA++ datasets demonstrate substantial improvements, with\nour method achieving a mean Average Precision (mAP) of up to 59.70\\% in dense\nsymbol environments, achieving comparable results to object detection.\nFurthermore, using traditional computer vision techniques, we add a parallel\nstep for staff detection to infer the pitch for the recognised symbols. This\nstudy emphasises the role of pixel-wise segmentation in advancing accurate\nmusic symbol recognition, contributing to knowledge discovery in OMR. Our\nfindings indicate that instance segmentation provides more precise\nrepresentations of musical symbols, particularly in densely populated scores,\nadvancing OMR technology. We make our implementation, pre-processing scripts,\ntrained models, and evaluation results publicly available to support further\nresearch and development."
                },
                "authors": [
                    {
                        "name": "Elona Shatri"
                    },
                    {
                        "name": "George Fazekas"
                    }
                ],
                "author_detail": {
                    "name": "George Fazekas"
                },
                "author": "George Fazekas",
                "arxiv_comment": "8 pages content and one references, accepted version at the\n  International Conference on Knowledge Discovery and Information Retrieval\n  2024, Porto, Portugal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02850v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02850v5",
                "updated": "2024-08-28T17:19:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    19,
                    27,
                    2,
                    241,
                    0
                ],
                "published": "2024-05-05T08:43:07Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    8,
                    43,
                    7,
                    6,
                    126,
                    0
                ],
                "title": "Halfway Escape Optimization: A Quantum-Inspired Solution for General\n  Optimization Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Halfway Escape Optimization: A Quantum-Inspired Solution for General\n  Optimization Problems"
                },
                "summary": "This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a\nquantum-inspired metaheuristic designed to address general optimization\nproblems characterized by rugged landscapes and high-dimensionality with an\nefficient convergence rate. The study presents a comprehensive comparative\nevaluation of HEO's performance against established optimization algorithms,\nincluding Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Artificial\nFish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behaved\nParticle Swarm Optimization (QPSO). The primary analysis encompasses 14\nbenchmark functions with dimension 30, demonstrating HEO's effectiveness and\nadaptability in navigating general optimization problems and providing valuable\ninsights into its performance. The test of HEO in Pressure Vessel Design and\nTubular Column Design infers its feasibility and potential in real-time\napplications. Further validation in Osmancik-97 and Cammeo Rice Classification\nproves the effectiveness of HEO and achieves a higher accuracy record.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a\nquantum-inspired metaheuristic designed to address general optimization\nproblems characterized by rugged landscapes and high-dimensionality with an\nefficient convergence rate. The study presents a comprehensive comparative\nevaluation of HEO's performance against established optimization algorithms,\nincluding Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Artificial\nFish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behaved\nParticle Swarm Optimization (QPSO). The primary analysis encompasses 14\nbenchmark functions with dimension 30, demonstrating HEO's effectiveness and\nadaptability in navigating general optimization problems and providing valuable\ninsights into its performance. The test of HEO in Pressure Vessel Design and\nTubular Column Design infers its feasibility and potential in real-time\napplications. Further validation in Osmancik-97 and Cammeo Rice Classification\nproves the effectiveness of HEO and achieves a higher accuracy record."
                },
                "authors": [
                    {
                        "name": "Jiawen Li"
                    },
                    {
                        "name": "Anwar PP Abdul Majeed"
                    },
                    {
                        "name": "Pascal Lefevre"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Lefevre"
                },
                "author": "Pascal Lefevre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02850v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02850v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14975v1",
                "updated": "2024-08-27T11:31:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    11,
                    31,
                    47,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T11:31:47Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    11,
                    31,
                    47,
                    1,
                    240,
                    0
                ],
                "title": "MegActor-$Σ$: Unlocking Flexible Mixed-Modal Control in Portrait\n  Animation with Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegActor-$Σ$: Unlocking Flexible Mixed-Modal Control in Portrait\n  Animation with Diffusion Transformer"
                },
                "summary": "Diffusion models have demonstrated superior performance in the field of\nportrait animation. However, current approaches relied on either visual or\naudio modality to control character movements, failing to exploit the potential\nof mixed-modal control. This challenge arises from the difficulty in balancing\nthe weak control strength of audio modality and the strong control strength of\nvisual modality. To address this issue, we introduce MegActor-$\\Sigma$: a\nmixed-modal conditional diffusion transformer (DiT), which can flexibly inject\naudio and visual modality control signals into portrait animation.\nSpecifically, we make substantial advancements over its predecessor, MegActor,\nby leveraging the promising model structure of DiT and integrating audio and\nvisual conditions through advanced modules within the DiT framework. To further\nachieve flexible combinations of mixed-modal control signals, we propose a\n``Modality Decoupling Control\" training strategy to balance the control\nstrength between visual and audio modalities, along with the ``Amplitude\nAdjustment\" inference strategy to freely regulate the motion amplitude of each\nmodality. Finally, to facilitate extensive studies in this field, we design\nseveral dataset evaluation metrics to filter out public datasets and solely use\nthis filtered dataset to train MegActor-$\\Sigma$. Extensive experiments\ndemonstrate the superiority of our approach in generating vivid portrait\nanimations, outperforming previous methods trained on private dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated superior performance in the field of\nportrait animation. However, current approaches relied on either visual or\naudio modality to control character movements, failing to exploit the potential\nof mixed-modal control. This challenge arises from the difficulty in balancing\nthe weak control strength of audio modality and the strong control strength of\nvisual modality. To address this issue, we introduce MegActor-$\\Sigma$: a\nmixed-modal conditional diffusion transformer (DiT), which can flexibly inject\naudio and visual modality control signals into portrait animation.\nSpecifically, we make substantial advancements over its predecessor, MegActor,\nby leveraging the promising model structure of DiT and integrating audio and\nvisual conditions through advanced modules within the DiT framework. To further\nachieve flexible combinations of mixed-modal control signals, we propose a\n``Modality Decoupling Control\" training strategy to balance the control\nstrength between visual and audio modalities, along with the ``Amplitude\nAdjustment\" inference strategy to freely regulate the motion amplitude of each\nmodality. Finally, to facilitate extensive studies in this field, we design\nseveral dataset evaluation metrics to filter out public datasets and solely use\nthis filtered dataset to train MegActor-$\\Sigma$. Extensive experiments\ndemonstrate the superiority of our approach in generating vivid portrait\nanimations, outperforming previous methods trained on private dataset."
                },
                "authors": [
                    {
                        "name": "Shurong Yang"
                    },
                    {
                        "name": "Huadong Li"
                    },
                    {
                        "name": "Juhao Wu"
                    },
                    {
                        "name": "Minhao Jing"
                    },
                    {
                        "name": "Linze Li"
                    },
                    {
                        "name": "Renhe Ji"
                    },
                    {
                        "name": "Jiajun Liang"
                    },
                    {
                        "name": "Haoqiang Fan"
                    },
                    {
                        "name": "Jin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jin Wang"
                },
                "author": "Jin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14972v1",
                "updated": "2024-08-27T11:24:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    11,
                    24,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T11:24:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    11,
                    24,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "AgentMonitor: A Plug-and-Play Framework for Predictive and Secure\n  Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentMonitor: A Plug-and-Play Framework for Predictive and Secure\n  Multi-Agent Systems"
                },
                "summary": "The rapid advancement of large language models (LLMs) has led to the rise of\nLLM-based agents. Recent research shows that multi-agent systems (MAS), where\neach agent plays a specific role, can outperform individual LLMs. However,\nconfiguring an MAS for a task remains challenging, with performance only\nobservable post-execution. Inspired by scaling laws in LLM development, we\ninvestigate whether MAS performance can be predicted beforehand. We introduce\nAgentMonitor, a framework that integrates at the agent level to capture inputs\nand outputs, transforming them into statistics for training a regression model\nto predict task performance. Additionally, it can further apply real-time\ncorrections to address security risks posed by malicious agents, mitigating\nnegative impacts and enhancing MAS security. Experiments demonstrate that an\nXGBoost model achieves a Spearman correlation of 0.89 in-domain and 0.58 in\nmore challenging scenarios. Furthermore, using AgentMonitor reduces harmful\ncontent by 6.2% and increases helpful content by 1.8% on average, enhancing\nsafety and reliability. Code is available at\n\\url{https://github.com/chanchimin/AgentMonitor}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has led to the rise of\nLLM-based agents. Recent research shows that multi-agent systems (MAS), where\neach agent plays a specific role, can outperform individual LLMs. However,\nconfiguring an MAS for a task remains challenging, with performance only\nobservable post-execution. Inspired by scaling laws in LLM development, we\ninvestigate whether MAS performance can be predicted beforehand. We introduce\nAgentMonitor, a framework that integrates at the agent level to capture inputs\nand outputs, transforming them into statistics for training a regression model\nto predict task performance. Additionally, it can further apply real-time\ncorrections to address security risks posed by malicious agents, mitigating\nnegative impacts and enhancing MAS security. Experiments demonstrate that an\nXGBoost model achieves a Spearman correlation of 0.89 in-domain and 0.58 in\nmore challenging scenarios. Furthermore, using AgentMonitor reduces harmful\ncontent by 6.2% and increases helpful content by 1.8% on average, enhancing\nsafety and reliability. Code is available at\n\\url{https://github.com/chanchimin/AgentMonitor}."
                },
                "authors": [
                    {
                        "name": "Chi-Min Chan"
                    },
                    {
                        "name": "Jianxuan Yu"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Chunyang Jiang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10672v2",
                "updated": "2024-08-27T11:13:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    11,
                    13,
                    43,
                    1,
                    240,
                    0
                ],
                "published": "2024-03-15T20:48:41Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    20,
                    48,
                    41,
                    4,
                    75,
                    0
                ],
                "title": "Riemannian Flow Matching Policy for Robot Motion Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Riemannian Flow Matching Policy for Robot Motion Learning"
                },
                "summary": "We introduce Riemannian Flow Matching Policies (RFMP), a novel model for\nlearning and synthesizing robot visuomotor policies. RFMP leverages the\nefficient training and inference capabilities of flow matching methods. By\ndesign, RFMP inherits the strengths of flow matching: the ability to encode\nhigh-dimensional multimodal distributions, commonly encountered in robotic\ntasks, and a very simple and fast inference process. We demonstrate the\napplicability of RFMP to both state-based and vision-conditioned robot motion\npolicies. Notably, as the robot state resides on a Riemannian manifold, RFMP\ninherently incorporates geometric awareness, which is crucial for realistic\nrobotic tasks. To evaluate RFMP, we conduct two proof-of-concept experiments,\ncomparing its performance against Diffusion Policies. Although both approaches\nsuccessfully learn the considered tasks, our results show that RFMP provides\nsmoother action trajectories with significantly lower inference times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Riemannian Flow Matching Policies (RFMP), a novel model for\nlearning and synthesizing robot visuomotor policies. RFMP leverages the\nefficient training and inference capabilities of flow matching methods. By\ndesign, RFMP inherits the strengths of flow matching: the ability to encode\nhigh-dimensional multimodal distributions, commonly encountered in robotic\ntasks, and a very simple and fast inference process. We demonstrate the\napplicability of RFMP to both state-based and vision-conditioned robot motion\npolicies. Notably, as the robot state resides on a Riemannian manifold, RFMP\ninherently incorporates geometric awareness, which is crucial for realistic\nrobotic tasks. To evaluate RFMP, we conduct two proof-of-concept experiments,\ncomparing its performance against Diffusion Policies. Although both approaches\nsuccessfully learn the considered tasks, our results show that RFMP provides\nsmoother action trajectories with significantly lower inference times."
                },
                "authors": [
                    {
                        "name": "Max Braun"
                    },
                    {
                        "name": "Noémie Jaquier"
                    },
                    {
                        "name": "Leonel Rozo"
                    },
                    {
                        "name": "Tamim Asfour"
                    }
                ],
                "author_detail": {
                    "name": "Tamim Asfour"
                },
                "author": "Tamim Asfour",
                "arxiv_comment": "Accepted for publication at IROS'24. 8 pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09702v2",
                "updated": "2024-08-27T11:12:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    11,
                    12,
                    12,
                    1,
                    240,
                    0
                ],
                "published": "2023-10-15T01:41:42Z",
                "published_parsed": [
                    2023,
                    10,
                    15,
                    1,
                    41,
                    42,
                    6,
                    288,
                    0
                ],
                "title": "Inference with Mondrian Random Forests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Mondrian Random Forests"
                },
                "summary": "Random forests are popular methods for regression and classification\nanalysis, and many different variants have been proposed in recent years. One\ninteresting example is the Mondrian random forest, in which the underlying\nconstituent trees are constructed via a Mondrian process. We give precise bias\nand variance characterizations, along with a Berry-Esseen-type central limit\ntheorem, for the Mondrian random forest regression estimator. By combining\nthese results with a carefully crafted debiasing approach and an accurate\nvariance estimator, we present valid statistical inference methods for the\nunknown regression function. These methods come with explicitly characterized\nerror bounds in terms of the sample size, tree complexity parameter, and number\nof trees in the forest, and include coverage error rates for feasible\nconfidence interval estimators. Our novel debiasing procedure for the Mondrian\nrandom forest also allows it to achieve the minimax-optimal point estimation\nconvergence rate in mean squared error for multivariate $\\beta$-H\\\"older\nregression functions, for all $\\beta > 0$, provided that the underlying tuning\nparameters are chosen appropriately. Efficient and implementable algorithms are\ndevised for both batch and online learning settings, and we carefully study the\ncomputational complexity of different Mondrian random forest implementations.\nFinally, simulations with synthetic data validate our theory and methodology,\ndemonstrating their excellent finite-sample properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random forests are popular methods for regression and classification\nanalysis, and many different variants have been proposed in recent years. One\ninteresting example is the Mondrian random forest, in which the underlying\nconstituent trees are constructed via a Mondrian process. We give precise bias\nand variance characterizations, along with a Berry-Esseen-type central limit\ntheorem, for the Mondrian random forest regression estimator. By combining\nthese results with a carefully crafted debiasing approach and an accurate\nvariance estimator, we present valid statistical inference methods for the\nunknown regression function. These methods come with explicitly characterized\nerror bounds in terms of the sample size, tree complexity parameter, and number\nof trees in the forest, and include coverage error rates for feasible\nconfidence interval estimators. Our novel debiasing procedure for the Mondrian\nrandom forest also allows it to achieve the minimax-optimal point estimation\nconvergence rate in mean squared error for multivariate $\\beta$-H\\\"older\nregression functions, for all $\\beta > 0$, provided that the underlying tuning\nparameters are chosen appropriately. Efficient and implementable algorithms are\ndevised for both batch and online learning settings, and we carefully study the\ncomputational complexity of different Mondrian random forest implementations.\nFinally, simulations with synthetic data validate our theory and methodology,\ndemonstrating their excellent finite-sample properties."
                },
                "authors": [
                    {
                        "name": "Matias D. Cattaneo"
                    },
                    {
                        "name": "Jason M. Klusowski"
                    },
                    {
                        "name": "William G. Underwood"
                    }
                ],
                "author_detail": {
                    "name": "William G. Underwood"
                },
                "author": "William G. Underwood",
                "arxiv_comment": "64 pages, 1 figure, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G08 (Primary), 62G05, 62G20 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14964v1",
                "updated": "2024-08-27T11:10:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    11,
                    10,
                    39,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T11:10:39Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    11,
                    10,
                    39,
                    1,
                    240,
                    0
                ],
                "title": "Cross-Modal Learning for Chemistry Property Prediction: Large Language\n  Models Meet Graph Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Modal Learning for Chemistry Property Prediction: Large Language\n  Models Meet Graph Machine Learning"
                },
                "summary": "In the field of chemistry, the objective is to create novel molecules with\ndesired properties, facilitating accurate property predictions for applications\nsuch as material design and drug screening. However, existing graph deep\nlearning methods face limitations that curb their expressive power. To address\nthis, we explore the integration of vast molecular domain knowledge from Large\nLanguage Models (LLMs) with the complementary strengths of Graph Neural\nNetworks (GNNs) to enhance performance in property prediction tasks. We\nintroduce a Multi-Modal Fusion (MMF) framework that synergistically harnesses\nthe analytical prowess of GNNs and the linguistic generative and predictive\nabilities of LLMs, thereby improving accuracy and robustness in predicting\nmolecular properties. Our framework combines the effectiveness of GNNs in\nmodeling graph-structured data with the zero-shot and few-shot learning\ncapabilities of LLMs, enabling improved predictions while reducing the risk of\noverfitting. Furthermore, our approach effectively addresses distributional\nshifts, a common challenge in real-world applications, and showcases the\nefficacy of learning cross-modal representations, surpassing state-of-the-art\nbaselines on benchmark datasets for property prediction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of chemistry, the objective is to create novel molecules with\ndesired properties, facilitating accurate property predictions for applications\nsuch as material design and drug screening. However, existing graph deep\nlearning methods face limitations that curb their expressive power. To address\nthis, we explore the integration of vast molecular domain knowledge from Large\nLanguage Models (LLMs) with the complementary strengths of Graph Neural\nNetworks (GNNs) to enhance performance in property prediction tasks. We\nintroduce a Multi-Modal Fusion (MMF) framework that synergistically harnesses\nthe analytical prowess of GNNs and the linguistic generative and predictive\nabilities of LLMs, thereby improving accuracy and robustness in predicting\nmolecular properties. Our framework combines the effectiveness of GNNs in\nmodeling graph-structured data with the zero-shot and few-shot learning\ncapabilities of LLMs, enabling improved predictions while reducing the risk of\noverfitting. Furthermore, our approach effectively addresses distributional\nshifts, a common challenge in real-world applications, and showcases the\nefficacy of learning cross-modal representations, surpassing state-of-the-art\nbaselines on benchmark datasets for property prediction tasks."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "arxiv_comment": "Paper Accepted at Workshop on Robustness of Few-shot and Zero-shot\n  Learning in Foundation Models at NeurIPS 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15504v2",
                "updated": "2024-08-27T10:07:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    10,
                    7,
                    27,
                    1,
                    240,
                    0
                ],
                "published": "2024-06-19T16:43:56Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    16,
                    43,
                    56,
                    2,
                    171,
                    0
                ],
                "title": "Dr.E Bridges Graphs with Large Language Models through Words",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dr.E Bridges Graphs with Large Language Models through Words"
                },
                "summary": "Significant efforts have been dedicated to integrating the powerful Large\nLanguage Models (LLMs) with diverse modalities, particularly focusing on the\nfusion of language, vision and audio data. However, the graph-structured data,\nwhich is inherently rich in structural and domain-specific knowledge, has not\nyet been gracefully adapted to LLMs. Existing methods either describe the graph\nwith raw text, suffering the loss of graph structural information, or feed\nGraph Neural Network (GNN) embeddings into LLMs at the cost of losing\nexplainable prompt semantics. To bridge this gap, we introduce an end-to-end\nmodality-aligning framework for LLM-graph alignment: Dual-Residual Vector\nQuantized-Variational AutoEncoder, namely Dr.E. Our approach is purposefully\ndesigned to facilitate token-level alignment with LLMs, enabling an effective\ntranslation of the intrinsic `language' of graphs into comprehensible natural\nlanguage. We also manage to enhance LLMs' more robust structural understanding\nof graphs by incorporating multiple views of the central nodes based on their\nsurrounding nodes at various distances. Our experimental evaluations on\nstandard graph tasks demonstrate competitive performance against other\nstate-of-the-art (SOTA) approaches. Additionally, our framework ensures certain\nvisual interpretability, efficiency, and robustness, marking the promising\nsuccessful endeavor to achieve token-level alignment between LLMs and GNNs. Our\ncode is available at: https://anonymous.4open.science/r/dre-817.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant efforts have been dedicated to integrating the powerful Large\nLanguage Models (LLMs) with diverse modalities, particularly focusing on the\nfusion of language, vision and audio data. However, the graph-structured data,\nwhich is inherently rich in structural and domain-specific knowledge, has not\nyet been gracefully adapted to LLMs. Existing methods either describe the graph\nwith raw text, suffering the loss of graph structural information, or feed\nGraph Neural Network (GNN) embeddings into LLMs at the cost of losing\nexplainable prompt semantics. To bridge this gap, we introduce an end-to-end\nmodality-aligning framework for LLM-graph alignment: Dual-Residual Vector\nQuantized-Variational AutoEncoder, namely Dr.E. Our approach is purposefully\ndesigned to facilitate token-level alignment with LLMs, enabling an effective\ntranslation of the intrinsic `language' of graphs into comprehensible natural\nlanguage. We also manage to enhance LLMs' more robust structural understanding\nof graphs by incorporating multiple views of the central nodes based on their\nsurrounding nodes at various distances. Our experimental evaluations on\nstandard graph tasks demonstrate competitive performance against other\nstate-of-the-art (SOTA) approaches. Additionally, our framework ensures certain\nvisual interpretability, efficiency, and robustness, marking the promising\nsuccessful endeavor to achieve token-level alignment between LLMs and GNNs. Our\ncode is available at: https://anonymous.4open.science/r/dre-817."
                },
                "authors": [
                    {
                        "name": "Zipeng Liu"
                    },
                    {
                        "name": "Likang Wu"
                    },
                    {
                        "name": "Ming He"
                    },
                    {
                        "name": "Zhong Guan"
                    },
                    {
                        "name": "Hongke Zhao"
                    },
                    {
                        "name": "Nan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Nan Feng"
                },
                "author": "Nan Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14915v1",
                "updated": "2024-08-27T09:44:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    44,
                    1,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:44:01Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    44,
                    1,
                    1,
                    240,
                    0
                ],
                "title": "Can Transformers Do Enumerative Geometry?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Transformers Do Enumerative Geometry?"
                },
                "summary": "How can Transformers model and learn enumerative geometry? What is a robust\nprocedure for using Transformers in abductive knowledge discovery within a\nmathematician-machine collaboration? In this work, we introduce a new paradigm\nin computational enumerative geometry in analyzing the $\\psi$-class\nintersection numbers on the moduli space of curves. By formulating the\nenumerative problem as a continuous optimization task, we develop a\nTransformer-based model for computing $\\psi$-class intersection numbers based\non the underlying quantum Airy structure. For a finite range of genera, our\nmodel is capable of regressing intersection numbers that span an extremely wide\nrange of values, from $10^{-45}$ to $10^{45}$. To provide a proper inductive\nbias for capturing the recursive behavior of intersection numbers, we propose a\nnew activation function, Dynamic Range Activator (DRA). Moreover, given the\nsevere heteroscedasticity of $\\psi$-class intersections and the required\nprecision, we quantify the uncertainty of the predictions using Conformal\nPrediction with a dynamic sliding window that is aware of the number of marked\npoints. Next, we go beyond merely computing intersection numbers and explore\nthe enumerative \"world-model\" of the Transformers. Through a series of causal\ninference and correlational interpretability analyses, we demonstrate that\nTransformers are actually modeling Virasoro constraints in a purely data-driven\nmanner. Additionally, we provide evidence for the comprehension of several\nvalues appearing in the large genus asymptotic of $\\psi$-class intersection\nnumbers through abductive hypothesis testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can Transformers model and learn enumerative geometry? What is a robust\nprocedure for using Transformers in abductive knowledge discovery within a\nmathematician-machine collaboration? In this work, we introduce a new paradigm\nin computational enumerative geometry in analyzing the $\\psi$-class\nintersection numbers on the moduli space of curves. By formulating the\nenumerative problem as a continuous optimization task, we develop a\nTransformer-based model for computing $\\psi$-class intersection numbers based\non the underlying quantum Airy structure. For a finite range of genera, our\nmodel is capable of regressing intersection numbers that span an extremely wide\nrange of values, from $10^{-45}$ to $10^{45}$. To provide a proper inductive\nbias for capturing the recursive behavior of intersection numbers, we propose a\nnew activation function, Dynamic Range Activator (DRA). Moreover, given the\nsevere heteroscedasticity of $\\psi$-class intersections and the required\nprecision, we quantify the uncertainty of the predictions using Conformal\nPrediction with a dynamic sliding window that is aware of the number of marked\npoints. Next, we go beyond merely computing intersection numbers and explore\nthe enumerative \"world-model\" of the Transformers. Through a series of causal\ninference and correlational interpretability analyses, we demonstrate that\nTransformers are actually modeling Virasoro constraints in a purely data-driven\nmanner. Additionally, we provide evidence for the comprehension of several\nvalues appearing in the large genus asymptotic of $\\psi$-class intersection\nnumbers through abductive hypothesis testing."
                },
                "authors": [
                    {
                        "name": "Baran Hashemi"
                    },
                    {
                        "name": "Roderic G. Corominas"
                    },
                    {
                        "name": "Alessandro Giacchetto"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Giacchetto"
                },
                "author": "Alessandro Giacchetto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14909v1",
                "updated": "2024-08-27T09:35:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    35,
                    49,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:35:49Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    35,
                    49,
                    1,
                    240,
                    0
                ],
                "title": "SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking\n  State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking\n  State Space Models"
                },
                "summary": "Known as low energy consumption networks, spiking neural networks (SNNs) have\ngained a lot of attention within the past decades. While SNNs are increasing\ncompetitive with artificial neural networks (ANNs) for vision tasks, they are\nrarely used for long sequence tasks, despite their intrinsic temporal dynamics.\nIn this work, we develop spiking state space models (SpikingSSMs) for long\nsequence learning by leveraging on the sequence learning abilities of state\nspace models (SSMs). Inspired by dendritic neuron structure, we hierarchically\nintegrate neuronal dynamics with the original SSM block, meanwhile realizing\nsparse synaptic computation. Furthermore, to solve the conflict of event-driven\nneuronal dynamics with parallel computing, we propose a light-weight surrogate\ndynamic network which accurately predicts the after-reset membrane potential\nand compatible to learnable thresholds, enabling orders of acceleration in\ntraining speed compared with conventional iterative methods. On the long range\narena benchmark task, SpikingSSM achieves competitive performance to\nstate-of-the-art SSMs meanwhile realizing on average 90\\% of network sparsity.\nOn language modeling, our network significantly surpasses existing spiking\nlarge language models (spikingLLMs) on the WikiText-103 dataset with only a\nthird of the model size, demonstrating its potential as backbone architecture\nfor low computation cost LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Known as low energy consumption networks, spiking neural networks (SNNs) have\ngained a lot of attention within the past decades. While SNNs are increasing\ncompetitive with artificial neural networks (ANNs) for vision tasks, they are\nrarely used for long sequence tasks, despite their intrinsic temporal dynamics.\nIn this work, we develop spiking state space models (SpikingSSMs) for long\nsequence learning by leveraging on the sequence learning abilities of state\nspace models (SSMs). Inspired by dendritic neuron structure, we hierarchically\nintegrate neuronal dynamics with the original SSM block, meanwhile realizing\nsparse synaptic computation. Furthermore, to solve the conflict of event-driven\nneuronal dynamics with parallel computing, we propose a light-weight surrogate\ndynamic network which accurately predicts the after-reset membrane potential\nand compatible to learnable thresholds, enabling orders of acceleration in\ntraining speed compared with conventional iterative methods. On the long range\narena benchmark task, SpikingSSM achieves competitive performance to\nstate-of-the-art SSMs meanwhile realizing on average 90\\% of network sparsity.\nOn language modeling, our network significantly surpasses existing spiking\nlarge language models (spikingLLMs) on the WikiText-103 dataset with only a\nthird of the model size, demonstrating its potential as backbone architecture\nfor low computation cost LLMs."
                },
                "authors": [
                    {
                        "name": "Shuaijie Shen"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Renzhuo Huang"
                    },
                    {
                        "name": "Yan Zhong"
                    },
                    {
                        "name": "Qinghai Guo"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Luziwei Leng"
                    }
                ],
                "author_detail": {
                    "name": "Luziwei Leng"
                },
                "author": "Luziwei Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10779v2",
                "updated": "2024-08-27T09:28:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    28,
                    35,
                    1,
                    240,
                    0
                ],
                "published": "2024-05-17T13:40:59Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    13,
                    40,
                    59,
                    4,
                    138,
                    0
                ],
                "title": "Baseline Results for Selected Nonlinear System Identification Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Baseline Results for Selected Nonlinear System Identification Benchmarks"
                },
                "summary": "Nonlinear system identification remains an important open challenge across\nresearch and academia. Large numbers of novel approaches are seen published\neach year, each presenting improvements or extensions to existing methods. It\nis natural, therefore, to consider how one might choose between these competing\nmodels. Benchmark datasets provide one clear way to approach this question.\nHowever, to make meaningful inference based on benchmark performance it is\nimportant to understand how well a new method performs comparatively to results\navailable with well-established methods. This paper presents a set of ten\nbaseline techniques and their relative performances on five popular benchmarks.\nThe aim of this contribution is to stimulate thought and discussion regarding\nobjective comparison of identification methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlinear system identification remains an important open challenge across\nresearch and academia. Large numbers of novel approaches are seen published\neach year, each presenting improvements or extensions to existing methods. It\nis natural, therefore, to consider how one might choose between these competing\nmodels. Benchmark datasets provide one clear way to approach this question.\nHowever, to make meaningful inference based on benchmark performance it is\nimportant to understand how well a new method performs comparatively to results\navailable with well-established methods. This paper presents a set of ten\nbaseline techniques and their relative performances on five popular benchmarks.\nThe aim of this contribution is to stimulate thought and discussion regarding\nobjective comparison of identification methodologies."
                },
                "authors": [
                    {
                        "name": "Max D. Champneys"
                    },
                    {
                        "name": "Gerben I. Beintema"
                    },
                    {
                        "name": "Roland Tóth"
                    },
                    {
                        "name": "Maarten Schoukens"
                    },
                    {
                        "name": "Timothy J. Rogers"
                    }
                ],
                "author_detail": {
                    "name": "Timothy J. Rogers"
                },
                "author": "Timothy J. Rogers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05655v2",
                "updated": "2024-08-27T09:24:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    24,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-10-09T12:10:51Z",
                "published_parsed": [
                    2023,
                    10,
                    9,
                    12,
                    10,
                    51,
                    0,
                    282,
                    0
                ],
                "title": "Causal structure learning with momentum: Sampling distributions over\n  Markov Equivalence Classes of DAGs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal structure learning with momentum: Sampling distributions over\n  Markov Equivalence Classes of DAGs"
                },
                "summary": "In the context of inferring a Bayesian network structure (directed acyclic\ngraph, DAG for short), we devise a non-reversible continuous time Markov chain,\nthe ``Causal Zig-Zag sampler'', that targets a probability distribution over\nclasses of observationally equivalent (Markov equivalent) DAGs. The classes are\nrepresented as completed partially directed acyclic graphs (CPDAGs). The\nnon-reversible Markov chain relies on the operators used in Chickering's Greedy\nEquivalence Search (GES) and is endowed with a momentum variable, which\nimproves mixing significantly as we show empirically. The possible target\ndistributions include posterior distributions based on a prior over DAGs and a\nMarkov equivalent likelihood. We offer an efficient implementation wherein we\ndevelop new algorithms for listing, counting, uniformly sampling, and applying\npossible moves of the GES operators, all of which significantly improve upon\nthe state-of-the-art run-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context of inferring a Bayesian network structure (directed acyclic\ngraph, DAG for short), we devise a non-reversible continuous time Markov chain,\nthe ``Causal Zig-Zag sampler'', that targets a probability distribution over\nclasses of observationally equivalent (Markov equivalent) DAGs. The classes are\nrepresented as completed partially directed acyclic graphs (CPDAGs). The\nnon-reversible Markov chain relies on the operators used in Chickering's Greedy\nEquivalence Search (GES) and is endowed with a momentum variable, which\nimproves mixing significantly as we show empirically. The possible target\ndistributions include posterior distributions based on a prior over DAGs and a\nMarkov equivalent likelihood. We offer an efficient implementation wherein we\ndevelop new algorithms for listing, counting, uniformly sampling, and applying\npossible moves of the GES operators, all of which significantly improve upon\nthe state-of-the-art run-time."
                },
                "authors": [
                    {
                        "name": "Moritz Schauer"
                    },
                    {
                        "name": "Marcel Wienöbst"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Wienöbst"
                },
                "author": "Marcel Wienöbst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T37 (primary) 60J99 (secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18005v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18005v3",
                "updated": "2024-08-27T09:17:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    17,
                    22,
                    1,
                    240,
                    0
                ],
                "published": "2024-05-28T09:48:29Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    9,
                    48,
                    29,
                    1,
                    149,
                    0
                ],
                "title": "Persistence Diagram Estimation : Beyond Plug-in Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistence Diagram Estimation : Beyond Plug-in Approaches"
                },
                "summary": "Persistent homology is a tool from Topological Data Analysis (TDA) used to\nsummarize the topology underlying data. It can be conveniently represented\nthrough persistence diagrams. Observing a noisy signal, common strategies to\ninfer its persistence diagram involve plug-in estimators, and convergence\nproperties are then derived from sup-norm stability. This dependence on the\nsup-norm convergence of the preliminary estimator is restrictive, as it\nessentially imposes to consider regular classes of signals. Departing from\nthese approaches, we design an estimator based on image persistence. In the\ncontext of the Gaussian white noise model, and for large classes of\npiecewise-constant signals, we prove that the proposed estimator is consistent\nand achieves parametric rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent homology is a tool from Topological Data Analysis (TDA) used to\nsummarize the topology underlying data. It can be conveniently represented\nthrough persistence diagrams. Observing a noisy signal, common strategies to\ninfer its persistence diagram involve plug-in estimators, and convergence\nproperties are then derived from sup-norm stability. This dependence on the\nsup-norm convergence of the preliminary estimator is restrictive, as it\nessentially imposes to consider regular classes of signals. Departing from\nthese approaches, we design an estimator based on image persistence. In the\ncontext of the Gaussian white noise model, and for large classes of\npiecewise-constant signals, we prove that the proposed estimator is consistent\nand achieves parametric rates."
                },
                "authors": [
                    {
                        "name": "Hugo Henneuse"
                    }
                ],
                "author_detail": {
                    "name": "Hugo Henneuse"
                },
                "author": "Hugo Henneuse",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18005v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18005v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15449v2",
                "updated": "2024-08-27T09:12:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    12,
                    8,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-22T08:07:33Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    8,
                    7,
                    33,
                    0,
                    204,
                    0
                ],
                "title": "Persistence-based Modes Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistence-based Modes Inference"
                },
                "summary": "We address the problem of estimating multiple modes of a multivariate\ndensity, using persistent homology, a key tool from Topological Data Analysis.\nWe propose a procedure, based on a preliminary estimation of the\n$H_{0}-$persistence diagram, to estimate the number of modes, their locations,\nand the associated local maxima. For large classes of piecewise-continuous\nfunctions, we show that these estimators are consistent and achieve nearly\nminimax rates. These classes involve geometric control over the set of\ndiscontinuities of the density, and differ from commonly considered function\nclasses in mode(s) inference. Interestingly, we do not suppose regularity or\neven continuity in any neighborhood of the modes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of estimating multiple modes of a multivariate\ndensity, using persistent homology, a key tool from Topological Data Analysis.\nWe propose a procedure, based on a preliminary estimation of the\n$H_{0}-$persistence diagram, to estimate the number of modes, their locations,\nand the associated local maxima. For large classes of piecewise-continuous\nfunctions, we show that these estimators are consistent and achieve nearly\nminimax rates. These classes involve geometric control over the set of\ndiscontinuities of the density, and differ from commonly considered function\nclasses in mode(s) inference. Interestingly, we do not suppose regularity or\neven continuity in any neighborhood of the modes."
                },
                "authors": [
                    {
                        "name": "Hugo Henneuse"
                    }
                ],
                "author_detail": {
                    "name": "Hugo Henneuse"
                },
                "author": "Hugo Henneuse",
                "arxiv_comment": "37 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20146v2",
                "updated": "2024-08-27T09:09:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    9,
                    55,
                    1,
                    240,
                    0
                ],
                "published": "2024-05-30T15:23:44Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    15,
                    23,
                    44,
                    3,
                    151,
                    0
                ],
                "title": "High-energy neutrinos from the vicinity of the supermassive black hole\n  in NGC 1068",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-energy neutrinos from the vicinity of the supermassive black hole\n  in NGC 1068"
                },
                "summary": "We present a comprehensive multi-messenger study of NGC 1068, the prototype\nSeyfert II galaxy recently associated with high-energy IceCube neutrinos.\nVarious aspects of the source, including its nuclear activity, jet, outflow,\nand starburst region, are analyzed in detail using a multi-wavelength approach\nand relevant luminosities are derived. We then explore its gamma-ray and\nneutrino emissions and investigate potential mechanisms underlying these\nphenomena and their relations with the different astrophysical components to\ntry to understand which one is responsible for the IceCube neutrinos. By first\nusing simple order-of-magnitude arguments and then applying specific\ntheoretical models, we infer that only the region close to the accretion disc\naround the supermassive black hole has both the right density of X-ray photons\nneeded to provide the targets for protons to sustain neutrino production and of\noptical/infrared photons required to absorb the associated but unobserved gamma\nrays. We conclude by highlighting ongoing efforts to constrain a possible broad\nconnection between neutrinos and active galactic nuclei, as well as future\nsynergies between astronomical and neutrino facilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive multi-messenger study of NGC 1068, the prototype\nSeyfert II galaxy recently associated with high-energy IceCube neutrinos.\nVarious aspects of the source, including its nuclear activity, jet, outflow,\nand starburst region, are analyzed in detail using a multi-wavelength approach\nand relevant luminosities are derived. We then explore its gamma-ray and\nneutrino emissions and investigate potential mechanisms underlying these\nphenomena and their relations with the different astrophysical components to\ntry to understand which one is responsible for the IceCube neutrinos. By first\nusing simple order-of-magnitude arguments and then applying specific\ntheoretical models, we infer that only the region close to the accretion disc\naround the supermassive black hole has both the right density of X-ray photons\nneeded to provide the targets for protons to sustain neutrino production and of\noptical/infrared photons required to absorb the associated but unobserved gamma\nrays. We conclude by highlighting ongoing efforts to constrain a possible broad\nconnection between neutrinos and active galactic nuclei, as well as future\nsynergies between astronomical and neutrino facilities."
                },
                "authors": [
                    {
                        "name": "P. Padovani"
                    },
                    {
                        "name": "E. Resconi"
                    },
                    {
                        "name": "M. Ajello"
                    },
                    {
                        "name": "C. Bellenghi"
                    },
                    {
                        "name": "S. Bianchi"
                    },
                    {
                        "name": "P. Blasi"
                    },
                    {
                        "name": "K. -Y. Huang"
                    },
                    {
                        "name": "S. Gabici"
                    },
                    {
                        "name": "V. Gámez Rosas"
                    },
                    {
                        "name": "H. Niederhausen"
                    },
                    {
                        "name": "E. Peretti"
                    },
                    {
                        "name": "B. Eichmann"
                    },
                    {
                        "name": "D. Guetta"
                    },
                    {
                        "name": "A. Lamastra"
                    },
                    {
                        "name": "T. Shimizu"
                    }
                ],
                "author_detail": {
                    "name": "T. Shimizu"
                },
                "author": "T. Shimizu",
                "arxiv_comment": "18 pages, 4 figures, review paper accepted for publication in Nature\n  Astronomy; authors' version before editorial review. Revised version matches\n  the title of the published paper; no other changes were made to the text",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14866v1",
                "updated": "2024-08-27T08:38:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    8,
                    38,
                    48,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T08:38:48Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    8,
                    38,
                    48,
                    1,
                    240,
                    0
                ],
                "title": "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language\n  Models"
                },
                "summary": "Language Language Models (LLMs) face safety concerns due to potential misuse\nby malicious users. Recent red-teaming efforts have identified adversarial\nsuffixes capable of jailbreaking LLMs using the gradient-based search algorithm\nGreedy Coordinate Gradient (GCG). However, GCG struggles with computational\ninefficiency, limiting further investigations regarding suffix transferability\nand scalability across models and data. In this work, we bridge the connection\nbetween search efficiency and suffix transferability. We propose a two-stage\ntransfer learning framework, DeGCG, which decouples the search process into\nbehavior-agnostic pre-searching and behavior-relevant post-searching.\nSpecifically, we employ direct first target token optimization in pre-searching\nto facilitate the search process. We apply our approach to cross-model,\ncross-data, and self-transfer scenarios. Furthermore, we introduce an\ninterleaved variant of our approach, i-DeGCG, which iteratively leverages\nself-transferability to accelerate the search process. Experiments on HarmBench\ndemonstrate the efficiency of our approach across various models and domains.\nNotably, our i-DeGCG outperforms the baseline on Llama2-chat-7b with ASRs of\n$43.9$ ($+22.2$) and $39.0$ ($+19.5$) on valid and test sets, respectively.\nFurther analysis on cross-model transfer indicates the pivotal role of first\ntarget token optimization in leveraging suffix transferability for efficient\nsearching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Language Models (LLMs) face safety concerns due to potential misuse\nby malicious users. Recent red-teaming efforts have identified adversarial\nsuffixes capable of jailbreaking LLMs using the gradient-based search algorithm\nGreedy Coordinate Gradient (GCG). However, GCG struggles with computational\ninefficiency, limiting further investigations regarding suffix transferability\nand scalability across models and data. In this work, we bridge the connection\nbetween search efficiency and suffix transferability. We propose a two-stage\ntransfer learning framework, DeGCG, which decouples the search process into\nbehavior-agnostic pre-searching and behavior-relevant post-searching.\nSpecifically, we employ direct first target token optimization in pre-searching\nto facilitate the search process. We apply our approach to cross-model,\ncross-data, and self-transfer scenarios. Furthermore, we introduce an\ninterleaved variant of our approach, i-DeGCG, which iteratively leverages\nself-transferability to accelerate the search process. Experiments on HarmBench\ndemonstrate the efficiency of our approach across various models and domains.\nNotably, our i-DeGCG outperforms the baseline on Llama2-chat-7b with ASRs of\n$43.9$ ($+22.2$) and $39.0$ ($+19.5$) on valid and test sets, respectively.\nFurther analysis on cross-model transfer indicates the pivotal role of first\ntarget token optimization in leveraging suffix transferability for efficient\nsearching."
                },
                "authors": [
                    {
                        "name": "Hongfu Liu"
                    },
                    {
                        "name": "Yuxi Xie"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Michael Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Shieh"
                },
                "author": "Michael Shieh",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00737v2",
                "updated": "2024-08-27T08:33:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    8,
                    33,
                    31,
                    1,
                    240,
                    0
                ],
                "published": "2024-06-30T15:50:32Z",
                "published_parsed": [
                    2024,
                    6,
                    30,
                    15,
                    50,
                    32,
                    6,
                    182,
                    0
                ],
                "title": "LLM4GEN: Leveraging Semantic Representation of LLMs for Text-to-Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4GEN: Leveraging Semantic Representation of LLMs for Text-to-Image\n  Generation"
                },
                "summary": "Diffusion models have exhibited substantial success in text-to-image\ngeneration. However, they often encounter challenges when dealing with complex\nand dense prompts involving multiple objects, attribute binding, and long\ndescriptions. In this paper, we propose a novel framework called\n\\textbf{LLM4GEN}, which enhances the semantic understanding of text-to-image\ndiffusion models by leveraging the representation of Large Language Models\n(LLMs). It can be seamlessly incorporated into various diffusion models as a\nplug-and-play component. A specially designed Cross-Adapter Module (CAM)\nintegrates the original text features of text-to-image models with LLM\nfeatures, thereby enhancing text-to-image generation. Additionally, to\nfacilitate and correct entity-attribute relationships in text prompts, we\ndevelop an entity-guided regularization loss to further improve generation\nperformance. We also introduce DensePrompts, which contains $7,000$ dense\nprompts to provide a comprehensive evaluation for the text-to-image generation\ntask. Experiments indicate that LLM4GEN significantly improves the semantic\nalignment of SD1.5 and SDXL, demonstrating increases of 9.69\\% and 12.90\\% in\ncolor on T2I-CompBench, respectively. Moreover, it surpasses existing models in\nterms of sample quality, image-text alignment, and human evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have exhibited substantial success in text-to-image\ngeneration. However, they often encounter challenges when dealing with complex\nand dense prompts involving multiple objects, attribute binding, and long\ndescriptions. In this paper, we propose a novel framework called\n\\textbf{LLM4GEN}, which enhances the semantic understanding of text-to-image\ndiffusion models by leveraging the representation of Large Language Models\n(LLMs). It can be seamlessly incorporated into various diffusion models as a\nplug-and-play component. A specially designed Cross-Adapter Module (CAM)\nintegrates the original text features of text-to-image models with LLM\nfeatures, thereby enhancing text-to-image generation. Additionally, to\nfacilitate and correct entity-attribute relationships in text prompts, we\ndevelop an entity-guided regularization loss to further improve generation\nperformance. We also introduce DensePrompts, which contains $7,000$ dense\nprompts to provide a comprehensive evaluation for the text-to-image generation\ntask. Experiments indicate that LLM4GEN significantly improves the semantic\nalignment of SD1.5 and SDXL, demonstrating increases of 9.69\\% and 12.90\\% in\ncolor on T2I-CompBench, respectively. Moreover, it surpasses existing models in\nterms of sample quality, image-text alignment, and human evaluation."
                },
                "authors": [
                    {
                        "name": "Mushui Liu"
                    },
                    {
                        "name": "Yuhang Ma"
                    },
                    {
                        "name": "Yang Zhen"
                    },
                    {
                        "name": "Jun Dan"
                    },
                    {
                        "name": "Yunlong Yu"
                    },
                    {
                        "name": "Zeng Zhao"
                    },
                    {
                        "name": "Zhipeng Hu"
                    },
                    {
                        "name": "Bai Liu"
                    },
                    {
                        "name": "Changjie Fan"
                    }
                ],
                "author_detail": {
                    "name": "Changjie Fan"
                },
                "author": "Changjie Fan",
                "arxiv_comment": "11 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03870v2",
                "updated": "2024-08-27T08:31:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    8,
                    31,
                    4,
                    1,
                    240,
                    0
                ],
                "published": "2024-03-06T17:23:28Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    17,
                    23,
                    28,
                    2,
                    66,
                    0
                ],
                "title": "Learning to Decode Collaboratively with Multiple Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Decode Collaboratively with Multiple Language Models"
                },
                "summary": "We propose a method to teach multiple large language models (LLM) to\ncollaborate by interleaving their generations at the token level. We model the\ndecision of which LLM generates the next token as a latent variable. By\noptimizing the marginal likelihood of a training set under our latent variable\nmodel, the base LLM automatically learns when to generate itself and when to\ncall on one of the ``assistant'' language models to generate, all without\ndirect supervision. Token-level collaboration during decoding allows for a\nfusion of each model's expertise in a manner tailored to the specific task at\nhand. Our collaborative decoding is especially useful in cross-domain settings\nwhere a generalist base LLM learns to invoke domain expert models. On\ninstruction-following, domain-specific QA, and reasoning tasks, we show that\nthe performance of the joint system exceeds that of the individual models.\nThrough qualitative analysis of the learned latent decisions, we show models\ntrained with our method exhibit several interesting collaboration patterns,\ne.g., template-filling. Our code is available at\nhttps://github.com/clinicalml/co-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a method to teach multiple large language models (LLM) to\ncollaborate by interleaving their generations at the token level. We model the\ndecision of which LLM generates the next token as a latent variable. By\noptimizing the marginal likelihood of a training set under our latent variable\nmodel, the base LLM automatically learns when to generate itself and when to\ncall on one of the ``assistant'' language models to generate, all without\ndirect supervision. Token-level collaboration during decoding allows for a\nfusion of each model's expertise in a manner tailored to the specific task at\nhand. Our collaborative decoding is especially useful in cross-domain settings\nwhere a generalist base LLM learns to invoke domain expert models. On\ninstruction-following, domain-specific QA, and reasoning tasks, we show that\nthe performance of the joint system exceeds that of the individual models.\nThrough qualitative analysis of the learned latent decisions, we show models\ntrained with our method exhibit several interesting collaboration patterns,\ne.g., template-filling. Our code is available at\nhttps://github.com/clinicalml/co-llm."
                },
                "authors": [
                    {
                        "name": "Shannon Zejiang Shen"
                    },
                    {
                        "name": "Hunter Lang"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Yoon Kim"
                    },
                    {
                        "name": "David Sontag"
                    }
                ],
                "author_detail": {
                    "name": "David Sontag"
                },
                "author": "David Sontag",
                "arxiv_comment": "16 pages, 4 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.16580v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.16580v4",
                "updated": "2024-08-27T08:13:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    8,
                    13,
                    1,
                    1,
                    240,
                    0
                ],
                "published": "2023-05-26T02:09:48Z",
                "published_parsed": [
                    2023,
                    5,
                    26,
                    2,
                    9,
                    48,
                    4,
                    146,
                    0
                ],
                "title": "TFDet: Target-Aware Fusion for RGB-T Pedestrian Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TFDet: Target-Aware Fusion for RGB-T Pedestrian Detection"
                },
                "summary": "Pedestrian detection plays a critical role in computer vision as it\ncontributes to ensuring traffic safety. Existing methods that rely solely on\nRGB images suffer from performance degradation under low-light conditions due\nto the lack of useful information. To address this issue, recent multispectral\ndetection approaches have combined thermal images to provide complementary\ninformation and have obtained enhanced performances. Nevertheless, few\napproaches focus on the negative effects of false positives caused by noisy\nfused feature maps. Different from them, we comprehensively analyze the impacts\nof false positives on the detection performance and find that enhancing feature\ncontrast can significantly reduce these false positives. In this paper, we\npropose a novel target-aware fusion strategy for multispectral pedestrian\ndetection, named TFDet. TFDet achieves state-of-the-art performance on two\nmultispectral pedestrian benchmarks, KAIST and LLVIP. TFDet can easily extend\nto multi-class object detection scenarios. It outperforms the previous best\napproaches on two multispectral object detection benchmarks, FLIR and M3FD.\nImportantly, TFDet has comparable inference efficiency to the previous\napproaches, and has remarkably good detection performance even under low-light\nconditions, which is a significant advancement for ensuring road safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pedestrian detection plays a critical role in computer vision as it\ncontributes to ensuring traffic safety. Existing methods that rely solely on\nRGB images suffer from performance degradation under low-light conditions due\nto the lack of useful information. To address this issue, recent multispectral\ndetection approaches have combined thermal images to provide complementary\ninformation and have obtained enhanced performances. Nevertheless, few\napproaches focus on the negative effects of false positives caused by noisy\nfused feature maps. Different from them, we comprehensively analyze the impacts\nof false positives on the detection performance and find that enhancing feature\ncontrast can significantly reduce these false positives. In this paper, we\npropose a novel target-aware fusion strategy for multispectral pedestrian\ndetection, named TFDet. TFDet achieves state-of-the-art performance on two\nmultispectral pedestrian benchmarks, KAIST and LLVIP. TFDet can easily extend\nto multi-class object detection scenarios. It outperforms the previous best\napproaches on two multispectral object detection benchmarks, FLIR and M3FD.\nImportantly, TFDet has comparable inference efficiency to the previous\napproaches, and has remarkably good detection performance even under low-light\nconditions, which is a significant advancement for ensuring road safety."
                },
                "authors": [
                    {
                        "name": "Xue Zhang"
                    },
                    {
                        "name": "Xiaohan Zhang"
                    },
                    {
                        "name": "Jiangtao Wang"
                    },
                    {
                        "name": "Jiacheng Ying"
                    },
                    {
                        "name": "Zehua Sheng"
                    },
                    {
                        "name": "Heng Yu"
                    },
                    {
                        "name": "Chunguang Li"
                    },
                    {
                        "name": "Hui-Liang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Hui-Liang Shen"
                },
                "author": "Hui-Liang Shen",
                "arxiv_doi": "10.1109/TNNLS.2024.3443455",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TNNLS.2024.3443455",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.16580v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.16580v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted by IEEE T-NNLS journal. Please jump to\n  External DOI to view the official version",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14853v1",
                "updated": "2024-08-27T08:12:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    8,
                    12,
                    8,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T08:12:08Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    8,
                    12,
                    8,
                    1,
                    240,
                    0
                ],
                "title": "Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have become a focal point in the rapidly\nevolving field of artificial intelligence. However, a critical concern is the\npresence of toxic content within the pre-training corpus of these models, which\ncan lead to the generation of inappropriate outputs. Investigating methods for\ndetecting internal faults in LLMs can help us understand their limitations and\nimprove their security. Existing methods primarily focus on jailbreaking\nattacks, which involve manually or automatically constructing adversarial\ncontent to prompt the target LLM to generate unexpected responses. These\nmethods rely heavily on prompt engineering, which is time-consuming and usually\nrequires specially designed questions. To address these challenges, this paper\nproposes a target-driven attack paradigm that focuses on directly eliciting the\ntarget response instead of optimizing the prompts. We introduce the use of\nanother LLM as the detector for toxic content, referred to as ToxDet. Given a\ntarget toxic response, ToxDet can generate a possible question and a\npreliminary answer to provoke the target model into producing desired toxic\nresponses with meanings equivalent to the provided one. ToxDet is trained by\ninteracting with the target LLM and receiving reward signals from it, utilizing\nreinforcement learning for the optimization process. While the primary focus of\nthe target models is on open-source LLMs, the fine-tuned ToxDet can also be\ntransferred to attack black-box models such as GPT-4o, achieving notable\nresults. Experimental results on AdvBench and HH-Harmless datasets demonstrate\nthe effectiveness of our methods in detecting the tendencies of target LLMs to\ngenerate harmful responses. This algorithm not only exposes vulnerabilities but\nalso provides a valuable resource for researchers to strengthen their models\nagainst such attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become a focal point in the rapidly\nevolving field of artificial intelligence. However, a critical concern is the\npresence of toxic content within the pre-training corpus of these models, which\ncan lead to the generation of inappropriate outputs. Investigating methods for\ndetecting internal faults in LLMs can help us understand their limitations and\nimprove their security. Existing methods primarily focus on jailbreaking\nattacks, which involve manually or automatically constructing adversarial\ncontent to prompt the target LLM to generate unexpected responses. These\nmethods rely heavily on prompt engineering, which is time-consuming and usually\nrequires specially designed questions. To address these challenges, this paper\nproposes a target-driven attack paradigm that focuses on directly eliciting the\ntarget response instead of optimizing the prompts. We introduce the use of\nanother LLM as the detector for toxic content, referred to as ToxDet. Given a\ntarget toxic response, ToxDet can generate a possible question and a\npreliminary answer to provoke the target model into producing desired toxic\nresponses with meanings equivalent to the provided one. ToxDet is trained by\ninteracting with the target LLM and receiving reward signals from it, utilizing\nreinforcement learning for the optimization process. While the primary focus of\nthe target models is on open-source LLMs, the fine-tuned ToxDet can also be\ntransferred to attack black-box models such as GPT-4o, achieving notable\nresults. Experimental results on AdvBench and HH-Harmless datasets demonstrate\nthe effectiveness of our methods in detecting the tendencies of target LLMs to\ngenerate harmful responses. This algorithm not only exposes vulnerabilities but\nalso provides a valuable resource for researchers to strengthen their models\nagainst such attacks."
                },
                "authors": [
                    {
                        "name": "Yuhao Du"
                    },
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Pengyu Cheng"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Anningzhe Gao"
                    }
                ],
                "author_detail": {
                    "name": "Anningzhe Gao"
                },
                "author": "Anningzhe Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14846v1",
                "updated": "2024-08-27T07:57:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    57,
                    58,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T07:57:58Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    57,
                    58,
                    1,
                    240,
                    0
                ],
                "title": "Diffusion-Occ: 3D Point Cloud Completion via Occupancy Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-Occ: 3D Point Cloud Completion via Occupancy Diffusion"
                },
                "summary": "Point clouds are crucial for capturing three-dimensional data but often\nsuffer from incompleteness due to limitations such as resolution and occlusion.\nTraditional methods typically rely on point-based approaches within\ndiscriminative frameworks for point cloud completion. In this paper, we\nintroduce \\textbf{Diffusion-Occ}, a novel framework for Diffusion Point Cloud\nCompletion. Diffusion-Occ utilizes a two-stage coarse-to-fine approach. In the\nfirst stage, the Coarse Density Voxel Prediction Network (CDNet) processes\npartial points to predict coarse density voxels, streamlining global feature\nextraction through voxel classification, as opposed to previous\nregression-based methods. In the second stage, we introduce the Occupancy\nGeneration Network (OccGen), a conditional occupancy diffusion model based on a\ntransformer architecture and enhanced by our Point-Voxel Fuse (PVF) block. This\nblock integrates coarse density voxels with partial points to leverage both\nglobal and local features for comprehensive completion. By thresholding the\noccupancy field, we convert it into a complete point cloud. Additionally, our\nmethod employs diverse training mixtures and efficient diffusion\nparameterization to enable effective one-step sampling during both training and\ninference. Experimental results demonstrate that Diffusion-Occ outperforms\nexisting discriminative and generative methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point clouds are crucial for capturing three-dimensional data but often\nsuffer from incompleteness due to limitations such as resolution and occlusion.\nTraditional methods typically rely on point-based approaches within\ndiscriminative frameworks for point cloud completion. In this paper, we\nintroduce \\textbf{Diffusion-Occ}, a novel framework for Diffusion Point Cloud\nCompletion. Diffusion-Occ utilizes a two-stage coarse-to-fine approach. In the\nfirst stage, the Coarse Density Voxel Prediction Network (CDNet) processes\npartial points to predict coarse density voxels, streamlining global feature\nextraction through voxel classification, as opposed to previous\nregression-based methods. In the second stage, we introduce the Occupancy\nGeneration Network (OccGen), a conditional occupancy diffusion model based on a\ntransformer architecture and enhanced by our Point-Voxel Fuse (PVF) block. This\nblock integrates coarse density voxels with partial points to leverage both\nglobal and local features for comprehensive completion. By thresholding the\noccupancy field, we convert it into a complete point cloud. Additionally, our\nmethod employs diverse training mixtures and efficient diffusion\nparameterization to enable effective one-step sampling during both training and\ninference. Experimental results demonstrate that Diffusion-Occ outperforms\nexisting discriminative and generative methods."
                },
                "authors": [
                    {
                        "name": "Guoqing Zhang"
                    },
                    {
                        "name": "Jian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Liu"
                },
                "author": "Jian Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14845v1",
                "updated": "2024-08-27T07:56:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    56,
                    35,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T07:56:35Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    56,
                    35,
                    1,
                    240,
                    0
                ],
                "title": "AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark"
                },
                "summary": "Detecting biases in natural language understanding (NLU) for African American\nVernacular English (AAVE) is crucial to developing inclusive natural language\nprocessing (NLP) systems. To address dialect-induced performance discrepancies,\nwe introduce AAVENUE ({AAVE} {N}atural Language {U}nderstanding {E}valuation),\na benchmark for evaluating large language model (LLM) performance on NLU tasks\nin AAVE and Standard American English (SAE). AAVENUE builds upon and extends\nexisting benchmarks like VALUE, replacing deterministic syntactic and\nmorphological transformations with a more flexible methodology leveraging\nLLM-based translation with few-shot prompting, improving performance across our\nevaluation metrics when translating key tasks from the GLUE and SuperGLUE\nbenchmarks. We compare AAVENUE and VALUE translations using five popular LLMs\nand a comprehensive set of metrics including fluency, BARTScore, quality,\ncoherence, and understandability. Additionally, we recruit fluent AAVE speakers\nto validate our translations for authenticity. Our evaluations reveal that LLMs\nconsistently perform better on SAE tasks than AAVE-translated versions,\nunderscoring inherent biases and highlighting the need for more inclusive NLP\nmodels. We have open-sourced our source code on GitHub and created a website to\nshowcase our work at https://aavenue.live.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting biases in natural language understanding (NLU) for African American\nVernacular English (AAVE) is crucial to developing inclusive natural language\nprocessing (NLP) systems. To address dialect-induced performance discrepancies,\nwe introduce AAVENUE ({AAVE} {N}atural Language {U}nderstanding {E}valuation),\na benchmark for evaluating large language model (LLM) performance on NLU tasks\nin AAVE and Standard American English (SAE). AAVENUE builds upon and extends\nexisting benchmarks like VALUE, replacing deterministic syntactic and\nmorphological transformations with a more flexible methodology leveraging\nLLM-based translation with few-shot prompting, improving performance across our\nevaluation metrics when translating key tasks from the GLUE and SuperGLUE\nbenchmarks. We compare AAVENUE and VALUE translations using five popular LLMs\nand a comprehensive set of metrics including fluency, BARTScore, quality,\ncoherence, and understandability. Additionally, we recruit fluent AAVE speakers\nto validate our translations for authenticity. Our evaluations reveal that LLMs\nconsistently perform better on SAE tasks than AAVE-translated versions,\nunderscoring inherent biases and highlighting the need for more inclusive NLP\nmodels. We have open-sourced our source code on GitHub and created a website to\nshowcase our work at https://aavenue.live."
                },
                "authors": [
                    {
                        "name": "Abhay Gupta"
                    },
                    {
                        "name": "Philip Meng"
                    },
                    {
                        "name": "Ece Yurtseven"
                    },
                    {
                        "name": "Sean O'Brien"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14843v1",
                "updated": "2024-08-27T07:54:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    54,
                    15,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T07:54:15Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    54,
                    15,
                    1,
                    240,
                    0
                ],
                "title": "Correntropy-Based Improper Likelihood Model for Robust\n  Electrophysiological Source Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correntropy-Based Improper Likelihood Model for Robust\n  Electrophysiological Source Imaging"
                },
                "summary": "Bayesian learning provides a unified skeleton to solve the\nelectrophysiological source imaging task. From this perspective, existing\nsource imaging algorithms utilize the Gaussian assumption for the observation\nnoise to build the likelihood function for Bayesian inference. However, the\nelectromagnetic measurements of brain activity are usually affected by\nmiscellaneous artifacts, leading to a potentially non-Gaussian distribution for\nthe observation noise. Hence the conventional Gaussian likelihood model is a\nsuboptimal choice for the real-world source imaging task. In this study, we aim\nto solve this problem by proposing a new likelihood model which is robust with\nrespect to non-Gaussian noises. Motivated by the robust maximum correntropy\ncriterion, we propose a new improper distribution model concerning the noise\nassumption. This new noise distribution is leveraged to structure a robust\nlikelihood function and integrated with hierarchical prior distributions to\nestimate source activities by variational inference. In particular, the score\nmatching is adopted to determine the hyperparameters for the improper\nlikelihood model. A comprehensive performance evaluation is performed to\ncompare the proposed noise assumption to the conventional Gaussian model.\nSimulation results show that, the proposed method can realize more precise\nsource reconstruction by designing known ground-truth. The real-world dataset\nalso demonstrates the superiority of our new method with the visual perception\ntask. This study provides a new backbone for Bayesian source imaging, which\nwould facilitate its application using real-world noisy brain signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian learning provides a unified skeleton to solve the\nelectrophysiological source imaging task. From this perspective, existing\nsource imaging algorithms utilize the Gaussian assumption for the observation\nnoise to build the likelihood function for Bayesian inference. However, the\nelectromagnetic measurements of brain activity are usually affected by\nmiscellaneous artifacts, leading to a potentially non-Gaussian distribution for\nthe observation noise. Hence the conventional Gaussian likelihood model is a\nsuboptimal choice for the real-world source imaging task. In this study, we aim\nto solve this problem by proposing a new likelihood model which is robust with\nrespect to non-Gaussian noises. Motivated by the robust maximum correntropy\ncriterion, we propose a new improper distribution model concerning the noise\nassumption. This new noise distribution is leveraged to structure a robust\nlikelihood function and integrated with hierarchical prior distributions to\nestimate source activities by variational inference. In particular, the score\nmatching is adopted to determine the hyperparameters for the improper\nlikelihood model. A comprehensive performance evaluation is performed to\ncompare the proposed noise assumption to the conventional Gaussian model.\nSimulation results show that, the proposed method can realize more precise\nsource reconstruction by designing known ground-truth. The real-world dataset\nalso demonstrates the superiority of our new method with the visual perception\ntask. This study provides a new backbone for Bayesian source imaging, which\nwould facilitate its application using real-world noisy brain signal."
                },
                "authors": [
                    {
                        "name": "Yuanhao Li"
                    },
                    {
                        "name": "Badong Chen"
                    },
                    {
                        "name": "Zhongxu Hu"
                    },
                    {
                        "name": "Keita Suzuki"
                    },
                    {
                        "name": "Wenjun Bai"
                    },
                    {
                        "name": "Yasuharu Koike"
                    },
                    {
                        "name": "Okito Yamashita"
                    }
                ],
                "author_detail": {
                    "name": "Okito Yamashita"
                },
                "author": "Okito Yamashita",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14834v1",
                "updated": "2024-08-27T07:44:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    44,
                    16,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T07:44:16Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    44,
                    16,
                    1,
                    240,
                    0
                ],
                "title": "Strategic Optimization and Challenges of Large Language Models in\n  Object-Oriented Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Optimization and Challenges of Large Language Models in\n  Object-Oriented Programming"
                },
                "summary": "In the area of code generation research, the emphasis has transitioned from\ncrafting individual functions to developing class-level method code that\nintegrates contextual information. This shift has brought several benchmarks\nsuch as ClassEval and CoderEval, which consider class-level contexts.\nNevertheless, the influence of specific contextual factors at the method level\nremains less explored.\n  This research focused on method-level code generation within the\nObject-Oriented Programming (OOP) framework. Based on CoderEval, we devised\nexperiments that varied the extent of contextual information in the prompts,\nranging from method-specific to project-wide details. We introduced the\ninnovative metric of \"Prompt-Token Cost-Effectiveness\" to evaluate the economic\nviability of incorporating additional contextual layers. Our findings indicate\nthat prompts enriched with method invocation details yield the highest\ncost-effectiveness. Additionally, our study revealed disparities among Large\nLanguage Models (LLMs) regarding error type distributions and the level of\nassistance they provide to developers. Notably, larger LLMs do not invariably\nperform better. We also observed that tasks with higher degrees of coupling\npresent more substantial challenges, suggesting that the choice of LLM should\nbe tailored to the task's coupling degree. For example, GPT-4 exhibited\nimproved performance in low-coupling scenarios, whereas GPT-3.5 seemed better\nsuited for tasks with high coupling. By meticulously curating prompt content\nand selecting the appropriate LLM, developers can optimize code quality while\nmaximizing cost-efficiency during the development process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the area of code generation research, the emphasis has transitioned from\ncrafting individual functions to developing class-level method code that\nintegrates contextual information. This shift has brought several benchmarks\nsuch as ClassEval and CoderEval, which consider class-level contexts.\nNevertheless, the influence of specific contextual factors at the method level\nremains less explored.\n  This research focused on method-level code generation within the\nObject-Oriented Programming (OOP) framework. Based on CoderEval, we devised\nexperiments that varied the extent of contextual information in the prompts,\nranging from method-specific to project-wide details. We introduced the\ninnovative metric of \"Prompt-Token Cost-Effectiveness\" to evaluate the economic\nviability of incorporating additional contextual layers. Our findings indicate\nthat prompts enriched with method invocation details yield the highest\ncost-effectiveness. Additionally, our study revealed disparities among Large\nLanguage Models (LLMs) regarding error type distributions and the level of\nassistance they provide to developers. Notably, larger LLMs do not invariably\nperform better. We also observed that tasks with higher degrees of coupling\npresent more substantial challenges, suggesting that the choice of LLM should\nbe tailored to the task's coupling degree. For example, GPT-4 exhibited\nimproved performance in low-coupling scenarios, whereas GPT-3.5 seemed better\nsuited for tasks with high coupling. By meticulously curating prompt content\nand selecting the appropriate LLM, developers can optimize code quality while\nmaximizing cost-efficiency during the development process."
                },
                "authors": [
                    {
                        "name": "Zinan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zinan Wang"
                },
                "author": "Zinan Wang",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14830v1",
                "updated": "2024-08-27T07:27:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    27,
                    16,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T07:27:16Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    27,
                    16,
                    1,
                    240,
                    0
                ],
                "title": "PolicyLR: A Logic Representation For Privacy Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolicyLR: A Logic Representation For Privacy Policies"
                },
                "summary": "Privacy policies are crucial in the online ecosystem, defining how services\nhandle user data and adhere to regulations such as GDPR and CCPA. However,\ntheir complexity and frequent updates often make them difficult for\nstakeholders to understand and analyze. Current automated analysis methods,\nwhich utilize natural language processing, have limitations. They typically\nfocus on individual tasks and fail to capture the full context of the policies.\nWe propose PolicyLR, a new paradigm that offers a comprehensive\nmachine-readable representation of privacy policies, serving as an all-in-one\nsolution for multiple downstream tasks. PolicyLR converts privacy policies into\na machine-readable format using valuations of atomic formulae, allowing for\nformal definitions of tasks like compliance and consistency. We have developed\na compiler that transforms unstructured policy text into this format using\noff-the-shelf Large Language Models (LLMs). This compiler breaks down the\ntransformation task into a two-stage translation and entailment procedure. This\nprocedure considers the full context of the privacy policy to infer a complex\nformula, where each formula consists of simpler atomic formulae. The advantage\nof this model is that PolicyLR is interpretable by design and grounded in\nsegments of the privacy policy. We evaluated the compiler using ToS;DR, a\ncommunity-annotated privacy policy entailment dataset. Utilizing open-source\nLLMs, our compiler achieves precision and recall values of 0.91 and 0.88,\nrespectively. Finally, we demonstrate the utility of PolicyLR in three privacy\ntasks: Policy Compliance, Inconsistency Detection, and Privacy Comparison\nShopping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy policies are crucial in the online ecosystem, defining how services\nhandle user data and adhere to regulations such as GDPR and CCPA. However,\ntheir complexity and frequent updates often make them difficult for\nstakeholders to understand and analyze. Current automated analysis methods,\nwhich utilize natural language processing, have limitations. They typically\nfocus on individual tasks and fail to capture the full context of the policies.\nWe propose PolicyLR, a new paradigm that offers a comprehensive\nmachine-readable representation of privacy policies, serving as an all-in-one\nsolution for multiple downstream tasks. PolicyLR converts privacy policies into\na machine-readable format using valuations of atomic formulae, allowing for\nformal definitions of tasks like compliance and consistency. We have developed\na compiler that transforms unstructured policy text into this format using\noff-the-shelf Large Language Models (LLMs). This compiler breaks down the\ntransformation task into a two-stage translation and entailment procedure. This\nprocedure considers the full context of the privacy policy to infer a complex\nformula, where each formula consists of simpler atomic formulae. The advantage\nof this model is that PolicyLR is interpretable by design and grounded in\nsegments of the privacy policy. We evaluated the compiler using ToS;DR, a\ncommunity-annotated privacy policy entailment dataset. Utilizing open-source\nLLMs, our compiler achieves precision and recall values of 0.91 and 0.88,\nrespectively. Finally, we demonstrate the utility of PolicyLR in three privacy\ntasks: Policy Compliance, Inconsistency Detection, and Privacy Comparison\nShopping."
                },
                "authors": [
                    {
                        "name": "Ashish Hooda"
                    },
                    {
                        "name": "Rishabh Khandelwal"
                    },
                    {
                        "name": "Prasad Chalasani"
                    },
                    {
                        "name": "Kassem Fawaz"
                    },
                    {
                        "name": "Somesh Jha"
                    }
                ],
                "author_detail": {
                    "name": "Somesh Jha"
                },
                "author": "Somesh Jha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14826v1",
                "updated": "2024-08-27T07:13:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    13,
                    44,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T07:13:44Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    13,
                    44,
                    1,
                    240,
                    0
                ],
                "title": "Alfie: Democratising RGBA Image Generation With No $$$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alfie: Democratising RGBA Image Generation With No $$$"
                },
                "summary": "Designs and artworks are ubiquitous across various creative fields, requiring\ngraphic design skills and dedicated software to create compositions that\ninclude many graphical elements, such as logos, icons, symbols, and art scenes,\nwhich are integral to visual storytelling. Automating the generation of such\nvisual elements improves graphic designers' productivity, democratizes and\ninnovates the creative industry, and helps generate more realistic synthetic\ndata for related tasks. These illustration elements are mostly RGBA images with\nirregular shapes and cutouts, facilitating blending and scene composition.\nHowever, most image generation models are incapable of generating such images\nand achieving this capability requires expensive computational resources,\nspecific training recipes, or post-processing solutions. In this work, we\npropose a fully-automated approach for obtaining RGBA illustrations by\nmodifying the inference-time behavior of a pre-trained Diffusion Transformer\nmodel, exploiting the prompt-guided controllability and visual quality offered\nby such models with no additional computational cost. We force the generation\nof entire subjects without sharp croppings, whose background is easily removed\nfor seamless integration into design projects or artistic scenes. We show with\na user study that, in most cases, users prefer our solution over generating and\nthen matting an image, and we show that our generated illustrations yield good\nresults when used as inputs for composite scene generation pipelines. We\nrelease the code at https://github.com/aimagelab/Alfie.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designs and artworks are ubiquitous across various creative fields, requiring\ngraphic design skills and dedicated software to create compositions that\ninclude many graphical elements, such as logos, icons, symbols, and art scenes,\nwhich are integral to visual storytelling. Automating the generation of such\nvisual elements improves graphic designers' productivity, democratizes and\ninnovates the creative industry, and helps generate more realistic synthetic\ndata for related tasks. These illustration elements are mostly RGBA images with\nirregular shapes and cutouts, facilitating blending and scene composition.\nHowever, most image generation models are incapable of generating such images\nand achieving this capability requires expensive computational resources,\nspecific training recipes, or post-processing solutions. In this work, we\npropose a fully-automated approach for obtaining RGBA illustrations by\nmodifying the inference-time behavior of a pre-trained Diffusion Transformer\nmodel, exploiting the prompt-guided controllability and visual quality offered\nby such models with no additional computational cost. We force the generation\nof entire subjects without sharp croppings, whose background is easily removed\nfor seamless integration into design projects or artistic scenes. We show with\na user study that, in most cases, users prefer our solution over generating and\nthen matting an image, and we show that our generated illustrations yield good\nresults when used as inputs for composite scene generation pipelines. We\nrelease the code at https://github.com/aimagelab/Alfie."
                },
                "authors": [
                    {
                        "name": "Fabio Quattrini"
                    },
                    {
                        "name": "Vittorio Pippi"
                    },
                    {
                        "name": "Silvia Cascianelli"
                    },
                    {
                        "name": "Rita Cucchiara"
                    }
                ],
                "author_detail": {
                    "name": "Rita Cucchiara"
                },
                "author": "Rita Cucchiara",
                "arxiv_comment": "Accepted at ECCV AI for Visual Arts Workshop and Challenges",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13423v2",
                "updated": "2024-08-27T07:12:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    12,
                    52,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-24T01:33:28Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    1,
                    33,
                    28,
                    5,
                    237,
                    0
                ],
                "title": "Training-free Long Video Generation with Chain of Diffusion Model\n  Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free Long Video Generation with Chain of Diffusion Model\n  Experts"
                },
                "summary": "Video generation models hold substantial potential in areas such as\nfilmmaking. However, current video diffusion models need high computational\ncosts and produce suboptimal results due to high complexity of video generation\ntask. In this paper, we propose \\textbf{ConFiner}, an efficient high-quality\nvideo generation framework that decouples video generation into easier\nsubtasks: structure \\textbf{con}trol and spatial-temporal re\\textbf{fine}ment.\nIt can generate high-quality videos with chain of off-the-shelf diffusion model\nexperts, each expert responsible for a decoupled subtask. During the\nrefinement, we introduce coordinated denoising, which can merge multiple\ndiffusion experts' capabilities into a single sampling. Furthermore, we design\nConFiner-Long framework, which can generate long coherent video with three\nconstraint strategies on ConFiner. Experimental results indicate that with only\n10\\% of the inference cost, our ConFiner surpasses representative models like\nLavie and Modelscope across all objective and subjective metrics. And\nConFiner-Long can generate high-quality and coherent videos with up to 600\nframes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models hold substantial potential in areas such as\nfilmmaking. However, current video diffusion models need high computational\ncosts and produce suboptimal results due to high complexity of video generation\ntask. In this paper, we propose \\textbf{ConFiner}, an efficient high-quality\nvideo generation framework that decouples video generation into easier\nsubtasks: structure \\textbf{con}trol and spatial-temporal re\\textbf{fine}ment.\nIt can generate high-quality videos with chain of off-the-shelf diffusion model\nexperts, each expert responsible for a decoupled subtask. During the\nrefinement, we introduce coordinated denoising, which can merge multiple\ndiffusion experts' capabilities into a single sampling. Furthermore, we design\nConFiner-Long framework, which can generate long coherent video with three\nconstraint strategies on ConFiner. Experimental results indicate that with only\n10\\% of the inference cost, our ConFiner surpasses representative models like\nLavie and Modelscope across all objective and subjective metrics. And\nConFiner-Long can generate high-quality and coherent videos with up to 600\nframes."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yichao Cao"
                    },
                    {
                        "name": "Xiu Su"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Shan You"
                    },
                    {
                        "name": "Mingkai Zheng"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05885v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05885v2",
                "updated": "2024-08-27T06:53:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    53,
                    16,
                    1,
                    240,
                    0
                ],
                "published": "2024-06-09T18:45:41Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    18,
                    45,
                    41,
                    6,
                    161,
                    0
                ],
                "title": "Are Large Language Models Actually Good at Text Style Transfer?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Actually Good at Text Style Transfer?"
                },
                "summary": "We analyze the performance of large language models (LLMs) on Text Style\nTransfer (TST), specifically focusing on sentiment transfer and text\ndetoxification across three languages: English, Hindi, and Bengali. Text Style\nTransfer involves modifying the linguistic style of a text while preserving its\ncore content. We evaluate the capabilities of pre-trained LLMs using zero-shot\nand few-shot prompting as well as parameter-efficient finetuning on publicly\navailable datasets. Our evaluation using automatic metrics, GPT-4 and human\nevaluations reveals that while some prompted LLMs perform well in English,\ntheir performance in on other languages (Hindi, Bengali) remains average.\nHowever, finetuning significantly improves results compared to zero-shot and\nfew-shot prompting, making them comparable to previous state-of-the-art. This\nunderscores the necessity of dedicated datasets and specialized models for\neffective TST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze the performance of large language models (LLMs) on Text Style\nTransfer (TST), specifically focusing on sentiment transfer and text\ndetoxification across three languages: English, Hindi, and Bengali. Text Style\nTransfer involves modifying the linguistic style of a text while preserving its\ncore content. We evaluate the capabilities of pre-trained LLMs using zero-shot\nand few-shot prompting as well as parameter-efficient finetuning on publicly\navailable datasets. Our evaluation using automatic metrics, GPT-4 and human\nevaluations reveals that while some prompted LLMs perform well in English,\ntheir performance in on other languages (Hindi, Bengali) remains average.\nHowever, finetuning significantly improves results compared to zero-shot and\nfew-shot prompting, making them comparable to previous state-of-the-art. This\nunderscores the necessity of dedicated datasets and specialized models for\neffective TST."
                },
                "authors": [
                    {
                        "name": "Sourabrata Mukherjee"
                    },
                    {
                        "name": "Atul Kr. Ojha"
                    },
                    {
                        "name": "Ondřej Dušek"
                    }
                ],
                "author_detail": {
                    "name": "Ondřej Dušek"
                },
                "author": "Ondřej Dušek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05885v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05885v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20805v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20805v3",
                "updated": "2024-08-27T06:51:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    51,
                    0,
                    1,
                    240,
                    0
                ],
                "published": "2024-05-31T14:05:27Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    14,
                    5,
                    27,
                    4,
                    152,
                    0
                ],
                "title": "Multilingual Text Style Transfer: Datasets & Models for Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Text Style Transfer: Datasets & Models for Indian Languages"
                },
                "summary": "Text style transfer (TST) involves altering the linguistic style of a text\nwhile preserving its core content. This paper focuses on sentiment transfer, a\npopular TST subtask, across a spectrum of Indian languages: Hindi, Magahi,\nMalayalam, Marathi, Punjabi, Odia, Telugu, and Urdu, expanding upon previous\nwork on English-Bangla sentiment transfer (Mukherjee et al., 2023). We\nintroduce dedicated datasets of 1,000 positive and 1,000 negative\nstyle-parallel sentences for each of these eight languages. We then evaluate\nthe performance of various benchmark models categorized into parallel,\nnon-parallel, cross-lingual, and shared learning approaches, including the\nLlama2 and GPT-3.5 large language models (LLMs). Our experiments highlight the\nsignificance of parallel data in TST and demonstrate the effectiveness of the\nMasked Style Filling (MSF) approach (Mukherjee et al., 2023) in non-parallel\ntechniques. Moreover, cross-lingual and joint multilingual learning methods\nshow promise, offering insights into selecting optimal models tailored to the\nspecific language and task requirements. To the best of our knowledge, this\nwork represents the first comprehensive exploration of the TST task as\nsentiment transfer across a diverse set of languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text style transfer (TST) involves altering the linguistic style of a text\nwhile preserving its core content. This paper focuses on sentiment transfer, a\npopular TST subtask, across a spectrum of Indian languages: Hindi, Magahi,\nMalayalam, Marathi, Punjabi, Odia, Telugu, and Urdu, expanding upon previous\nwork on English-Bangla sentiment transfer (Mukherjee et al., 2023). We\nintroduce dedicated datasets of 1,000 positive and 1,000 negative\nstyle-parallel sentences for each of these eight languages. We then evaluate\nthe performance of various benchmark models categorized into parallel,\nnon-parallel, cross-lingual, and shared learning approaches, including the\nLlama2 and GPT-3.5 large language models (LLMs). Our experiments highlight the\nsignificance of parallel data in TST and demonstrate the effectiveness of the\nMasked Style Filling (MSF) approach (Mukherjee et al., 2023) in non-parallel\ntechniques. Moreover, cross-lingual and joint multilingual learning methods\nshow promise, offering insights into selecting optimal models tailored to the\nspecific language and task requirements. To the best of our knowledge, this\nwork represents the first comprehensive exploration of the TST task as\nsentiment transfer across a diverse set of languages."
                },
                "authors": [
                    {
                        "name": "Sourabrata Mukherjee"
                    },
                    {
                        "name": "Atul Kr. Ojha"
                    },
                    {
                        "name": "Akanksha Bansal"
                    },
                    {
                        "name": "Deepak Alok"
                    },
                    {
                        "name": "John P. McCrae"
                    },
                    {
                        "name": "Ondřej Dušek"
                    }
                ],
                "author_detail": {
                    "name": "Ondřej Dušek"
                },
                "author": "Ondřej Dušek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20805v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20805v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14812v1",
                "updated": "2024-08-27T06:50:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    50,
                    28,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T06:50:28Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    50,
                    28,
                    1,
                    240,
                    0
                ],
                "title": "HPT++: Hierarchically Prompting Vision-Language Models with\n  Multi-Granularity Knowledge Generation and Improved Structure Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPT++: Hierarchically Prompting Vision-Language Models with\n  Multi-Granularity Knowledge Generation and Improved Structure Modeling"
                },
                "summary": "Prompt learning has become a prevalent strategy for adapting vision-language\nfoundation models (VLMs) such as CLIP to downstream tasks. With the emergence\nof large language models (LLMs), recent studies have explored the potential of\nusing category-related descriptions to enhance prompt effectiveness. However,\nconventional descriptions lack explicit structured information necessary to\nrepresent the interconnections among key elements like entities or attributes\nwith relation to a particular category. Since existing prompt tuning methods\ngive little consideration to managing structured knowledge, this paper\nadvocates leveraging LLMs to construct a graph for each description to\nprioritize such structured knowledge. Consequently, we propose a novel approach\ncalled Hierarchical Prompt Tuning (HPT), enabling simultaneous modeling of both\nstructured and conventional linguistic knowledge. Specifically, we introduce a\nrelationship-guided attention module to capture pair-wise associations among\nentities and attributes for low-level prompt learning. In addition, by\nincorporating high-level and global-level prompts modeling overall semantics,\nthe proposed hierarchical structure forges cross-level interlinks and empowers\nthe model to handle more complex and long-term relationships. Finally, by\nenhancing multi-granularity knowledge generation, redesigning the\nrelationship-driven attention re-weighting module, and incorporating consistent\nconstraints on the hierarchical text encoder, we propose HPT++, which further\nimproves the performance of HPT. Our experiments are conducted across a wide\nrange of evaluation settings, including base-to-new generalization,\ncross-dataset evaluation, and domain generalization. Extensive results and\nablation studies demonstrate the effectiveness of our methods, which\nconsistently outperform existing SOTA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt learning has become a prevalent strategy for adapting vision-language\nfoundation models (VLMs) such as CLIP to downstream tasks. With the emergence\nof large language models (LLMs), recent studies have explored the potential of\nusing category-related descriptions to enhance prompt effectiveness. However,\nconventional descriptions lack explicit structured information necessary to\nrepresent the interconnections among key elements like entities or attributes\nwith relation to a particular category. Since existing prompt tuning methods\ngive little consideration to managing structured knowledge, this paper\nadvocates leveraging LLMs to construct a graph for each description to\nprioritize such structured knowledge. Consequently, we propose a novel approach\ncalled Hierarchical Prompt Tuning (HPT), enabling simultaneous modeling of both\nstructured and conventional linguistic knowledge. Specifically, we introduce a\nrelationship-guided attention module to capture pair-wise associations among\nentities and attributes for low-level prompt learning. In addition, by\nincorporating high-level and global-level prompts modeling overall semantics,\nthe proposed hierarchical structure forges cross-level interlinks and empowers\nthe model to handle more complex and long-term relationships. Finally, by\nenhancing multi-granularity knowledge generation, redesigning the\nrelationship-driven attention re-weighting module, and incorporating consistent\nconstraints on the hierarchical text encoder, we propose HPT++, which further\nimproves the performance of HPT. Our experiments are conducted across a wide\nrange of evaluation settings, including base-to-new generalization,\ncross-dataset evaluation, and domain generalization. Extensive results and\nablation studies demonstrate the effectiveness of our methods, which\nconsistently outperform existing SOTA methods."
                },
                "authors": [
                    {
                        "name": "Yubin Wang"
                    },
                    {
                        "name": "Xinyang Jiang"
                    },
                    {
                        "name": "De Cheng"
                    },
                    {
                        "name": "Wenli Sun"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Cairong Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Cairong Zhao"
                },
                "author": "Cairong Zhao",
                "arxiv_comment": "19 pages, 7 figures, 7 tables. arXiv admin note: substantial text\n  overlap with arXiv:2312.06323",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13976v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13976v2",
                "updated": "2024-08-27T06:49:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    49,
                    19,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-26T01:48:57Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    1,
                    48,
                    57,
                    0,
                    239,
                    0
                ],
                "title": "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking\n  the Generated Code Candidates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking\n  the Generated Code Candidates"
                },
                "summary": "Large Language Models (LLMs), such as GPT-4, StarCoder, and CodeLlama, are\ntransforming the way developers approach programming by automatically\ngenerating code based on given natural language descriptions. Despite\nadvancements, generating syntactically and semantically correct code remains\nchallenging, especially for complex programming tasks. Typically, individuals\ngenerate multiple candidate solutions using LLMs to increase the likelihood of\nproducing correct code. However, selecting the correct code from these\ncandidates-a process known as code ranking-remains a major challenge. Current\nresearch on code ranking can be categorized into execution-based and\nnon-execution-based methods. Execution-based methods, although effective,\nencounter notable limitations, such as scarcity of quality unit tests and\nsecurity risks. Non-execution-based methods like CodeRanker, which rely solely\non classification labels to train a code ranker, struggle to capture subtle\nerrors and provide detailed error insights. Recognizing the strengths and\nlimitations of both approaches, we propose a new method. The key insight of our\nwork is that an effective code ranker is expected to genuinely comprehend the\nunderlying causes of erroneous code, as relying solely on classification labels\nis insufficient. Inspired by this, this paper puts forward RankEF, an\ninnovative approach for code ranking that leverages execution feedback. RankEF\nemploys multi-task learning to integrate code classification with execution\nfeedback generation. This approach enables the model to understand the reasons\nbehind incorrect code, distinguishing between correct and incorrect solutions\nwithout the need to execute the code during the ranking phase. Experiments on\nthree code generation benchmarks demonstrate that RankEF significantly\noutperforms the state-of-the-art CodeRanker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT-4, StarCoder, and CodeLlama, are\ntransforming the way developers approach programming by automatically\ngenerating code based on given natural language descriptions. Despite\nadvancements, generating syntactically and semantically correct code remains\nchallenging, especially for complex programming tasks. Typically, individuals\ngenerate multiple candidate solutions using LLMs to increase the likelihood of\nproducing correct code. However, selecting the correct code from these\ncandidates-a process known as code ranking-remains a major challenge. Current\nresearch on code ranking can be categorized into execution-based and\nnon-execution-based methods. Execution-based methods, although effective,\nencounter notable limitations, such as scarcity of quality unit tests and\nsecurity risks. Non-execution-based methods like CodeRanker, which rely solely\non classification labels to train a code ranker, struggle to capture subtle\nerrors and provide detailed error insights. Recognizing the strengths and\nlimitations of both approaches, we propose a new method. The key insight of our\nwork is that an effective code ranker is expected to genuinely comprehend the\nunderlying causes of erroneous code, as relying solely on classification labels\nis insufficient. Inspired by this, this paper puts forward RankEF, an\ninnovative approach for code ranking that leverages execution feedback. RankEF\nemploys multi-task learning to integrate code classification with execution\nfeedback generation. This approach enables the model to understand the reasons\nbehind incorrect code, distinguishing between correct and incorrect solutions\nwithout the need to execute the code during the ranking phase. Experiments on\nthree code generation benchmarks demonstrate that RankEF significantly\noutperforms the state-of-the-art CodeRanker."
                },
                "authors": [
                    {
                        "name": "Zhihong Sun"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Chen Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Lyu"
                },
                "author": "Chen Lyu",
                "arxiv_comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13976v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13976v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08841v2",
                "updated": "2024-08-27T06:23:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    23,
                    45,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-16T17:00:11Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    0,
                    11,
                    4,
                    229,
                    0
                ],
                "title": "FLEXTAF: Enhancing Table Reasoning with Flexible Tabular Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLEXTAF: Enhancing Table Reasoning with Flexible Tabular Formats"
                },
                "summary": "The table reasoning task aims to answer the question according to the given\ntable. Currently, using Large Language Models (LLMs) is the predominant method\nfor table reasoning. Most existing methods employ a fixed tabular format to\nrepresent the table, which could limit the performance. Given that each\ninstance requires different capabilities and models possess varying abilities,\nwe assert that different instances and models suit different tabular formats.\nWe prove the aforementioned claim through quantitative analysis of experimental\nresults, where different instances and models achieve different performances\nusing various tabular formats. Building on this discussion, we propose\nFLEXTAF-Single and FLEXTAF-Vote to enhance table reasoning performance by\nemploying flexible tabular formats. Specifically, (i) FLEXTAF-Single trains a\nclassifier to predict the most suitable tabular format based on the instance\nand the LLM. (ii) FLEXTAF-Vote integrates the results across different formats.\nOur experiments on WikiTableQuestions and TabFact reveal significant\nimprovements, with average gains of 2.3% and 4.8% compared to the best\nperformance achieved using a fixed tabular format with greedy decoding and\nself-consistency decoding, thereby validating the effectiveness of our methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The table reasoning task aims to answer the question according to the given\ntable. Currently, using Large Language Models (LLMs) is the predominant method\nfor table reasoning. Most existing methods employ a fixed tabular format to\nrepresent the table, which could limit the performance. Given that each\ninstance requires different capabilities and models possess varying abilities,\nwe assert that different instances and models suit different tabular formats.\nWe prove the aforementioned claim through quantitative analysis of experimental\nresults, where different instances and models achieve different performances\nusing various tabular formats. Building on this discussion, we propose\nFLEXTAF-Single and FLEXTAF-Vote to enhance table reasoning performance by\nemploying flexible tabular formats. Specifically, (i) FLEXTAF-Single trains a\nclassifier to predict the most suitable tabular format based on the instance\nand the LLM. (ii) FLEXTAF-Vote integrates the results across different formats.\nOur experiments on WikiTableQuestions and TabFact reveal significant\nimprovements, with average gains of 2.3% and 4.8% compared to the best\nperformance achieved using a fixed tabular format with greedy decoding and\nself-consistency decoding, thereby validating the effectiveness of our methods."
                },
                "authors": [
                    {
                        "name": "Xuanliang Zhang"
                    },
                    {
                        "name": "Dingzirui Wang"
                    },
                    {
                        "name": "Longxu Dou"
                    },
                    {
                        "name": "Baoxin Wang"
                    },
                    {
                        "name": "Dayong Wu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14043v2",
                "updated": "2024-08-27T06:18:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    18,
                    5,
                    1,
                    240,
                    0
                ],
                "published": "2024-06-20T07:06:58Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    7,
                    6,
                    58,
                    3,
                    172,
                    0
                ],
                "title": "Taxonomy-Guided Zero-Shot Recommendations with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taxonomy-Guided Zero-Shot Recommendations with LLMs"
                },
                "summary": "With the emergence of large language models (LLMs) and their ability to\nperform a variety of tasks, their application in recommender systems (RecSys)\nhas shown promise. However, we are facing significant challenges when deploying\nLLMs into RecSys, such as limited prompt length, unstructured item information,\nand un-constrained generation of recommendations, leading to sub-optimal\nperformance. To address these issues, we propose a novel method using a\ntaxonomy dictionary. This method provides a systematic framework for\ncategorizing and organizing items, improving the clarity and structure of item\ninformation. By incorporating the taxonomy dictionary into LLM prompts, we\nachieve efficient token utilization and controlled feature generation, leading\nto more accurate and contextually relevant recommendations. Our Taxonomy-guided\nRecommendation (TaxRec) approach features a two-step process: one-time taxonomy\ncategorization and LLM-based recommendation, enabling zero-shot recommendations\nwithout the need for domain-specific fine-tuning. Experimental results\ndemonstrate TaxRec significantly enhances recommendation quality compared to\ntraditional zero-shot approaches, showcasing its efficacy as personal\nrecommender with LLMs. Code is available at\nhttps://github.com/yueqingliang1/TaxRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of large language models (LLMs) and their ability to\nperform a variety of tasks, their application in recommender systems (RecSys)\nhas shown promise. However, we are facing significant challenges when deploying\nLLMs into RecSys, such as limited prompt length, unstructured item information,\nand un-constrained generation of recommendations, leading to sub-optimal\nperformance. To address these issues, we propose a novel method using a\ntaxonomy dictionary. This method provides a systematic framework for\ncategorizing and organizing items, improving the clarity and structure of item\ninformation. By incorporating the taxonomy dictionary into LLM prompts, we\nachieve efficient token utilization and controlled feature generation, leading\nto more accurate and contextually relevant recommendations. Our Taxonomy-guided\nRecommendation (TaxRec) approach features a two-step process: one-time taxonomy\ncategorization and LLM-based recommendation, enabling zero-shot recommendations\nwithout the need for domain-specific fine-tuning. Experimental results\ndemonstrate TaxRec significantly enhances recommendation quality compared to\ntraditional zero-shot approaches, showcasing its efficacy as personal\nrecommender with LLMs. Code is available at\nhttps://github.com/yueqingliang1/TaxRec."
                },
                "authors": [
                    {
                        "name": "Yueqing Liang"
                    },
                    {
                        "name": "Liangwei Yang"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Xiongxiao Xu"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Kai Shu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Shu"
                },
                "author": "Kai Shu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08779v2",
                "updated": "2024-08-27T06:14:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    14,
                    54,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-16T14:43:15Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    43,
                    15,
                    4,
                    229,
                    0
                ],
                "title": "DAC: Decomposed Automation Correction for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAC: Decomposed Automation Correction for Text-to-SQL"
                },
                "summary": "Text-to-SQL is an important task that helps people obtain information from\ndatabases by automatically generating SQL queries. Considering the brilliant\nperformance, approaches based on Large Language Models (LLMs) become the\nmainstream for text-to-SQL. Among these approaches, automated correction is an\neffective approach that further enhances performance by correcting the mistakes\nin the generated results. The existing correction methods require LLMs to\ndirectly correct with generated SQL, while previous research shows that LLMs do\nnot know how to detect mistakes, leading to poor performance. Therefore, in\nthis paper, we propose to employ the decomposed correction to enhance\ntext-to-SQL performance. We first demonstrate that decomposed correction\noutperforms direct correction since detecting and fixing mistakes with the\nresults of the decomposed sub-tasks is easier than with SQL. Based on this\nanalysis, we introduce Decomposed Automation Correction (DAC), which corrects\nSQL by decomposing text-to-SQL into entity linking and skeleton parsing. DAC\nfirst generates the entity and skeleton corresponding to the question and then\ncompares the differences between the initial SQL and the generated entities and\nskeleton as feedback for correction. Experimental results show that our method\nimproves performance by $3.7\\%$ on average of Spider, Bird, and KaggleDBQA\ncompared with the baseline method, demonstrating the effectiveness of DAC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is an important task that helps people obtain information from\ndatabases by automatically generating SQL queries. Considering the brilliant\nperformance, approaches based on Large Language Models (LLMs) become the\nmainstream for text-to-SQL. Among these approaches, automated correction is an\neffective approach that further enhances performance by correcting the mistakes\nin the generated results. The existing correction methods require LLMs to\ndirectly correct with generated SQL, while previous research shows that LLMs do\nnot know how to detect mistakes, leading to poor performance. Therefore, in\nthis paper, we propose to employ the decomposed correction to enhance\ntext-to-SQL performance. We first demonstrate that decomposed correction\noutperforms direct correction since detecting and fixing mistakes with the\nresults of the decomposed sub-tasks is easier than with SQL. Based on this\nanalysis, we introduce Decomposed Automation Correction (DAC), which corrects\nSQL by decomposing text-to-SQL into entity linking and skeleton parsing. DAC\nfirst generates the entity and skeleton corresponding to the question and then\ncompares the differences between the initial SQL and the generated entities and\nskeleton as feedback for correction. Experimental results show that our method\nimproves performance by $3.7\\%$ on average of Spider, Bird, and KaggleDBQA\ncompared with the baseline method, demonstrating the effectiveness of DAC."
                },
                "authors": [
                    {
                        "name": "Dingzirui Wang"
                    },
                    {
                        "name": "Longxu Dou"
                    },
                    {
                        "name": "Xuanliang Zhang"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12775v2",
                "updated": "2024-08-27T06:02:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    2,
                    55,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-23T00:49:36Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    49,
                    36,
                    4,
                    236,
                    0
                ],
                "title": "Intelligent OPC Engineer Assistant for Semiconductor Manufacturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent OPC Engineer Assistant for Semiconductor Manufacturing"
                },
                "summary": "Advancements in chip design and manufacturing have enabled the processing of\ncomplex tasks such as deep learning and natural language processing, paving the\nway for the development of artificial general intelligence (AGI). AI, on the\nother hand, can be leveraged to innovate and streamline semiconductor\ntechnology from planning and implementation to manufacturing. In this paper, we\npresent \\textit{Intelligent OPC Engineer Assistant}, an AI/LLM-powered\nmethodology designed to solve the core manufacturing-aware optimization problem\nknown as optical proximity correction (OPC). The methodology involves a\nreinforcement learning-based OPC recipe search and a customized multi-modal\nagent system for recipe summarization. Experiments demonstrate that our\nmethodology can efficiently build OPC recipes on various chip designs with\nspecially handled design topologies, a task that typically requires the\nfull-time effort of OPC engineers with years of experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in chip design and manufacturing have enabled the processing of\ncomplex tasks such as deep learning and natural language processing, paving the\nway for the development of artificial general intelligence (AGI). AI, on the\nother hand, can be leveraged to innovate and streamline semiconductor\ntechnology from planning and implementation to manufacturing. In this paper, we\npresent \\textit{Intelligent OPC Engineer Assistant}, an AI/LLM-powered\nmethodology designed to solve the core manufacturing-aware optimization problem\nknown as optical proximity correction (OPC). The methodology involves a\nreinforcement learning-based OPC recipe search and a customized multi-modal\nagent system for recipe summarization. Experiments demonstrate that our\nmethodology can efficiently build OPC recipes on various chip designs with\nspecially handled design topologies, a task that typically requires the\nfull-time effort of OPC engineers with years of experience."
                },
                "authors": [
                    {
                        "name": "Guojin Chen"
                    },
                    {
                        "name": "Haoyu Yang"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Haoxing Ren"
                    }
                ],
                "author_detail": {
                    "name": "Haoxing Ren"
                },
                "author": "Haoxing Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02869v2",
                "updated": "2024-08-27T04:53:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    4,
                    53,
                    11,
                    1,
                    240,
                    0
                ],
                "published": "2024-06-05T02:30:55Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    2,
                    30,
                    55,
                    2,
                    157,
                    0
                ],
                "title": "Mesoscopic Bayesian Inference by Solvable Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mesoscopic Bayesian Inference by Solvable Models"
                },
                "summary": "The rapid advancement of data science and artificial intelligence has\naffected physics in numerous ways, including the application of Bayesian\ninference, setting the stage for a revolution in research methodology. Our\ngroup has proposed Bayesian measurement, a framework that applies Bayesian\ninference to measurement science with broad applicability across various\nnatural sciences. This framework enables the determination of posterior\nprobability distributions of system parameters, model selection, and the\nintegration of multiple measurement datasets. However, applying Bayesian\nmeasurement to real data analysis requires a more sophisticated approach than\ntraditional statistical methods like Akaike information criterion (AIC) and\nBayesian information criterion (BIC), which are designed for an infinite number\nof measurements $N$. Therefore, in this paper, we propose an analytical theory\nthat explicitly addresses the case where $N$ is finite in the linear regression\nmodel. We introduce $O(1)$ mesoscopic variables for $N$ observation noises.\nUsing this mesoscopic theory, we analyze the three core principles of Bayesian\nmeasurement: parameter estimation, model selection, and measurement\nintegration. Furthermore, by introducing these mesoscopic variables, we\ndemonstrate that the difference in free energies, critical for both model\nselection and measurement integration, can be analytically reduced by two\nmesoscopic variables of $N$ observation noises. This provides a deeper\nqualitative understanding of model selection and measurement integration and\nfurther provides deeper insights into actual measurements for nonlinear models.\nOur framework presents a novel approach to understanding Bayesian measurement\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of data science and artificial intelligence has\naffected physics in numerous ways, including the application of Bayesian\ninference, setting the stage for a revolution in research methodology. Our\ngroup has proposed Bayesian measurement, a framework that applies Bayesian\ninference to measurement science with broad applicability across various\nnatural sciences. This framework enables the determination of posterior\nprobability distributions of system parameters, model selection, and the\nintegration of multiple measurement datasets. However, applying Bayesian\nmeasurement to real data analysis requires a more sophisticated approach than\ntraditional statistical methods like Akaike information criterion (AIC) and\nBayesian information criterion (BIC), which are designed for an infinite number\nof measurements $N$. Therefore, in this paper, we propose an analytical theory\nthat explicitly addresses the case where $N$ is finite in the linear regression\nmodel. We introduce $O(1)$ mesoscopic variables for $N$ observation noises.\nUsing this mesoscopic theory, we analyze the three core principles of Bayesian\nmeasurement: parameter estimation, model selection, and measurement\nintegration. Furthermore, by introducing these mesoscopic variables, we\ndemonstrate that the difference in free energies, critical for both model\nselection and measurement integration, can be analytically reduced by two\nmesoscopic variables of $N$ observation noises. This provides a deeper\nqualitative understanding of model selection and measurement integration and\nfurther provides deeper insights into actual measurements for nonlinear models.\nOur framework presents a novel approach to understanding Bayesian measurement\nresults."
                },
                "authors": [
                    {
                        "name": "Shun Katakami"
                    },
                    {
                        "name": "Shuhei Kashiwamura"
                    },
                    {
                        "name": "Kenji Nagata"
                    },
                    {
                        "name": "Masaichiro Mizumaki"
                    },
                    {
                        "name": "Masato Okada"
                    }
                ],
                "author_detail": {
                    "name": "Masato Okada"
                },
                "author": "Masato Okada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08072v2",
                "updated": "2024-08-27T04:50:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    4,
                    50,
                    12,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-15T10:44:38Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    44,
                    38,
                    3,
                    228,
                    0
                ],
                "title": "I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative\n  Self-Enhancement Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative\n  Self-Enhancement Paradigm"
                },
                "summary": "Large Language Models (LLMs) have achieved significant advancements, however,\nthe common learning paradigm treats LLMs as passive information repositories,\nneglecting their potential for active learning and alignment. Some approaches\ntrain LLMs using their own generated synthetic data, exploring the possibility\nof active alignment. However, there is still a huge gap between these one-time\nalignment methods and the continuous automatic alignment of humans. In this\npaper, we introduce \\textbf{I-SHEEP}, an \\textbf{I}terative\n\\textbf{S}elf-En\\textbf{H}anc\\textbf{E}m\\textbf{E}nt \\textbf{P}aradigm.This\nhuman-like paradigm enables LLMs to \\textbf{continuously self-align from\nscratch with nothing}. Compared to the one-time alignment method Dromedary\n\\cite{sun2023principledriven}, which refers to the first iteration in this\npaper, I-SHEEP can significantly enhance capacities on both Qwen and Llama\nmodels. I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca\nEval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval\naccuracy over subsequent iterations in Qwen-1.5 72B model. Additionally,\nI-SHEEP surpasses the base model in various standard benchmark generation\ntasks, achieving an average improvement of 24.77\\% in code generation tasks,\n12.04\\% in TrivialQA, and 20.29\\% in SQuAD. We also provide new insights based\non the experiment results. Our codes, datasets, and models are available at\n\\textbf{https://anonymous.4open.science/r/I-SHEEP}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant advancements, however,\nthe common learning paradigm treats LLMs as passive information repositories,\nneglecting their potential for active learning and alignment. Some approaches\ntrain LLMs using their own generated synthetic data, exploring the possibility\nof active alignment. However, there is still a huge gap between these one-time\nalignment methods and the continuous automatic alignment of humans. In this\npaper, we introduce \\textbf{I-SHEEP}, an \\textbf{I}terative\n\\textbf{S}elf-En\\textbf{H}anc\\textbf{E}m\\textbf{E}nt \\textbf{P}aradigm.This\nhuman-like paradigm enables LLMs to \\textbf{continuously self-align from\nscratch with nothing}. Compared to the one-time alignment method Dromedary\n\\cite{sun2023principledriven}, which refers to the first iteration in this\npaper, I-SHEEP can significantly enhance capacities on both Qwen and Llama\nmodels. I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca\nEval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval\naccuracy over subsequent iterations in Qwen-1.5 72B model. Additionally,\nI-SHEEP surpasses the base model in various standard benchmark generation\ntasks, achieving an average improvement of 24.77\\% in code generation tasks,\n12.04\\% in TrivialQA, and 20.29\\% in SQuAD. We also provide new insights based\non the experiment results. Our codes, datasets, and models are available at\n\\textbf{https://anonymous.4open.science/r/I-SHEEP}."
                },
                "authors": [
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Jiawei Guo"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Zhenzhu Yang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14776v1",
                "updated": "2024-08-27T04:45:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    4,
                    45,
                    53,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T04:45:53Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    4,
                    45,
                    53,
                    1,
                    240,
                    0
                ],
                "title": "MROVSeg: Breaking the Resolution Curse of Vision-Language Models in\n  Open-Vocabulary Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MROVSeg: Breaking the Resolution Curse of Vision-Language Models in\n  Open-Vocabulary Semantic Segmentation"
                },
                "summary": "Open-vocabulary semantic segmentation aims to segment and recognize\nsemantically meaningful regions based on text-based descriptions during\ninference. A typical solution to address this task is to leverage powerful\nvision-language models (VLMs), such as CLIP, to bridge the gap between open-\nand close-vocabulary recognition. As VLMs are usually pretrained with\nlow-resolution images (e.g. $224\\times224$), most previous methods operate only\non downscaled images. We question this design as low resolution features often\nfail to preserve fine details. Although employing additional image backbones\nfor high-resolution inputs can mitigate this issue, it may also introduce\nsignificant computation overhead. Therefore, we propose MROVSeg, a\nmulti-resolution training framework for open-vocabulary semantic segmentation\nwith a single pretrained CLIP backbone, that uses sliding windows to slice the\nhigh-resolution input into uniform patches, each matching the input size of the\nwell-trained image encoder. Its key components include a Multi-Res Adapter,\nwhich restores the spatial geometry and grasps local-global correspondences\nacross patches by learnable convolutional and scale attention layers. To\nachieve accurate segmentation, we introduce Multi-grained Masked Attention\nscheme to aggregate multi-grained semantics by performing cross-attention\nbetween object queries and multi-resolution CLIP features within the region of\ninterests. Through comprehensive experiments, we demonstrate the superiority of\nMROVSeg on well-established open-vocabulary semantic segmentation benchmarks,\nparticularly for high-resolution inputs, establishing new standards for\nopen-vocabulary semantic segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-vocabulary semantic segmentation aims to segment and recognize\nsemantically meaningful regions based on text-based descriptions during\ninference. A typical solution to address this task is to leverage powerful\nvision-language models (VLMs), such as CLIP, to bridge the gap between open-\nand close-vocabulary recognition. As VLMs are usually pretrained with\nlow-resolution images (e.g. $224\\times224$), most previous methods operate only\non downscaled images. We question this design as low resolution features often\nfail to preserve fine details. Although employing additional image backbones\nfor high-resolution inputs can mitigate this issue, it may also introduce\nsignificant computation overhead. Therefore, we propose MROVSeg, a\nmulti-resolution training framework for open-vocabulary semantic segmentation\nwith a single pretrained CLIP backbone, that uses sliding windows to slice the\nhigh-resolution input into uniform patches, each matching the input size of the\nwell-trained image encoder. Its key components include a Multi-Res Adapter,\nwhich restores the spatial geometry and grasps local-global correspondences\nacross patches by learnable convolutional and scale attention layers. To\nachieve accurate segmentation, we introduce Multi-grained Masked Attention\nscheme to aggregate multi-grained semantics by performing cross-attention\nbetween object queries and multi-resolution CLIP features within the region of\ninterests. Through comprehensive experiments, we demonstrate the superiority of\nMROVSeg on well-established open-vocabulary semantic segmentation benchmarks,\nparticularly for high-resolution inputs, establishing new standards for\nopen-vocabulary semantic segmentation."
                },
                "authors": [
                    {
                        "name": "Yuanbing Zhu"
                    },
                    {
                        "name": "Bingke Zhu"
                    },
                    {
                        "name": "Zhen Chen"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Ming Tang"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06537v2",
                "updated": "2024-08-27T04:43:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    4,
                    43,
                    59,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-09T04:17:39Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    4,
                    17,
                    39,
                    1,
                    191,
                    0
                ],
                "title": "Efficient and Accurate Memorable Conversation Model using DPO based on\n  sLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Accurate Memorable Conversation Model using DPO based on\n  sLLM"
                },
                "summary": "In multi-session dialog system, it is essential to continuously update the\nmemory as the session progresses. Simply accumulating memory can make it\ndifficult to focus on the content of the conversation for inference due to the\nlimited input sentence size. Therefore, efficient and accurate conversation\nmodel that is capable of managing memory to reflect the conversation history\ncontinuously is necessary. This paper presents a conversation model that\nefficiently manages memory as sessions progress and incorporates this into the\nmodel to reflect the conversation history accurately with 3 methodologies: SFT,\nDPO and DPO with SFT model. Our model using DPO algorithm shows an improvement\nabout 0.0591 of BERTScore in memory accuracy, and the rate of responses\nreflecting the memory increased as well. Also, response generation performance\nenhanced about 4.292 in fluency, 3.935 in coherence, and 2.896 in consistency.\nThis paper describes a training method that yields better performance than\nmodels with more than twice the parameter size, even when the model size is\nsmaller. Thus, our model demonstrates efficiency not only in terms of accuracy\nbut also in resource utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-session dialog system, it is essential to continuously update the\nmemory as the session progresses. Simply accumulating memory can make it\ndifficult to focus on the content of the conversation for inference due to the\nlimited input sentence size. Therefore, efficient and accurate conversation\nmodel that is capable of managing memory to reflect the conversation history\ncontinuously is necessary. This paper presents a conversation model that\nefficiently manages memory as sessions progress and incorporates this into the\nmodel to reflect the conversation history accurately with 3 methodologies: SFT,\nDPO and DPO with SFT model. Our model using DPO algorithm shows an improvement\nabout 0.0591 of BERTScore in memory accuracy, and the rate of responses\nreflecting the memory increased as well. Also, response generation performance\nenhanced about 4.292 in fluency, 3.935 in coherence, and 2.896 in consistency.\nThis paper describes a training method that yields better performance than\nmodels with more than twice the parameter size, even when the model size is\nsmaller. Thus, our model demonstrates efficiency not only in terms of accuracy\nbut also in resource utilization."
                },
                "authors": [
                    {
                        "name": "Youngkyung Seo"
                    },
                    {
                        "name": "Yoonseok Heo"
                    },
                    {
                        "name": "Jun-Seok Koh"
                    },
                    {
                        "name": "Du-Seong Chang"
                    }
                ],
                "author_detail": {
                    "name": "Du-Seong Chang"
                },
                "author": "Du-Seong Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04814v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04814v4",
                "updated": "2024-08-27T04:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    4,
                    43,
                    10,
                    1,
                    240,
                    0
                ],
                "published": "2024-04-07T05:47:41Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    5,
                    47,
                    41,
                    6,
                    98,
                    0
                ],
                "title": "Inference-Time Rule Eraser: Fair Recognition via Distilling and Removing\n  Biased Rules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Rule Eraser: Fair Recognition via Distilling and Removing\n  Biased Rules"
                },
                "summary": "Machine learning models often make predictions based on biased features such\nas gender, race, and other social attributes, posing significant fairness\nrisks, especially in societal applications, such as hiring, banking, and\ncriminal justice. Traditional approaches to addressing this issue involve\nretraining or fine-tuning neural networks with fairness-aware optimization\nobjectives. However, these methods can be impractical due to significant\ncomputational resources, complex industrial tests, and the associated CO2\nfootprint. Additionally, regular users often fail to fine-tune models because\nthey lack access to model parameters In this paper, we introduce the\nInference-Time Rule Eraser (Eraser), a novel method designed to address\nfairness concerns by removing biased decision-making rules from deployed models\nduring inference without altering model weights. We begin by establishing a\ntheoretical foundation for modifying model outputs to eliminate biased rules\nthrough Bayesian analysis. Next, we present a specific implementation of Eraser\nthat involves two stages: (1) distilling the biased rules from the deployed\nmodel into an additional patch model, and (2) removing these biased rules from\nthe output of the deployed model during inference. Extensive experiments\nvalidate the effectiveness of our approach, showcasing its superior performance\nin addressing fairness concerns in AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models often make predictions based on biased features such\nas gender, race, and other social attributes, posing significant fairness\nrisks, especially in societal applications, such as hiring, banking, and\ncriminal justice. Traditional approaches to addressing this issue involve\nretraining or fine-tuning neural networks with fairness-aware optimization\nobjectives. However, these methods can be impractical due to significant\ncomputational resources, complex industrial tests, and the associated CO2\nfootprint. Additionally, regular users often fail to fine-tune models because\nthey lack access to model parameters In this paper, we introduce the\nInference-Time Rule Eraser (Eraser), a novel method designed to address\nfairness concerns by removing biased decision-making rules from deployed models\nduring inference without altering model weights. We begin by establishing a\ntheoretical foundation for modifying model outputs to eliminate biased rules\nthrough Bayesian analysis. Next, we present a specific implementation of Eraser\nthat involves two stages: (1) distilling the biased rules from the deployed\nmodel into an additional patch model, and (2) removing these biased rules from\nthe output of the deployed model during inference. Extensive experiments\nvalidate the effectiveness of our approach, showcasing its superior performance\nin addressing fairness concerns in AI systems."
                },
                "authors": [
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Dongyuan Lu"
                    },
                    {
                        "name": "Jitao Sang"
                    }
                ],
                "author_detail": {
                    "name": "Jitao Sang"
                },
                "author": "Jitao Sang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04814v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04814v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15773v2",
                "updated": "2024-08-27T04:41:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    4,
                    41,
                    40,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-22T16:25:41Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    16,
                    25,
                    41,
                    0,
                    204,
                    0
                ],
                "title": "STAMP: Outlier-Aware Test-Time Adaptation with Stable Memory Replay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAMP: Outlier-Aware Test-Time Adaptation with Stable Memory Replay"
                },
                "summary": "Test-time adaptation (TTA) aims to address the distribution shift between the\ntraining and test data with only unlabeled data at test time. Existing TTA\nmethods often focus on improving recognition performance specifically for test\ndata associated with classes in the training set. However, during the\nopen-world inference process, there are inevitably test data instances from\nunknown classes, commonly referred to as outliers. This paper pays attention to\nthe problem that conducts both sample recognition and outlier rejection during\ninference while outliers exist. To address this problem, we propose a new\napproach called STAble Memory rePlay (STAMP), which performs optimization over\na stable memory bank instead of the risky mini-batch. In particular, the memory\nbank is dynamically updated by selecting low-entropy and label-consistent\nsamples in a class-balanced manner. In addition, we develop a self-weighted\nentropy minimization strategy that assigns higher weight to low-entropy\nsamples. Extensive results demonstrate that STAMP outperforms existing TTA\nmethods in terms of both recognition and outlier detection performance. The\ncode is released at https://github.com/yuyongcan/STAMP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) aims to address the distribution shift between the\ntraining and test data with only unlabeled data at test time. Existing TTA\nmethods often focus on improving recognition performance specifically for test\ndata associated with classes in the training set. However, during the\nopen-world inference process, there are inevitably test data instances from\nunknown classes, commonly referred to as outliers. This paper pays attention to\nthe problem that conducts both sample recognition and outlier rejection during\ninference while outliers exist. To address this problem, we propose a new\napproach called STAble Memory rePlay (STAMP), which performs optimization over\na stable memory bank instead of the risky mini-batch. In particular, the memory\nbank is dynamically updated by selecting low-entropy and label-consistent\nsamples in a class-balanced manner. In addition, we develop a self-weighted\nentropy minimization strategy that assigns higher weight to low-entropy\nsamples. Extensive results demonstrate that STAMP outperforms existing TTA\nmethods in terms of both recognition and outlier detection performance. The\ncode is released at https://github.com/yuyongcan/STAMP."
                },
                "authors": [
                    {
                        "name": "Yongcan Yu"
                    },
                    {
                        "name": "Lijun Sheng"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Jian Liang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Liang"
                },
                "author": "Jian Liang",
                "arxiv_comment": "Accepted by ECCV 2024; Fixed a bug in calculating OOD score of STAMP\n  and updated the results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14774v1",
                "updated": "2024-08-27T04:31:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    4,
                    31,
                    58,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T04:31:58Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    4,
                    31,
                    58,
                    1,
                    240,
                    0
                ],
                "title": "Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning"
                },
                "summary": "We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just $4$K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in $20\\%$ of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just $4$K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in $20\\%$ of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings."
                },
                "authors": [
                    {
                        "name": "Simran Kaur"
                    },
                    {
                        "name": "Simon Park"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Sanjeev Arora"
                    }
                ],
                "author_detail": {
                    "name": "Sanjeev Arora"
                },
                "author": "Sanjeev Arora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14766v1",
                "updated": "2024-08-27T03:56:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    3,
                    56,
                    33,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T03:56:33Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    3,
                    56,
                    33,
                    1,
                    240,
                    0
                ],
                "title": "Differentially Private Estimation of Weighted Average Treatment Effects\n  for Binary Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Estimation of Weighted Average Treatment Effects\n  for Binary Outcomes"
                },
                "summary": "In the social and health sciences, researchers often make causal inferences\nusing sensitive variables. These researchers, as well as the data holders\nthemselves, may be ethically and perhaps legally obligated to protect the\nconfidentiality of study participants' data. It is now known that releasing any\nstatistics, including estimates of causal effects, computed with confidential\ndata leaks information about the underlying data values. Thus, analysts may\ndesire to use causal estimators that can provably bound this information\nleakage. Motivated by this goal, we develop algorithms for estimating weighted\naverage treatment effects with binary outcomes that satisfy the criterion of\ndifferential privacy. We present theoretical results on the accuracy of several\ndifferentially private estimators of weighted average treatment effects. We\nillustrate the empirical performance of these estimators using simulated data\nand a causal analysis using data on education and income.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the social and health sciences, researchers often make causal inferences\nusing sensitive variables. These researchers, as well as the data holders\nthemselves, may be ethically and perhaps legally obligated to protect the\nconfidentiality of study participants' data. It is now known that releasing any\nstatistics, including estimates of causal effects, computed with confidential\ndata leaks information about the underlying data values. Thus, analysts may\ndesire to use causal estimators that can provably bound this information\nleakage. Motivated by this goal, we develop algorithms for estimating weighted\naverage treatment effects with binary outcomes that satisfy the criterion of\ndifferential privacy. We present theoretical results on the accuracy of several\ndifferentially private estimators of weighted average treatment effects. We\nillustrate the empirical performance of these estimators using simulated data\nand a causal analysis using data on education and income."
                },
                "authors": [
                    {
                        "name": "Sharmistha Guha"
                    },
                    {
                        "name": "Jerome P. Reiter"
                    }
                ],
                "author_detail": {
                    "name": "Jerome P. Reiter"
                },
                "author": "Jerome P. Reiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14470v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14470v2",
                "updated": "2024-08-27T03:56:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    3,
                    56,
                    11,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-26T17:58:53Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    58,
                    53,
                    0,
                    239,
                    0
                ],
                "title": "Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large\n  Language Models"
                },
                "summary": "Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. A class of parameter-efficient fine-tuning\n(PEFT) aims to mitigate these computational challenges by selectively\nfine-tuning only a small fraction of the model parameters. Although\ncomputationally efficient, these techniques often fail to match the performance\nof fully fine-tuned models, primarily due to inherent biases introduced during\nparameter selection. Traditional selective PEFT techniques use a fixed set of\nparameters based on a predefined budget (a process also known as unmasking),\nfailing to capture parameter importance dynamically and often ending up\nexceeding the budget. We introduce $\\text{ID}^3$, a novel selective PEFT method\nthat calculates parameter importance continually and dynamically unmasks\nparameters by balancing exploration and exploitation in parameter selection.\nOur empirical study on 15 tasks spanning natural language understanding and\ngenerative tasks demonstrates the effectiveness of our method compared to\nfixed-masking-based PEFT techniques. We analytically show that $\\text{ID}^3$\nreduces the number of gradient updates by a factor of two, enhancing\ncomputational efficiency. $\\text{ID}^3$ is robust to random initialization of\nneurons and, therefore, can be seamlessly integrated into existing additive and\nreparametrization-based PEFT modules such as adapters and LoRA for dynamic\nsparsification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. A class of parameter-efficient fine-tuning\n(PEFT) aims to mitigate these computational challenges by selectively\nfine-tuning only a small fraction of the model parameters. Although\ncomputationally efficient, these techniques often fail to match the performance\nof fully fine-tuned models, primarily due to inherent biases introduced during\nparameter selection. Traditional selective PEFT techniques use a fixed set of\nparameters based on a predefined budget (a process also known as unmasking),\nfailing to capture parameter importance dynamically and often ending up\nexceeding the budget. We introduce $\\text{ID}^3$, a novel selective PEFT method\nthat calculates parameter importance continually and dynamically unmasks\nparameters by balancing exploration and exploitation in parameter selection.\nOur empirical study on 15 tasks spanning natural language understanding and\ngenerative tasks demonstrates the effectiveness of our method compared to\nfixed-masking-based PEFT techniques. We analytically show that $\\text{ID}^3$\nreduces the number of gradient updates by a factor of two, enhancing\ncomputational efficiency. $\\text{ID}^3$ is robust to random initialization of\nneurons and, therefore, can be seamlessly integrated into existing additive and\nreparametrization-based PEFT modules such as adapters and LoRA for dynamic\nsparsification."
                },
                "authors": [
                    {
                        "name": "Aradhye Agarwal"
                    },
                    {
                        "name": "Suhas K Ramesh"
                    },
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "15 pages, 7 tables, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14470v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14470v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14077v2",
                "updated": "2024-08-27T03:36:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    3,
                    36,
                    54,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-26T08:00:47Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    8,
                    0,
                    47,
                    0,
                    239,
                    0
                ],
                "title": "The effect of vorticity on the dynamical magnetic fields in heavy-ion\n  collisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effect of vorticity on the dynamical magnetic fields in heavy-ion\n  collisions"
                },
                "summary": "Magnetic fields in heavy-ion collisions are pivotal and subject to diverse\nfactors. In this study, we quantitatively investigate the impact of fluid\nvorticity on the evolution of magnetic fields in the 20-50\\% centrality class\nin Au+Au collisions, with collision energies of $\\sqrt{s_{NN}}=(7.7, 14.5,\n19.6, 27, 39, 62.4, 200)$ GeV. Our results indicate that fluid vorticity leads\nto a delay in the evolution of the magnetic field, in which this effect becomes\nmore pronounced as the collision energy decreases. Additionally, we have\ncalculated the mean magnetic field values on the freeze-out hypersurface for\nvarious collision energies. Our simulation results align with the values\ninferred from experimental data of $\\bar{\\Lambda}-\\Lambda$, within the error\nmargins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic fields in heavy-ion collisions are pivotal and subject to diverse\nfactors. In this study, we quantitatively investigate the impact of fluid\nvorticity on the evolution of magnetic fields in the 20-50\\% centrality class\nin Au+Au collisions, with collision energies of $\\sqrt{s_{NN}}=(7.7, 14.5,\n19.6, 27, 39, 62.4, 200)$ GeV. Our results indicate that fluid vorticity leads\nto a delay in the evolution of the magnetic field, in which this effect becomes\nmore pronounced as the collision energy decreases. Additionally, we have\ncalculated the mean magnetic field values on the freeze-out hypersurface for\nvarious collision energies. Our simulation results align with the values\ninferred from experimental data of $\\bar{\\Lambda}-\\Lambda$, within the error\nmargins."
                },
                "authors": [
                    {
                        "name": "Anping Huang"
                    },
                    {
                        "name": "Xiang-Yu Wu"
                    },
                    {
                        "name": "Mei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Mei Huang"
                },
                "author": "Mei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13184v2",
                "updated": "2024-08-27T03:27:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    3,
                    27,
                    8,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-23T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    2,
                    54,
                    4,
                    236,
                    0
                ],
                "title": "Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating\n  the Hallucination for Path Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating\n  the Hallucination for Path Planning"
                },
                "summary": "Spatial reasoning in Large Language Models (LLMs) is the foundation for\nembodied intelligence. However, even in simple maze environments, LLMs still\nencounter challenges in long-term path-planning, primarily influenced by their\nspatial hallucination and context inconsistency hallucination by long-term\nreasoning. To address this challenge, this study proposes an innovative model,\nSpatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To\naddress the spatial hallucination of LLMs, we propose the Spatial-to-Relational\napproach, which transforms spatial prompts into entity relations and paths\nrepresenting entity relation chains. This approach fully taps the potential of\nLLMs in terms of sequential thinking. As a result, we design a path-planning\nalgorithm based on Q-learning to mitigate the context inconsistency\nhallucination, which enhances the reasoning ability of LLMs. Using the Q-value\nof state-action as auxiliary information for prompts, we correct the\nhallucinations of LLMs, thereby guiding LLMs to learn the optimal path.\nFinally, we propose a reverse curriculum learning technique based on LLMs to\nfurther mitigate the context inconsistency hallucination. LLMs can rapidly\naccumulate successful experiences by reducing task difficulty and leveraging\nthem to tackle more complex tasks. We performed comprehensive experiments based\non Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our\nS2RCQL achieved a 23%--40% improvement in both success and optimality rates\ncompared with advanced prompt engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial reasoning in Large Language Models (LLMs) is the foundation for\nembodied intelligence. However, even in simple maze environments, LLMs still\nencounter challenges in long-term path-planning, primarily influenced by their\nspatial hallucination and context inconsistency hallucination by long-term\nreasoning. To address this challenge, this study proposes an innovative model,\nSpatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To\naddress the spatial hallucination of LLMs, we propose the Spatial-to-Relational\napproach, which transforms spatial prompts into entity relations and paths\nrepresenting entity relation chains. This approach fully taps the potential of\nLLMs in terms of sequential thinking. As a result, we design a path-planning\nalgorithm based on Q-learning to mitigate the context inconsistency\nhallucination, which enhances the reasoning ability of LLMs. Using the Q-value\nof state-action as auxiliary information for prompts, we correct the\nhallucinations of LLMs, thereby guiding LLMs to learn the optimal path.\nFinally, we propose a reverse curriculum learning technique based on LLMs to\nfurther mitigate the context inconsistency hallucination. LLMs can rapidly\naccumulate successful experiences by reducing task difficulty and leveraging\nthem to tackle more complex tasks. We performed comprehensive experiments based\non Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our\nS2RCQL achieved a 23%--40% improvement in both success and optimality rates\ncompared with advanced prompt engineering."
                },
                "authors": [
                    {
                        "name": "Hourui Deng"
                    },
                    {
                        "name": "Hongjie Zhang"
                    },
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Chaosheng Feng"
                    }
                ],
                "author_detail": {
                    "name": "Chaosheng Feng"
                },
                "author": "Chaosheng Feng",
                "arxiv_comment": "Submitted to ICASSP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.03865v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.03865v3",
                "updated": "2024-08-27T03:25:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    3,
                    25,
                    58,
                    1,
                    240,
                    0
                ],
                "published": "2023-11-07T10:28:17Z",
                "published_parsed": [
                    2023,
                    11,
                    7,
                    10,
                    28,
                    17,
                    1,
                    311,
                    0
                ],
                "title": "When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary\n  Classifiers via Membership Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary\n  Classifiers via Membership Inference Attacks"
                },
                "summary": "Previous studies have developed fairness methods for biased models that\nexhibit discriminatory behaviors towards specific subgroups. While these models\nhave shown promise in achieving fair predictions, recent research has\nidentified their potential vulnerability to score-based membership inference\nattacks (MIAs). In these attacks, adversaries can infer whether a particular\ndata sample was used during training by analyzing the model's prediction\nscores. However, our investigations reveal that these score-based MIAs are\nineffective when targeting fairness-enhanced models in binary classifications.\nThe attack models trained to launch the MIAs degrade into simplistic threshold\nmodels, resulting in lower attack performance. Meanwhile, we observe that\nfairness methods often lead to prediction performance degradation for the\nmajority subgroups of the training data. This raises the barrier to successful\nattacks and widens the prediction gaps between member and non-member data.\nBuilding upon these insights, we propose an efficient MIA method against\nfairness-enhanced models based on fairness discrepancy results (FD-MIA). It\nleverages the difference in the predictions from both the original and\nfairness-enhanced models and exploits the observed prediction gaps as attack\nclues. We also explore potential strategies for mitigating privacy leakages.\nExtensive experiments validate our findings and demonstrate the efficacy of the\nproposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous studies have developed fairness methods for biased models that\nexhibit discriminatory behaviors towards specific subgroups. While these models\nhave shown promise in achieving fair predictions, recent research has\nidentified their potential vulnerability to score-based membership inference\nattacks (MIAs). In these attacks, adversaries can infer whether a particular\ndata sample was used during training by analyzing the model's prediction\nscores. However, our investigations reveal that these score-based MIAs are\nineffective when targeting fairness-enhanced models in binary classifications.\nThe attack models trained to launch the MIAs degrade into simplistic threshold\nmodels, resulting in lower attack performance. Meanwhile, we observe that\nfairness methods often lead to prediction performance degradation for the\nmajority subgroups of the training data. This raises the barrier to successful\nattacks and widens the prediction gaps between member and non-member data.\nBuilding upon these insights, we propose an efficient MIA method against\nfairness-enhanced models based on fairness discrepancy results (FD-MIA). It\nleverages the difference in the predictions from both the original and\nfairness-enhanced models and exploits the observed prediction gaps as attack\nclues. We also explore potential strategies for mitigating privacy leakages.\nExtensive experiments validate our findings and demonstrate the efficacy of the\nproposed method."
                },
                "authors": [
                    {
                        "name": "Huan Tian"
                    },
                    {
                        "name": "Guangsheng Zhang"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Tianqing Zhu"
                    },
                    {
                        "name": "Ming Ding"
                    },
                    {
                        "name": "Wanlei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wanlei Zhou"
                },
                "author": "Wanlei Zhou",
                "arxiv_doi": "10.24963/ijcai.2024/57",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.24963/ijcai.2024/57",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.03865v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.03865v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IJCAI 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01262v3",
                "updated": "2024-08-27T03:13:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    3,
                    13,
                    50,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-02T13:35:11Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    13,
                    35,
                    11,
                    4,
                    215,
                    0
                ],
                "title": "RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems have demonstrated their\nadvantages in alleviating the hallucination of Large Language Models (LLMs).\nExisting RAG benchmarks mainly focus on evaluating whether LLMs can correctly\nanswer the general knowledge. However, they are unable to evaluate the\neffectiveness of the RAG system in dealing with the data from different\nvertical domains. This paper introduces RAGEval, a framework for automatically\ngenerating evaluation datasets to evaluate the knowledge usage ability of\ndifferent LLMs in different scenarios. Specifically, RAGEval summarizes a\nschema from seed documents, applies the configurations to generate diverse\ndocuments, and constructs question-answering pairs according to both articles\nand configurations. We propose three novel metrics, Completeness,\nHallucination, and Irrelevance, to carefully evaluate the responses generated\nby LLMs. By benchmarking RAG models in vertical domains, RAGEval has the\nability to better evaluate the knowledge usage ability of LLMs, which avoids\nthe confusion regarding the source of knowledge in answering question in\nexisting QA datasets--whether it comes from parameterized memory or retrieval.\nThe code and dataset will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems have demonstrated their\nadvantages in alleviating the hallucination of Large Language Models (LLMs).\nExisting RAG benchmarks mainly focus on evaluating whether LLMs can correctly\nanswer the general knowledge. However, they are unable to evaluate the\neffectiveness of the RAG system in dealing with the data from different\nvertical domains. This paper introduces RAGEval, a framework for automatically\ngenerating evaluation datasets to evaluate the knowledge usage ability of\ndifferent LLMs in different scenarios. Specifically, RAGEval summarizes a\nschema from seed documents, applies the configurations to generate diverse\ndocuments, and constructs question-answering pairs according to both articles\nand configurations. We propose three novel metrics, Completeness,\nHallucination, and Irrelevance, to carefully evaluate the responses generated\nby LLMs. By benchmarking RAG models in vertical domains, RAGEval has the\nability to better evaluate the knowledge usage ability of LLMs, which avoids\nthe confusion regarding the source of knowledge in answering question in\nexisting QA datasets--whether it comes from parameterized memory or retrieval.\nThe code and dataset will be released."
                },
                "authors": [
                    {
                        "name": "Kunlun Zhu"
                    },
                    {
                        "name": "Yifan Luo"
                    },
                    {
                        "name": "Dingling Xu"
                    },
                    {
                        "name": "Ruobing Wang"
                    },
                    {
                        "name": "Shi Yu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "add github repo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10903v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10903v4",
                "updated": "2024-08-27T02:58:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    58,
                    39,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-20T14:47:38Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    47,
                    38,
                    1,
                    233,
                    0
                ],
                "title": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model"
                },
                "summary": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue."
                },
                "authors": [
                    {
                        "name": "Yeyong Yu"
                    },
                    {
                        "name": "Runsheng Yu"
                    },
                    {
                        "name": "Haojie Wei"
                    },
                    {
                        "name": "Zhanqiu Zhang"
                    },
                    {
                        "name": "Quan Qian"
                    }
                ],
                "author_detail": {
                    "name": "Quan Qian"
                },
                "author": "Quan Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10903v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10903v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13391v2",
                "updated": "2024-08-27T02:55:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    55,
                    28,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-23T22:06:54Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    22,
                    6,
                    54,
                    4,
                    236,
                    0
                ],
                "title": "Generating Analytic Specifications for Data Visualization from Natural\n  Language Queries using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Analytic Specifications for Data Visualization from Natural\n  Language Queries using Large Language Models"
                },
                "summary": "Recently, large language models (LLMs) have shown great promise in\ntranslating natural language (NL) queries into visualizations, but their\n\"black-box\" nature often limits explainability and debuggability. In response,\nwe present a comprehensive text prompt that, given a tabular dataset and an NL\nquery about the dataset, generates an analytic specification including\n(detected) data attributes, (inferred) analytic tasks, and (recommended)\nvisualizations. This specification captures key aspects of the query\ntranslation process, affording both explainability and debuggability. For\ninstance, it provides mappings from the detected entities to the corresponding\nphrases in the input query, as well as the specific visual design principles\nthat determined the visualization recommendations. Moreover, unlike prior\nLLM-based approaches, our prompt supports conversational interaction and\nambiguity detection capabilities. In this paper, we detail the iterative\nprocess of curating our prompt, present a preliminary performance evaluation\nusing GPT-4, and discuss the strengths and limitations of LLMs at various\nstages of query translation. The prompt is open-source and integrated into\nNL4DV, a popular Python-based natural language toolkit for visualization, which\ncan be accessed at https://nl4dv.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have shown great promise in\ntranslating natural language (NL) queries into visualizations, but their\n\"black-box\" nature often limits explainability and debuggability. In response,\nwe present a comprehensive text prompt that, given a tabular dataset and an NL\nquery about the dataset, generates an analytic specification including\n(detected) data attributes, (inferred) analytic tasks, and (recommended)\nvisualizations. This specification captures key aspects of the query\ntranslation process, affording both explainability and debuggability. For\ninstance, it provides mappings from the detected entities to the corresponding\nphrases in the input query, as well as the specific visual design principles\nthat determined the visualization recommendations. Moreover, unlike prior\nLLM-based approaches, our prompt supports conversational interaction and\nambiguity detection capabilities. In this paper, we detail the iterative\nprocess of curating our prompt, present a preliminary performance evaluation\nusing GPT-4, and discuss the strengths and limitations of LLMs at various\nstages of query translation. The prompt is open-source and integrated into\nNL4DV, a popular Python-based natural language toolkit for visualization, which\ncan be accessed at https://nl4dv.github.io."
                },
                "authors": [
                    {
                        "name": "Subham Sah"
                    },
                    {
                        "name": "Rishab Mitra"
                    },
                    {
                        "name": "Arpit Narechania"
                    },
                    {
                        "name": "Alex Endert"
                    },
                    {
                        "name": "John Stasko"
                    },
                    {
                        "name": "Wenwen Dou"
                    }
                ],
                "author_detail": {
                    "name": "Wenwen Dou"
                },
                "author": "Wenwen Dou",
                "arxiv_comment": "6 pages, 3 figures. To appear in NLVIZ workshop 2024 (IEEE VIS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14744v1",
                "updated": "2024-08-27T02:45:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    45,
                    26,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:45:26Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    45,
                    26,
                    1,
                    240,
                    0
                ],
                "title": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with\n  Rich Linguistic Semantics from Openly Available Data and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with\n  Rich Linguistic Semantics from Openly Available Data and Large Language\n  Models"
                },
                "summary": "Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1 million RS images, each\naccompanied by multiple descriptive captions. Extensive experiments demonstrate\nthat RSTeller enhances the performance of multiple existing vision language\nmodels for RS scene understanding through continual pre-training. Our\nmethodology significantly reduces the manual effort and expertise needed for\nannotating remote sensing imagery while democratizing access to high-quality\nannotated data. This advancement fosters progress in visual language modeling\nand encourages broader participation in remote sensing research and\napplications. The RSTeller dataset is available at\nhttps://github.com/SlytherinGe/RSTeller.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1 million RS images, each\naccompanied by multiple descriptive captions. Extensive experiments demonstrate\nthat RSTeller enhances the performance of multiple existing vision language\nmodels for RS scene understanding through continual pre-training. Our\nmethodology significantly reduces the manual effort and expertise needed for\nannotating remote sensing imagery while democratizing access to high-quality\nannotated data. This advancement fosters progress in visual language modeling\nand encourages broader participation in remote sensing research and\napplications. The RSTeller dataset is available at\nhttps://github.com/SlytherinGe/RSTeller."
                },
                "authors": [
                    {
                        "name": "Junyao Ge"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Kaitai Guo"
                    },
                    {
                        "name": "Jimin Liang"
                    }
                ],
                "author_detail": {
                    "name": "Jimin Liang"
                },
                "author": "Jimin Liang",
                "arxiv_comment": "Submitted to ISPRS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12569v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12569v3",
                "updated": "2024-08-27T02:31:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    31,
                    42,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-22T17:37:27Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    37,
                    27,
                    3,
                    235,
                    0
                ],
                "title": "Sapiens: Foundation for Human Vision Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sapiens: Foundation for Human Vision Models"
                },
                "summary": "We present Sapiens, a family of models for four fundamental human-centric\nvision tasks -- 2D pose estimation, body-part segmentation, depth estimation,\nand surface normal prediction. Our models natively support 1K high-resolution\ninference and are extremely easy to adapt for individual tasks by simply\nfine-tuning models pretrained on over 300 million in-the-wild human images. We\nobserve that, given the same computational budget, self-supervised pretraining\non a curated dataset of human images significantly boosts the performance for a\ndiverse set of human-centric tasks. The resulting models exhibit remarkable\ngeneralization to in-the-wild data, even when labeled data is scarce or\nentirely synthetic. Our simple model design also brings scalability -- model\nperformance across tasks improves as we scale the number of parameters from 0.3\nto 2 billion. Sapiens consistently surpasses existing baselines across various\nhuman-centric benchmarks. We achieve significant improvements over the prior\nstate-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1\nmIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5%\nrelative angular error. Project page:\nhttps://about.meta.com/realitylabs/codecavatars/sapiens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Sapiens, a family of models for four fundamental human-centric\nvision tasks -- 2D pose estimation, body-part segmentation, depth estimation,\nand surface normal prediction. Our models natively support 1K high-resolution\ninference and are extremely easy to adapt for individual tasks by simply\nfine-tuning models pretrained on over 300 million in-the-wild human images. We\nobserve that, given the same computational budget, self-supervised pretraining\non a curated dataset of human images significantly boosts the performance for a\ndiverse set of human-centric tasks. The resulting models exhibit remarkable\ngeneralization to in-the-wild data, even when labeled data is scarce or\nentirely synthetic. Our simple model design also brings scalability -- model\nperformance across tasks improves as we scale the number of parameters from 0.3\nto 2 billion. Sapiens consistently surpasses existing baselines across various\nhuman-centric benchmarks. We achieve significant improvements over the prior\nstate-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1\nmIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5%\nrelative angular error. Project page:\nhttps://about.meta.com/realitylabs/codecavatars/sapiens."
                },
                "authors": [
                    {
                        "name": "Rawal Khirodkar"
                    },
                    {
                        "name": "Timur Bagautdinov"
                    },
                    {
                        "name": "Julieta Martinez"
                    },
                    {
                        "name": "Su Zhaoen"
                    },
                    {
                        "name": "Austin James"
                    },
                    {
                        "name": "Peter Selednik"
                    },
                    {
                        "name": "Stuart Anderson"
                    },
                    {
                        "name": "Shunsuke Saito"
                    }
                ],
                "author_detail": {
                    "name": "Shunsuke Saito"
                },
                "author": "Shunsuke Saito",
                "arxiv_comment": "ECCV 2024 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12569v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12569v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05706v2",
                "updated": "2024-08-27T02:24:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    24,
                    30,
                    1,
                    240,
                    0
                ],
                "published": "2024-02-08T14:35:09Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    14,
                    35,
                    9,
                    3,
                    39,
                    0
                ],
                "title": "Integrating Paralinguistics in Speech-Empowered Large Language Models\n  for Natural Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Paralinguistics in Speech-Empowered Large Language Models\n  for Natural Conversation"
                },
                "summary": "Recent work shows promising results in expanding the capabilities of large\nlanguage models (LLM) to directly understand and synthesize speech. However, an\nLLM-based strategy for modeling spoken dialogs remains elusive, calling for\nfurther investigation. This paper introduces an extensive speech-text LLM\nframework, the Unified Spoken Dialog Model (USDM), designed to generate\ncoherent spoken responses with naturally occurring prosodic features relevant\nto the given input speech without relying on explicit automatic speech\nrecognition (ASR) or text-to-speech (TTS) systems. We have verified the\ninclusion of prosody in speech tokens that predominantly contain semantic\ninformation and have used this foundation to construct a prosody-infused\nspeech-text model. Additionally, we propose a generalized speech-text\npretraining scheme that enhances the capture of cross-modal semantics. To\nconstruct USDM, we fine-tune our speech-text model on spoken dialog data using\na multi-step spoken dialog template that stimulates the chain-of-reasoning\ncapabilities exhibited by the underlying LLM. Automatic and human evaluations\non the DailyTalk dataset demonstrate that our approach effectively generates\nnatural-sounding spoken responses, surpassing previous and cascaded baselines.\nWe will make our code and checkpoints publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work shows promising results in expanding the capabilities of large\nlanguage models (LLM) to directly understand and synthesize speech. However, an\nLLM-based strategy for modeling spoken dialogs remains elusive, calling for\nfurther investigation. This paper introduces an extensive speech-text LLM\nframework, the Unified Spoken Dialog Model (USDM), designed to generate\ncoherent spoken responses with naturally occurring prosodic features relevant\nto the given input speech without relying on explicit automatic speech\nrecognition (ASR) or text-to-speech (TTS) systems. We have verified the\ninclusion of prosody in speech tokens that predominantly contain semantic\ninformation and have used this foundation to construct a prosody-infused\nspeech-text model. Additionally, we propose a generalized speech-text\npretraining scheme that enhances the capture of cross-modal semantics. To\nconstruct USDM, we fine-tune our speech-text model on spoken dialog data using\na multi-step spoken dialog template that stimulates the chain-of-reasoning\ncapabilities exhibited by the underlying LLM. Automatic and human evaluations\non the DailyTalk dataset demonstrate that our approach effectively generates\nnatural-sounding spoken responses, surpassing previous and cascaded baselines.\nWe will make our code and checkpoints publicly available."
                },
                "authors": [
                    {
                        "name": "Heeseung Kim"
                    },
                    {
                        "name": "Soonshin Seo"
                    },
                    {
                        "name": "Kyeongseok Jeong"
                    },
                    {
                        "name": "Ohsung Kwon"
                    },
                    {
                        "name": "Soyoon Kim"
                    },
                    {
                        "name": "Jungwhan Kim"
                    },
                    {
                        "name": "Jaehong Lee"
                    },
                    {
                        "name": "Eunwoo Song"
                    },
                    {
                        "name": "Myungwoo Oh"
                    },
                    {
                        "name": "Jung-Woo Ha"
                    },
                    {
                        "name": "Sungroh Yoon"
                    },
                    {
                        "name": "Kang Min Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Kang Min Yoo"
                },
                "author": "Kang Min Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.15240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15240v1",
                "updated": "2024-08-27T17:57:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    57,
                    45,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:57:45Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    57,
                    45,
                    1,
                    240,
                    0
                ],
                "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Verifiers: Reward Modeling as Next-Token Prediction"
                },
                "summary": "Verifiers or reward models are often used to enhance the reasoning\nperformance of large language models (LLMs). A common approach is the Best-of-N\nmethod, where N candidate solutions generated by the LLM are ranked by a\nverifier, and the best one is selected. While LLM-based verifiers are typically\ntrained as discriminative classifiers to score solutions, they do not utilize\nthe text generation capabilities of pretrained LLMs. To overcome this\nlimitation, we instead propose training verifiers using the ubiquitous\nnext-token prediction objective, jointly on verification and solution\ngeneration. Compared to standard verifiers, such generative verifiers (GenRM)\ncan benefit from several advantages of LLMs: they integrate seamlessly with\ninstruction tuning, enable chain-of-thought reasoning, and can utilize\nadditional inference-time compute via majority voting for better verification.\nWe demonstrate that when using Gemma-based verifiers on algorithmic and\ngrade-school math reasoning tasks, GenRM outperforms discriminative verifiers\nand LLM-as-a-Judge, showing a 16-64% improvement in the percentage of problems\nsolved with Best-of-N. Furthermore, we show that GenRM scales favorably across\ndataset size, model capacity, and inference-time compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifiers or reward models are often used to enhance the reasoning\nperformance of large language models (LLMs). A common approach is the Best-of-N\nmethod, where N candidate solutions generated by the LLM are ranked by a\nverifier, and the best one is selected. While LLM-based verifiers are typically\ntrained as discriminative classifiers to score solutions, they do not utilize\nthe text generation capabilities of pretrained LLMs. To overcome this\nlimitation, we instead propose training verifiers using the ubiquitous\nnext-token prediction objective, jointly on verification and solution\ngeneration. Compared to standard verifiers, such generative verifiers (GenRM)\ncan benefit from several advantages of LLMs: they integrate seamlessly with\ninstruction tuning, enable chain-of-thought reasoning, and can utilize\nadditional inference-time compute via majority voting for better verification.\nWe demonstrate that when using Gemma-based verifiers on algorithmic and\ngrade-school math reasoning tasks, GenRM outperforms discriminative verifiers\nand LLM-as-a-Judge, showing a 16-64% improvement in the percentage of problems\nsolved with Best-of-N. Furthermore, we show that GenRM scales favorably across\ndataset size, model capacity, and inference-time compute."
                },
                "authors": [
                    {
                        "name": "Lunjun Zhang"
                    },
                    {
                        "name": "Arian Hosseini"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Aviral Kumar"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Agarwal"
                },
                "author": "Rishabh Agarwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16553v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16553v7",
                "updated": "2024-08-27T17:57:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    57,
                    7,
                    1,
                    240,
                    0
                ],
                "published": "2024-01-29T20:44:10Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    20,
                    44,
                    10,
                    0,
                    29,
                    0
                ],
                "title": "SelectLLM: Can LLMs Select Important Instructions to Annotate?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Can LLMs Select Important Instructions to Annotate?"
                },
                "summary": "Instruction tuning benefits from large and diverse datasets; however,\ncreating such datasets involves a high cost of human labeling. While synthetic\ndatasets generated by large language models (LLMs) have partly solved this\nissue, they often contain low-quality data. One effective solution is\nselectively annotating unlabelled instructions, especially given the relative\nease of acquiring unlabeled instructions or texts from various sources.\nHowever, how to select unlabelled instructions is not well-explored, especially\nin the context of LLMs. Therefore, we introduce SelectLLM, an alternative\nframework that leverages the capabilities of LLMs to select unlabeled\ninstructions more effectively. Specifically, SelectLLM consists of two key\nsteps: Coreset-based clustering of unlabelled instructions for enlarging\ndiversity and prompting of LLM to identify the most beneficial instructions\nwithin each cluster. We evaluate SelectLLM on AlpacaEval2 and MT-Bench,\ndemonstrating its ability to outperform state-of-the-art methods like\nAlpagasus. In addition, we compare the performance and compatibility of\nSelectLLM with various LLMs, such as ChatGPT, LLaMA-3.1-70B, and Gemma-2-27b.\nSelectLLM's adaptability and robustness are further evidenced by its ability to\nmaintain high performance across both human and synthetic datasets. All code\nand data are publicly available (https://github.com/minnesotanlp/select-llm).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning benefits from large and diverse datasets; however,\ncreating such datasets involves a high cost of human labeling. While synthetic\ndatasets generated by large language models (LLMs) have partly solved this\nissue, they often contain low-quality data. One effective solution is\nselectively annotating unlabelled instructions, especially given the relative\nease of acquiring unlabeled instructions or texts from various sources.\nHowever, how to select unlabelled instructions is not well-explored, especially\nin the context of LLMs. Therefore, we introduce SelectLLM, an alternative\nframework that leverages the capabilities of LLMs to select unlabeled\ninstructions more effectively. Specifically, SelectLLM consists of two key\nsteps: Coreset-based clustering of unlabelled instructions for enlarging\ndiversity and prompting of LLM to identify the most beneficial instructions\nwithin each cluster. We evaluate SelectLLM on AlpacaEval2 and MT-Bench,\ndemonstrating its ability to outperform state-of-the-art methods like\nAlpagasus. In addition, we compare the performance and compatibility of\nSelectLLM with various LLMs, such as ChatGPT, LLaMA-3.1-70B, and Gemma-2-27b.\nSelectLLM's adaptability and robustness are further evidenced by its ability to\nmaintain high performance across both human and synthetic datasets. All code\nand data are publicly available (https://github.com/minnesotanlp/select-llm)."
                },
                "authors": [
                    {
                        "name": "Ritik Sachin Parkar"
                    },
                    {
                        "name": "Jaehyung Kim"
                    },
                    {
                        "name": "Jong Inn Park"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "arxiv_comment": "First Authors: Ritik Sachin Parkar and Jaehyung Kim | Second Author:\n  Jong Inn Park | PI: Dongyeop Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16553v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16553v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15237v1",
                "updated": "2024-08-27T17:56:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    56,
                    11,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:56:11Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    56,
                    11,
                    1,
                    240,
                    0
                ],
                "title": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models"
                },
                "summary": "Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model."
                },
                "authors": [
                    {
                        "name": "Junxiong Wang"
                    },
                    {
                        "name": "Daniele Paliotta"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Alexander M. Rush"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "Code is open-sourced at https://github.com/jxiw/MambaInLlama",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15231v1",
                "updated": "2024-08-27T17:48:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    48,
                    29,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:48:29Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    48,
                    29,
                    1,
                    240,
                    0
                ],
                "title": "DCT-CryptoNets: Scaling Private Inference in the Frequency Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCT-CryptoNets: Scaling Private Inference in the Frequency Domain"
                },
                "summary": "The convergence of fully homomorphic encryption (FHE) and machine learning\noffers unprecedented opportunities for private inference of sensitive data. FHE\nenables computation directly on encrypted data, safeguarding the entire machine\nlearning pipeline, including data and model confidentiality. However, existing\nFHE-based implementations for deep neural networks face significant challenges\nin computational cost, latency, and scalability, limiting their practical\ndeployment. This paper introduces DCT-CryptoNets, a novel approach that\nleverages frequency-domain learning to tackle these issues. Our method operates\ndirectly in the frequency domain, utilizing the discrete cosine transform (DCT)\ncommonly employed in JPEG compression. This approach is inherently compatible\nwith remote computing services, where images are usually transmitted and stored\nin compressed formats. DCT-CryptoNets reduces the computational burden of\nhomomorphic operations by focusing on perceptually relevant low-frequency\ncomponents. This is demonstrated by substantial latency reduction of up to\n5.3$\\times$ compared to prior work on image classification tasks, including a\nnovel demonstration of ImageNet inference within 2.5 hours, down from 12.5\nhours compared to prior work on equivalent compute resources. Moreover,\nDCT-CryptoNets improves the reliability of encrypted accuracy by reducing\nvariability (e.g., from $\\pm$2.5\\% to $\\pm$1.0\\% on ImageNet). This study\ndemonstrates a promising avenue for achieving efficient and practical\nprivacy-preserving deep learning on high resolution images seen in real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The convergence of fully homomorphic encryption (FHE) and machine learning\noffers unprecedented opportunities for private inference of sensitive data. FHE\nenables computation directly on encrypted data, safeguarding the entire machine\nlearning pipeline, including data and model confidentiality. However, existing\nFHE-based implementations for deep neural networks face significant challenges\nin computational cost, latency, and scalability, limiting their practical\ndeployment. This paper introduces DCT-CryptoNets, a novel approach that\nleverages frequency-domain learning to tackle these issues. Our method operates\ndirectly in the frequency domain, utilizing the discrete cosine transform (DCT)\ncommonly employed in JPEG compression. This approach is inherently compatible\nwith remote computing services, where images are usually transmitted and stored\nin compressed formats. DCT-CryptoNets reduces the computational burden of\nhomomorphic operations by focusing on perceptually relevant low-frequency\ncomponents. This is demonstrated by substantial latency reduction of up to\n5.3$\\times$ compared to prior work on image classification tasks, including a\nnovel demonstration of ImageNet inference within 2.5 hours, down from 12.5\nhours compared to prior work on equivalent compute resources. Moreover,\nDCT-CryptoNets improves the reliability of encrypted accuracy by reducing\nvariability (e.g., from $\\pm$2.5\\% to $\\pm$1.0\\% on ImageNet). This study\ndemonstrates a promising avenue for achieving efficient and practical\nprivacy-preserving deep learning on high resolution images seen in real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Arjun Roy"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "Under Review; 10 pages content, 3 pages appendix, 4 figures, 8\n  tables; Code TBD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01419v2",
                "updated": "2024-08-27T17:45:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    45,
                    27,
                    1,
                    240,
                    0
                ],
                "published": "2023-11-02T17:33:47Z",
                "published_parsed": [
                    2023,
                    11,
                    2,
                    17,
                    33,
                    47,
                    3,
                    306,
                    0
                ],
                "title": "C3DM: Constrained-Context Conditional Diffusion Models for Imitation\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C3DM: Constrained-Context Conditional Diffusion Models for Imitation\n  Learning"
                },
                "summary": "Behavior Cloning (BC) methods are effective at learning complex manipulation\ntasks. However, they are prone to spurious correlation - expressive models may\nfocus on distractors that are irrelevant to action prediction - and are thus\nfragile in real-world deployment. Prior methods have addressed this challenge\nby exploring different model architectures and action representations. However,\nnone were able to balance between sample efficiency and robustness against\ndistractors for solving manipulation tasks with a complex action space. We\npresent \\textbf{C}onstrained-\\textbf{C}ontext \\textbf{C}onditional\n\\textbf{D}iffusion \\textbf{M}odel (C3DM), a diffusion model policy for solving\n6-DoF robotic manipulation tasks with robustness to distractions that can learn\ndeployable robot policies from as little as five demonstrations. A key\ncomponent of C3DM is a fixation step that helps the action denoiser to focus on\ntask-relevant regions around a predicted fixation point while ignoring\ndistractors in the context. We empirically show that C3DM is robust to\nout-of-distribution distractors, and consistently achieves high success rates\non a wide array of tasks, ranging from table-top manipulation to industrial\nkitting that require varying levels of precision and robustness to distractors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavior Cloning (BC) methods are effective at learning complex manipulation\ntasks. However, they are prone to spurious correlation - expressive models may\nfocus on distractors that are irrelevant to action prediction - and are thus\nfragile in real-world deployment. Prior methods have addressed this challenge\nby exploring different model architectures and action representations. However,\nnone were able to balance between sample efficiency and robustness against\ndistractors for solving manipulation tasks with a complex action space. We\npresent \\textbf{C}onstrained-\\textbf{C}ontext \\textbf{C}onditional\n\\textbf{D}iffusion \\textbf{M}odel (C3DM), a diffusion model policy for solving\n6-DoF robotic manipulation tasks with robustness to distractions that can learn\ndeployable robot policies from as little as five demonstrations. A key\ncomponent of C3DM is a fixation step that helps the action denoiser to focus on\ntask-relevant regions around a predicted fixation point while ignoring\ndistractors in the context. We empirically show that C3DM is robust to\nout-of-distribution distractors, and consistently achieves high success rates\non a wide array of tasks, ranging from table-top manipulation to industrial\nkitting that require varying levels of precision and robustness to distractors."
                },
                "authors": [
                    {
                        "name": "Vaibhav Saxena"
                    },
                    {
                        "name": "Yotto Koga"
                    },
                    {
                        "name": "Danfei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Danfei Xu"
                },
                "author": "Danfei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.01419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15221v1",
                "updated": "2024-08-27T17:33:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    33,
                    30,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:33:30Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    33,
                    30,
                    1,
                    240,
                    0
                ],
                "title": "LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet"
                },
                "summary": "Recent large language model (LLM) defenses have greatly improved models'\nability to refuse harmful queries, even when adversarially attacked. However,\nLLM defenses are primarily evaluated against automated adversarial attacks in a\nsingle turn of conversation, an insufficient threat model for real-world\nmalicious use. We demonstrate that multi-turn human jailbreaks uncover\nsignificant vulnerabilities, exceeding 70% attack success rate (ASR) on\nHarmBench against defenses that report single-digit ASRs with automated\nsingle-turn attacks. Human jailbreaks also reveal vulnerabilities in machine\nunlearning defenses, successfully recovering dual-use biosecurity knowledge\nfrom unlearned models. We compile these results into Multi-Turn Human\nJailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks.\nWe publicly release MHJ alongside a compendium of jailbreak tactics developed\nacross dozens of commercial red teaming engagements, supporting research\ntowards stronger LLM defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model (LLM) defenses have greatly improved models'\nability to refuse harmful queries, even when adversarially attacked. However,\nLLM defenses are primarily evaluated against automated adversarial attacks in a\nsingle turn of conversation, an insufficient threat model for real-world\nmalicious use. We demonstrate that multi-turn human jailbreaks uncover\nsignificant vulnerabilities, exceeding 70% attack success rate (ASR) on\nHarmBench against defenses that report single-digit ASRs with automated\nsingle-turn attacks. Human jailbreaks also reveal vulnerabilities in machine\nunlearning defenses, successfully recovering dual-use biosecurity knowledge\nfrom unlearned models. We compile these results into Multi-Turn Human\nJailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks.\nWe publicly release MHJ alongside a compendium of jailbreak tactics developed\nacross dozens of commercial red teaming engagements, supporting research\ntowards stronger LLM defenses."
                },
                "authors": [
                    {
                        "name": "Nathaniel Li"
                    },
                    {
                        "name": "Ziwen Han"
                    },
                    {
                        "name": "Ian Steneker"
                    },
                    {
                        "name": "Willow Primack"
                    },
                    {
                        "name": "Riley Goodside"
                    },
                    {
                        "name": "Hugh Zhang"
                    },
                    {
                        "name": "Zifan Wang"
                    },
                    {
                        "name": "Cristina Menghini"
                    },
                    {
                        "name": "Summer Yue"
                    }
                ],
                "author_detail": {
                    "name": "Summer Yue"
                },
                "author": "Summer Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15219v1",
                "updated": "2024-08-27T17:31:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    31,
                    26,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:31:26Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    31,
                    26,
                    1,
                    240,
                    0
                ],
                "title": "FRAMER/Miu: Tagged Pointer-based Capability and Fundamental Cost of\n  Memory Safety & Coherence (Position Paper)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRAMER/Miu: Tagged Pointer-based Capability and Fundamental Cost of\n  Memory Safety & Coherence (Position Paper)"
                },
                "summary": "Ensuring system correctness, such as memory safety, can eliminate security\nvulnerabilities that attackers could exploit in the first place. However, high\nand unpredictable performance degradation remains a primary challenge.\n  Recognizing that it is extremely difficult to achieve complete system\ncorrectness for production deployment, researchers make trade-offs between\nperformance, detection coverage, interoperability, precision, and detection\ntiming.\n  This research strikes a balance between comprehensive system protection and\nthe costs required to obtain it, identifies the desirable roles of software and\nhardware, and presents a tagged pointer-based capability system as a\nstand-alone software solution and a prototype for future hardware design. This\npaper presents follow-up plans for the FRAMER/Miu generic framework to achieve\nthese goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring system correctness, such as memory safety, can eliminate security\nvulnerabilities that attackers could exploit in the first place. However, high\nand unpredictable performance degradation remains a primary challenge.\n  Recognizing that it is extremely difficult to achieve complete system\ncorrectness for production deployment, researchers make trade-offs between\nperformance, detection coverage, interoperability, precision, and detection\ntiming.\n  This research strikes a balance between comprehensive system protection and\nthe costs required to obtain it, identifies the desirable roles of software and\nhardware, and presents a tagged pointer-based capability system as a\nstand-alone software solution and a prototype for future hardware design. This\npaper presents follow-up plans for the FRAMER/Miu generic framework to achieve\nthese goals."
                },
                "authors": [
                    {
                        "name": "Myoung Jin Nam"
                    }
                ],
                "author_detail": {
                    "name": "Myoung Jin Nam"
                },
                "author": "Myoung Jin Nam",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15207v1",
                "updated": "2024-08-27T17:14:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    14,
                    21,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:14:21Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    14,
                    21,
                    1,
                    240,
                    0
                ],
                "title": "Investigating Coverage Criteria in Large Language Models: An In-Depth\n  Study Through Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Coverage Criteria in Large Language Models: An In-Depth\n  Study Through Jailbreak Attacks"
                },
                "summary": "The swift advancement of large language models (LLMs) has profoundly shaped\nthe landscape of artificial intelligence; however, their deployment in\nsensitive domains raises grave concerns, particularly due to their\nsusceptibility to malicious exploitation. This situation underscores the\ninsufficiencies in pre-deployment testing, highlighting the urgent need for\nmore rigorous and comprehensive evaluation methods. This study presents a\ncomprehensive empirical analysis assessing the efficacy of conventional\ncoverage criteria in identifying these vulnerabilities, with a particular\nemphasis on the pressing issue of jailbreak attacks. Our investigation begins\nwith a clustering analysis of the hidden states in LLMs, demonstrating that\nintrinsic characteristics of these states can distinctly differentiate between\nvarious types of queries. Subsequently, we assess the performance of these\ncriteria across three critical dimensions: criterion level, layer level, and\ntoken level. Our findings uncover significant disparities in neuron activation\npatterns between the processing of normal and jailbreak queries, thereby\ncorroborating the clustering results. Leveraging these findings, we propose an\ninnovative approach for the real-time detection of jailbreak attacks by\nutilizing neural activation features. Our classifier demonstrates remarkable\naccuracy, averaging 96.33% in identifying jailbreak queries, including those\nthat could lead to adversarial attacks. The importance of our research lies in\nits comprehensive approach to addressing the intricate challenges of LLM\nsecurity. By enabling instantaneous detection from the model's first token\noutput, our method holds promise for future systems integrating LLMs, offering\nrobust real-time detection capabilities. This study advances our understanding\nof LLM security testing, and lays a critical foundation for the development of\nmore resilient AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The swift advancement of large language models (LLMs) has profoundly shaped\nthe landscape of artificial intelligence; however, their deployment in\nsensitive domains raises grave concerns, particularly due to their\nsusceptibility to malicious exploitation. This situation underscores the\ninsufficiencies in pre-deployment testing, highlighting the urgent need for\nmore rigorous and comprehensive evaluation methods. This study presents a\ncomprehensive empirical analysis assessing the efficacy of conventional\ncoverage criteria in identifying these vulnerabilities, with a particular\nemphasis on the pressing issue of jailbreak attacks. Our investigation begins\nwith a clustering analysis of the hidden states in LLMs, demonstrating that\nintrinsic characteristics of these states can distinctly differentiate between\nvarious types of queries. Subsequently, we assess the performance of these\ncriteria across three critical dimensions: criterion level, layer level, and\ntoken level. Our findings uncover significant disparities in neuron activation\npatterns between the processing of normal and jailbreak queries, thereby\ncorroborating the clustering results. Leveraging these findings, we propose an\ninnovative approach for the real-time detection of jailbreak attacks by\nutilizing neural activation features. Our classifier demonstrates remarkable\naccuracy, averaging 96.33% in identifying jailbreak queries, including those\nthat could lead to adversarial attacks. The importance of our research lies in\nits comprehensive approach to addressing the intricate challenges of LLM\nsecurity. By enabling instantaneous detection from the model's first token\noutput, our method holds promise for future systems integrating LLMs, offering\nrobust real-time detection capabilities. This study advances our understanding\nof LLM security testing, and lays a critical foundation for the development of\nmore resilient AI systems."
                },
                "authors": [
                    {
                        "name": "Shide Zhou"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15204v1",
                "updated": "2024-08-27T17:03:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    3,
                    18,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:03:18Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    3,
                    18,
                    1,
                    240,
                    0
                ],
                "title": "Can Unconfident LLM Annotations Be Used for Confident Conclusions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Unconfident LLM Annotations Be Used for Confident Conclusions?"
                },
                "summary": "Large language models (LLMs) have shown high agreement with human raters\nacross a variety of tasks, demonstrating potential to ease the challenges of\nhuman data collection. In computational social science (CSS), researchers are\nincreasingly leveraging LLM annotations to complement slow and expensive human\nannotations. Still, guidelines for collecting and using LLM annotations,\nwithout compromising the validity of downstream conclusions, remain limited. We\nintroduce Confidence-Driven Inference: a method that combines LLM annotations\nand LLM confidence indicators to strategically select which human annotations\nshould be collected, with the goal of producing accurate statistical estimates\nand provably valid confidence intervals while reducing the number of human\nannotations needed. Our approach comes with safeguards against LLM annotations\nof poor quality, guaranteeing that the conclusions will be both valid and no\nless accurate than if we only relied on human annotations. We demonstrate the\neffectiveness of Confidence-Driven Inference over baselines in statistical\nestimation tasks across three CSS settings--text politeness, stance, and\nbias--reducing the needed number of human annotations by over 25% in each.\nAlthough we use CSS settings for demonstration, Confidence-Driven Inference can\nbe used to estimate most standard quantities across a broad range of NLP\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown high agreement with human raters\nacross a variety of tasks, demonstrating potential to ease the challenges of\nhuman data collection. In computational social science (CSS), researchers are\nincreasingly leveraging LLM annotations to complement slow and expensive human\nannotations. Still, guidelines for collecting and using LLM annotations,\nwithout compromising the validity of downstream conclusions, remain limited. We\nintroduce Confidence-Driven Inference: a method that combines LLM annotations\nand LLM confidence indicators to strategically select which human annotations\nshould be collected, with the goal of producing accurate statistical estimates\nand provably valid confidence intervals while reducing the number of human\nannotations needed. Our approach comes with safeguards against LLM annotations\nof poor quality, guaranteeing that the conclusions will be both valid and no\nless accurate than if we only relied on human annotations. We demonstrate the\neffectiveness of Confidence-Driven Inference over baselines in statistical\nestimation tasks across three CSS settings--text politeness, stance, and\nbias--reducing the needed number of human annotations by over 25% in each.\nAlthough we use CSS settings for demonstration, Confidence-Driven Inference can\nbe used to estimate most standard quantities across a broad range of NLP\nproblems."
                },
                "authors": [
                    {
                        "name": "Kristina Gligorić"
                    },
                    {
                        "name": "Tijana Zrnic"
                    },
                    {
                        "name": "Cinoo Lee"
                    },
                    {
                        "name": "Emmanuel J. Candès"
                    },
                    {
                        "name": "Dan Jurafsky"
                    }
                ],
                "author_detail": {
                    "name": "Dan Jurafsky"
                },
                "author": "Dan Jurafsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10738v3",
                "updated": "2024-08-27T16:38:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    38,
                    50,
                    1,
                    240,
                    0
                ],
                "published": "2024-04-16T17:11:44Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    17,
                    11,
                    44,
                    1,
                    107,
                    0
                ],
                "title": "Quantum teleportation coexisting with classical communications in\n  optical fiber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum teleportation coexisting with classical communications in\n  optical fiber"
                },
                "summary": "The ability for quantum and conventional networks to operate in the same\noptical fibers would aid the deployment of quantum network technology on a\nlarge scale. Quantum teleportation is a fundamental operation in quantum\nnetworking, but has yet to be demonstrated in fibers populated with high-power\nconventional optical signals. Here we report to the best of our knowledge the\nfirst demonstration of quantum teleportation over fibers carrying conventional\ntelecommunications traffic. Quantum state transfer is achieved over a 30.2-km\nfiber carrying 400-Gbps C-band classical traffic with a Bell state measurement\nperformed at the fiber midpoint. To protect quantum fidelity from spontaneous\nRaman scattering noise, we use optimal O-band quantum channels, narrow\nspectro-temporal filtering, and multi-photon coincidence detection. Fidelity is\nshown to be well maintained with an elevated C-band classical power of 18.7\ndBm, which could support multiple classical channels totaling many terabits/s\naggregate data rates. These results show the feasibility of advanced quantum\nand classical network applications operating within a unified fiber\ninfrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability for quantum and conventional networks to operate in the same\noptical fibers would aid the deployment of quantum network technology on a\nlarge scale. Quantum teleportation is a fundamental operation in quantum\nnetworking, but has yet to be demonstrated in fibers populated with high-power\nconventional optical signals. Here we report to the best of our knowledge the\nfirst demonstration of quantum teleportation over fibers carrying conventional\ntelecommunications traffic. Quantum state transfer is achieved over a 30.2-km\nfiber carrying 400-Gbps C-band classical traffic with a Bell state measurement\nperformed at the fiber midpoint. To protect quantum fidelity from spontaneous\nRaman scattering noise, we use optimal O-band quantum channels, narrow\nspectro-temporal filtering, and multi-photon coincidence detection. Fidelity is\nshown to be well maintained with an elevated C-band classical power of 18.7\ndBm, which could support multiple classical channels totaling many terabits/s\naggregate data rates. These results show the feasibility of advanced quantum\nand classical network applications operating within a unified fiber\ninfrastructure."
                },
                "authors": [
                    {
                        "name": "Jordan M. Thomas"
                    },
                    {
                        "name": "Fei I. Yeh"
                    },
                    {
                        "name": "Jim Hao Chen"
                    },
                    {
                        "name": "Joe J. Mambretti"
                    },
                    {
                        "name": "Scott J. Kohlert"
                    },
                    {
                        "name": "Gregory S. Kanter"
                    },
                    {
                        "name": "Prem Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Prem Kumar"
                },
                "author": "Prem Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15172v1",
                "updated": "2024-08-27T16:10:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    10,
                    21,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T16:10:21Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    10,
                    21,
                    1,
                    240,
                    0
                ],
                "title": "X-Reflect: Cross-Reflection Prompting for Multimodal Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Reflect: Cross-Reflection Prompting for Multimodal Recommendation"
                },
                "summary": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) have been\nshown to enhance the effectiveness of enriching item descriptions, thereby\nimproving the accuracy of recommendation systems. However, most existing\napproaches either rely on text-only prompting or employ basic multimodal\nstrategies that do not fully exploit the complementary information available\nfrom both textual and visual modalities. This paper introduces a novel\nframework, Cross-Reflection Prompting, termed X-Reflect, designed to address\nthese limitations by prompting LMMs to explicitly identify and reconcile\nsupportive and conflicting information between text and images. By capturing\nnuanced insights from both modalities, this approach generates more\ncomprehensive and contextually richer item representations. Extensive\nexperiments conducted on two widely used benchmarks demonstrate that our method\noutperforms existing prompting baselines in downstream recommendation accuracy.\nAdditionally, we evaluate the generalizability of our framework across\ndifferent LMM backbones and the robustness of the prompting strategies,\noffering insights for optimization. This work underscores the importance of\nintegrating multimodal information and presents a novel solution for improving\nitem understanding in multimodal recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) have been\nshown to enhance the effectiveness of enriching item descriptions, thereby\nimproving the accuracy of recommendation systems. However, most existing\napproaches either rely on text-only prompting or employ basic multimodal\nstrategies that do not fully exploit the complementary information available\nfrom both textual and visual modalities. This paper introduces a novel\nframework, Cross-Reflection Prompting, termed X-Reflect, designed to address\nthese limitations by prompting LMMs to explicitly identify and reconcile\nsupportive and conflicting information between text and images. By capturing\nnuanced insights from both modalities, this approach generates more\ncomprehensive and contextually richer item representations. Extensive\nexperiments conducted on two widely used benchmarks demonstrate that our method\noutperforms existing prompting baselines in downstream recommendation accuracy.\nAdditionally, we evaluate the generalizability of our framework across\ndifferent LMM backbones and the robustness of the prompting strategies,\noffering insights for optimization. This work underscores the importance of\nintegrating multimodal information and presents a novel solution for improving\nitem understanding in multimodal recommendation systems."
                },
                "authors": [
                    {
                        "name": "Hanjia Lyu"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Md Mehrab Tanjim"
                    },
                    {
                        "name": "Stefano Petrangeli"
                    },
                    {
                        "name": "Somdeb Sarkhel"
                    },
                    {
                        "name": "Jiebo Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jiebo Luo"
                },
                "author": "Jiebo Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15171v1",
                "updated": "2024-08-27T16:09:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    9,
                    56,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T16:09:56Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    9,
                    56,
                    1,
                    240,
                    0
                ],
                "title": "Measuring text summarization factuality using atomic facts entailment\n  metrics in the context of retrieval augmented generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring text summarization factuality using atomic facts entailment\n  metrics in the context of retrieval augmented generation"
                },
                "summary": "The use of large language models (LLMs) has significantly increased since the\nintroduction of ChatGPT in 2022, demonstrating their value across various\napplications. However, a major challenge for enterprise and commercial adoption\nof LLMs is their tendency to generate inaccurate information, a phenomenon\nknown as \"hallucination.\" This project proposes a method for estimating the\nfactuality of a summary generated by LLMs when compared to a source text. Our\napproach utilizes Naive Bayes classification to assess the accuracy of the\ncontent produced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) has significantly increased since the\nintroduction of ChatGPT in 2022, demonstrating their value across various\napplications. However, a major challenge for enterprise and commercial adoption\nof LLMs is their tendency to generate inaccurate information, a phenomenon\nknown as \"hallucination.\" This project proposes a method for estimating the\nfactuality of a summary generated by LLMs when compared to a source text. Our\napproach utilizes Naive Bayes classification to assess the accuracy of the\ncontent produced."
                },
                "authors": [
                    {
                        "name": "N. E. Kriman"
                    }
                ],
                "author_detail": {
                    "name": "N. E. Kriman"
                },
                "author": "N. E. Kriman",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13993v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13993v3",
                "updated": "2024-08-27T15:56:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    56,
                    33,
                    1,
                    240,
                    0
                ],
                "published": "2024-04-22T08:59:35Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    8,
                    59,
                    35,
                    0,
                    113,
                    0
                ],
                "title": "Zero-Shot Character Identification and Speaker Prediction in Comics via\n  Iterative Multimodal Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Character Identification and Speaker Prediction in Comics via\n  Iterative Multimodal Fusion"
                },
                "summary": "Recognizing characters and predicting speakers of dialogue are critical for\ncomic processing tasks, such as voice generation or translation. However,\nbecause characters vary by comic title, supervised learning approaches like\ntraining character classifiers which require specific annotations for each\ncomic title are infeasible. This motivates us to propose a novel zero-shot\napproach, allowing machines to identify characters and predict speaker names\nbased solely on unannotated comic images. In spite of their importance in\nreal-world applications, these task have largely remained unexplored due to\nchallenges in story comprehension and multimodal integration. Recent large\nlanguage models (LLMs) have shown great capability for text understanding and\nreasoning, while their application to multimodal content analysis is still an\nopen problem. To address this problem, we propose an iterative multimodal\nframework, the first to employ multimodal information for both character\nidentification and speaker prediction tasks. Our experiments demonstrate the\neffectiveness of the proposed framework, establishing a robust baseline for\nthese tasks. Furthermore, since our method requires no training data or\nannotations, it can be used as-is on any comic series.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing characters and predicting speakers of dialogue are critical for\ncomic processing tasks, such as voice generation or translation. However,\nbecause characters vary by comic title, supervised learning approaches like\ntraining character classifiers which require specific annotations for each\ncomic title are infeasible. This motivates us to propose a novel zero-shot\napproach, allowing machines to identify characters and predict speaker names\nbased solely on unannotated comic images. In spite of their importance in\nreal-world applications, these task have largely remained unexplored due to\nchallenges in story comprehension and multimodal integration. Recent large\nlanguage models (LLMs) have shown great capability for text understanding and\nreasoning, while their application to multimodal content analysis is still an\nopen problem. To address this problem, we propose an iterative multimodal\nframework, the first to employ multimodal information for both character\nidentification and speaker prediction tasks. Our experiments demonstrate the\neffectiveness of the proposed framework, establishing a robust baseline for\nthese tasks. Furthermore, since our method requires no training data or\nannotations, it can be used as-is on any comic series."
                },
                "authors": [
                    {
                        "name": "Yingxuan Li"
                    },
                    {
                        "name": "Ryota Hinami"
                    },
                    {
                        "name": "Kiyoharu Aizawa"
                    },
                    {
                        "name": "Yusuke Matsui"
                    }
                ],
                "author_detail": {
                    "name": "Yusuke Matsui"
                },
                "author": "Yusuke Matsui",
                "arxiv_comment": "Accepted to ACM Multimedia 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13993v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13993v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15147v1",
                "updated": "2024-08-27T15:40:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    40,
                    11,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T15:40:11Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    40,
                    11,
                    1,
                    240,
                    0
                ],
                "title": "Blackbox optimization for origami-inspired bistable structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blackbox optimization for origami-inspired bistable structures"
                },
                "summary": "Bistable mechanical systems exhibit two stable configurations where the\nelastic energy is locally minimized. To realize such systems, origami\ntechniques have been proposed as a versatile platform to design deployable\nstructures with both compact and functional stable states. Conceptually, a\nbistable origami motif is composed of two-dimensional surfaces connected by\none-dimensional fold lines. This leads to stable configurations exhibiting\nzero-energy local minima. Physically, origami-inspired structures are\nthree-dimensional, comprising facets and hinges fabricated in a distinct stable\nstate where residual stresses are minimized. This leads to the dominance of one\nstable state over the other. To improve mechanical performance, one can solve\nthe constrained optimization problem of maximizing the bistability of origami\nstructures, defined as the amount of elastic energy required to switch between\nstable states, while ensuring materials used for the facets and hinges remain\nwithin their elastic regime. In this study, the Mesh Adaptive Direct Search\n(MADS) algorithm, a blackbox optimization technique, is used to solve the\nconstrained optimization problem. The bistable waterbomb-base origami motif is\nselected as a case-study to present the methodology. The elastic energy of this\norigami pattern under deployment is calculated via Finite Element simulations\nwhich serve as the blackbox in the MADS optimization loop. To validate the\nresults, optimized waterbomb-base geometries are built via Fused Filament\nFabrication and their response under loading is characterized experimentally on\na Uniaxial Test Machine. Ultimately, our method offers a general framework for\noptimizing bistability in mechanical systems, presenting opportunities for\nadvancement across various engineering applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bistable mechanical systems exhibit two stable configurations where the\nelastic energy is locally minimized. To realize such systems, origami\ntechniques have been proposed as a versatile platform to design deployable\nstructures with both compact and functional stable states. Conceptually, a\nbistable origami motif is composed of two-dimensional surfaces connected by\none-dimensional fold lines. This leads to stable configurations exhibiting\nzero-energy local minima. Physically, origami-inspired structures are\nthree-dimensional, comprising facets and hinges fabricated in a distinct stable\nstate where residual stresses are minimized. This leads to the dominance of one\nstable state over the other. To improve mechanical performance, one can solve\nthe constrained optimization problem of maximizing the bistability of origami\nstructures, defined as the amount of elastic energy required to switch between\nstable states, while ensuring materials used for the facets and hinges remain\nwithin their elastic regime. In this study, the Mesh Adaptive Direct Search\n(MADS) algorithm, a blackbox optimization technique, is used to solve the\nconstrained optimization problem. The bistable waterbomb-base origami motif is\nselected as a case-study to present the methodology. The elastic energy of this\norigami pattern under deployment is calculated via Finite Element simulations\nwhich serve as the blackbox in the MADS optimization loop. To validate the\nresults, optimized waterbomb-base geometries are built via Fused Filament\nFabrication and their response under loading is characterized experimentally on\na Uniaxial Test Machine. Ultimately, our method offers a general framework for\noptimizing bistability in mechanical systems, presenting opportunities for\nadvancement across various engineering applications."
                },
                "authors": [
                    {
                        "name": "Luca Boisneault"
                    },
                    {
                        "name": "Charles Audet"
                    },
                    {
                        "name": "David Melancon"
                    }
                ],
                "author_detail": {
                    "name": "David Melancon"
                },
                "author": "David Melancon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07531v2",
                "updated": "2024-08-27T15:16:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    16,
                    6,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-14T13:03:41Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    3,
                    41,
                    2,
                    227,
                    0
                ],
                "title": "Development of a Large Language Model-based Multi-Agent Clinical\n  Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based\n  Triage and Treatment Planning in Emergency Departments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a Large Language Model-based Multi-Agent Clinical\n  Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based\n  Triage and Treatment Planning in Emergency Departments"
                },
                "summary": "Emergency department (ED) overcrowding and the complexity of rapid\ndecision-making in critical care settings pose significant challenges to\nhealthcare systems worldwide. While clinical decision support systems (CDSS)\nhave shown promise, the integration of large language models (LLMs) offers new\npossibilities for enhancing triage accuracy and clinical decision-making. This\nstudy presents an LLM-driven CDSS designed to assist ED physicians and nurses\nin patient triage, treatment planning, and overall emergency care management.\n  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM,\norchestrated by CrewAI and Langchain. The system comprises four AI agents\nemulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED\nCoordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for\ntriage assessment and integrates with the RxNorm API for medication management.\n  The model was evaluated using the Asclepius dataset, with performance\nassessed by a clinical emergency medicine specialist. The CDSS demonstrated\nhigh accuracy in triage decision-making compared to the baseline of a\nsingle-agent system. Furthermore, the system exhibited strong performance in\ncritical areas, including primary diagnosis, critical findings identification,\ndisposition decision-making, treatment planning, and resource allocation.\n  Our multi-agent CDSS demonstrates significant potential for supporting\ncomprehensive emergency care management. By leveraging state-of-the-art AI\ntechnologies, this system offers a scalable and adaptable tool that could\nenhance emergency medical care delivery, potentially alleviating ED\novercrowding and improving patient outcomes. This work contributes to the\ngrowing field of AI applications in emergency medicine and offers a promising\ndirection for future research and clinical implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency department (ED) overcrowding and the complexity of rapid\ndecision-making in critical care settings pose significant challenges to\nhealthcare systems worldwide. While clinical decision support systems (CDSS)\nhave shown promise, the integration of large language models (LLMs) offers new\npossibilities for enhancing triage accuracy and clinical decision-making. This\nstudy presents an LLM-driven CDSS designed to assist ED physicians and nurses\nin patient triage, treatment planning, and overall emergency care management.\n  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM,\norchestrated by CrewAI and Langchain. The system comprises four AI agents\nemulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED\nCoordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for\ntriage assessment and integrates with the RxNorm API for medication management.\n  The model was evaluated using the Asclepius dataset, with performance\nassessed by a clinical emergency medicine specialist. The CDSS demonstrated\nhigh accuracy in triage decision-making compared to the baseline of a\nsingle-agent system. Furthermore, the system exhibited strong performance in\ncritical areas, including primary diagnosis, critical findings identification,\ndisposition decision-making, treatment planning, and resource allocation.\n  Our multi-agent CDSS demonstrates significant potential for supporting\ncomprehensive emergency care management. By leveraging state-of-the-art AI\ntechnologies, this system offers a scalable and adaptable tool that could\nenhance emergency medical care delivery, potentially alleviating ED\novercrowding and improving patient outcomes. This work contributes to the\ngrowing field of AI applications in emergency medicine and offers a promising\ndirection for future research and clinical implementation."
                },
                "authors": [
                    {
                        "name": "Seungjun Han"
                    },
                    {
                        "name": "Wongyung Choi"
                    }
                ],
                "author_detail": {
                    "name": "Wongyung Choi"
                },
                "author": "Wongyung Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15133v1",
                "updated": "2024-08-27T15:13:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    13,
                    6,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T15:13:06Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    13,
                    6,
                    1,
                    240,
                    0
                ],
                "title": "Using LLMs for Explaining Sets of Counterfactual Examples to Final Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs for Explaining Sets of Counterfactual Examples to Final Users"
                },
                "summary": "Causality is vital for understanding true cause-and-effect relationships\nbetween variables within predictive models, rather than relying on mere\ncorrelations, making it highly relevant in the field of Explainable AI. In an\nautomated decision-making scenario, causal inference methods can analyze the\nunderlying data-generation process, enabling explanations of a model's decision\nby manipulating features and creating counterfactual examples. These\ncounterfactuals explore hypothetical scenarios where a minimal number of\nfactors are altered, providing end-users with valuable information on how to\nchange their situation. However, interpreting a set of multiple counterfactuals\ncan be challenging for end-users who are not used to analyzing raw data\nrecords. In our work, we propose a novel multi-step pipeline that uses\ncounterfactuals to generate natural language explanations of actions that will\nlead to a change in outcome in classifiers of tabular data using LLMs. This\npipeline is designed to guide the LLM through smaller tasks that mimic human\nreasoning when explaining a decision based on counterfactual cases. We\nconducted various experiments using a public dataset and proposed a method of\nclosed-loop evaluation to assess the coherence of the final explanation with\nthe counterfactuals, as well as the quality of the content. Results are\npromising, although further experiments with other datasets and human\nevaluations should be carried out.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causality is vital for understanding true cause-and-effect relationships\nbetween variables within predictive models, rather than relying on mere\ncorrelations, making it highly relevant in the field of Explainable AI. In an\nautomated decision-making scenario, causal inference methods can analyze the\nunderlying data-generation process, enabling explanations of a model's decision\nby manipulating features and creating counterfactual examples. These\ncounterfactuals explore hypothetical scenarios where a minimal number of\nfactors are altered, providing end-users with valuable information on how to\nchange their situation. However, interpreting a set of multiple counterfactuals\ncan be challenging for end-users who are not used to analyzing raw data\nrecords. In our work, we propose a novel multi-step pipeline that uses\ncounterfactuals to generate natural language explanations of actions that will\nlead to a change in outcome in classifiers of tabular data using LLMs. This\npipeline is designed to guide the LLM through smaller tasks that mimic human\nreasoning when explaining a decision based on counterfactual cases. We\nconducted various experiments using a public dataset and proposed a method of\nclosed-loop evaluation to assess the coherence of the final explanation with\nthe counterfactuals, as well as the quality of the content. Results are\npromising, although further experiments with other datasets and human\nevaluations should be carried out."
                },
                "authors": [
                    {
                        "name": "Arturo Fredes"
                    },
                    {
                        "name": "Jordi Vitria"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Vitria"
                },
                "author": "Jordi Vitria",
                "arxiv_comment": "Presented as a poster in the 2nd Workshop on Causal Inference and\n  Machine Learning in Practice at KDD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.16264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.16264v3",
                "updated": "2024-08-27T15:09:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    9,
                    40,
                    1,
                    240,
                    0
                ],
                "published": "2023-08-30T18:45:21Z",
                "published_parsed": [
                    2023,
                    8,
                    30,
                    18,
                    45,
                    21,
                    2,
                    242,
                    0
                ],
                "title": "Resource Placement for Rate and Fidelity Maximization in Quantum\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Placement for Rate and Fidelity Maximization in Quantum\n  Networks"
                },
                "summary": "Existing classical optical network infrastructure cannot be immediately used\nfor quantum network applications due to photon loss. The first step towards\nenabling quantum networks is the integration of quantum repeaters into optical\nnetworks. However, the expenses and intrinsic noise inherent in quantum\nhardware underscore the need for an efficient deployment strategy that\noptimizes the allocation of quantum repeaters and memories. In this paper, we\npresent a comprehensive framework for network planning, aiming to efficiently\ndistributing quantum repeaters across existing infrastructure, with the\nobjective of maximizing quantum network utility within an entanglement\ndistribution network. We apply our framework to several cases including a\npreliminary illustration of a dumbbell network topology and real-world cases of\nthe SURFnet and ESnet. We explore the effect of quantum memory multiplexing\nwithin quantum repeaters, as well as the influence of memory coherence time on\nquantum network utility. We further examine the effects of different fairness\nassumptions on network planning, uncovering their impacts on real-time network\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing classical optical network infrastructure cannot be immediately used\nfor quantum network applications due to photon loss. The first step towards\nenabling quantum networks is the integration of quantum repeaters into optical\nnetworks. However, the expenses and intrinsic noise inherent in quantum\nhardware underscore the need for an efficient deployment strategy that\noptimizes the allocation of quantum repeaters and memories. In this paper, we\npresent a comprehensive framework for network planning, aiming to efficiently\ndistributing quantum repeaters across existing infrastructure, with the\nobjective of maximizing quantum network utility within an entanglement\ndistribution network. We apply our framework to several cases including a\npreliminary illustration of a dumbbell network topology and real-world cases of\nthe SURFnet and ESnet. We explore the effect of quantum memory multiplexing\nwithin quantum repeaters, as well as the influence of memory coherence time on\nquantum network utility. We further examine the effects of different fairness\nassumptions on network planning, uncovering their impacts on real-time network\nperformance."
                },
                "authors": [
                    {
                        "name": "Shahrooz Pouryousef"
                    },
                    {
                        "name": "Hassan Shapourian"
                    },
                    {
                        "name": "Alireza Shabani"
                    },
                    {
                        "name": "Ramana Kompella"
                    },
                    {
                        "name": "Don Towsley"
                    }
                ],
                "author_detail": {
                    "name": "Don Towsley"
                },
                "author": "Don Towsley",
                "arxiv_comment": "18 pages, 8 figures, 3 appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.16264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.16264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13960v2",
                "updated": "2024-08-27T15:06:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    6,
                    17,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-25T23:48:11Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    23,
                    48,
                    11,
                    6,
                    238,
                    0
                ],
                "title": "Time Series Analysis for Education: Methods, Applications, and Future\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Analysis for Education: Methods, Applications, and Future\n  Directions"
                },
                "summary": "Recent advancements in the collection and analysis of sequential educational\ndata have brought time series analysis to a pivotal position in educational\nresearch, highlighting its essential role in facilitating data-driven\ndecision-making. However, there is a lack of comprehensive summaries that\nconsolidate these advancements. To the best of our knowledge, this paper is the\nfirst to provide a comprehensive review of time series analysis techniques\nspecifically within the educational context. We begin by exploring the\nlandscape of educational data analytics, categorizing various data sources and\ntypes relevant to education. We then review four prominent time series\nmethods-forecasting, classification, clustering, and anomaly\ndetection-illustrating their specific application points in educational\nsettings. Subsequently, we present a range of educational scenarios and\napplications, focusing on how these methods are employed to address diverse\neducational tasks, which highlights the practical integration of multiple time\nseries methods to solve complex educational problems. Finally, we conclude with\na discussion on future directions, including personalized learning analytics,\nmultimodal data fusion, and the role of large language models (LLMs) in\neducational time series. The contributions of this paper include a detailed\ntaxonomy of educational data, a synthesis of time series techniques with\nspecific educational applications, and a forward-looking perspective on\nemerging trends and future research opportunities in educational analysis. The\nrelated papers and resources are available and regularly updated at the project\npage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the collection and analysis of sequential educational\ndata have brought time series analysis to a pivotal position in educational\nresearch, highlighting its essential role in facilitating data-driven\ndecision-making. However, there is a lack of comprehensive summaries that\nconsolidate these advancements. To the best of our knowledge, this paper is the\nfirst to provide a comprehensive review of time series analysis techniques\nspecifically within the educational context. We begin by exploring the\nlandscape of educational data analytics, categorizing various data sources and\ntypes relevant to education. We then review four prominent time series\nmethods-forecasting, classification, clustering, and anomaly\ndetection-illustrating their specific application points in educational\nsettings. Subsequently, we present a range of educational scenarios and\napplications, focusing on how these methods are employed to address diverse\neducational tasks, which highlights the practical integration of multiple time\nseries methods to solve complex educational problems. Finally, we conclude with\na discussion on future directions, including personalized learning analytics,\nmultimodal data fusion, and the role of large language models (LLMs) in\neducational time series. The contributions of this paper include a detailed\ntaxonomy of educational data, a synthesis of time series techniques with\nspecific educational applications, and a forward-looking perspective on\nemerging trends and future research opportunities in educational analysis. The\nrelated papers and resources are available and regularly updated at the project\npage."
                },
                "authors": [
                    {
                        "name": "Shengzhong Mao"
                    },
                    {
                        "name": "Chaoli Zhang"
                    },
                    {
                        "name": "Yichi Song"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Xiao-Jun Zeng"
                    },
                    {
                        "name": "Zenglin Xu"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "24 pages, 3 figures, 6 tables, project page: see\n  https://github.com/ai-for-edu/time-series-analysis-for-education",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15116v1",
                "updated": "2024-08-27T14:55:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    55,
                    15,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T14:55:15Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    55,
                    15,
                    1,
                    240,
                    0
                ],
                "title": "Evaluating Stability of Unreflective Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Stability of Unreflective Alignment"
                },
                "summary": "Many theoretical obstacles to AI alignment are consequences of reflective\nstability - the problem of designing alignment mechanisms that the AI would not\ndisable if given the option. However, problems stemming from reflective\nstability are not obviously present in current LLMs, leading to disagreement\nover whether they will need to be solved to enable safe delegation of cognitive\nlabor. In this paper, we propose Counterfactual Priority Change (CPC)\ndestabilization as a mechanism by which reflective stability problems may arise\nin future LLMs. We describe two risk factors for CPC-destabilization: 1)\nCPC-based stepping back and 2) preference instability. We develop preliminary\nevaluations for each of these risk factors, and apply them to frontier LLMs.\nOur findings indicate that in current LLMs, increased scale and capability are\nassociated with increases in both CPC-based stepping back and preference\ninstability, suggesting that CPC-destabilization may cause reflective stability\nproblems in future LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many theoretical obstacles to AI alignment are consequences of reflective\nstability - the problem of designing alignment mechanisms that the AI would not\ndisable if given the option. However, problems stemming from reflective\nstability are not obviously present in current LLMs, leading to disagreement\nover whether they will need to be solved to enable safe delegation of cognitive\nlabor. In this paper, we propose Counterfactual Priority Change (CPC)\ndestabilization as a mechanism by which reflective stability problems may arise\nin future LLMs. We describe two risk factors for CPC-destabilization: 1)\nCPC-based stepping back and 2) preference instability. We develop preliminary\nevaluations for each of these risk factors, and apply them to frontier LLMs.\nOur findings indicate that in current LLMs, increased scale and capability are\nassociated with increases in both CPC-based stepping back and preference\ninstability, suggesting that CPC-destabilization may cause reflective stability\nproblems in future LLMs."
                },
                "authors": [
                    {
                        "name": "James Lucassen"
                    },
                    {
                        "name": "Mark Henry"
                    },
                    {
                        "name": "Philippa Wright"
                    },
                    {
                        "name": "Owen Yeung"
                    }
                ],
                "author_detail": {
                    "name": "Owen Yeung"
                },
                "author": "Owen Yeung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10146v2",
                "updated": "2024-08-27T14:51:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    51,
                    52,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-19T16:45:45Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    45,
                    45,
                    0,
                    232,
                    0
                ],
                "title": "Epitaxial Films and Devices of Transparent Conducting Oxides:\n  La:BaSnO$_3$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epitaxial Films and Devices of Transparent Conducting Oxides:\n  La:BaSnO$_3$"
                },
                "summary": "This paper reviews recent developments in materials science and device\nphysics of high-quality epitaxial films of the transparent perovskite La-doped\nbarium stannate, La:BaSnO$_3$. It presents current efforts in the synthesis\nscience of epitaxial La:BaSnO$_3$ films for achieving reduced defect densities\nand high electron mobility at room temperature. We discuss the scattering\nmechanisms and the route towards engineering defect-free epitaxial La:BaSnO$_3$\nheterostructures. By combining chemical surface characterization and electronic\ntransport studies, a special emphasis is laid on the proper correlation between\nthe transport properties and the electronic band structure of La:BaSnO$_3$\nfilms and heterostructures. For application purposes, interesting optical\nproperties of La:BaSnO$_3$ films are discussed. Finally, for their potential\napplication in oxide electronics, an overview of current progress in the\nfabrication of La:BaSnO$_3$-based thin-film field-effect transistors is\npresented together with recent progress in the the fundamental realization of\ntwo-dimensional electron gases with high electron mobility in\nLa:BaSnO$_3$-based heterostructures. Future experimental studies to reveal the\npotential deployment of La:BaSnO$_3$ films in optoelectronic and transparent\nelectronics are also discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper reviews recent developments in materials science and device\nphysics of high-quality epitaxial films of the transparent perovskite La-doped\nbarium stannate, La:BaSnO$_3$. It presents current efforts in the synthesis\nscience of epitaxial La:BaSnO$_3$ films for achieving reduced defect densities\nand high electron mobility at room temperature. We discuss the scattering\nmechanisms and the route towards engineering defect-free epitaxial La:BaSnO$_3$\nheterostructures. By combining chemical surface characterization and electronic\ntransport studies, a special emphasis is laid on the proper correlation between\nthe transport properties and the electronic band structure of La:BaSnO$_3$\nfilms and heterostructures. For application purposes, interesting optical\nproperties of La:BaSnO$_3$ films are discussed. Finally, for their potential\napplication in oxide electronics, an overview of current progress in the\nfabrication of La:BaSnO$_3$-based thin-film field-effect transistors is\npresented together with recent progress in the the fundamental realization of\ntwo-dimensional electron gases with high electron mobility in\nLa:BaSnO$_3$-based heterostructures. Future experimental studies to reveal the\npotential deployment of La:BaSnO$_3$ films in optoelectronic and transparent\nelectronics are also discussed."
                },
                "authors": [
                    {
                        "name": "Prosper Ngabonziza"
                    },
                    {
                        "name": "Arnaud P. Nono Tchiomo"
                    }
                ],
                "author_detail": {
                    "name": "Arnaud P. Nono Tchiomo"
                },
                "author": "Arnaud P. Nono Tchiomo",
                "arxiv_comment": "45 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.05195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.05195v2",
                "updated": "2024-08-27T14:20:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    20,
                    57,
                    1,
                    240,
                    0
                ],
                "published": "2023-11-09T08:19:34Z",
                "published_parsed": [
                    2023,
                    11,
                    9,
                    8,
                    19,
                    34,
                    3,
                    313,
                    0
                ],
                "title": "PRODIGy: a PROfile-based DIalogue Generation dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRODIGy: a PROfile-based DIalogue Generation dataset"
                },
                "summary": "Providing dialogue agents with a profile representation can improve their\nconsistency and coherence, leading to better conversations. However, current\nprofile-based dialogue datasets for training such agents contain either\nexplicit profile representations that are simple and dialogue-specific, or\nimplicit representations that are difficult to collect. In this work, we\npropose a unified framework in which we bring together both standard and more\nsophisticated profile representations by creating a new resource where each\ndialogue is aligned with all possible speaker representations such as\ncommunication style, biographies, and personality. This framework allows to\ntest several baselines built using generative language models with several\nprofile configurations. The automatic evaluation shows that profile-based\nmodels have better generalisation capabilities than models trained on dialogues\nonly, both in-domain and cross-domain settings. These results are consistent\nfor fine-tuned models and instruction-based LLMs. Additionally, human\nevaluation demonstrates a clear preference for generations consistent with both\nprofile and context. Finally, to account for possible privacy concerns, all\nexperiments are done under two configurations: inter-character and\nintra-character. In the former, the LM stores the information about the\ncharacter in its internal representation, while in the latter, the LM does not\nretain any personal information but uses it only at inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing dialogue agents with a profile representation can improve their\nconsistency and coherence, leading to better conversations. However, current\nprofile-based dialogue datasets for training such agents contain either\nexplicit profile representations that are simple and dialogue-specific, or\nimplicit representations that are difficult to collect. In this work, we\npropose a unified framework in which we bring together both standard and more\nsophisticated profile representations by creating a new resource where each\ndialogue is aligned with all possible speaker representations such as\ncommunication style, biographies, and personality. This framework allows to\ntest several baselines built using generative language models with several\nprofile configurations. The automatic evaluation shows that profile-based\nmodels have better generalisation capabilities than models trained on dialogues\nonly, both in-domain and cross-domain settings. These results are consistent\nfor fine-tuned models and instruction-based LLMs. Additionally, human\nevaluation demonstrates a clear preference for generations consistent with both\nprofile and context. Finally, to account for possible privacy concerns, all\nexperiments are done under two configurations: inter-character and\nintra-character. In the former, the LM stores the information about the\ncharacter in its internal representation, while in the latter, the LM does not\nretain any personal information but uses it only at inference time."
                },
                "authors": [
                    {
                        "name": "Daniela Occhipinti"
                    },
                    {
                        "name": "Serra Sinem Tekiroglu"
                    },
                    {
                        "name": "Marco Guerini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Guerini"
                },
                "author": "Marco Guerini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.05195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.05195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14340v2",
                "updated": "2024-08-27T14:09:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    9,
                    44,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-26T15:13:14Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    13,
                    14,
                    0,
                    239,
                    0
                ],
                "title": "Foundation Models for Music: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models for Music: A Survey"
                },
                "summary": "In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm."
                },
                "authors": [
                    {
                        "name": "Yinghao Ma"
                    },
                    {
                        "name": "Anders Øland"
                    },
                    {
                        "name": "Anton Ragni"
                    },
                    {
                        "name": "Bleiz MacSen Del Sette"
                    },
                    {
                        "name": "Charalampos Saitis"
                    },
                    {
                        "name": "Chris Donahue"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Christos Plachouras"
                    },
                    {
                        "name": "Emmanouil Benetos"
                    },
                    {
                        "name": "Elio Quinton"
                    },
                    {
                        "name": "Elona Shatri"
                    },
                    {
                        "name": "Fabio Morreale"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "György Fazekas"
                    },
                    {
                        "name": "Gus Xia"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Ilaria Manco"
                    },
                    {
                        "name": "Jiawen Huang"
                    },
                    {
                        "name": "Julien Guinot"
                    },
                    {
                        "name": "Liwei Lin"
                    },
                    {
                        "name": "Luca Marinelli"
                    },
                    {
                        "name": "Max W. Y. Lam"
                    },
                    {
                        "name": "Megha Sharma"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Roger B. Dannenberg"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Shangda Wu"
                    },
                    {
                        "name": "Shih-Lun Wu"
                    },
                    {
                        "name": "Shuqi Dai"
                    },
                    {
                        "name": "Shun Lei"
                    },
                    {
                        "name": "Shiyin Kang"
                    },
                    {
                        "name": "Simon Dixon"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xingjian Du"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Zeyue Tian"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Zhizheng Wu"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Ziyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ziyu Wang"
                },
                "author": "Ziyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15079v1",
                "updated": "2024-08-27T14:08:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    8,
                    23,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T14:08:23Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    8,
                    23,
                    1,
                    240,
                    0
                ],
                "title": "BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and\n  Deduplication by Introducing a Competitive Large Language Model Baseline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and\n  Deduplication by Introducing a Competitive Large Language Model Baseline"
                },
                "summary": "The general capabilities of Large Language Models (LLM) highly rely on the\ncomposition and selection on extensive pretraining datasets, treated as\ncommercial secrets by several institutions. To mitigate this issue, we\nopen-source the details of a universally applicable data processing pipeline\nand validate its effectiveness and potential by introducing a competitive LLM\nbaseline. Specifically, the data processing pipeline consists of broad\ncollection to scale up and reweighting to improve quality. We then pretrain a\n7B model BaichuanSEED with 3T tokens processed by our pipeline without any\ndeliberate downstream task-related optimization, followed by an easy but\neffective supervised fine-tuning stage. BaichuanSEED demonstrates consistency\nand predictability throughout training and achieves comparable performance on\ncomprehensive benchmarks with several commercial advanced large language\nmodels, such as Qwen1.5 and Llama3. We also conduct several heuristic\nexperiments to discuss the potential for further optimization of downstream\ntasks, such as mathematics and coding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The general capabilities of Large Language Models (LLM) highly rely on the\ncomposition and selection on extensive pretraining datasets, treated as\ncommercial secrets by several institutions. To mitigate this issue, we\nopen-source the details of a universally applicable data processing pipeline\nand validate its effectiveness and potential by introducing a competitive LLM\nbaseline. Specifically, the data processing pipeline consists of broad\ncollection to scale up and reweighting to improve quality. We then pretrain a\n7B model BaichuanSEED with 3T tokens processed by our pipeline without any\ndeliberate downstream task-related optimization, followed by an easy but\neffective supervised fine-tuning stage. BaichuanSEED demonstrates consistency\nand predictability throughout training and achieves comparable performance on\ncomprehensive benchmarks with several commercial advanced large language\nmodels, such as Qwen1.5 and Llama3. We also conduct several heuristic\nexperiments to discuss the potential for further optimization of downstream\ntasks, such as mathematics and coding."
                },
                "authors": [
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Yiding Sun"
                    },
                    {
                        "name": "Shusen Zhang"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Yanjun Shen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Haoze Sun"
                    },
                    {
                        "name": "Tianpeng Li"
                    },
                    {
                        "name": "Mingan Lin"
                    },
                    {
                        "name": "Jianhua Xu"
                    },
                    {
                        "name": "Yufan Zhang"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Lei Su"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Jiaxin Mao"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "19 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15066v1",
                "updated": "2024-08-27T13:50:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    50,
                    37,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T13:50:37Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    50,
                    37,
                    1,
                    240,
                    0
                ],
                "title": "Constraining Participation: Affordances of Feedback Features in\n  Interfaces to Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining Participation: Affordances of Feedback Features in\n  Interfaces to Large Language Models"
                },
                "summary": "Large language models (LLMs) are now accessible to anyone with a computer, a\nweb browser, and an internet connection via browser-based interfaces, shifting\nthe dynamics of participation in AI development. This paper examines the\naffordances of interactive feedback features in ChatGPT's interface, analysing\nhow they shape user input and participation in LLM iteration. Drawing on a\nsurvey of ChatGPT users and applying the mechanisms and conditions framework of\naffordances, we demonstrate that these features encourage simple, frequent, and\nperformance-focused feedback while discouraging collective input and\ndiscussions among users. We argue that this feedback format significantly\nconstrains user participation, reinforcing power imbalances between users, the\npublic, and companies developing LLMs. Our analysis contributes to the growing\nbody of literature on participatory AI by critically examining the limitations\nof existing feedback processes and proposing directions for their redesign. To\nenable more meaningful public participation in AI development, we advocate for\na shift away from processes focused on aligning model outputs with specific\nuser preferences. Instead, we emphasise the need for processes that facilitate\ndialogue between companies and diverse 'publics' about the purpose and\napplications of LLMs. This approach requires attention to the ongoing work of\ninfrastructuring - creating and sustaining the social, technical, and\ninstitutional structures necessary to address matters of concern to groups\nimpacted by AI development and deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are now accessible to anyone with a computer, a\nweb browser, and an internet connection via browser-based interfaces, shifting\nthe dynamics of participation in AI development. This paper examines the\naffordances of interactive feedback features in ChatGPT's interface, analysing\nhow they shape user input and participation in LLM iteration. Drawing on a\nsurvey of ChatGPT users and applying the mechanisms and conditions framework of\naffordances, we demonstrate that these features encourage simple, frequent, and\nperformance-focused feedback while discouraging collective input and\ndiscussions among users. We argue that this feedback format significantly\nconstrains user participation, reinforcing power imbalances between users, the\npublic, and companies developing LLMs. Our analysis contributes to the growing\nbody of literature on participatory AI by critically examining the limitations\nof existing feedback processes and proposing directions for their redesign. To\nenable more meaningful public participation in AI development, we advocate for\na shift away from processes focused on aligning model outputs with specific\nuser preferences. Instead, we emphasise the need for processes that facilitate\ndialogue between companies and diverse 'publics' about the purpose and\napplications of LLMs. This approach requires attention to the ongoing work of\ninfrastructuring - creating and sustaining the social, technical, and\ninstitutional structures necessary to address matters of concern to groups\nimpacted by AI development and deployment."
                },
                "authors": [
                    {
                        "name": "Ned Cooper"
                    },
                    {
                        "name": "Alexandra Zafiroglu"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Zafiroglu"
                },
                "author": "Alexandra Zafiroglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14745v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14745v3",
                "updated": "2024-08-27T13:36:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    36,
                    12,
                    1,
                    240,
                    0
                ],
                "published": "2024-04-23T04:54:32Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    4,
                    54,
                    32,
                    1,
                    114,
                    0
                ],
                "title": "TAAT: Think and Act from Arbitrary Texts in Text2Motion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAAT: Think and Act from Arbitrary Texts in Text2Motion"
                },
                "summary": "Text to Motion aims to generate human motions from texts. Existing settings\nassume that texts include action labels, which limits flexibility in practical\nscenarios. This paper extends this task with a more realistic assumption that\nthe texts are arbitrary. Specifically, in our setting, arbitrary texts include\nexisting action texts composed of action labels and introduce scene texts\nwithout explicit action labels. To address this practical issue, we extend the\naction texts in the HUMANML3D dataset by incorporating additional scene texts,\nthereby creating a new dataset, HUMANML3D++. Concurrently, we propose a simple\nframework that extracts action representations from arbitrary texts using a\nLarge Language Model (LLM) and subsequently generates motions. Furthermore, we\nenhance the existing evaluation methodologies to address their inadequacies.\nExtensive experiments are conducted under different application scenarios to\nvalidate the effectiveness of the proposed framework on existing and proposed\ndatasets. The results indicate that Text to Motion in this realistic setting is\nvery challenging, fostering new research in this practical direction. Our\ndataset and code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text to Motion aims to generate human motions from texts. Existing settings\nassume that texts include action labels, which limits flexibility in practical\nscenarios. This paper extends this task with a more realistic assumption that\nthe texts are arbitrary. Specifically, in our setting, arbitrary texts include\nexisting action texts composed of action labels and introduce scene texts\nwithout explicit action labels. To address this practical issue, we extend the\naction texts in the HUMANML3D dataset by incorporating additional scene texts,\nthereby creating a new dataset, HUMANML3D++. Concurrently, we propose a simple\nframework that extracts action representations from arbitrary texts using a\nLarge Language Model (LLM) and subsequently generates motions. Furthermore, we\nenhance the existing evaluation methodologies to address their inadequacies.\nExtensive experiments are conducted under different application scenarios to\nvalidate the effectiveness of the proposed framework on existing and proposed\ndatasets. The results indicate that Text to Motion in this realistic setting is\nvery challenging, fostering new research in this practical direction. Our\ndataset and code will be released."
                },
                "authors": [
                    {
                        "name": "Runqi Wang"
                    },
                    {
                        "name": "Caoyuan Ma"
                    },
                    {
                        "name": "Guopeng Li"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang",
                "arxiv_comment": "Updated errors in author information",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14745v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14745v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05520v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05520v3",
                "updated": "2024-08-27T13:17:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    17,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-04-08T13:41:32Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    13,
                    41,
                    32,
                    0,
                    99,
                    0
                ],
                "title": "The Fact Selection Problem in LLM-Based Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fact Selection Problem in LLM-Based Program Repair"
                },
                "summary": "Recent research has shown that incorporating bug-related facts, such as stack\ntraces and GitHub issues, into prompts enhances the bug-fixing capabilities of\nlarge language models (LLMs). Considering the ever-increasing context window of\nthese models, a critical question arises: what and how many facts should be\nincluded in prompts to maximise the chance of correctly fixing bugs? To answer\nthis question, we conducted a large-scale study, employing over 19K prompts\nfeaturing various combinations of seven diverse facts to rectify 314 bugs from\nopen-source Python projects within the BugsInPy benchmark. Our findings\nrevealed that each fact, ranging from simple syntactic details like code\ncontext to semantic information previously unexplored in the context of LLMs\nsuch as angelic values, is beneficial. Specifically, each fact aids in fixing\nsome bugs that would remain unresolved or only be fixed with a low success rate\nwithout it. Importantly, we discovered that the effectiveness of program repair\nprompts is non-monotonic over the number of used facts; using too many facts\nleads to subpar outcomes. These insights led us to define the fact selection\nproblem: determining the optimal set of facts for inclusion in a prompt to\nmaximise LLM's performance on a given task instance. We found that there is no\none-size-fits-all set of facts for bug repair. Therefore, we developed a basic\nstatistical model, named Maniple, which selects facts specific to a given bug\nto include in the prompt. This model significantly surpasses the performance of\nthe best generic fact set. To underscore the significance of the fact selection\nproblem, we benchmarked Maniple against the state-of-the-art zero-shot,\nnon-conversational LLM-based bug repair methods. On our testing dataset of 157\nbugs, Maniple repairs 88 bugs, 17% above the best configuration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that incorporating bug-related facts, such as stack\ntraces and GitHub issues, into prompts enhances the bug-fixing capabilities of\nlarge language models (LLMs). Considering the ever-increasing context window of\nthese models, a critical question arises: what and how many facts should be\nincluded in prompts to maximise the chance of correctly fixing bugs? To answer\nthis question, we conducted a large-scale study, employing over 19K prompts\nfeaturing various combinations of seven diverse facts to rectify 314 bugs from\nopen-source Python projects within the BugsInPy benchmark. Our findings\nrevealed that each fact, ranging from simple syntactic details like code\ncontext to semantic information previously unexplored in the context of LLMs\nsuch as angelic values, is beneficial. Specifically, each fact aids in fixing\nsome bugs that would remain unresolved or only be fixed with a low success rate\nwithout it. Importantly, we discovered that the effectiveness of program repair\nprompts is non-monotonic over the number of used facts; using too many facts\nleads to subpar outcomes. These insights led us to define the fact selection\nproblem: determining the optimal set of facts for inclusion in a prompt to\nmaximise LLM's performance on a given task instance. We found that there is no\none-size-fits-all set of facts for bug repair. Therefore, we developed a basic\nstatistical model, named Maniple, which selects facts specific to a given bug\nto include in the prompt. This model significantly surpasses the performance of\nthe best generic fact set. To underscore the significance of the fact selection\nproblem, we benchmarked Maniple against the state-of-the-art zero-shot,\nnon-conversational LLM-based bug repair methods. On our testing dataset of 157\nbugs, Maniple repairs 88 bugs, 17% above the best configuration."
                },
                "authors": [
                    {
                        "name": "Nikhil Parasaram"
                    },
                    {
                        "name": "Huijie Yan"
                    },
                    {
                        "name": "Boyu Yang"
                    },
                    {
                        "name": "Zineb Flahy"
                    },
                    {
                        "name": "Abriele Qudsi"
                    },
                    {
                        "name": "Damian Ziaber"
                    },
                    {
                        "name": "Earl Barr"
                    },
                    {
                        "name": "Sergey Mechtaev"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Mechtaev"
                },
                "author": "Sergey Mechtaev",
                "arxiv_comment": "Code, scripts and data necessary to reproduce this work are available\n  at https://github.com/PyRepair/maniple",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05520v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05520v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15045v2",
                "updated": "2024-08-28T08:32:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    32,
                    44,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-27T13:13:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    13,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "DocLayLLM: An Efficient and Effective Multi-modal Extension of Large\n  Language Models for Text-rich Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocLayLLM: An Efficient and Effective Multi-modal Extension of Large\n  Language Models for Text-rich Document Understanding"
                },
                "summary": "Text-rich document understanding (TDU) refers to analyzing and comprehending\ndocuments containing substantial textual content. With the rapid evolution of\nlarge language models (LLMs), they have been widely leveraged for TDU due to\ntheir remarkable versatility and generalization. In this paper, we introduce\nDocLayLLM, an efficient and effective multi-modal extension of LLMs\nspecifically designed for TDU. By integrating visual patch tokens and 2D\npositional tokens into LLMs and encoding the document content using the LLMs\nthemselves, we fully take advantage of the document comprehension capability of\nLLMs and enhance their perception of OCR information. We have also deeply\nconsidered the role of the chain-of-thought (CoT) and innovatively proposed the\ntechniques of CoT Pre-training and CoT Annealing. Our DocLayLLM can achieve\nremarkable performances with lightweight training settings, showcasing its\nefficiency and effectiveness. Experimental results demonstrate that our\nDocLayLLM surpasses existing OCR-dependent methods and also outperforms\nOCR-free competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-rich document understanding (TDU) refers to analyzing and comprehending\ndocuments containing substantial textual content. With the rapid evolution of\nlarge language models (LLMs), they have been widely leveraged for TDU due to\ntheir remarkable versatility and generalization. In this paper, we introduce\nDocLayLLM, an efficient and effective multi-modal extension of LLMs\nspecifically designed for TDU. By integrating visual patch tokens and 2D\npositional tokens into LLMs and encoding the document content using the LLMs\nthemselves, we fully take advantage of the document comprehension capability of\nLLMs and enhance their perception of OCR information. We have also deeply\nconsidered the role of the chain-of-thought (CoT) and innovatively proposed the\ntechniques of CoT Pre-training and CoT Annealing. Our DocLayLLM can achieve\nremarkable performances with lightweight training settings, showcasing its\nefficiency and effectiveness. Experimental results demonstrate that our\nDocLayLLM surpasses existing OCR-dependent methods and also outperforms\nOCR-free competitors."
                },
                "authors": [
                    {
                        "name": "Wenhui Liao"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Hongliang Li"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Lianwen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Lianwen Jin"
                },
                "author": "Lianwen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15040v2",
                "updated": "2024-08-28T03:56:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    56,
                    37,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-27T13:10:05Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    10,
                    5,
                    1,
                    240,
                    0
                ],
                "title": "A Survey of Large Language Models for European Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Models for European Languages"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention due to their\nhigh performance on a wide range of natural language tasks since the release of\nChatGPT. The LLMs learn to understand and generate language by training\nbillions of model parameters on vast volumes of text data. Despite being a\nrelatively new field, LLM research is rapidly advancing in various directions.\nIn this paper, we present an overview of LLM families, including LLaMA, PaLM,\nGPT, and MoE, and the methods developed to create and enhance LLMs for official\nEuropean Union (EU) languages. We provide a comprehensive summary of common\nmonolingual and multilingual datasets used for pretraining large language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention due to their\nhigh performance on a wide range of natural language tasks since the release of\nChatGPT. The LLMs learn to understand and generate language by training\nbillions of model parameters on vast volumes of text data. Despite being a\nrelatively new field, LLM research is rapidly advancing in various directions.\nIn this paper, we present an overview of LLM families, including LLaMA, PaLM,\nGPT, and MoE, and the methods developed to create and enhance LLMs for official\nEuropean Union (EU) languages. We provide a comprehensive summary of common\nmonolingual and multilingual datasets used for pretraining large language\nmodels."
                },
                "authors": [
                    {
                        "name": "Wazir Ali"
                    },
                    {
                        "name": "Sampo Pyysalo"
                    }
                ],
                "author_detail": {
                    "name": "Sampo Pyysalo"
                },
                "author": "Sampo Pyysalo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15037v1",
                "updated": "2024-08-27T13:07:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    7,
                    7,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T13:07:07Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    7,
                    7,
                    1,
                    240,
                    0
                ],
                "title": "Evidence-Enhanced Triplet Generation Framework for Hallucination\n  Alleviation in Generative Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence-Enhanced Triplet Generation Framework for Hallucination\n  Alleviation in Generative Question Answering"
                },
                "summary": "To address the hallucination in generative question answering (GQA) where the\nanswer can not be derived from the document, we propose a novel\nevidence-enhanced triplet generation framework, EATQA, encouraging the model to\npredict all the combinations of (Question, Evidence, Answer) triplet by\nflipping the source pair and the target label to understand their logical\nrelationships, i.e., predict Answer(A), Question(Q), and Evidence(E) given a\nQE, EA, and QA pairs, respectively. Furthermore, we bridge the distribution gap\nto distill the knowledge from evidence in inference stage. Our framework\nensures the model to learn the logical relation between query, evidence and\nanswer, which simultaneously improves the evidence generation and query\nanswering. In this paper, we apply EATQA to LLama and it outperforms other\nLLMs-based methods and hallucination mitigation approaches on two challenging\nGQA benchmarks. Further analysis shows that our method not only keeps prior\nknowledge within LLM, but also mitigates hallucination and generates faithful\nanswers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the hallucination in generative question answering (GQA) where the\nanswer can not be derived from the document, we propose a novel\nevidence-enhanced triplet generation framework, EATQA, encouraging the model to\npredict all the combinations of (Question, Evidence, Answer) triplet by\nflipping the source pair and the target label to understand their logical\nrelationships, i.e., predict Answer(A), Question(Q), and Evidence(E) given a\nQE, EA, and QA pairs, respectively. Furthermore, we bridge the distribution gap\nto distill the knowledge from evidence in inference stage. Our framework\nensures the model to learn the logical relation between query, evidence and\nanswer, which simultaneously improves the evidence generation and query\nanswering. In this paper, we apply EATQA to LLama and it outperforms other\nLLMs-based methods and hallucination mitigation approaches on two challenging\nGQA benchmarks. Further analysis shows that our method not only keeps prior\nknowledge within LLM, but also mitigates hallucination and generates faithful\nanswers."
                },
                "authors": [
                    {
                        "name": "Haowei Du"
                    },
                    {
                        "name": "Huishuai Zhang"
                    },
                    {
                        "name": "Dongyan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongyan Zhao"
                },
                "author": "Dongyan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14972v1",
                "updated": "2024-08-27T11:24:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    11,
                    24,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T11:24:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    11,
                    24,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "AgentMonitor: A Plug-and-Play Framework for Predictive and Secure\n  Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentMonitor: A Plug-and-Play Framework for Predictive and Secure\n  Multi-Agent Systems"
                },
                "summary": "The rapid advancement of large language models (LLMs) has led to the rise of\nLLM-based agents. Recent research shows that multi-agent systems (MAS), where\neach agent plays a specific role, can outperform individual LLMs. However,\nconfiguring an MAS for a task remains challenging, with performance only\nobservable post-execution. Inspired by scaling laws in LLM development, we\ninvestigate whether MAS performance can be predicted beforehand. We introduce\nAgentMonitor, a framework that integrates at the agent level to capture inputs\nand outputs, transforming them into statistics for training a regression model\nto predict task performance. Additionally, it can further apply real-time\ncorrections to address security risks posed by malicious agents, mitigating\nnegative impacts and enhancing MAS security. Experiments demonstrate that an\nXGBoost model achieves a Spearman correlation of 0.89 in-domain and 0.58 in\nmore challenging scenarios. Furthermore, using AgentMonitor reduces harmful\ncontent by 6.2% and increases helpful content by 1.8% on average, enhancing\nsafety and reliability. Code is available at\n\\url{https://github.com/chanchimin/AgentMonitor}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has led to the rise of\nLLM-based agents. Recent research shows that multi-agent systems (MAS), where\neach agent plays a specific role, can outperform individual LLMs. However,\nconfiguring an MAS for a task remains challenging, with performance only\nobservable post-execution. Inspired by scaling laws in LLM development, we\ninvestigate whether MAS performance can be predicted beforehand. We introduce\nAgentMonitor, a framework that integrates at the agent level to capture inputs\nand outputs, transforming them into statistics for training a regression model\nto predict task performance. Additionally, it can further apply real-time\ncorrections to address security risks posed by malicious agents, mitigating\nnegative impacts and enhancing MAS security. Experiments demonstrate that an\nXGBoost model achieves a Spearman correlation of 0.89 in-domain and 0.58 in\nmore challenging scenarios. Furthermore, using AgentMonitor reduces harmful\ncontent by 6.2% and increases helpful content by 1.8% on average, enhancing\nsafety and reliability. Code is available at\n\\url{https://github.com/chanchimin/AgentMonitor}."
                },
                "authors": [
                    {
                        "name": "Chi-Min Chan"
                    },
                    {
                        "name": "Jianxuan Yu"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Chunyang Jiang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14964v1",
                "updated": "2024-08-27T11:10:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    11,
                    10,
                    39,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T11:10:39Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    11,
                    10,
                    39,
                    1,
                    240,
                    0
                ],
                "title": "Cross-Modal Learning for Chemistry Property Prediction: Large Language\n  Models Meet Graph Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Modal Learning for Chemistry Property Prediction: Large Language\n  Models Meet Graph Machine Learning"
                },
                "summary": "In the field of chemistry, the objective is to create novel molecules with\ndesired properties, facilitating accurate property predictions for applications\nsuch as material design and drug screening. However, existing graph deep\nlearning methods face limitations that curb their expressive power. To address\nthis, we explore the integration of vast molecular domain knowledge from Large\nLanguage Models (LLMs) with the complementary strengths of Graph Neural\nNetworks (GNNs) to enhance performance in property prediction tasks. We\nintroduce a Multi-Modal Fusion (MMF) framework that synergistically harnesses\nthe analytical prowess of GNNs and the linguistic generative and predictive\nabilities of LLMs, thereby improving accuracy and robustness in predicting\nmolecular properties. Our framework combines the effectiveness of GNNs in\nmodeling graph-structured data with the zero-shot and few-shot learning\ncapabilities of LLMs, enabling improved predictions while reducing the risk of\noverfitting. Furthermore, our approach effectively addresses distributional\nshifts, a common challenge in real-world applications, and showcases the\nefficacy of learning cross-modal representations, surpassing state-of-the-art\nbaselines on benchmark datasets for property prediction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of chemistry, the objective is to create novel molecules with\ndesired properties, facilitating accurate property predictions for applications\nsuch as material design and drug screening. However, existing graph deep\nlearning methods face limitations that curb their expressive power. To address\nthis, we explore the integration of vast molecular domain knowledge from Large\nLanguage Models (LLMs) with the complementary strengths of Graph Neural\nNetworks (GNNs) to enhance performance in property prediction tasks. We\nintroduce a Multi-Modal Fusion (MMF) framework that synergistically harnesses\nthe analytical prowess of GNNs and the linguistic generative and predictive\nabilities of LLMs, thereby improving accuracy and robustness in predicting\nmolecular properties. Our framework combines the effectiveness of GNNs in\nmodeling graph-structured data with the zero-shot and few-shot learning\ncapabilities of LLMs, enabling improved predictions while reducing the risk of\noverfitting. Furthermore, our approach effectively addresses distributional\nshifts, a common challenge in real-world applications, and showcases the\nefficacy of learning cross-modal representations, surpassing state-of-the-art\nbaselines on benchmark datasets for property prediction tasks."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "arxiv_comment": "Paper Accepted at Workshop on Robustness of Few-shot and Zero-shot\n  Learning in Foundation Models at NeurIPS 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14963v1",
                "updated": "2024-08-27T11:10:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    11,
                    10,
                    37,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T11:10:37Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    11,
                    10,
                    37,
                    1,
                    240,
                    0
                ],
                "title": "The future of offshore wind power production: wake and climate impacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The future of offshore wind power production: wake and climate impacts"
                },
                "summary": "Rapid deployment of offshore wind is expected within the coming decades to\nhelp meet climate goals. With offshore wind turbine lifetimes of 25-30 years,\nand new offshore leases spanning 60 years, it is vital to consider long-term\nchanges in potential wind power resource at the farm planning stage. Such\nchanges may arise from multiple sources, including climate change, and\nincreasing wake-induced power losses. In this work, we investigate and compare\nthese two sources of long-term change in wind power, for a case study\nconsisting of 21 wind farms within the German Bight. Consistent with previous\nstudies, we find a small but significant reduction in wind resource due to\nclimate change by the end of the 21st century under the high-emission RCP8.5\nscenario, compared with a historical period, with a mean power reduction (over\nan ensemble of seven climate models) of 2.1%. To assess the impact of\nwake-induced losses due to increasingly dense farm build-out, we model wakes\nwithin the German Bight region using an engineering wake model, under various\nstages of (planned) build-out corresponding to the years 2010-2027. By\nidentifying clusters of wind farms, we decompose wake effects into long-range\n(inter-cluster), medium-range (intra-cluster) and short-range (intra-farm)\neffects. Inter-cluster wake-induced losses increase from 0 for the 2010\nscenario to 2.5% for the 2027 scenario, with intra-cluster losses also\nincreasing from 0 to 4.3%. Intra-farm losses are relatively constant, at around\n13%. While the evolution of wake effects therefore outweighs the climate\neffect, and impacts over a shorter timescale, both factors are significant. We\nalso find evidence of non-linear interactions between the climate and wake\neffects. Both climate change and evolving wake effects must therefore be\nconsidered within resource assessment and wind farm planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid deployment of offshore wind is expected within the coming decades to\nhelp meet climate goals. With offshore wind turbine lifetimes of 25-30 years,\nand new offshore leases spanning 60 years, it is vital to consider long-term\nchanges in potential wind power resource at the farm planning stage. Such\nchanges may arise from multiple sources, including climate change, and\nincreasing wake-induced power losses. In this work, we investigate and compare\nthese two sources of long-term change in wind power, for a case study\nconsisting of 21 wind farms within the German Bight. Consistent with previous\nstudies, we find a small but significant reduction in wind resource due to\nclimate change by the end of the 21st century under the high-emission RCP8.5\nscenario, compared with a historical period, with a mean power reduction (over\nan ensemble of seven climate models) of 2.1%. To assess the impact of\nwake-induced losses due to increasingly dense farm build-out, we model wakes\nwithin the German Bight region using an engineering wake model, under various\nstages of (planned) build-out corresponding to the years 2010-2027. By\nidentifying clusters of wind farms, we decompose wake effects into long-range\n(inter-cluster), medium-range (intra-cluster) and short-range (intra-farm)\neffects. Inter-cluster wake-induced losses increase from 0 for the 2010\nscenario to 2.5% for the 2027 scenario, with intra-cluster losses also\nincreasing from 0 to 4.3%. Intra-farm losses are relatively constant, at around\n13%. While the evolution of wake effects therefore outweighs the climate\neffect, and impacts over a shorter timescale, both factors are significant. We\nalso find evidence of non-linear interactions between the climate and wake\neffects. Both climate change and evolving wake effects must therefore be\nconsidered within resource assessment and wind farm planning."
                },
                "authors": [
                    {
                        "name": "Simon C Warder"
                    },
                    {
                        "name": "Matthew D Piggott"
                    }
                ],
                "author_detail": {
                    "name": "Matthew D Piggott"
                },
                "author": "Matthew D Piggott",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15504v2",
                "updated": "2024-08-27T10:07:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    10,
                    7,
                    27,
                    1,
                    240,
                    0
                ],
                "published": "2024-06-19T16:43:56Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    16,
                    43,
                    56,
                    2,
                    171,
                    0
                ],
                "title": "Dr.E Bridges Graphs with Large Language Models through Words",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dr.E Bridges Graphs with Large Language Models through Words"
                },
                "summary": "Significant efforts have been dedicated to integrating the powerful Large\nLanguage Models (LLMs) with diverse modalities, particularly focusing on the\nfusion of language, vision and audio data. However, the graph-structured data,\nwhich is inherently rich in structural and domain-specific knowledge, has not\nyet been gracefully adapted to LLMs. Existing methods either describe the graph\nwith raw text, suffering the loss of graph structural information, or feed\nGraph Neural Network (GNN) embeddings into LLMs at the cost of losing\nexplainable prompt semantics. To bridge this gap, we introduce an end-to-end\nmodality-aligning framework for LLM-graph alignment: Dual-Residual Vector\nQuantized-Variational AutoEncoder, namely Dr.E. Our approach is purposefully\ndesigned to facilitate token-level alignment with LLMs, enabling an effective\ntranslation of the intrinsic `language' of graphs into comprehensible natural\nlanguage. We also manage to enhance LLMs' more robust structural understanding\nof graphs by incorporating multiple views of the central nodes based on their\nsurrounding nodes at various distances. Our experimental evaluations on\nstandard graph tasks demonstrate competitive performance against other\nstate-of-the-art (SOTA) approaches. Additionally, our framework ensures certain\nvisual interpretability, efficiency, and robustness, marking the promising\nsuccessful endeavor to achieve token-level alignment between LLMs and GNNs. Our\ncode is available at: https://anonymous.4open.science/r/dre-817.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant efforts have been dedicated to integrating the powerful Large\nLanguage Models (LLMs) with diverse modalities, particularly focusing on the\nfusion of language, vision and audio data. However, the graph-structured data,\nwhich is inherently rich in structural and domain-specific knowledge, has not\nyet been gracefully adapted to LLMs. Existing methods either describe the graph\nwith raw text, suffering the loss of graph structural information, or feed\nGraph Neural Network (GNN) embeddings into LLMs at the cost of losing\nexplainable prompt semantics. To bridge this gap, we introduce an end-to-end\nmodality-aligning framework for LLM-graph alignment: Dual-Residual Vector\nQuantized-Variational AutoEncoder, namely Dr.E. Our approach is purposefully\ndesigned to facilitate token-level alignment with LLMs, enabling an effective\ntranslation of the intrinsic `language' of graphs into comprehensible natural\nlanguage. We also manage to enhance LLMs' more robust structural understanding\nof graphs by incorporating multiple views of the central nodes based on their\nsurrounding nodes at various distances. Our experimental evaluations on\nstandard graph tasks demonstrate competitive performance against other\nstate-of-the-art (SOTA) approaches. Additionally, our framework ensures certain\nvisual interpretability, efficiency, and robustness, marking the promising\nsuccessful endeavor to achieve token-level alignment between LLMs and GNNs. Our\ncode is available at: https://anonymous.4open.science/r/dre-817."
                },
                "authors": [
                    {
                        "name": "Zipeng Liu"
                    },
                    {
                        "name": "Likang Wu"
                    },
                    {
                        "name": "Ming He"
                    },
                    {
                        "name": "Zhong Guan"
                    },
                    {
                        "name": "Hongke Zhao"
                    },
                    {
                        "name": "Nan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Nan Feng"
                },
                "author": "Nan Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.12084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.12084v2",
                "updated": "2024-08-27T09:55:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    55,
                    37,
                    1,
                    240,
                    0
                ],
                "published": "2023-11-20T11:08:06Z",
                "published_parsed": [
                    2023,
                    11,
                    20,
                    11,
                    8,
                    6,
                    0,
                    324,
                    0
                ],
                "title": "ODDR: Outlier Detection & Dimension Reduction Based Defense Against\n  Adversarial Patches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ODDR: Outlier Detection & Dimension Reduction Based Defense Against\n  Adversarial Patches"
                },
                "summary": "Adversarial attacks present a significant challenge to the dependable\ndeployment of machine learning models, with patch-based attacks being\nparticularly potent. These attacks introduce adversarial perturbations in\nlocalized regions of an image, deceiving even well-trained models. In this\npaper, we propose Outlier Detection and Dimension Reduction (ODDR), a\ncomprehensive defense strategy engineered to counteract patch-based adversarial\nattacks through advanced statistical methodologies. Our approach is based on\nthe observation that input features corresponding to adversarial\npatches-whether naturalistic or synthetic-deviate from the intrinsic\ndistribution of the remaining image data and can thus be identified as\noutliers. ODDR operates through a robust three-stage pipeline: Fragmentation,\nSegregation, and Neutralization. This model-agnostic framework is versatile,\noffering protection across various tasks, including image classification,\nobject detection, and depth estimation, and is proved effective in both\nCNN-based and Transformer-based architectures. In the Fragmentation stage,\nimage samples are divided into smaller segments, preparing them for the\nSegregation stage, where advanced outlier detection techniques isolate\nanomalous features linked to adversarial perturbations. The Neutralization\nstage then applies dimension reduction techniques to these outliers,\neffectively neutralizing the adversarial impact while preserving critical\ninformation for the machine learning task. Extensive evaluation on benchmark\ndatasets against state-of-the-art adversarial patches underscores the efficacy\nof ODDR. Our method enhances model accuracy from 39.26% to 79.1% under the\nGoogleAp attack, outperforming leading defenses such as LGS (53.86%), Jujutsu\n(60%), and Jedi (64.34%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial attacks present a significant challenge to the dependable\ndeployment of machine learning models, with patch-based attacks being\nparticularly potent. These attacks introduce adversarial perturbations in\nlocalized regions of an image, deceiving even well-trained models. In this\npaper, we propose Outlier Detection and Dimension Reduction (ODDR), a\ncomprehensive defense strategy engineered to counteract patch-based adversarial\nattacks through advanced statistical methodologies. Our approach is based on\nthe observation that input features corresponding to adversarial\npatches-whether naturalistic or synthetic-deviate from the intrinsic\ndistribution of the remaining image data and can thus be identified as\noutliers. ODDR operates through a robust three-stage pipeline: Fragmentation,\nSegregation, and Neutralization. This model-agnostic framework is versatile,\noffering protection across various tasks, including image classification,\nobject detection, and depth estimation, and is proved effective in both\nCNN-based and Transformer-based architectures. In the Fragmentation stage,\nimage samples are divided into smaller segments, preparing them for the\nSegregation stage, where advanced outlier detection techniques isolate\nanomalous features linked to adversarial perturbations. The Neutralization\nstage then applies dimension reduction techniques to these outliers,\neffectively neutralizing the adversarial impact while preserving critical\ninformation for the machine learning task. Extensive evaluation on benchmark\ndatasets against state-of-the-art adversarial patches underscores the efficacy\nof ODDR. Our method enhances model accuracy from 39.26% to 79.1% under the\nGoogleAp attack, outperforming leading defenses such as LGS (53.86%), Jujutsu\n(60%), and Jedi (64.34%)."
                },
                "authors": [
                    {
                        "name": "Nandish Chattopadhyay"
                    },
                    {
                        "name": "Amira Guesmi"
                    },
                    {
                        "name": "Muhammad Abdullah Hanif"
                    },
                    {
                        "name": "Bassem Ouni"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.12084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.12084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14909v1",
                "updated": "2024-08-27T09:35:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    35,
                    49,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:35:49Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    35,
                    49,
                    1,
                    240,
                    0
                ],
                "title": "SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking\n  State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking\n  State Space Models"
                },
                "summary": "Known as low energy consumption networks, spiking neural networks (SNNs) have\ngained a lot of attention within the past decades. While SNNs are increasing\ncompetitive with artificial neural networks (ANNs) for vision tasks, they are\nrarely used for long sequence tasks, despite their intrinsic temporal dynamics.\nIn this work, we develop spiking state space models (SpikingSSMs) for long\nsequence learning by leveraging on the sequence learning abilities of state\nspace models (SSMs). Inspired by dendritic neuron structure, we hierarchically\nintegrate neuronal dynamics with the original SSM block, meanwhile realizing\nsparse synaptic computation. Furthermore, to solve the conflict of event-driven\nneuronal dynamics with parallel computing, we propose a light-weight surrogate\ndynamic network which accurately predicts the after-reset membrane potential\nand compatible to learnable thresholds, enabling orders of acceleration in\ntraining speed compared with conventional iterative methods. On the long range\narena benchmark task, SpikingSSM achieves competitive performance to\nstate-of-the-art SSMs meanwhile realizing on average 90\\% of network sparsity.\nOn language modeling, our network significantly surpasses existing spiking\nlarge language models (spikingLLMs) on the WikiText-103 dataset with only a\nthird of the model size, demonstrating its potential as backbone architecture\nfor low computation cost LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Known as low energy consumption networks, spiking neural networks (SNNs) have\ngained a lot of attention within the past decades. While SNNs are increasing\ncompetitive with artificial neural networks (ANNs) for vision tasks, they are\nrarely used for long sequence tasks, despite their intrinsic temporal dynamics.\nIn this work, we develop spiking state space models (SpikingSSMs) for long\nsequence learning by leveraging on the sequence learning abilities of state\nspace models (SSMs). Inspired by dendritic neuron structure, we hierarchically\nintegrate neuronal dynamics with the original SSM block, meanwhile realizing\nsparse synaptic computation. Furthermore, to solve the conflict of event-driven\nneuronal dynamics with parallel computing, we propose a light-weight surrogate\ndynamic network which accurately predicts the after-reset membrane potential\nand compatible to learnable thresholds, enabling orders of acceleration in\ntraining speed compared with conventional iterative methods. On the long range\narena benchmark task, SpikingSSM achieves competitive performance to\nstate-of-the-art SSMs meanwhile realizing on average 90\\% of network sparsity.\nOn language modeling, our network significantly surpasses existing spiking\nlarge language models (spikingLLMs) on the WikiText-103 dataset with only a\nthird of the model size, demonstrating its potential as backbone architecture\nfor low computation cost LLMs."
                },
                "authors": [
                    {
                        "name": "Shuaijie Shen"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Renzhuo Huang"
                    },
                    {
                        "name": "Yan Zhong"
                    },
                    {
                        "name": "Qinghai Guo"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Luziwei Leng"
                    }
                ],
                "author_detail": {
                    "name": "Luziwei Leng"
                },
                "author": "Luziwei Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08448v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08448v3",
                "updated": "2024-08-27T09:04:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    4,
                    35,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-15T22:57:39Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    22,
                    57,
                    39,
                    3,
                    228,
                    0
                ],
                "title": "Exploring Cross-model Neuronal Correlations in the Context of Predicting\n  Model Performance and Generalizability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Cross-model Neuronal Correlations in the Context of Predicting\n  Model Performance and Generalizability"
                },
                "summary": "As Artificial Intelligence (AI) models are increasingly integrated into\ncritical systems, the need for a robust framework to establish the\ntrustworthiness of AI is increasingly paramount. While collaborative efforts\nhave established conceptual foundations for such a framework, there remains a\nsignificant gap in developing concrete, technically robust methods for\nassessing AI model quality and performance. A critical drawback in the\ntraditional methods for assessing the validity and generalizability of models\nis their dependence on internal developer datasets, rendering it challenging to\nindependently assess and verify their performance claims. This paper introduces\na novel approach for assessing a newly trained model's performance based on\nanother known model by calculating correlation between neural networks. The\nproposed method evaluates correlations by determining if, for each neuron in\none network, there exists a neuron in the other network that produces similar\noutput. This approach has implications for memory efficiency, allowing for the\nuse of smaller networks when high correlation exists between networks of\ndifferent sizes. Additionally, the method provides insights into robustness,\nsuggesting that if two highly correlated networks are compared and one\ndemonstrates robustness when operating in production environments, the other is\nlikely to exhibit similar robustness. This contribution advances the technical\ntoolkit for responsible AI, supporting more comprehensive and nuanced\nevaluations of AI models to ensure their safe and effective deployment. Code is\navailable at https://github.com/aheldis/Cross-model-correlation.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Artificial Intelligence (AI) models are increasingly integrated into\ncritical systems, the need for a robust framework to establish the\ntrustworthiness of AI is increasingly paramount. While collaborative efforts\nhave established conceptual foundations for such a framework, there remains a\nsignificant gap in developing concrete, technically robust methods for\nassessing AI model quality and performance. A critical drawback in the\ntraditional methods for assessing the validity and generalizability of models\nis their dependence on internal developer datasets, rendering it challenging to\nindependently assess and verify their performance claims. This paper introduces\na novel approach for assessing a newly trained model's performance based on\nanother known model by calculating correlation between neural networks. The\nproposed method evaluates correlations by determining if, for each neuron in\none network, there exists a neuron in the other network that produces similar\noutput. This approach has implications for memory efficiency, allowing for the\nuse of smaller networks when high correlation exists between networks of\ndifferent sizes. Additionally, the method provides insights into robustness,\nsuggesting that if two highly correlated networks are compared and one\ndemonstrates robustness when operating in production environments, the other is\nlikely to exhibit similar robustness. This contribution advances the technical\ntoolkit for responsible AI, supporting more comprehensive and nuanced\nevaluations of AI models to ensure their safe and effective deployment. Code is\navailable at https://github.com/aheldis/Cross-model-correlation.git."
                },
                "authors": [
                    {
                        "name": "Haniyeh Ehsani Oskouie"
                    },
                    {
                        "name": "Lionel Levine"
                    },
                    {
                        "name": "Majid Sarrafzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Majid Sarrafzadeh"
                },
                "author": "Majid Sarrafzadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08448v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08448v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14866v1",
                "updated": "2024-08-27T08:38:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    8,
                    38,
                    48,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T08:38:48Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    8,
                    38,
                    48,
                    1,
                    240,
                    0
                ],
                "title": "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language\n  Models"
                },
                "summary": "Language Language Models (LLMs) face safety concerns due to potential misuse\nby malicious users. Recent red-teaming efforts have identified adversarial\nsuffixes capable of jailbreaking LLMs using the gradient-based search algorithm\nGreedy Coordinate Gradient (GCG). However, GCG struggles with computational\ninefficiency, limiting further investigations regarding suffix transferability\nand scalability across models and data. In this work, we bridge the connection\nbetween search efficiency and suffix transferability. We propose a two-stage\ntransfer learning framework, DeGCG, which decouples the search process into\nbehavior-agnostic pre-searching and behavior-relevant post-searching.\nSpecifically, we employ direct first target token optimization in pre-searching\nto facilitate the search process. We apply our approach to cross-model,\ncross-data, and self-transfer scenarios. Furthermore, we introduce an\ninterleaved variant of our approach, i-DeGCG, which iteratively leverages\nself-transferability to accelerate the search process. Experiments on HarmBench\ndemonstrate the efficiency of our approach across various models and domains.\nNotably, our i-DeGCG outperforms the baseline on Llama2-chat-7b with ASRs of\n$43.9$ ($+22.2$) and $39.0$ ($+19.5$) on valid and test sets, respectively.\nFurther analysis on cross-model transfer indicates the pivotal role of first\ntarget token optimization in leveraging suffix transferability for efficient\nsearching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Language Models (LLMs) face safety concerns due to potential misuse\nby malicious users. Recent red-teaming efforts have identified adversarial\nsuffixes capable of jailbreaking LLMs using the gradient-based search algorithm\nGreedy Coordinate Gradient (GCG). However, GCG struggles with computational\ninefficiency, limiting further investigations regarding suffix transferability\nand scalability across models and data. In this work, we bridge the connection\nbetween search efficiency and suffix transferability. We propose a two-stage\ntransfer learning framework, DeGCG, which decouples the search process into\nbehavior-agnostic pre-searching and behavior-relevant post-searching.\nSpecifically, we employ direct first target token optimization in pre-searching\nto facilitate the search process. We apply our approach to cross-model,\ncross-data, and self-transfer scenarios. Furthermore, we introduce an\ninterleaved variant of our approach, i-DeGCG, which iteratively leverages\nself-transferability to accelerate the search process. Experiments on HarmBench\ndemonstrate the efficiency of our approach across various models and domains.\nNotably, our i-DeGCG outperforms the baseline on Llama2-chat-7b with ASRs of\n$43.9$ ($+22.2$) and $39.0$ ($+19.5$) on valid and test sets, respectively.\nFurther analysis on cross-model transfer indicates the pivotal role of first\ntarget token optimization in leveraging suffix transferability for efficient\nsearching."
                },
                "authors": [
                    {
                        "name": "Hongfu Liu"
                    },
                    {
                        "name": "Yuxi Xie"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Michael Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Shieh"
                },
                "author": "Michael Shieh",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00737v2",
                "updated": "2024-08-27T08:33:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    8,
                    33,
                    31,
                    1,
                    240,
                    0
                ],
                "published": "2024-06-30T15:50:32Z",
                "published_parsed": [
                    2024,
                    6,
                    30,
                    15,
                    50,
                    32,
                    6,
                    182,
                    0
                ],
                "title": "LLM4GEN: Leveraging Semantic Representation of LLMs for Text-to-Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4GEN: Leveraging Semantic Representation of LLMs for Text-to-Image\n  Generation"
                },
                "summary": "Diffusion models have exhibited substantial success in text-to-image\ngeneration. However, they often encounter challenges when dealing with complex\nand dense prompts involving multiple objects, attribute binding, and long\ndescriptions. In this paper, we propose a novel framework called\n\\textbf{LLM4GEN}, which enhances the semantic understanding of text-to-image\ndiffusion models by leveraging the representation of Large Language Models\n(LLMs). It can be seamlessly incorporated into various diffusion models as a\nplug-and-play component. A specially designed Cross-Adapter Module (CAM)\nintegrates the original text features of text-to-image models with LLM\nfeatures, thereby enhancing text-to-image generation. Additionally, to\nfacilitate and correct entity-attribute relationships in text prompts, we\ndevelop an entity-guided regularization loss to further improve generation\nperformance. We also introduce DensePrompts, which contains $7,000$ dense\nprompts to provide a comprehensive evaluation for the text-to-image generation\ntask. Experiments indicate that LLM4GEN significantly improves the semantic\nalignment of SD1.5 and SDXL, demonstrating increases of 9.69\\% and 12.90\\% in\ncolor on T2I-CompBench, respectively. Moreover, it surpasses existing models in\nterms of sample quality, image-text alignment, and human evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have exhibited substantial success in text-to-image\ngeneration. However, they often encounter challenges when dealing with complex\nand dense prompts involving multiple objects, attribute binding, and long\ndescriptions. In this paper, we propose a novel framework called\n\\textbf{LLM4GEN}, which enhances the semantic understanding of text-to-image\ndiffusion models by leveraging the representation of Large Language Models\n(LLMs). It can be seamlessly incorporated into various diffusion models as a\nplug-and-play component. A specially designed Cross-Adapter Module (CAM)\nintegrates the original text features of text-to-image models with LLM\nfeatures, thereby enhancing text-to-image generation. Additionally, to\nfacilitate and correct entity-attribute relationships in text prompts, we\ndevelop an entity-guided regularization loss to further improve generation\nperformance. We also introduce DensePrompts, which contains $7,000$ dense\nprompts to provide a comprehensive evaluation for the text-to-image generation\ntask. Experiments indicate that LLM4GEN significantly improves the semantic\nalignment of SD1.5 and SDXL, demonstrating increases of 9.69\\% and 12.90\\% in\ncolor on T2I-CompBench, respectively. Moreover, it surpasses existing models in\nterms of sample quality, image-text alignment, and human evaluation."
                },
                "authors": [
                    {
                        "name": "Mushui Liu"
                    },
                    {
                        "name": "Yuhang Ma"
                    },
                    {
                        "name": "Yang Zhen"
                    },
                    {
                        "name": "Jun Dan"
                    },
                    {
                        "name": "Yunlong Yu"
                    },
                    {
                        "name": "Zeng Zhao"
                    },
                    {
                        "name": "Zhipeng Hu"
                    },
                    {
                        "name": "Bai Liu"
                    },
                    {
                        "name": "Changjie Fan"
                    }
                ],
                "author_detail": {
                    "name": "Changjie Fan"
                },
                "author": "Changjie Fan",
                "arxiv_comment": "11 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03870v2",
                "updated": "2024-08-27T08:31:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    8,
                    31,
                    4,
                    1,
                    240,
                    0
                ],
                "published": "2024-03-06T17:23:28Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    17,
                    23,
                    28,
                    2,
                    66,
                    0
                ],
                "title": "Learning to Decode Collaboratively with Multiple Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Decode Collaboratively with Multiple Language Models"
                },
                "summary": "We propose a method to teach multiple large language models (LLM) to\ncollaborate by interleaving their generations at the token level. We model the\ndecision of which LLM generates the next token as a latent variable. By\noptimizing the marginal likelihood of a training set under our latent variable\nmodel, the base LLM automatically learns when to generate itself and when to\ncall on one of the ``assistant'' language models to generate, all without\ndirect supervision. Token-level collaboration during decoding allows for a\nfusion of each model's expertise in a manner tailored to the specific task at\nhand. Our collaborative decoding is especially useful in cross-domain settings\nwhere a generalist base LLM learns to invoke domain expert models. On\ninstruction-following, domain-specific QA, and reasoning tasks, we show that\nthe performance of the joint system exceeds that of the individual models.\nThrough qualitative analysis of the learned latent decisions, we show models\ntrained with our method exhibit several interesting collaboration patterns,\ne.g., template-filling. Our code is available at\nhttps://github.com/clinicalml/co-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a method to teach multiple large language models (LLM) to\ncollaborate by interleaving their generations at the token level. We model the\ndecision of which LLM generates the next token as a latent variable. By\noptimizing the marginal likelihood of a training set under our latent variable\nmodel, the base LLM automatically learns when to generate itself and when to\ncall on one of the ``assistant'' language models to generate, all without\ndirect supervision. Token-level collaboration during decoding allows for a\nfusion of each model's expertise in a manner tailored to the specific task at\nhand. Our collaborative decoding is especially useful in cross-domain settings\nwhere a generalist base LLM learns to invoke domain expert models. On\ninstruction-following, domain-specific QA, and reasoning tasks, we show that\nthe performance of the joint system exceeds that of the individual models.\nThrough qualitative analysis of the learned latent decisions, we show models\ntrained with our method exhibit several interesting collaboration patterns,\ne.g., template-filling. Our code is available at\nhttps://github.com/clinicalml/co-llm."
                },
                "authors": [
                    {
                        "name": "Shannon Zejiang Shen"
                    },
                    {
                        "name": "Hunter Lang"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Yoon Kim"
                    },
                    {
                        "name": "David Sontag"
                    }
                ],
                "author_detail": {
                    "name": "David Sontag"
                },
                "author": "David Sontag",
                "arxiv_comment": "16 pages, 4 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14853v1",
                "updated": "2024-08-27T08:12:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    8,
                    12,
                    8,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T08:12:08Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    8,
                    12,
                    8,
                    1,
                    240,
                    0
                ],
                "title": "Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have become a focal point in the rapidly\nevolving field of artificial intelligence. However, a critical concern is the\npresence of toxic content within the pre-training corpus of these models, which\ncan lead to the generation of inappropriate outputs. Investigating methods for\ndetecting internal faults in LLMs can help us understand their limitations and\nimprove their security. Existing methods primarily focus on jailbreaking\nattacks, which involve manually or automatically constructing adversarial\ncontent to prompt the target LLM to generate unexpected responses. These\nmethods rely heavily on prompt engineering, which is time-consuming and usually\nrequires specially designed questions. To address these challenges, this paper\nproposes a target-driven attack paradigm that focuses on directly eliciting the\ntarget response instead of optimizing the prompts. We introduce the use of\nanother LLM as the detector for toxic content, referred to as ToxDet. Given a\ntarget toxic response, ToxDet can generate a possible question and a\npreliminary answer to provoke the target model into producing desired toxic\nresponses with meanings equivalent to the provided one. ToxDet is trained by\ninteracting with the target LLM and receiving reward signals from it, utilizing\nreinforcement learning for the optimization process. While the primary focus of\nthe target models is on open-source LLMs, the fine-tuned ToxDet can also be\ntransferred to attack black-box models such as GPT-4o, achieving notable\nresults. Experimental results on AdvBench and HH-Harmless datasets demonstrate\nthe effectiveness of our methods in detecting the tendencies of target LLMs to\ngenerate harmful responses. This algorithm not only exposes vulnerabilities but\nalso provides a valuable resource for researchers to strengthen their models\nagainst such attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become a focal point in the rapidly\nevolving field of artificial intelligence. However, a critical concern is the\npresence of toxic content within the pre-training corpus of these models, which\ncan lead to the generation of inappropriate outputs. Investigating methods for\ndetecting internal faults in LLMs can help us understand their limitations and\nimprove their security. Existing methods primarily focus on jailbreaking\nattacks, which involve manually or automatically constructing adversarial\ncontent to prompt the target LLM to generate unexpected responses. These\nmethods rely heavily on prompt engineering, which is time-consuming and usually\nrequires specially designed questions. To address these challenges, this paper\nproposes a target-driven attack paradigm that focuses on directly eliciting the\ntarget response instead of optimizing the prompts. We introduce the use of\nanother LLM as the detector for toxic content, referred to as ToxDet. Given a\ntarget toxic response, ToxDet can generate a possible question and a\npreliminary answer to provoke the target model into producing desired toxic\nresponses with meanings equivalent to the provided one. ToxDet is trained by\ninteracting with the target LLM and receiving reward signals from it, utilizing\nreinforcement learning for the optimization process. While the primary focus of\nthe target models is on open-source LLMs, the fine-tuned ToxDet can also be\ntransferred to attack black-box models such as GPT-4o, achieving notable\nresults. Experimental results on AdvBench and HH-Harmless datasets demonstrate\nthe effectiveness of our methods in detecting the tendencies of target LLMs to\ngenerate harmful responses. This algorithm not only exposes vulnerabilities but\nalso provides a valuable resource for researchers to strengthen their models\nagainst such attacks."
                },
                "authors": [
                    {
                        "name": "Yuhao Du"
                    },
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Pengyu Cheng"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Anningzhe Gao"
                    }
                ],
                "author_detail": {
                    "name": "Anningzhe Gao"
                },
                "author": "Anningzhe Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13740v2",
                "updated": "2024-08-27T08:05:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    8,
                    5,
                    45,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-25T07:01:37Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    7,
                    1,
                    37,
                    6,
                    238,
                    0
                ],
                "title": "PIE: Parkour with Implicit-Explicit Learning Framework for Legged Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIE: Parkour with Implicit-Explicit Learning Framework for Legged Robots"
                },
                "summary": "Parkour presents a highly challenging task for legged robots, requiring them\nto traverse various terrains with agile and smooth locomotion. This\nnecessitates comprehensive understanding of both the robot's own state and the\nsurrounding terrain, despite the inherent unreliability of robot perception and\nactuation. Current state-of-the-art methods either rely on complex pre-trained\nhigh-level terrain reconstruction modules or limit the maximum potential of\nrobot parkour to avoid failure due to inaccurate perception. In this paper, we\npropose a one-stage end-to-end learning-based parkour framework: Parkour with\nImplicit-Explicit learning framework for legged robots (PIE) that leverages\ndual-level implicit-explicit estimation. With this mechanism, even a low-cost\nquadruped robot equipped with an unreliable egocentric depth camera can achieve\nexceptional performance on challenging parkour terrains using a relatively\nsimple training process and reward function. While the training process is\nconducted entirely in simulation, our real-world validation demonstrates\nsuccessful zero-shot deployment of our framework, showcasing superior parkour\nperformance on harsh terrains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parkour presents a highly challenging task for legged robots, requiring them\nto traverse various terrains with agile and smooth locomotion. This\nnecessitates comprehensive understanding of both the robot's own state and the\nsurrounding terrain, despite the inherent unreliability of robot perception and\nactuation. Current state-of-the-art methods either rely on complex pre-trained\nhigh-level terrain reconstruction modules or limit the maximum potential of\nrobot parkour to avoid failure due to inaccurate perception. In this paper, we\npropose a one-stage end-to-end learning-based parkour framework: Parkour with\nImplicit-Explicit learning framework for legged robots (PIE) that leverages\ndual-level implicit-explicit estimation. With this mechanism, even a low-cost\nquadruped robot equipped with an unreliable egocentric depth camera can achieve\nexceptional performance on challenging parkour terrains using a relatively\nsimple training process and reward function. While the training process is\nconducted entirely in simulation, our real-world validation demonstrates\nsuccessful zero-shot deployment of our framework, showcasing superior parkour\nperformance on harsh terrains."
                },
                "authors": [
                    {
                        "name": "Shixin Luo"
                    },
                    {
                        "name": "Songbo Li"
                    },
                    {
                        "name": "Ruiqi Yu"
                    },
                    {
                        "name": "Zhicheng Wang"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Qiuguo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Qiuguo Zhu"
                },
                "author": "Qiuguo Zhu",
                "arxiv_comment": "Accepted for IEEE Robotics and Automation Letters (RA-L)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14845v1",
                "updated": "2024-08-27T07:56:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    56,
                    35,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T07:56:35Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    56,
                    35,
                    1,
                    240,
                    0
                ],
                "title": "AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark"
                },
                "summary": "Detecting biases in natural language understanding (NLU) for African American\nVernacular English (AAVE) is crucial to developing inclusive natural language\nprocessing (NLP) systems. To address dialect-induced performance discrepancies,\nwe introduce AAVENUE ({AAVE} {N}atural Language {U}nderstanding {E}valuation),\na benchmark for evaluating large language model (LLM) performance on NLU tasks\nin AAVE and Standard American English (SAE). AAVENUE builds upon and extends\nexisting benchmarks like VALUE, replacing deterministic syntactic and\nmorphological transformations with a more flexible methodology leveraging\nLLM-based translation with few-shot prompting, improving performance across our\nevaluation metrics when translating key tasks from the GLUE and SuperGLUE\nbenchmarks. We compare AAVENUE and VALUE translations using five popular LLMs\nand a comprehensive set of metrics including fluency, BARTScore, quality,\ncoherence, and understandability. Additionally, we recruit fluent AAVE speakers\nto validate our translations for authenticity. Our evaluations reveal that LLMs\nconsistently perform better on SAE tasks than AAVE-translated versions,\nunderscoring inherent biases and highlighting the need for more inclusive NLP\nmodels. We have open-sourced our source code on GitHub and created a website to\nshowcase our work at https://aavenue.live.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting biases in natural language understanding (NLU) for African American\nVernacular English (AAVE) is crucial to developing inclusive natural language\nprocessing (NLP) systems. To address dialect-induced performance discrepancies,\nwe introduce AAVENUE ({AAVE} {N}atural Language {U}nderstanding {E}valuation),\na benchmark for evaluating large language model (LLM) performance on NLU tasks\nin AAVE and Standard American English (SAE). AAVENUE builds upon and extends\nexisting benchmarks like VALUE, replacing deterministic syntactic and\nmorphological transformations with a more flexible methodology leveraging\nLLM-based translation with few-shot prompting, improving performance across our\nevaluation metrics when translating key tasks from the GLUE and SuperGLUE\nbenchmarks. We compare AAVENUE and VALUE translations using five popular LLMs\nand a comprehensive set of metrics including fluency, BARTScore, quality,\ncoherence, and understandability. Additionally, we recruit fluent AAVE speakers\nto validate our translations for authenticity. Our evaluations reveal that LLMs\nconsistently perform better on SAE tasks than AAVE-translated versions,\nunderscoring inherent biases and highlighting the need for more inclusive NLP\nmodels. We have open-sourced our source code on GitHub and created a website to\nshowcase our work at https://aavenue.live."
                },
                "authors": [
                    {
                        "name": "Abhay Gupta"
                    },
                    {
                        "name": "Philip Meng"
                    },
                    {
                        "name": "Ece Yurtseven"
                    },
                    {
                        "name": "Sean O'Brien"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14834v1",
                "updated": "2024-08-27T07:44:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    44,
                    16,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T07:44:16Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    44,
                    16,
                    1,
                    240,
                    0
                ],
                "title": "Strategic Optimization and Challenges of Large Language Models in\n  Object-Oriented Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Optimization and Challenges of Large Language Models in\n  Object-Oriented Programming"
                },
                "summary": "In the area of code generation research, the emphasis has transitioned from\ncrafting individual functions to developing class-level method code that\nintegrates contextual information. This shift has brought several benchmarks\nsuch as ClassEval and CoderEval, which consider class-level contexts.\nNevertheless, the influence of specific contextual factors at the method level\nremains less explored.\n  This research focused on method-level code generation within the\nObject-Oriented Programming (OOP) framework. Based on CoderEval, we devised\nexperiments that varied the extent of contextual information in the prompts,\nranging from method-specific to project-wide details. We introduced the\ninnovative metric of \"Prompt-Token Cost-Effectiveness\" to evaluate the economic\nviability of incorporating additional contextual layers. Our findings indicate\nthat prompts enriched with method invocation details yield the highest\ncost-effectiveness. Additionally, our study revealed disparities among Large\nLanguage Models (LLMs) regarding error type distributions and the level of\nassistance they provide to developers. Notably, larger LLMs do not invariably\nperform better. We also observed that tasks with higher degrees of coupling\npresent more substantial challenges, suggesting that the choice of LLM should\nbe tailored to the task's coupling degree. For example, GPT-4 exhibited\nimproved performance in low-coupling scenarios, whereas GPT-3.5 seemed better\nsuited for tasks with high coupling. By meticulously curating prompt content\nand selecting the appropriate LLM, developers can optimize code quality while\nmaximizing cost-efficiency during the development process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the area of code generation research, the emphasis has transitioned from\ncrafting individual functions to developing class-level method code that\nintegrates contextual information. This shift has brought several benchmarks\nsuch as ClassEval and CoderEval, which consider class-level contexts.\nNevertheless, the influence of specific contextual factors at the method level\nremains less explored.\n  This research focused on method-level code generation within the\nObject-Oriented Programming (OOP) framework. Based on CoderEval, we devised\nexperiments that varied the extent of contextual information in the prompts,\nranging from method-specific to project-wide details. We introduced the\ninnovative metric of \"Prompt-Token Cost-Effectiveness\" to evaluate the economic\nviability of incorporating additional contextual layers. Our findings indicate\nthat prompts enriched with method invocation details yield the highest\ncost-effectiveness. Additionally, our study revealed disparities among Large\nLanguage Models (LLMs) regarding error type distributions and the level of\nassistance they provide to developers. Notably, larger LLMs do not invariably\nperform better. We also observed that tasks with higher degrees of coupling\npresent more substantial challenges, suggesting that the choice of LLM should\nbe tailored to the task's coupling degree. For example, GPT-4 exhibited\nimproved performance in low-coupling scenarios, whereas GPT-3.5 seemed better\nsuited for tasks with high coupling. By meticulously curating prompt content\nand selecting the appropriate LLM, developers can optimize code quality while\nmaximizing cost-efficiency during the development process."
                },
                "authors": [
                    {
                        "name": "Zinan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zinan Wang"
                },
                "author": "Zinan Wang",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14830v1",
                "updated": "2024-08-27T07:27:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    27,
                    16,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T07:27:16Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    27,
                    16,
                    1,
                    240,
                    0
                ],
                "title": "PolicyLR: A Logic Representation For Privacy Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolicyLR: A Logic Representation For Privacy Policies"
                },
                "summary": "Privacy policies are crucial in the online ecosystem, defining how services\nhandle user data and adhere to regulations such as GDPR and CCPA. However,\ntheir complexity and frequent updates often make them difficult for\nstakeholders to understand and analyze. Current automated analysis methods,\nwhich utilize natural language processing, have limitations. They typically\nfocus on individual tasks and fail to capture the full context of the policies.\nWe propose PolicyLR, a new paradigm that offers a comprehensive\nmachine-readable representation of privacy policies, serving as an all-in-one\nsolution for multiple downstream tasks. PolicyLR converts privacy policies into\na machine-readable format using valuations of atomic formulae, allowing for\nformal definitions of tasks like compliance and consistency. We have developed\na compiler that transforms unstructured policy text into this format using\noff-the-shelf Large Language Models (LLMs). This compiler breaks down the\ntransformation task into a two-stage translation and entailment procedure. This\nprocedure considers the full context of the privacy policy to infer a complex\nformula, where each formula consists of simpler atomic formulae. The advantage\nof this model is that PolicyLR is interpretable by design and grounded in\nsegments of the privacy policy. We evaluated the compiler using ToS;DR, a\ncommunity-annotated privacy policy entailment dataset. Utilizing open-source\nLLMs, our compiler achieves precision and recall values of 0.91 and 0.88,\nrespectively. Finally, we demonstrate the utility of PolicyLR in three privacy\ntasks: Policy Compliance, Inconsistency Detection, and Privacy Comparison\nShopping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy policies are crucial in the online ecosystem, defining how services\nhandle user data and adhere to regulations such as GDPR and CCPA. However,\ntheir complexity and frequent updates often make them difficult for\nstakeholders to understand and analyze. Current automated analysis methods,\nwhich utilize natural language processing, have limitations. They typically\nfocus on individual tasks and fail to capture the full context of the policies.\nWe propose PolicyLR, a new paradigm that offers a comprehensive\nmachine-readable representation of privacy policies, serving as an all-in-one\nsolution for multiple downstream tasks. PolicyLR converts privacy policies into\na machine-readable format using valuations of atomic formulae, allowing for\nformal definitions of tasks like compliance and consistency. We have developed\na compiler that transforms unstructured policy text into this format using\noff-the-shelf Large Language Models (LLMs). This compiler breaks down the\ntransformation task into a two-stage translation and entailment procedure. This\nprocedure considers the full context of the privacy policy to infer a complex\nformula, where each formula consists of simpler atomic formulae. The advantage\nof this model is that PolicyLR is interpretable by design and grounded in\nsegments of the privacy policy. We evaluated the compiler using ToS;DR, a\ncommunity-annotated privacy policy entailment dataset. Utilizing open-source\nLLMs, our compiler achieves precision and recall values of 0.91 and 0.88,\nrespectively. Finally, we demonstrate the utility of PolicyLR in three privacy\ntasks: Policy Compliance, Inconsistency Detection, and Privacy Comparison\nShopping."
                },
                "authors": [
                    {
                        "name": "Ashish Hooda"
                    },
                    {
                        "name": "Rishabh Khandelwal"
                    },
                    {
                        "name": "Prasad Chalasani"
                    },
                    {
                        "name": "Kassem Fawaz"
                    },
                    {
                        "name": "Somesh Jha"
                    }
                ],
                "author_detail": {
                    "name": "Somesh Jha"
                },
                "author": "Somesh Jha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09193v2",
                "updated": "2024-08-27T07:02:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    2,
                    7,
                    1,
                    240,
                    0
                ],
                "published": "2024-04-14T09:01:26Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    9,
                    1,
                    26,
                    6,
                    105,
                    0
                ],
                "title": "FaceCat: Enhancing Face Recognition Security with a Unified Diffusion\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaceCat: Enhancing Face Recognition Security with a Unified Diffusion\n  Model"
                },
                "summary": "Face anti-spoofing (FAS) and adversarial detection (FAD) have been regarded\nas critical technologies to ensure the safety of face recognition systems.\nHowever, due to limited practicality, complex deployment, and the additional\ncomputational overhead, it is necessary to implement both detection techniques\nwithin a unified framework. This paper aims to achieve this goal by breaking\nthrough two primary obstacles: 1) the suboptimal face feature representation\nand 2) the scarcity of training data. To address the limited performance caused\nby existing feature representations, motivated by the rich structural and\ndetailed features of face diffusion models, we propose FaceCat, the first\napproach leveraging the diffusion model to simultaneously enhance the\nperformance of FAS and FAD. Specifically, FaceCat elaborately designs a\nhierarchical fusion mechanism to capture rich face semantic features of the\ndiffusion model. These features then serve as a robust foundation for a\nlightweight head, designed to execute FAS and FAD simultaneously. Due to the\nlimitations in feature representation that arise from relying solely on\nsingle-modality image data, we further propose a novel text-guided multi-modal\nalignment strategy that utilizes text prompts to enrich feature representation,\nthereby enhancing performance. To combat data scarcity, we build a\ncomprehensive dataset with a wide range of 28 attack types, offering greater\npotential for a unified framework in facial security. Extensive experiments\nvalidate the effectiveness of FaceCat generalizes significantly better and\nobtains excellent robustness against common input transformations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face anti-spoofing (FAS) and adversarial detection (FAD) have been regarded\nas critical technologies to ensure the safety of face recognition systems.\nHowever, due to limited practicality, complex deployment, and the additional\ncomputational overhead, it is necessary to implement both detection techniques\nwithin a unified framework. This paper aims to achieve this goal by breaking\nthrough two primary obstacles: 1) the suboptimal face feature representation\nand 2) the scarcity of training data. To address the limited performance caused\nby existing feature representations, motivated by the rich structural and\ndetailed features of face diffusion models, we propose FaceCat, the first\napproach leveraging the diffusion model to simultaneously enhance the\nperformance of FAS and FAD. Specifically, FaceCat elaborately designs a\nhierarchical fusion mechanism to capture rich face semantic features of the\ndiffusion model. These features then serve as a robust foundation for a\nlightweight head, designed to execute FAS and FAD simultaneously. Due to the\nlimitations in feature representation that arise from relying solely on\nsingle-modality image data, we further propose a novel text-guided multi-modal\nalignment strategy that utilizes text prompts to enrich feature representation,\nthereby enhancing performance. To combat data scarcity, we build a\ncomprehensive dataset with a wide range of 28 attack types, offering greater\npotential for a unified framework in facial security. Extensive experiments\nvalidate the effectiveness of FaceCat generalizes significantly better and\nobtains excellent robustness against common input transformations."
                },
                "authors": [
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Zhaoxia Yin"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoxia Yin"
                },
                "author": "Zhaoxia Yin",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05885v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05885v2",
                "updated": "2024-08-27T06:53:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    53,
                    16,
                    1,
                    240,
                    0
                ],
                "published": "2024-06-09T18:45:41Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    18,
                    45,
                    41,
                    6,
                    161,
                    0
                ],
                "title": "Are Large Language Models Actually Good at Text Style Transfer?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Actually Good at Text Style Transfer?"
                },
                "summary": "We analyze the performance of large language models (LLMs) on Text Style\nTransfer (TST), specifically focusing on sentiment transfer and text\ndetoxification across three languages: English, Hindi, and Bengali. Text Style\nTransfer involves modifying the linguistic style of a text while preserving its\ncore content. We evaluate the capabilities of pre-trained LLMs using zero-shot\nand few-shot prompting as well as parameter-efficient finetuning on publicly\navailable datasets. Our evaluation using automatic metrics, GPT-4 and human\nevaluations reveals that while some prompted LLMs perform well in English,\ntheir performance in on other languages (Hindi, Bengali) remains average.\nHowever, finetuning significantly improves results compared to zero-shot and\nfew-shot prompting, making them comparable to previous state-of-the-art. This\nunderscores the necessity of dedicated datasets and specialized models for\neffective TST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze the performance of large language models (LLMs) on Text Style\nTransfer (TST), specifically focusing on sentiment transfer and text\ndetoxification across three languages: English, Hindi, and Bengali. Text Style\nTransfer involves modifying the linguistic style of a text while preserving its\ncore content. We evaluate the capabilities of pre-trained LLMs using zero-shot\nand few-shot prompting as well as parameter-efficient finetuning on publicly\navailable datasets. Our evaluation using automatic metrics, GPT-4 and human\nevaluations reveals that while some prompted LLMs perform well in English,\ntheir performance in on other languages (Hindi, Bengali) remains average.\nHowever, finetuning significantly improves results compared to zero-shot and\nfew-shot prompting, making them comparable to previous state-of-the-art. This\nunderscores the necessity of dedicated datasets and specialized models for\neffective TST."
                },
                "authors": [
                    {
                        "name": "Sourabrata Mukherjee"
                    },
                    {
                        "name": "Atul Kr. Ojha"
                    },
                    {
                        "name": "Ondřej Dušek"
                    }
                ],
                "author_detail": {
                    "name": "Ondřej Dušek"
                },
                "author": "Ondřej Dušek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05885v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05885v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20805v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20805v3",
                "updated": "2024-08-27T06:51:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    51,
                    0,
                    1,
                    240,
                    0
                ],
                "published": "2024-05-31T14:05:27Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    14,
                    5,
                    27,
                    4,
                    152,
                    0
                ],
                "title": "Multilingual Text Style Transfer: Datasets & Models for Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Text Style Transfer: Datasets & Models for Indian Languages"
                },
                "summary": "Text style transfer (TST) involves altering the linguistic style of a text\nwhile preserving its core content. This paper focuses on sentiment transfer, a\npopular TST subtask, across a spectrum of Indian languages: Hindi, Magahi,\nMalayalam, Marathi, Punjabi, Odia, Telugu, and Urdu, expanding upon previous\nwork on English-Bangla sentiment transfer (Mukherjee et al., 2023). We\nintroduce dedicated datasets of 1,000 positive and 1,000 negative\nstyle-parallel sentences for each of these eight languages. We then evaluate\nthe performance of various benchmark models categorized into parallel,\nnon-parallel, cross-lingual, and shared learning approaches, including the\nLlama2 and GPT-3.5 large language models (LLMs). Our experiments highlight the\nsignificance of parallel data in TST and demonstrate the effectiveness of the\nMasked Style Filling (MSF) approach (Mukherjee et al., 2023) in non-parallel\ntechniques. Moreover, cross-lingual and joint multilingual learning methods\nshow promise, offering insights into selecting optimal models tailored to the\nspecific language and task requirements. To the best of our knowledge, this\nwork represents the first comprehensive exploration of the TST task as\nsentiment transfer across a diverse set of languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text style transfer (TST) involves altering the linguistic style of a text\nwhile preserving its core content. This paper focuses on sentiment transfer, a\npopular TST subtask, across a spectrum of Indian languages: Hindi, Magahi,\nMalayalam, Marathi, Punjabi, Odia, Telugu, and Urdu, expanding upon previous\nwork on English-Bangla sentiment transfer (Mukherjee et al., 2023). We\nintroduce dedicated datasets of 1,000 positive and 1,000 negative\nstyle-parallel sentences for each of these eight languages. We then evaluate\nthe performance of various benchmark models categorized into parallel,\nnon-parallel, cross-lingual, and shared learning approaches, including the\nLlama2 and GPT-3.5 large language models (LLMs). Our experiments highlight the\nsignificance of parallel data in TST and demonstrate the effectiveness of the\nMasked Style Filling (MSF) approach (Mukherjee et al., 2023) in non-parallel\ntechniques. Moreover, cross-lingual and joint multilingual learning methods\nshow promise, offering insights into selecting optimal models tailored to the\nspecific language and task requirements. To the best of our knowledge, this\nwork represents the first comprehensive exploration of the TST task as\nsentiment transfer across a diverse set of languages."
                },
                "authors": [
                    {
                        "name": "Sourabrata Mukherjee"
                    },
                    {
                        "name": "Atul Kr. Ojha"
                    },
                    {
                        "name": "Akanksha Bansal"
                    },
                    {
                        "name": "Deepak Alok"
                    },
                    {
                        "name": "John P. McCrae"
                    },
                    {
                        "name": "Ondřej Dušek"
                    }
                ],
                "author_detail": {
                    "name": "Ondřej Dušek"
                },
                "author": "Ondřej Dušek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20805v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20805v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14812v1",
                "updated": "2024-08-27T06:50:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    50,
                    28,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T06:50:28Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    50,
                    28,
                    1,
                    240,
                    0
                ],
                "title": "HPT++: Hierarchically Prompting Vision-Language Models with\n  Multi-Granularity Knowledge Generation and Improved Structure Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPT++: Hierarchically Prompting Vision-Language Models with\n  Multi-Granularity Knowledge Generation and Improved Structure Modeling"
                },
                "summary": "Prompt learning has become a prevalent strategy for adapting vision-language\nfoundation models (VLMs) such as CLIP to downstream tasks. With the emergence\nof large language models (LLMs), recent studies have explored the potential of\nusing category-related descriptions to enhance prompt effectiveness. However,\nconventional descriptions lack explicit structured information necessary to\nrepresent the interconnections among key elements like entities or attributes\nwith relation to a particular category. Since existing prompt tuning methods\ngive little consideration to managing structured knowledge, this paper\nadvocates leveraging LLMs to construct a graph for each description to\nprioritize such structured knowledge. Consequently, we propose a novel approach\ncalled Hierarchical Prompt Tuning (HPT), enabling simultaneous modeling of both\nstructured and conventional linguistic knowledge. Specifically, we introduce a\nrelationship-guided attention module to capture pair-wise associations among\nentities and attributes for low-level prompt learning. In addition, by\nincorporating high-level and global-level prompts modeling overall semantics,\nthe proposed hierarchical structure forges cross-level interlinks and empowers\nthe model to handle more complex and long-term relationships. Finally, by\nenhancing multi-granularity knowledge generation, redesigning the\nrelationship-driven attention re-weighting module, and incorporating consistent\nconstraints on the hierarchical text encoder, we propose HPT++, which further\nimproves the performance of HPT. Our experiments are conducted across a wide\nrange of evaluation settings, including base-to-new generalization,\ncross-dataset evaluation, and domain generalization. Extensive results and\nablation studies demonstrate the effectiveness of our methods, which\nconsistently outperform existing SOTA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt learning has become a prevalent strategy for adapting vision-language\nfoundation models (VLMs) such as CLIP to downstream tasks. With the emergence\nof large language models (LLMs), recent studies have explored the potential of\nusing category-related descriptions to enhance prompt effectiveness. However,\nconventional descriptions lack explicit structured information necessary to\nrepresent the interconnections among key elements like entities or attributes\nwith relation to a particular category. Since existing prompt tuning methods\ngive little consideration to managing structured knowledge, this paper\nadvocates leveraging LLMs to construct a graph for each description to\nprioritize such structured knowledge. Consequently, we propose a novel approach\ncalled Hierarchical Prompt Tuning (HPT), enabling simultaneous modeling of both\nstructured and conventional linguistic knowledge. Specifically, we introduce a\nrelationship-guided attention module to capture pair-wise associations among\nentities and attributes for low-level prompt learning. In addition, by\nincorporating high-level and global-level prompts modeling overall semantics,\nthe proposed hierarchical structure forges cross-level interlinks and empowers\nthe model to handle more complex and long-term relationships. Finally, by\nenhancing multi-granularity knowledge generation, redesigning the\nrelationship-driven attention re-weighting module, and incorporating consistent\nconstraints on the hierarchical text encoder, we propose HPT++, which further\nimproves the performance of HPT. Our experiments are conducted across a wide\nrange of evaluation settings, including base-to-new generalization,\ncross-dataset evaluation, and domain generalization. Extensive results and\nablation studies demonstrate the effectiveness of our methods, which\nconsistently outperform existing SOTA methods."
                },
                "authors": [
                    {
                        "name": "Yubin Wang"
                    },
                    {
                        "name": "Xinyang Jiang"
                    },
                    {
                        "name": "De Cheng"
                    },
                    {
                        "name": "Wenli Sun"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Cairong Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Cairong Zhao"
                },
                "author": "Cairong Zhao",
                "arxiv_comment": "19 pages, 7 figures, 7 tables. arXiv admin note: substantial text\n  overlap with arXiv:2312.06323",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14811v1",
                "updated": "2024-08-27T06:49:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    49,
                    50,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T06:49:50Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    49,
                    50,
                    1,
                    240,
                    0
                ],
                "title": "Brain-inspired Artificial Intelligence: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-inspired Artificial Intelligence: A Comprehensive Review"
                },
                "summary": "Current artificial intelligence (AI) models often focus on enhancing\nperformance through meticulous parameter tuning and optimization techniques.\nHowever, the fundamental design principles behind these models receive\ncomparatively less attention, which can limit our understanding of their\npotential and constraints. This comprehensive review explores the diverse\ndesign inspirations that have shaped modern AI models, i.e., brain-inspired\nartificial intelligence (BIAI). We present a classification framework that\ncategorizes BIAI approaches into physical structure-inspired and human\nbehavior-inspired models. We also examine the real-world applications where\ndifferent BIAI models excel, highlighting their practical benefits and\ndeployment challenges. By delving into these areas, we provide new insights and\npropose future research directions to drive innovation and address current gaps\nin the field. This review offers researchers and practitioners a comprehensive\noverview of the BIAI landscape, helping them harness its potential and expedite\nadvancements in AI development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current artificial intelligence (AI) models often focus on enhancing\nperformance through meticulous parameter tuning and optimization techniques.\nHowever, the fundamental design principles behind these models receive\ncomparatively less attention, which can limit our understanding of their\npotential and constraints. This comprehensive review explores the diverse\ndesign inspirations that have shaped modern AI models, i.e., brain-inspired\nartificial intelligence (BIAI). We present a classification framework that\ncategorizes BIAI approaches into physical structure-inspired and human\nbehavior-inspired models. We also examine the real-world applications where\ndifferent BIAI models excel, highlighting their practical benefits and\ndeployment challenges. By delving into these areas, we provide new insights and\npropose future research directions to drive innovation and address current gaps\nin the field. This review offers researchers and practitioners a comprehensive\noverview of the BIAI landscape, helping them harness its potential and expedite\nadvancements in AI development."
                },
                "authors": [
                    {
                        "name": "Jing Ren"
                    },
                    {
                        "name": "Feng Xia"
                    }
                ],
                "author_detail": {
                    "name": "Feng Xia"
                },
                "author": "Feng Xia",
                "arxiv_comment": "35 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13976v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13976v2",
                "updated": "2024-08-27T06:49:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    49,
                    19,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-26T01:48:57Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    1,
                    48,
                    57,
                    0,
                    239,
                    0
                ],
                "title": "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking\n  the Generated Code Candidates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking\n  the Generated Code Candidates"
                },
                "summary": "Large Language Models (LLMs), such as GPT-4, StarCoder, and CodeLlama, are\ntransforming the way developers approach programming by automatically\ngenerating code based on given natural language descriptions. Despite\nadvancements, generating syntactically and semantically correct code remains\nchallenging, especially for complex programming tasks. Typically, individuals\ngenerate multiple candidate solutions using LLMs to increase the likelihood of\nproducing correct code. However, selecting the correct code from these\ncandidates-a process known as code ranking-remains a major challenge. Current\nresearch on code ranking can be categorized into execution-based and\nnon-execution-based methods. Execution-based methods, although effective,\nencounter notable limitations, such as scarcity of quality unit tests and\nsecurity risks. Non-execution-based methods like CodeRanker, which rely solely\non classification labels to train a code ranker, struggle to capture subtle\nerrors and provide detailed error insights. Recognizing the strengths and\nlimitations of both approaches, we propose a new method. The key insight of our\nwork is that an effective code ranker is expected to genuinely comprehend the\nunderlying causes of erroneous code, as relying solely on classification labels\nis insufficient. Inspired by this, this paper puts forward RankEF, an\ninnovative approach for code ranking that leverages execution feedback. RankEF\nemploys multi-task learning to integrate code classification with execution\nfeedback generation. This approach enables the model to understand the reasons\nbehind incorrect code, distinguishing between correct and incorrect solutions\nwithout the need to execute the code during the ranking phase. Experiments on\nthree code generation benchmarks demonstrate that RankEF significantly\noutperforms the state-of-the-art CodeRanker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT-4, StarCoder, and CodeLlama, are\ntransforming the way developers approach programming by automatically\ngenerating code based on given natural language descriptions. Despite\nadvancements, generating syntactically and semantically correct code remains\nchallenging, especially for complex programming tasks. Typically, individuals\ngenerate multiple candidate solutions using LLMs to increase the likelihood of\nproducing correct code. However, selecting the correct code from these\ncandidates-a process known as code ranking-remains a major challenge. Current\nresearch on code ranking can be categorized into execution-based and\nnon-execution-based methods. Execution-based methods, although effective,\nencounter notable limitations, such as scarcity of quality unit tests and\nsecurity risks. Non-execution-based methods like CodeRanker, which rely solely\non classification labels to train a code ranker, struggle to capture subtle\nerrors and provide detailed error insights. Recognizing the strengths and\nlimitations of both approaches, we propose a new method. The key insight of our\nwork is that an effective code ranker is expected to genuinely comprehend the\nunderlying causes of erroneous code, as relying solely on classification labels\nis insufficient. Inspired by this, this paper puts forward RankEF, an\ninnovative approach for code ranking that leverages execution feedback. RankEF\nemploys multi-task learning to integrate code classification with execution\nfeedback generation. This approach enables the model to understand the reasons\nbehind incorrect code, distinguishing between correct and incorrect solutions\nwithout the need to execute the code during the ranking phase. Experiments on\nthree code generation benchmarks demonstrate that RankEF significantly\noutperforms the state-of-the-art CodeRanker."
                },
                "authors": [
                    {
                        "name": "Zhihong Sun"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Chen Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Lyu"
                },
                "author": "Chen Lyu",
                "arxiv_comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13976v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13976v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08841v2",
                "updated": "2024-08-27T06:23:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    23,
                    45,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-16T17:00:11Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    0,
                    11,
                    4,
                    229,
                    0
                ],
                "title": "FLEXTAF: Enhancing Table Reasoning with Flexible Tabular Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLEXTAF: Enhancing Table Reasoning with Flexible Tabular Formats"
                },
                "summary": "The table reasoning task aims to answer the question according to the given\ntable. Currently, using Large Language Models (LLMs) is the predominant method\nfor table reasoning. Most existing methods employ a fixed tabular format to\nrepresent the table, which could limit the performance. Given that each\ninstance requires different capabilities and models possess varying abilities,\nwe assert that different instances and models suit different tabular formats.\nWe prove the aforementioned claim through quantitative analysis of experimental\nresults, where different instances and models achieve different performances\nusing various tabular formats. Building on this discussion, we propose\nFLEXTAF-Single and FLEXTAF-Vote to enhance table reasoning performance by\nemploying flexible tabular formats. Specifically, (i) FLEXTAF-Single trains a\nclassifier to predict the most suitable tabular format based on the instance\nand the LLM. (ii) FLEXTAF-Vote integrates the results across different formats.\nOur experiments on WikiTableQuestions and TabFact reveal significant\nimprovements, with average gains of 2.3% and 4.8% compared to the best\nperformance achieved using a fixed tabular format with greedy decoding and\nself-consistency decoding, thereby validating the effectiveness of our methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The table reasoning task aims to answer the question according to the given\ntable. Currently, using Large Language Models (LLMs) is the predominant method\nfor table reasoning. Most existing methods employ a fixed tabular format to\nrepresent the table, which could limit the performance. Given that each\ninstance requires different capabilities and models possess varying abilities,\nwe assert that different instances and models suit different tabular formats.\nWe prove the aforementioned claim through quantitative analysis of experimental\nresults, where different instances and models achieve different performances\nusing various tabular formats. Building on this discussion, we propose\nFLEXTAF-Single and FLEXTAF-Vote to enhance table reasoning performance by\nemploying flexible tabular formats. Specifically, (i) FLEXTAF-Single trains a\nclassifier to predict the most suitable tabular format based on the instance\nand the LLM. (ii) FLEXTAF-Vote integrates the results across different formats.\nOur experiments on WikiTableQuestions and TabFact reveal significant\nimprovements, with average gains of 2.3% and 4.8% compared to the best\nperformance achieved using a fixed tabular format with greedy decoding and\nself-consistency decoding, thereby validating the effectiveness of our methods."
                },
                "authors": [
                    {
                        "name": "Xuanliang Zhang"
                    },
                    {
                        "name": "Dingzirui Wang"
                    },
                    {
                        "name": "Longxu Dou"
                    },
                    {
                        "name": "Baoxin Wang"
                    },
                    {
                        "name": "Dayong Wu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14043v2",
                "updated": "2024-08-27T06:18:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    18,
                    5,
                    1,
                    240,
                    0
                ],
                "published": "2024-06-20T07:06:58Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    7,
                    6,
                    58,
                    3,
                    172,
                    0
                ],
                "title": "Taxonomy-Guided Zero-Shot Recommendations with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taxonomy-Guided Zero-Shot Recommendations with LLMs"
                },
                "summary": "With the emergence of large language models (LLMs) and their ability to\nperform a variety of tasks, their application in recommender systems (RecSys)\nhas shown promise. However, we are facing significant challenges when deploying\nLLMs into RecSys, such as limited prompt length, unstructured item information,\nand un-constrained generation of recommendations, leading to sub-optimal\nperformance. To address these issues, we propose a novel method using a\ntaxonomy dictionary. This method provides a systematic framework for\ncategorizing and organizing items, improving the clarity and structure of item\ninformation. By incorporating the taxonomy dictionary into LLM prompts, we\nachieve efficient token utilization and controlled feature generation, leading\nto more accurate and contextually relevant recommendations. Our Taxonomy-guided\nRecommendation (TaxRec) approach features a two-step process: one-time taxonomy\ncategorization and LLM-based recommendation, enabling zero-shot recommendations\nwithout the need for domain-specific fine-tuning. Experimental results\ndemonstrate TaxRec significantly enhances recommendation quality compared to\ntraditional zero-shot approaches, showcasing its efficacy as personal\nrecommender with LLMs. Code is available at\nhttps://github.com/yueqingliang1/TaxRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of large language models (LLMs) and their ability to\nperform a variety of tasks, their application in recommender systems (RecSys)\nhas shown promise. However, we are facing significant challenges when deploying\nLLMs into RecSys, such as limited prompt length, unstructured item information,\nand un-constrained generation of recommendations, leading to sub-optimal\nperformance. To address these issues, we propose a novel method using a\ntaxonomy dictionary. This method provides a systematic framework for\ncategorizing and organizing items, improving the clarity and structure of item\ninformation. By incorporating the taxonomy dictionary into LLM prompts, we\nachieve efficient token utilization and controlled feature generation, leading\nto more accurate and contextually relevant recommendations. Our Taxonomy-guided\nRecommendation (TaxRec) approach features a two-step process: one-time taxonomy\ncategorization and LLM-based recommendation, enabling zero-shot recommendations\nwithout the need for domain-specific fine-tuning. Experimental results\ndemonstrate TaxRec significantly enhances recommendation quality compared to\ntraditional zero-shot approaches, showcasing its efficacy as personal\nrecommender with LLMs. Code is available at\nhttps://github.com/yueqingliang1/TaxRec."
                },
                "authors": [
                    {
                        "name": "Yueqing Liang"
                    },
                    {
                        "name": "Liangwei Yang"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Xiongxiao Xu"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Kai Shu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Shu"
                },
                "author": "Kai Shu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08779v2",
                "updated": "2024-08-27T06:14:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    14,
                    54,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-16T14:43:15Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    43,
                    15,
                    4,
                    229,
                    0
                ],
                "title": "DAC: Decomposed Automation Correction for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAC: Decomposed Automation Correction for Text-to-SQL"
                },
                "summary": "Text-to-SQL is an important task that helps people obtain information from\ndatabases by automatically generating SQL queries. Considering the brilliant\nperformance, approaches based on Large Language Models (LLMs) become the\nmainstream for text-to-SQL. Among these approaches, automated correction is an\neffective approach that further enhances performance by correcting the mistakes\nin the generated results. The existing correction methods require LLMs to\ndirectly correct with generated SQL, while previous research shows that LLMs do\nnot know how to detect mistakes, leading to poor performance. Therefore, in\nthis paper, we propose to employ the decomposed correction to enhance\ntext-to-SQL performance. We first demonstrate that decomposed correction\noutperforms direct correction since detecting and fixing mistakes with the\nresults of the decomposed sub-tasks is easier than with SQL. Based on this\nanalysis, we introduce Decomposed Automation Correction (DAC), which corrects\nSQL by decomposing text-to-SQL into entity linking and skeleton parsing. DAC\nfirst generates the entity and skeleton corresponding to the question and then\ncompares the differences between the initial SQL and the generated entities and\nskeleton as feedback for correction. Experimental results show that our method\nimproves performance by $3.7\\%$ on average of Spider, Bird, and KaggleDBQA\ncompared with the baseline method, demonstrating the effectiveness of DAC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is an important task that helps people obtain information from\ndatabases by automatically generating SQL queries. Considering the brilliant\nperformance, approaches based on Large Language Models (LLMs) become the\nmainstream for text-to-SQL. Among these approaches, automated correction is an\neffective approach that further enhances performance by correcting the mistakes\nin the generated results. The existing correction methods require LLMs to\ndirectly correct with generated SQL, while previous research shows that LLMs do\nnot know how to detect mistakes, leading to poor performance. Therefore, in\nthis paper, we propose to employ the decomposed correction to enhance\ntext-to-SQL performance. We first demonstrate that decomposed correction\noutperforms direct correction since detecting and fixing mistakes with the\nresults of the decomposed sub-tasks is easier than with SQL. Based on this\nanalysis, we introduce Decomposed Automation Correction (DAC), which corrects\nSQL by decomposing text-to-SQL into entity linking and skeleton parsing. DAC\nfirst generates the entity and skeleton corresponding to the question and then\ncompares the differences between the initial SQL and the generated entities and\nskeleton as feedback for correction. Experimental results show that our method\nimproves performance by $3.7\\%$ on average of Spider, Bird, and KaggleDBQA\ncompared with the baseline method, demonstrating the effectiveness of DAC."
                },
                "authors": [
                    {
                        "name": "Dingzirui Wang"
                    },
                    {
                        "name": "Longxu Dou"
                    },
                    {
                        "name": "Xuanliang Zhang"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12775v2",
                "updated": "2024-08-27T06:02:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    6,
                    2,
                    55,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-23T00:49:36Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    49,
                    36,
                    4,
                    236,
                    0
                ],
                "title": "Intelligent OPC Engineer Assistant for Semiconductor Manufacturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent OPC Engineer Assistant for Semiconductor Manufacturing"
                },
                "summary": "Advancements in chip design and manufacturing have enabled the processing of\ncomplex tasks such as deep learning and natural language processing, paving the\nway for the development of artificial general intelligence (AGI). AI, on the\nother hand, can be leveraged to innovate and streamline semiconductor\ntechnology from planning and implementation to manufacturing. In this paper, we\npresent \\textit{Intelligent OPC Engineer Assistant}, an AI/LLM-powered\nmethodology designed to solve the core manufacturing-aware optimization problem\nknown as optical proximity correction (OPC). The methodology involves a\nreinforcement learning-based OPC recipe search and a customized multi-modal\nagent system for recipe summarization. Experiments demonstrate that our\nmethodology can efficiently build OPC recipes on various chip designs with\nspecially handled design topologies, a task that typically requires the\nfull-time effort of OPC engineers with years of experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in chip design and manufacturing have enabled the processing of\ncomplex tasks such as deep learning and natural language processing, paving the\nway for the development of artificial general intelligence (AGI). AI, on the\nother hand, can be leveraged to innovate and streamline semiconductor\ntechnology from planning and implementation to manufacturing. In this paper, we\npresent \\textit{Intelligent OPC Engineer Assistant}, an AI/LLM-powered\nmethodology designed to solve the core manufacturing-aware optimization problem\nknown as optical proximity correction (OPC). The methodology involves a\nreinforcement learning-based OPC recipe search and a customized multi-modal\nagent system for recipe summarization. Experiments demonstrate that our\nmethodology can efficiently build OPC recipes on various chip designs with\nspecially handled design topologies, a task that typically requires the\nfull-time effort of OPC engineers with years of experience."
                },
                "authors": [
                    {
                        "name": "Guojin Chen"
                    },
                    {
                        "name": "Haoyu Yang"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Haoxing Ren"
                    }
                ],
                "author_detail": {
                    "name": "Haoxing Ren"
                },
                "author": "Haoxing Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08072v2",
                "updated": "2024-08-27T04:50:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    4,
                    50,
                    12,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-15T10:44:38Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    44,
                    38,
                    3,
                    228,
                    0
                ],
                "title": "I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative\n  Self-Enhancement Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative\n  Self-Enhancement Paradigm"
                },
                "summary": "Large Language Models (LLMs) have achieved significant advancements, however,\nthe common learning paradigm treats LLMs as passive information repositories,\nneglecting their potential for active learning and alignment. Some approaches\ntrain LLMs using their own generated synthetic data, exploring the possibility\nof active alignment. However, there is still a huge gap between these one-time\nalignment methods and the continuous automatic alignment of humans. In this\npaper, we introduce \\textbf{I-SHEEP}, an \\textbf{I}terative\n\\textbf{S}elf-En\\textbf{H}anc\\textbf{E}m\\textbf{E}nt \\textbf{P}aradigm.This\nhuman-like paradigm enables LLMs to \\textbf{continuously self-align from\nscratch with nothing}. Compared to the one-time alignment method Dromedary\n\\cite{sun2023principledriven}, which refers to the first iteration in this\npaper, I-SHEEP can significantly enhance capacities on both Qwen and Llama\nmodels. I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca\nEval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval\naccuracy over subsequent iterations in Qwen-1.5 72B model. Additionally,\nI-SHEEP surpasses the base model in various standard benchmark generation\ntasks, achieving an average improvement of 24.77\\% in code generation tasks,\n12.04\\% in TrivialQA, and 20.29\\% in SQuAD. We also provide new insights based\non the experiment results. Our codes, datasets, and models are available at\n\\textbf{https://anonymous.4open.science/r/I-SHEEP}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant advancements, however,\nthe common learning paradigm treats LLMs as passive information repositories,\nneglecting their potential for active learning and alignment. Some approaches\ntrain LLMs using their own generated synthetic data, exploring the possibility\nof active alignment. However, there is still a huge gap between these one-time\nalignment methods and the continuous automatic alignment of humans. In this\npaper, we introduce \\textbf{I-SHEEP}, an \\textbf{I}terative\n\\textbf{S}elf-En\\textbf{H}anc\\textbf{E}m\\textbf{E}nt \\textbf{P}aradigm.This\nhuman-like paradigm enables LLMs to \\textbf{continuously self-align from\nscratch with nothing}. Compared to the one-time alignment method Dromedary\n\\cite{sun2023principledriven}, which refers to the first iteration in this\npaper, I-SHEEP can significantly enhance capacities on both Qwen and Llama\nmodels. I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca\nEval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval\naccuracy over subsequent iterations in Qwen-1.5 72B model. Additionally,\nI-SHEEP surpasses the base model in various standard benchmark generation\ntasks, achieving an average improvement of 24.77\\% in code generation tasks,\n12.04\\% in TrivialQA, and 20.29\\% in SQuAD. We also provide new insights based\non the experiment results. Our codes, datasets, and models are available at\n\\textbf{https://anonymous.4open.science/r/I-SHEEP}."
                },
                "authors": [
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Jiawei Guo"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Zhenzhu Yang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14774v1",
                "updated": "2024-08-27T04:31:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    4,
                    31,
                    58,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T04:31:58Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    4,
                    31,
                    58,
                    1,
                    240,
                    0
                ],
                "title": "Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning"
                },
                "summary": "We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just $4$K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in $20\\%$ of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just $4$K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in $20\\%$ of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings."
                },
                "authors": [
                    {
                        "name": "Simran Kaur"
                    },
                    {
                        "name": "Simon Park"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Sanjeev Arora"
                    }
                ],
                "author_detail": {
                    "name": "Sanjeev Arora"
                },
                "author": "Sanjeev Arora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14470v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14470v2",
                "updated": "2024-08-27T03:56:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    3,
                    56,
                    11,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-26T17:58:53Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    58,
                    53,
                    0,
                    239,
                    0
                ],
                "title": "Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large\n  Language Models"
                },
                "summary": "Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. A class of parameter-efficient fine-tuning\n(PEFT) aims to mitigate these computational challenges by selectively\nfine-tuning only a small fraction of the model parameters. Although\ncomputationally efficient, these techniques often fail to match the performance\nof fully fine-tuned models, primarily due to inherent biases introduced during\nparameter selection. Traditional selective PEFT techniques use a fixed set of\nparameters based on a predefined budget (a process also known as unmasking),\nfailing to capture parameter importance dynamically and often ending up\nexceeding the budget. We introduce $\\text{ID}^3$, a novel selective PEFT method\nthat calculates parameter importance continually and dynamically unmasks\nparameters by balancing exploration and exploitation in parameter selection.\nOur empirical study on 15 tasks spanning natural language understanding and\ngenerative tasks demonstrates the effectiveness of our method compared to\nfixed-masking-based PEFT techniques. We analytically show that $\\text{ID}^3$\nreduces the number of gradient updates by a factor of two, enhancing\ncomputational efficiency. $\\text{ID}^3$ is robust to random initialization of\nneurons and, therefore, can be seamlessly integrated into existing additive and\nreparametrization-based PEFT modules such as adapters and LoRA for dynamic\nsparsification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. A class of parameter-efficient fine-tuning\n(PEFT) aims to mitigate these computational challenges by selectively\nfine-tuning only a small fraction of the model parameters. Although\ncomputationally efficient, these techniques often fail to match the performance\nof fully fine-tuned models, primarily due to inherent biases introduced during\nparameter selection. Traditional selective PEFT techniques use a fixed set of\nparameters based on a predefined budget (a process also known as unmasking),\nfailing to capture parameter importance dynamically and often ending up\nexceeding the budget. We introduce $\\text{ID}^3$, a novel selective PEFT method\nthat calculates parameter importance continually and dynamically unmasks\nparameters by balancing exploration and exploitation in parameter selection.\nOur empirical study on 15 tasks spanning natural language understanding and\ngenerative tasks demonstrates the effectiveness of our method compared to\nfixed-masking-based PEFT techniques. We analytically show that $\\text{ID}^3$\nreduces the number of gradient updates by a factor of two, enhancing\ncomputational efficiency. $\\text{ID}^3$ is robust to random initialization of\nneurons and, therefore, can be seamlessly integrated into existing additive and\nreparametrization-based PEFT modules such as adapters and LoRA for dynamic\nsparsification."
                },
                "authors": [
                    {
                        "name": "Aradhye Agarwal"
                    },
                    {
                        "name": "Suhas K Ramesh"
                    },
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "15 pages, 7 tables, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14470v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14470v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13184v2",
                "updated": "2024-08-27T03:27:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    3,
                    27,
                    8,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-23T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    2,
                    54,
                    4,
                    236,
                    0
                ],
                "title": "Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating\n  the Hallucination for Path Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating\n  the Hallucination for Path Planning"
                },
                "summary": "Spatial reasoning in Large Language Models (LLMs) is the foundation for\nembodied intelligence. However, even in simple maze environments, LLMs still\nencounter challenges in long-term path-planning, primarily influenced by their\nspatial hallucination and context inconsistency hallucination by long-term\nreasoning. To address this challenge, this study proposes an innovative model,\nSpatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To\naddress the spatial hallucination of LLMs, we propose the Spatial-to-Relational\napproach, which transforms spatial prompts into entity relations and paths\nrepresenting entity relation chains. This approach fully taps the potential of\nLLMs in terms of sequential thinking. As a result, we design a path-planning\nalgorithm based on Q-learning to mitigate the context inconsistency\nhallucination, which enhances the reasoning ability of LLMs. Using the Q-value\nof state-action as auxiliary information for prompts, we correct the\nhallucinations of LLMs, thereby guiding LLMs to learn the optimal path.\nFinally, we propose a reverse curriculum learning technique based on LLMs to\nfurther mitigate the context inconsistency hallucination. LLMs can rapidly\naccumulate successful experiences by reducing task difficulty and leveraging\nthem to tackle more complex tasks. We performed comprehensive experiments based\non Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our\nS2RCQL achieved a 23%--40% improvement in both success and optimality rates\ncompared with advanced prompt engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial reasoning in Large Language Models (LLMs) is the foundation for\nembodied intelligence. However, even in simple maze environments, LLMs still\nencounter challenges in long-term path-planning, primarily influenced by their\nspatial hallucination and context inconsistency hallucination by long-term\nreasoning. To address this challenge, this study proposes an innovative model,\nSpatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To\naddress the spatial hallucination of LLMs, we propose the Spatial-to-Relational\napproach, which transforms spatial prompts into entity relations and paths\nrepresenting entity relation chains. This approach fully taps the potential of\nLLMs in terms of sequential thinking. As a result, we design a path-planning\nalgorithm based on Q-learning to mitigate the context inconsistency\nhallucination, which enhances the reasoning ability of LLMs. Using the Q-value\nof state-action as auxiliary information for prompts, we correct the\nhallucinations of LLMs, thereby guiding LLMs to learn the optimal path.\nFinally, we propose a reverse curriculum learning technique based on LLMs to\nfurther mitigate the context inconsistency hallucination. LLMs can rapidly\naccumulate successful experiences by reducing task difficulty and leveraging\nthem to tackle more complex tasks. We performed comprehensive experiments based\non Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our\nS2RCQL achieved a 23%--40% improvement in both success and optimality rates\ncompared with advanced prompt engineering."
                },
                "authors": [
                    {
                        "name": "Hourui Deng"
                    },
                    {
                        "name": "Hongjie Zhang"
                    },
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Chaosheng Feng"
                    }
                ],
                "author_detail": {
                    "name": "Chaosheng Feng"
                },
                "author": "Chaosheng Feng",
                "arxiv_comment": "Submitted to ICASSP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01262v3",
                "updated": "2024-08-27T03:13:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    3,
                    13,
                    50,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-02T13:35:11Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    13,
                    35,
                    11,
                    4,
                    215,
                    0
                ],
                "title": "RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems have demonstrated their\nadvantages in alleviating the hallucination of Large Language Models (LLMs).\nExisting RAG benchmarks mainly focus on evaluating whether LLMs can correctly\nanswer the general knowledge. However, they are unable to evaluate the\neffectiveness of the RAG system in dealing with the data from different\nvertical domains. This paper introduces RAGEval, a framework for automatically\ngenerating evaluation datasets to evaluate the knowledge usage ability of\ndifferent LLMs in different scenarios. Specifically, RAGEval summarizes a\nschema from seed documents, applies the configurations to generate diverse\ndocuments, and constructs question-answering pairs according to both articles\nand configurations. We propose three novel metrics, Completeness,\nHallucination, and Irrelevance, to carefully evaluate the responses generated\nby LLMs. By benchmarking RAG models in vertical domains, RAGEval has the\nability to better evaluate the knowledge usage ability of LLMs, which avoids\nthe confusion regarding the source of knowledge in answering question in\nexisting QA datasets--whether it comes from parameterized memory or retrieval.\nThe code and dataset will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems have demonstrated their\nadvantages in alleviating the hallucination of Large Language Models (LLMs).\nExisting RAG benchmarks mainly focus on evaluating whether LLMs can correctly\nanswer the general knowledge. However, they are unable to evaluate the\neffectiveness of the RAG system in dealing with the data from different\nvertical domains. This paper introduces RAGEval, a framework for automatically\ngenerating evaluation datasets to evaluate the knowledge usage ability of\ndifferent LLMs in different scenarios. Specifically, RAGEval summarizes a\nschema from seed documents, applies the configurations to generate diverse\ndocuments, and constructs question-answering pairs according to both articles\nand configurations. We propose three novel metrics, Completeness,\nHallucination, and Irrelevance, to carefully evaluate the responses generated\nby LLMs. By benchmarking RAG models in vertical domains, RAGEval has the\nability to better evaluate the knowledge usage ability of LLMs, which avoids\nthe confusion regarding the source of knowledge in answering question in\nexisting QA datasets--whether it comes from parameterized memory or retrieval.\nThe code and dataset will be released."
                },
                "authors": [
                    {
                        "name": "Kunlun Zhu"
                    },
                    {
                        "name": "Yifan Luo"
                    },
                    {
                        "name": "Dingling Xu"
                    },
                    {
                        "name": "Ruobing Wang"
                    },
                    {
                        "name": "Shi Yu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "add github repo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10903v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10903v4",
                "updated": "2024-08-27T02:58:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    58,
                    39,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-20T14:47:38Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    47,
                    38,
                    1,
                    233,
                    0
                ],
                "title": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model"
                },
                "summary": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue."
                },
                "authors": [
                    {
                        "name": "Yeyong Yu"
                    },
                    {
                        "name": "Runsheng Yu"
                    },
                    {
                        "name": "Haojie Wei"
                    },
                    {
                        "name": "Zhanqiu Zhang"
                    },
                    {
                        "name": "Quan Qian"
                    }
                ],
                "author_detail": {
                    "name": "Quan Qian"
                },
                "author": "Quan Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10903v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10903v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13391v2",
                "updated": "2024-08-27T02:55:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    55,
                    28,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-23T22:06:54Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    22,
                    6,
                    54,
                    4,
                    236,
                    0
                ],
                "title": "Generating Analytic Specifications for Data Visualization from Natural\n  Language Queries using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Analytic Specifications for Data Visualization from Natural\n  Language Queries using Large Language Models"
                },
                "summary": "Recently, large language models (LLMs) have shown great promise in\ntranslating natural language (NL) queries into visualizations, but their\n\"black-box\" nature often limits explainability and debuggability. In response,\nwe present a comprehensive text prompt that, given a tabular dataset and an NL\nquery about the dataset, generates an analytic specification including\n(detected) data attributes, (inferred) analytic tasks, and (recommended)\nvisualizations. This specification captures key aspects of the query\ntranslation process, affording both explainability and debuggability. For\ninstance, it provides mappings from the detected entities to the corresponding\nphrases in the input query, as well as the specific visual design principles\nthat determined the visualization recommendations. Moreover, unlike prior\nLLM-based approaches, our prompt supports conversational interaction and\nambiguity detection capabilities. In this paper, we detail the iterative\nprocess of curating our prompt, present a preliminary performance evaluation\nusing GPT-4, and discuss the strengths and limitations of LLMs at various\nstages of query translation. The prompt is open-source and integrated into\nNL4DV, a popular Python-based natural language toolkit for visualization, which\ncan be accessed at https://nl4dv.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have shown great promise in\ntranslating natural language (NL) queries into visualizations, but their\n\"black-box\" nature often limits explainability and debuggability. In response,\nwe present a comprehensive text prompt that, given a tabular dataset and an NL\nquery about the dataset, generates an analytic specification including\n(detected) data attributes, (inferred) analytic tasks, and (recommended)\nvisualizations. This specification captures key aspects of the query\ntranslation process, affording both explainability and debuggability. For\ninstance, it provides mappings from the detected entities to the corresponding\nphrases in the input query, as well as the specific visual design principles\nthat determined the visualization recommendations. Moreover, unlike prior\nLLM-based approaches, our prompt supports conversational interaction and\nambiguity detection capabilities. In this paper, we detail the iterative\nprocess of curating our prompt, present a preliminary performance evaluation\nusing GPT-4, and discuss the strengths and limitations of LLMs at various\nstages of query translation. The prompt is open-source and integrated into\nNL4DV, a popular Python-based natural language toolkit for visualization, which\ncan be accessed at https://nl4dv.github.io."
                },
                "authors": [
                    {
                        "name": "Subham Sah"
                    },
                    {
                        "name": "Rishab Mitra"
                    },
                    {
                        "name": "Arpit Narechania"
                    },
                    {
                        "name": "Alex Endert"
                    },
                    {
                        "name": "John Stasko"
                    },
                    {
                        "name": "Wenwen Dou"
                    }
                ],
                "author_detail": {
                    "name": "Wenwen Dou"
                },
                "author": "Wenwen Dou",
                "arxiv_comment": "6 pages, 3 figures. To appear in NLVIZ workshop 2024 (IEEE VIS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.10662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.10662v2",
                "updated": "2024-08-27T02:46:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    46,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2023-05-18T02:51:17Z",
                "published_parsed": [
                    2023,
                    5,
                    18,
                    2,
                    51,
                    17,
                    3,
                    138,
                    0
                ],
                "title": "Private Gradient Estimation is Useful for Generative Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private Gradient Estimation is Useful for Generative Modeling"
                },
                "summary": "While generative models have proved successful in many domains, they may pose\na privacy leakage risk in practical deployment. To address this issue,\ndifferentially private generative model learning has emerged as a solution to\ntrain private generative models for different downstream tasks. However,\nexisting private generative modeling approaches face significant challenges in\ngenerating high-dimensional data due to the inherent complexity involved in\nmodeling such data. In this work, we present a new private generative modeling\napproach where samples are generated via Hamiltonian dynamics with gradients of\nthe private dataset estimated by a well-trained network. In the approach, we\nachieve differential privacy by perturbing the projection vectors in the\nestimation of gradients with sliced score matching. In addition, we enhance the\nreconstruction ability of the model by incorporating a residual enhancement\nmodule during the score matching. For sampling, we perform Hamiltonian dynamics\nwith gradients estimated by the well-trained network, allowing the sampled data\nclose to the private dataset's manifold step by step. In this way, our model is\nable to generate data with a resolution of 256x256. Extensive experiments and\nanalysis clearly demonstrate the effectiveness and rationality of the proposed\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While generative models have proved successful in many domains, they may pose\na privacy leakage risk in practical deployment. To address this issue,\ndifferentially private generative model learning has emerged as a solution to\ntrain private generative models for different downstream tasks. However,\nexisting private generative modeling approaches face significant challenges in\ngenerating high-dimensional data due to the inherent complexity involved in\nmodeling such data. In this work, we present a new private generative modeling\napproach where samples are generated via Hamiltonian dynamics with gradients of\nthe private dataset estimated by a well-trained network. In the approach, we\nachieve differential privacy by perturbing the projection vectors in the\nestimation of gradients with sliced score matching. In addition, we enhance the\nreconstruction ability of the model by incorporating a residual enhancement\nmodule during the score matching. For sampling, we perform Hamiltonian dynamics\nwith gradients estimated by the well-trained network, allowing the sampled data\nclose to the private dataset's manifold step by step. In this way, our model is\nable to generate data with a resolution of 256x256. Extensive experiments and\nanalysis clearly demonstrate the effectiveness and rationality of the proposed\napproach."
                },
                "authors": [
                    {
                        "name": "Bochao Liu"
                    },
                    {
                        "name": "Pengju Wang"
                    },
                    {
                        "name": "Weijia Guo"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Liansheng Zhuang"
                    },
                    {
                        "name": "Weiping Wang"
                    },
                    {
                        "name": "Shiming Ge"
                    }
                ],
                "author_detail": {
                    "name": "Shiming Ge"
                },
                "author": "Shiming Ge",
                "arxiv_comment": "accepted by ACM MM 2024 Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.10662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.10662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14744v1",
                "updated": "2024-08-27T02:45:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    45,
                    26,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:45:26Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    45,
                    26,
                    1,
                    240,
                    0
                ],
                "title": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with\n  Rich Linguistic Semantics from Openly Available Data and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with\n  Rich Linguistic Semantics from Openly Available Data and Large Language\n  Models"
                },
                "summary": "Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1 million RS images, each\naccompanied by multiple descriptive captions. Extensive experiments demonstrate\nthat RSTeller enhances the performance of multiple existing vision language\nmodels for RS scene understanding through continual pre-training. Our\nmethodology significantly reduces the manual effort and expertise needed for\nannotating remote sensing imagery while democratizing access to high-quality\nannotated data. This advancement fosters progress in visual language modeling\nand encourages broader participation in remote sensing research and\napplications. The RSTeller dataset is available at\nhttps://github.com/SlytherinGe/RSTeller.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1 million RS images, each\naccompanied by multiple descriptive captions. Extensive experiments demonstrate\nthat RSTeller enhances the performance of multiple existing vision language\nmodels for RS scene understanding through continual pre-training. Our\nmethodology significantly reduces the manual effort and expertise needed for\nannotating remote sensing imagery while democratizing access to high-quality\nannotated data. This advancement fosters progress in visual language modeling\nand encourages broader participation in remote sensing research and\napplications. The RSTeller dataset is available at\nhttps://github.com/SlytherinGe/RSTeller."
                },
                "authors": [
                    {
                        "name": "Junyao Ge"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Kaitai Guo"
                    },
                    {
                        "name": "Jimin Liang"
                    }
                ],
                "author_detail": {
                    "name": "Jimin Liang"
                },
                "author": "Jimin Liang",
                "arxiv_comment": "Submitted to ISPRS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05706v2",
                "updated": "2024-08-27T02:24:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    24,
                    30,
                    1,
                    240,
                    0
                ],
                "published": "2024-02-08T14:35:09Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    14,
                    35,
                    9,
                    3,
                    39,
                    0
                ],
                "title": "Integrating Paralinguistics in Speech-Empowered Large Language Models\n  for Natural Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Paralinguistics in Speech-Empowered Large Language Models\n  for Natural Conversation"
                },
                "summary": "Recent work shows promising results in expanding the capabilities of large\nlanguage models (LLM) to directly understand and synthesize speech. However, an\nLLM-based strategy for modeling spoken dialogs remains elusive, calling for\nfurther investigation. This paper introduces an extensive speech-text LLM\nframework, the Unified Spoken Dialog Model (USDM), designed to generate\ncoherent spoken responses with naturally occurring prosodic features relevant\nto the given input speech without relying on explicit automatic speech\nrecognition (ASR) or text-to-speech (TTS) systems. We have verified the\ninclusion of prosody in speech tokens that predominantly contain semantic\ninformation and have used this foundation to construct a prosody-infused\nspeech-text model. Additionally, we propose a generalized speech-text\npretraining scheme that enhances the capture of cross-modal semantics. To\nconstruct USDM, we fine-tune our speech-text model on spoken dialog data using\na multi-step spoken dialog template that stimulates the chain-of-reasoning\ncapabilities exhibited by the underlying LLM. Automatic and human evaluations\non the DailyTalk dataset demonstrate that our approach effectively generates\nnatural-sounding spoken responses, surpassing previous and cascaded baselines.\nWe will make our code and checkpoints publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work shows promising results in expanding the capabilities of large\nlanguage models (LLM) to directly understand and synthesize speech. However, an\nLLM-based strategy for modeling spoken dialogs remains elusive, calling for\nfurther investigation. This paper introduces an extensive speech-text LLM\nframework, the Unified Spoken Dialog Model (USDM), designed to generate\ncoherent spoken responses with naturally occurring prosodic features relevant\nto the given input speech without relying on explicit automatic speech\nrecognition (ASR) or text-to-speech (TTS) systems. We have verified the\ninclusion of prosody in speech tokens that predominantly contain semantic\ninformation and have used this foundation to construct a prosody-infused\nspeech-text model. Additionally, we propose a generalized speech-text\npretraining scheme that enhances the capture of cross-modal semantics. To\nconstruct USDM, we fine-tune our speech-text model on spoken dialog data using\na multi-step spoken dialog template that stimulates the chain-of-reasoning\ncapabilities exhibited by the underlying LLM. Automatic and human evaluations\non the DailyTalk dataset demonstrate that our approach effectively generates\nnatural-sounding spoken responses, surpassing previous and cascaded baselines.\nWe will make our code and checkpoints publicly available."
                },
                "authors": [
                    {
                        "name": "Heeseung Kim"
                    },
                    {
                        "name": "Soonshin Seo"
                    },
                    {
                        "name": "Kyeongseok Jeong"
                    },
                    {
                        "name": "Ohsung Kwon"
                    },
                    {
                        "name": "Soyoon Kim"
                    },
                    {
                        "name": "Jungwhan Kim"
                    },
                    {
                        "name": "Jaehong Lee"
                    },
                    {
                        "name": "Eunwoo Song"
                    },
                    {
                        "name": "Myungwoo Oh"
                    },
                    {
                        "name": "Jung-Woo Ha"
                    },
                    {
                        "name": "Sungroh Yoon"
                    },
                    {
                        "name": "Kang Min Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Kang Min Yoo"
                },
                "author": "Kang Min Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11247v2",
                "updated": "2024-08-27T02:11:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    11,
                    32,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-20T23:54:26Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    23,
                    54,
                    26,
                    1,
                    233,
                    0
                ],
                "title": "Unboxing Occupational Bias: Grounded Debiasing of LLMs with U.S. Labor\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unboxing Occupational Bias: Grounded Debiasing of LLMs with U.S. Labor\n  Data"
                },
                "summary": "Large Language Models (LLMs) are prone to inheriting and amplifying societal\nbiases embedded within their training data, potentially reinforcing harmful\nstereotypes related to gender, occupation, and other sensitive categories. This\nissue becomes particularly problematic as biased LLMs can have far-reaching\nconsequences, leading to unfair practices and exacerbating social inequalities\nacross various domains, such as recruitment, online content moderation, or even\nthe criminal justice system. Although prior research has focused on detecting\nbias in LLMs using specialized datasets designed to highlight intrinsic biases,\nthere has been a notable lack of investigation into how these findings\ncorrelate with authoritative datasets, such as those from the U.S. National\nBureau of Labor Statistics (NBLS). To address this gap, we conduct empirical\nresearch that evaluates LLMs in a ``bias-out-of-the-box\" setting, analyzing how\nthe generated outputs compare with the distributions found in NBLS data.\nFurthermore, we propose a straightforward yet effective debiasing mechanism\nthat directly incorporates NBLS instances to mitigate bias within LLMs. Our\nstudy spans seven different LLMs, including instructable, base, and\nmixture-of-expert models, and reveals significant levels of bias that are often\noverlooked by existing bias detection techniques. Importantly, our debiasing\nmethod, which does not rely on external datasets, demonstrates a substantial\nreduction in bias scores, highlighting the efficacy of our approach in creating\nfairer and more reliable LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prone to inheriting and amplifying societal\nbiases embedded within their training data, potentially reinforcing harmful\nstereotypes related to gender, occupation, and other sensitive categories. This\nissue becomes particularly problematic as biased LLMs can have far-reaching\nconsequences, leading to unfair practices and exacerbating social inequalities\nacross various domains, such as recruitment, online content moderation, or even\nthe criminal justice system. Although prior research has focused on detecting\nbias in LLMs using specialized datasets designed to highlight intrinsic biases,\nthere has been a notable lack of investigation into how these findings\ncorrelate with authoritative datasets, such as those from the U.S. National\nBureau of Labor Statistics (NBLS). To address this gap, we conduct empirical\nresearch that evaluates LLMs in a ``bias-out-of-the-box\" setting, analyzing how\nthe generated outputs compare with the distributions found in NBLS data.\nFurthermore, we propose a straightforward yet effective debiasing mechanism\nthat directly incorporates NBLS instances to mitigate bias within LLMs. Our\nstudy spans seven different LLMs, including instructable, base, and\nmixture-of-expert models, and reveals significant levels of bias that are often\noverlooked by existing bias detection techniques. Importantly, our debiasing\nmethod, which does not rely on external datasets, demonstrates a substantial\nreduction in bias scores, highlighting the efficacy of our approach in creating\nfairer and more reliable LLMs."
                },
                "authors": [
                    {
                        "name": "Atmika Gorti"
                    },
                    {
                        "name": "Manas Gaur"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "arxiv_comment": "Accepted in AAAI Spring Symposium 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09130v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09130v4",
                "updated": "2024-08-27T01:28:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    1,
                    28,
                    12,
                    1,
                    240,
                    0
                ],
                "published": "2023-10-13T14:17:33Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    14,
                    17,
                    33,
                    4,
                    286,
                    0
                ],
                "title": "Split-and-Denoise: Protect large language model inference with local\n  differential privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split-and-Denoise: Protect large language model inference with local\n  differential privacy"
                },
                "summary": "Large Language Models (LLMs) excel in natural language understanding by\ncapturing hidden semantics in vector space. This process enriches the value of\ntext embeddings for various downstream tasks, thereby fostering the\nEmbedding-as-a-Service (EaaS) business model. However, the risk of privacy\nleakage due to direct text transmission to servers remains a critical concern.\nTo address this, we introduce Split-N-Denoise (SnD), an private inference\nframework that splits the model to execute the token embedding layer on the\nclient side at minimal computational cost. This allows the client to introduce\nnoise prior to transmitting the embeddings to the server, and subsequently\nreceive and denoise the perturbed output embeddings for downstream tasks. Our\napproach is designed for the inference stage of LLMs and requires no\nmodifications to the model parameters. Extensive experiments demonstrate SnD's\neffectiveness in optimizing the privacy-utility tradeoff across various LLM\narchitectures and diverse downstream tasks. The results reveal an improvement\nin performance under the same privacy budget compared to the baselines by over\n10\\% on average, offering clients a privacy-preserving solution for local\nprivacy protection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language understanding by\ncapturing hidden semantics in vector space. This process enriches the value of\ntext embeddings for various downstream tasks, thereby fostering the\nEmbedding-as-a-Service (EaaS) business model. However, the risk of privacy\nleakage due to direct text transmission to servers remains a critical concern.\nTo address this, we introduce Split-N-Denoise (SnD), an private inference\nframework that splits the model to execute the token embedding layer on the\nclient side at minimal computational cost. This allows the client to introduce\nnoise prior to transmitting the embeddings to the server, and subsequently\nreceive and denoise the perturbed output embeddings for downstream tasks. Our\napproach is designed for the inference stage of LLMs and requires no\nmodifications to the model parameters. Extensive experiments demonstrate SnD's\neffectiveness in optimizing the privacy-utility tradeoff across various LLM\narchitectures and diverse downstream tasks. The results reveal an improvement\nin performance under the same privacy budget compared to the baselines by over\n10\\% on average, offering clients a privacy-preserving solution for local\nprivacy protection."
                },
                "authors": [
                    {
                        "name": "Peihua Mai"
                    },
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Zhe Huang"
                    },
                    {
                        "name": "Youjia Yang"
                    },
                    {
                        "name": "Yan Pang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Pang"
                },
                "author": "Yan Pang",
                "arxiv_comment": "Proceedings of the 41st International Conference on Machine Learning,\n  22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09130v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09130v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14721v1",
                "updated": "2024-08-27T01:04:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    1,
                    4,
                    14,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T01:04:14Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    1,
                    4,
                    14,
                    1,
                    240,
                    0
                ],
                "title": "PAT: Pruning-Aware Tuning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAT: Pruning-Aware Tuning for Large Language Models"
                },
                "summary": "Large language models (LLMs) excel in language tasks, especially with\nsupervised fine-tuning after pre-training. However, their substantial memory\nand computational requirements hinder practical applications. Structural\npruning, which reduces less significant weight dimensions, is one solution.\nYet, traditional post-hoc pruning often leads to significant performance loss,\nwith limited recovery from further fine-tuning due to reduced capacity. Since\nthe model fine-tuning refines the general and chaotic knowledge in pre-trained\nmodels, we aim to incorporate structural pruning with the fine-tuning, and\npropose the Pruning-Aware Tuning (PAT) paradigm to eliminate model redundancy\nwhile preserving the model performance to the maximum extend. Specifically, we\ninsert the innovative Hybrid Sparsification Modules (HSMs) between the\nAttention and FFN components to accordingly sparsify the upstream and\ndownstream linear modules. The HSM comprises a lightweight operator and a\nglobally shared trainable mask. The lightweight operator maintains a training\noverhead comparable to that of LoRA, while the trainable mask unifies the\nchannels to be sparsified, ensuring structural pruning. Additionally, we\npropose the Identity Loss which decouples the transformation and scaling\nproperties of the HSMs to enhance training robustness. Extensive experiments\ndemonstrate that PAT excels in both performance and efficiency. For example,\nour Llama2-7b model with a 25\\% pruning ratio achieves 1.33$\\times$ speedup\nwhile outperforming the LoRA-finetuned model by up to 1.26\\% in accuracy with a\nsimilar training cost. Code:\nhttps://github.com/kriskrisliu/PAT_Pruning-Aware-Tuning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in language tasks, especially with\nsupervised fine-tuning after pre-training. However, their substantial memory\nand computational requirements hinder practical applications. Structural\npruning, which reduces less significant weight dimensions, is one solution.\nYet, traditional post-hoc pruning often leads to significant performance loss,\nwith limited recovery from further fine-tuning due to reduced capacity. Since\nthe model fine-tuning refines the general and chaotic knowledge in pre-trained\nmodels, we aim to incorporate structural pruning with the fine-tuning, and\npropose the Pruning-Aware Tuning (PAT) paradigm to eliminate model redundancy\nwhile preserving the model performance to the maximum extend. Specifically, we\ninsert the innovative Hybrid Sparsification Modules (HSMs) between the\nAttention and FFN components to accordingly sparsify the upstream and\ndownstream linear modules. The HSM comprises a lightweight operator and a\nglobally shared trainable mask. The lightweight operator maintains a training\noverhead comparable to that of LoRA, while the trainable mask unifies the\nchannels to be sparsified, ensuring structural pruning. Additionally, we\npropose the Identity Loss which decouples the transformation and scaling\nproperties of the HSMs to enhance training robustness. Extensive experiments\ndemonstrate that PAT excels in both performance and efficiency. For example,\nour Llama2-7b model with a 25\\% pruning ratio achieves 1.33$\\times$ speedup\nwhile outperforming the LoRA-finetuned model by up to 1.26\\% in accuracy with a\nsimilar training cost. Code:\nhttps://github.com/kriskrisliu/PAT_Pruning-Aware-Tuning"
                },
                "authors": [
                    {
                        "name": "Yijiang Liu"
                    },
                    {
                        "name": "Huanrui Yang"
                    },
                    {
                        "name": "Youxin Chen"
                    },
                    {
                        "name": "Rongyu Zhang"
                    },
                    {
                        "name": "Miao Wang"
                    },
                    {
                        "name": "Yuan Du"
                    },
                    {
                        "name": "Li Du"
                    }
                ],
                "author_detail": {
                    "name": "Li Du"
                },
                "author": "Li Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14713v1",
                "updated": "2024-08-27T00:37:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    0,
                    37,
                    7,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T00:37:07Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    0,
                    37,
                    7,
                    1,
                    240,
                    0
                ],
                "title": "StyleSpeech: Parameter-efficient Fine Tuning for Pre-trained\n  Controllable Text-to-Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StyleSpeech: Parameter-efficient Fine Tuning for Pre-trained\n  Controllable Text-to-Speech"
                },
                "summary": "This paper introduces StyleSpeech, a novel Text-to-Speech~(TTS) system that\nenhances the naturalness and accuracy of synthesized speech. Building upon\nexisting TTS technologies, StyleSpeech incorporates a unique Style Decorator\nstructure that enables deep learning models to simultaneously learn style and\nphoneme features, improving adaptability and efficiency through the principles\nof Lower Rank Adaptation~(LoRA). LoRA allows efficient adaptation of style\nfeatures in pre-trained models. Additionally, we introduce a novel automatic\nevaluation metric, the LLM-Guided Mean Opinion Score (LLM-MOS), which employs\nlarge language models to offer an objective and robust protocol for\nautomatically assessing TTS system performance. Extensive testing on benchmark\ndatasets shows that our approach markedly outperforms existing state-of-the-art\nbaseline methods in producing natural, accurate, and high-quality speech. These\nadvancements not only pushes the boundaries of current TTS system capabilities,\nbut also facilitate the application of TTS system in more dynamic and\nspecialized, such as interactive virtual assistants, adaptive audiobooks, and\ncustomized voice for gaming. Speech samples can be found in\nhttps://style-speech.vercel.app",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces StyleSpeech, a novel Text-to-Speech~(TTS) system that\nenhances the naturalness and accuracy of synthesized speech. Building upon\nexisting TTS technologies, StyleSpeech incorporates a unique Style Decorator\nstructure that enables deep learning models to simultaneously learn style and\nphoneme features, improving adaptability and efficiency through the principles\nof Lower Rank Adaptation~(LoRA). LoRA allows efficient adaptation of style\nfeatures in pre-trained models. Additionally, we introduce a novel automatic\nevaluation metric, the LLM-Guided Mean Opinion Score (LLM-MOS), which employs\nlarge language models to offer an objective and robust protocol for\nautomatically assessing TTS system performance. Extensive testing on benchmark\ndatasets shows that our approach markedly outperforms existing state-of-the-art\nbaseline methods in producing natural, accurate, and high-quality speech. These\nadvancements not only pushes the boundaries of current TTS system capabilities,\nbut also facilitate the application of TTS system in more dynamic and\nspecialized, such as interactive virtual assistants, adaptive audiobooks, and\ncustomized voice for gaming. Speech samples can be found in\nhttps://style-speech.vercel.app"
                },
                "authors": [
                    {
                        "name": "Haowei Lou"
                    },
                    {
                        "name": "Helen Paik"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Lina Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lina Yao"
                },
                "author": "Lina Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10960v2",
                "updated": "2024-08-27T00:27:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    0,
                    27,
                    12,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-15T17:55:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    55,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "Fast Matrix Multiplications for Lookup Table-Quantized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Matrix Multiplications for Lookup Table-Quantized LLMs"
                },
                "summary": "The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times."
                },
                "authors": [
                    {
                        "name": "Han Guo"
                    },
                    {
                        "name": "William Brandon"
                    },
                    {
                        "name": "Radostin Cholakov"
                    },
                    {
                        "name": "Jonathan Ragan-Kelley"
                    },
                    {
                        "name": "Eric P. Xing"
                    },
                    {
                        "name": "Yoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Yoon Kim"
                },
                "author": "Yoon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14690v1",
                "updated": "2024-08-26T23:30:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    23,
                    30,
                    15,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T23:30:15Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    23,
                    30,
                    15,
                    0,
                    239,
                    0
                ],
                "title": "Training-Free Activation Sparsity in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Activation Sparsity in Large Language Models"
                },
                "summary": "Activation sparsity can enable practical inference speedups in large language\nmodels (LLMs) by reducing the compute and memory-movement required for matrix\nmultiplications during the forward pass. However, existing methods face\nlimitations that inhibit widespread adoption. Some approaches are tailored\ntowards older models with ReLU-based sparsity, while others require extensive\ncontinued pre-training on up to hundreds of billions of tokens. This paper\ndescribes TEAL, a simple training-free method that applies magnitude-based\nactivation sparsity to hidden states throughout the entire model. TEAL achieves\n40-50% model-wide sparsity with minimal performance degradation across Llama-2,\nLlama-3, and Mistral families, with sizes varying from 7B to 70B. We improve\nexisting sparse kernels and demonstrate wall-clock decoding speed-ups of up to\n1.53$\\times$ and 1.8$\\times$ at 40% and 50% model-wide sparsity. TEAL is\ncompatible with weight quantization, enabling further efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation sparsity can enable practical inference speedups in large language\nmodels (LLMs) by reducing the compute and memory-movement required for matrix\nmultiplications during the forward pass. However, existing methods face\nlimitations that inhibit widespread adoption. Some approaches are tailored\ntowards older models with ReLU-based sparsity, while others require extensive\ncontinued pre-training on up to hundreds of billions of tokens. This paper\ndescribes TEAL, a simple training-free method that applies magnitude-based\nactivation sparsity to hidden states throughout the entire model. TEAL achieves\n40-50% model-wide sparsity with minimal performance degradation across Llama-2,\nLlama-3, and Mistral families, with sizes varying from 7B to 70B. We improve\nexisting sparse kernels and demonstrate wall-clock decoding speed-ups of up to\n1.53$\\times$ and 1.8$\\times$ at 40% and 50% model-wide sparsity. TEAL is\ncompatible with weight quantization, enabling further efficiency gains."
                },
                "authors": [
                    {
                        "name": "James Liu"
                    },
                    {
                        "name": "Pragaash Ponnusamy"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Han Guo"
                    },
                    {
                        "name": "Yoon Kim"
                    },
                    {
                        "name": "Ben Athiwaratkun"
                    }
                ],
                "author_detail": {
                    "name": "Ben Athiwaratkun"
                },
                "author": "Ben Athiwaratkun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12320v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12320v2",
                "updated": "2024-08-26T23:05:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    23,
                    5,
                    51,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-22T11:57:07Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    11,
                    57,
                    7,
                    3,
                    235,
                    0
                ],
                "title": "PolyRouter: A Multi-LLM Querying System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolyRouter: A Multi-LLM Querying System"
                },
                "summary": "With the rapid growth of Large Language Models (LLMs) across various domains,\nnumerous new LLMs have emerged, each possessing domain-specific expertise. This\nproliferation has highlighted the need for quick, high-quality, and\ncost-effective LLM query response methods. Yet, no single LLM exists to\nefficiently balance this trilemma. Some models are powerful but extremely\ncostly, while others are fast and inexpensive but qualitatively inferior. To\naddress this challenge, we present PolyRouter, a non-monolithic LLM querying\nsystem that seamlessly integrates various LLM experts into a single query\ninterface and dynamically routes incoming queries to the most high-performant\nexpert based on query's requirements. Through extensive experiments, we\ndemonstrate that when compared to standalone expert models, PolyRouter improves\nquery efficiency by up to 40%, and leads to significant cost reductions of up\nto 30%, while maintaining or enhancing model performance by up to 10%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of Large Language Models (LLMs) across various domains,\nnumerous new LLMs have emerged, each possessing domain-specific expertise. This\nproliferation has highlighted the need for quick, high-quality, and\ncost-effective LLM query response methods. Yet, no single LLM exists to\nefficiently balance this trilemma. Some models are powerful but extremely\ncostly, while others are fast and inexpensive but qualitatively inferior. To\naddress this challenge, we present PolyRouter, a non-monolithic LLM querying\nsystem that seamlessly integrates various LLM experts into a single query\ninterface and dynamically routes incoming queries to the most high-performant\nexpert based on query's requirements. Through extensive experiments, we\ndemonstrate that when compared to standalone expert models, PolyRouter improves\nquery efficiency by up to 40%, and leads to significant cost reductions of up\nto 30%, while maintaining or enhancing model performance by up to 10%."
                },
                "authors": [
                    {
                        "name": "Dimitris Stripelis"
                    },
                    {
                        "name": "Zijian Hu"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Alay Dilipbhai Shah"
                    },
                    {
                        "name": "Han Jin"
                    },
                    {
                        "name": "Yuhang Yao"
                    },
                    {
                        "name": "Salman Avestimehr"
                    },
                    {
                        "name": "Chaoyang He"
                    }
                ],
                "author_detail": {
                    "name": "Chaoyang He"
                },
                "author": "Chaoyang He",
                "arxiv_comment": "14 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12320v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12276v2",
                "updated": "2024-08-26T23:05:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    23,
                    5,
                    42,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-19T16:40:38Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    40,
                    38,
                    0,
                    50,
                    0
                ],
                "title": "Explain then Rank: Scale Calibration of Neural Rankers Using Natural\n  Language Explanations from LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explain then Rank: Scale Calibration of Neural Rankers Using Natural\n  Language Explanations from LLMs"
                },
                "summary": "In search settings, calibrating the scores during the ranking process to\nquantities such as click-through rates or relevance levels enhances a system's\nusefulness and trustworthiness for downstream users. While previous research\nhas improved this notion of calibration for low complexity learning-to-rank\nmodels, the larger data demands and parameter count specific to modern neural\ntext rankers produce unique obstacles that hamper the efficacy of methods\nintended for the learning-to-rank setting.\n  This paper proposes exploiting large language models (LLMs) to provide\nrelevance and uncertainty signals for these neural text rankers to produce\nscale-calibrated scores through Monte Carlo sampling of natural language\nexplanations (NLEs). Our approach transforms the neural ranking task from\nranking textual query-document pairs to ranking corresponding synthesized NLEs.\nComprehensive experiments on two popular document ranking datasets show that\nthe NLE-based calibration approach consistently outperforms past calibration\nmethods and LLM-based methods for ranking, calibration, and query performance\nprediction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In search settings, calibrating the scores during the ranking process to\nquantities such as click-through rates or relevance levels enhances a system's\nusefulness and trustworthiness for downstream users. While previous research\nhas improved this notion of calibration for low complexity learning-to-rank\nmodels, the larger data demands and parameter count specific to modern neural\ntext rankers produce unique obstacles that hamper the efficacy of methods\nintended for the learning-to-rank setting.\n  This paper proposes exploiting large language models (LLMs) to provide\nrelevance and uncertainty signals for these neural text rankers to produce\nscale-calibrated scores through Monte Carlo sampling of natural language\nexplanations (NLEs). Our approach transforms the neural ranking task from\nranking textual query-document pairs to ranking corresponding synthesized NLEs.\nComprehensive experiments on two popular document ranking datasets show that\nthe NLE-based calibration approach consistently outperforms past calibration\nmethods and LLM-based methods for ranking, calibration, and query performance\nprediction tasks."
                },
                "authors": [
                    {
                        "name": "Puxuan Yu"
                    },
                    {
                        "name": "Daniel Cohen"
                    },
                    {
                        "name": "Hemank Lamba"
                    },
                    {
                        "name": "Joel Tetreault"
                    },
                    {
                        "name": "Alex Jaimes"
                    }
                ],
                "author_detail": {
                    "name": "Alex Jaimes"
                },
                "author": "Alex Jaimes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14646v1",
                "updated": "2024-08-26T21:23:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    23,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T21:23:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    23,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "ParTEETor: A System for Partial Deployments of TEEs within Tor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParTEETor: A System for Partial Deployments of TEEs within Tor"
                },
                "summary": "The Tor anonymity network allows users such as political activists and those\nunder repressive governments to protect their privacy when communicating over\nthe internet. At the same time, Tor has been demonstrated to be vulnerable to\nseveral classes of deanonymizing attacks that expose user behavior and\nidentities. Prior work has shown that these threats can be mitigated by\nleveraging trusted execution environments (TEEs). However, previous proposals\nassume that all relays in the network will be TEE-based-which as a practical\nmatter is unrealistic. In this work, we introduce ParTEETor, a Tor-variant\nsystem, which leverages partial deployments of TEEs to thwart known attacks. We\nstudy two modes of operation: non-policy and policy. Non-policy mode uses the\nexisting Tor relay selection algorithm to provide users incident security.\nPolicy mode extends the relay selection algorithm to address the classes of\nattacks by enforcing a specific TEE circuit configuration. We evaluate\nParTEETor for security, performance, and privacy. Our evaluation demonstrates\nthat at even a small TEE penetration (e.g., 10% of relays are TEE-based), users\ncan reach performance of Tor today while enforcing a security policy to\nguarantee protection from at least two classes of attacks. Overall, we find\nthat partial deployments of TEEs can substantially improve the security of Tor,\nwithout a significant impact on performance or privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Tor anonymity network allows users such as political activists and those\nunder repressive governments to protect their privacy when communicating over\nthe internet. At the same time, Tor has been demonstrated to be vulnerable to\nseveral classes of deanonymizing attacks that expose user behavior and\nidentities. Prior work has shown that these threats can be mitigated by\nleveraging trusted execution environments (TEEs). However, previous proposals\nassume that all relays in the network will be TEE-based-which as a practical\nmatter is unrealistic. In this work, we introduce ParTEETor, a Tor-variant\nsystem, which leverages partial deployments of TEEs to thwart known attacks. We\nstudy two modes of operation: non-policy and policy. Non-policy mode uses the\nexisting Tor relay selection algorithm to provide users incident security.\nPolicy mode extends the relay selection algorithm to address the classes of\nattacks by enforcing a specific TEE circuit configuration. We evaluate\nParTEETor for security, performance, and privacy. Our evaluation demonstrates\nthat at even a small TEE penetration (e.g., 10% of relays are TEE-based), users\ncan reach performance of Tor today while enforcing a security policy to\nguarantee protection from at least two classes of attacks. Overall, we find\nthat partial deployments of TEEs can substantially improve the security of Tor,\nwithout a significant impact on performance or privacy."
                },
                "authors": [
                    {
                        "name": "Rachel King"
                    },
                    {
                        "name": "Quinn Burke"
                    },
                    {
                        "name": "Yohan Beugin"
                    },
                    {
                        "name": "Blaine Hoak"
                    },
                    {
                        "name": "Kunyang Li"
                    },
                    {
                        "name": "Eric Pauley"
                    },
                    {
                        "name": "Ryan Sheatsley"
                    },
                    {
                        "name": "Patrick McDaniel"
                    }
                ],
                "author_detail": {
                    "name": "Patrick McDaniel"
                },
                "author": "Patrick McDaniel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14627v1",
                "updated": "2024-08-26T20:45:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    20,
                    45,
                    48,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T20:45:48Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    20,
                    45,
                    48,
                    0,
                    239,
                    0
                ],
                "title": "Sustainable Data Democratization: A Multifaceted Investment for an\n  Equitable Future",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sustainable Data Democratization: A Multifaceted Investment for an\n  Equitable Future"
                },
                "summary": "The urgent need for data democratization in scientific research was the focal\npoint of a panel discussion at SC23 in Denver, Colorado, from November 12 to\n17, 2023. This article summarizes the outcomes of that discussion and\nsubsequent conversations. We advocate for strategic investments in financial,\nhuman, and technological resources for sustainable data democratization.\nEmphasizing that data is central to scientific discovery and AI deployment, we\nhighlight barriers such as limited access, inadequate financial incentives for\ncross-domain collaboration, and a shortage of workforce development\ninitiatives. Our recommendations aim to guide decision-makers in fostering an\ninclusive research community, breaking down research silos, and developing a\nskilled workforce to advance scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The urgent need for data democratization in scientific research was the focal\npoint of a panel discussion at SC23 in Denver, Colorado, from November 12 to\n17, 2023. This article summarizes the outcomes of that discussion and\nsubsequent conversations. We advocate for strategic investments in financial,\nhuman, and technological resources for sustainable data democratization.\nEmphasizing that data is central to scientific discovery and AI deployment, we\nhighlight barriers such as limited access, inadequate financial incentives for\ncross-domain collaboration, and a shortage of workforce development\ninitiatives. Our recommendations aim to guide decision-makers in fostering an\ninclusive research community, breaking down research silos, and developing a\nskilled workforce to advance scientific discovery."
                },
                "authors": [
                    {
                        "name": "Michela Taufer"
                    },
                    {
                        "name": "Valerio Pascucci"
                    },
                    {
                        "name": "Christine R. Kirkpatric"
                    },
                    {
                        "name": "Ian T. Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian T. Foster"
                },
                "author": "Ian T. Foster",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14623v1",
                "updated": "2024-08-26T20:36:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    20,
                    36,
                    52,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T20:36:52Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    20,
                    36,
                    52,
                    0,
                    239,
                    0
                ],
                "title": "MODOC: A Modular Interface for Flexible Interlinking of Text Retrieval\n  and Text Generation Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MODOC: A Modular Interface for Flexible Interlinking of Text Retrieval\n  and Text Generation Functions"
                },
                "summary": "Large Language Models (LLMs) produce eloquent texts but often the content\nthey generate needs to be verified. Traditional information retrieval systems\ncan assist with this task, but most systems have not been designed with\nLLM-generated queries in mind. As such, there is a compelling need for\nintegrated systems that provide both retrieval and generation functionality\nwithin a single user interface.\n  We present MODOC, a modular user interface that leverages the capabilities of\nLLMs and provides assistance with detecting their confabulations, promoting\nintegrity in scientific writing. MODOC represents a significant step forward in\nscientific writing assistance. Its modular architecture supports flexible\nfunctions for retrieving information and for writing and generating text in a\nsingle, user-friendly interface.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) produce eloquent texts but often the content\nthey generate needs to be verified. Traditional information retrieval systems\ncan assist with this task, but most systems have not been designed with\nLLM-generated queries in mind. As such, there is a compelling need for\nintegrated systems that provide both retrieval and generation functionality\nwithin a single user interface.\n  We present MODOC, a modular user interface that leverages the capabilities of\nLLMs and provides assistance with detecting their confabulations, promoting\nintegrity in scientific writing. MODOC represents a significant step forward in\nscientific writing assistance. Its modular architecture supports flexible\nfunctions for retrieving information and for writing and generating text in a\nsingle, user-friendly interface."
                },
                "authors": [
                    {
                        "name": "Yingqiang Gao"
                    },
                    {
                        "name": "Jhony Prada"
                    },
                    {
                        "name": "Nianlong Gu"
                    },
                    {
                        "name": "Jessica Lam"
                    },
                    {
                        "name": "Richard H. R. Hahnloser"
                    }
                ],
                "author_detail": {
                    "name": "Richard H. R. Hahnloser"
                },
                "author": "Richard H. R. Hahnloser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14622v1",
                "updated": "2024-08-26T20:35:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    20,
                    35,
                    42,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T20:35:42Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    20,
                    35,
                    42,
                    0,
                    239,
                    0
                ],
                "title": "What Makes a Good Story and How Can We Measure It? A Comprehensive\n  Survey of Story Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Makes a Good Story and How Can We Measure It? A Comprehensive\n  Survey of Story Evaluation"
                },
                "summary": "With the development of artificial intelligence, particularly the success of\nLarge Language Models (LLMs), the quantity and quality of automatically\ngenerated stories have significantly increased. This has led to the need for\nautomatic story evaluation to assess the generative capabilities of computing\nsystems and analyze the quality of both automatic-generated and human-written\nstories. Evaluating a story can be more challenging than other generation\nevaluation tasks. While tasks like machine translation primarily focus on\nassessing the aspects of fluency and accuracy, story evaluation demands complex\nadditional measures such as overall coherence, character development,\ninterestingness, etc. This requires a thorough review of relevant research. In\nthis survey, we first summarize existing storytelling tasks, including\ntext-to-text, visual-to-text, and text-to-visual. We highlight their evaluation\nchallenges, identify various human criteria to measure stories, and present\nexisting benchmark datasets. Then, we propose a taxonomy to organize evaluation\nmetrics that have been developed or can be adopted for story evaluation. We\nalso provide descriptions of these metrics, along with the discussion of their\nmerits and limitations. Later, we discuss the human-AI collaboration for story\nevaluation and generation. Finally, we suggest potential future research\ndirections, extending from story evaluation to general evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of artificial intelligence, particularly the success of\nLarge Language Models (LLMs), the quantity and quality of automatically\ngenerated stories have significantly increased. This has led to the need for\nautomatic story evaluation to assess the generative capabilities of computing\nsystems and analyze the quality of both automatic-generated and human-written\nstories. Evaluating a story can be more challenging than other generation\nevaluation tasks. While tasks like machine translation primarily focus on\nassessing the aspects of fluency and accuracy, story evaluation demands complex\nadditional measures such as overall coherence, character development,\ninterestingness, etc. This requires a thorough review of relevant research. In\nthis survey, we first summarize existing storytelling tasks, including\ntext-to-text, visual-to-text, and text-to-visual. We highlight their evaluation\nchallenges, identify various human criteria to measure stories, and present\nexisting benchmark datasets. Then, we propose a taxonomy to organize evaluation\nmetrics that have been developed or can be adopted for story evaluation. We\nalso provide descriptions of these metrics, along with the discussion of their\nmerits and limitations. Later, we discuss the human-AI collaboration for story\nevaluation and generation. Finally, we suggest potential future research\ndirections, extending from story evaluation to general evaluations."
                },
                "authors": [
                    {
                        "name": "Dingyi Yang"
                    },
                    {
                        "name": "Qin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Qin Jin"
                },
                "author": "Qin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "A.1; I.2.7; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01663v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01663v3",
                "updated": "2024-08-26T20:30:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    20,
                    30,
                    40,
                    0,
                    239,
                    0
                ],
                "published": "2024-04-02T06:07:35Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    6,
                    7,
                    35,
                    1,
                    93,
                    0
                ],
                "title": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small\n  Language Models"
                },
                "summary": "Open large language models (LLMs) have significantly advanced the field of\nnatural language processing, showcasing impressive performance across various\ntasks.Despite the significant advancements in LLMs, their effective operation\nstill relies heavily on human input to accurately guide the dialogue flow, with\nagent tuning being a crucial optimization technique that involves human\nadjustments to the model for better response to such guidance.Addressing this\ndependency, our work introduces the TinyAgent model, trained on a meticulously\ncurated high-quality dataset. We also present the Collaborative Multi-Agent\nTuning (CMAT) framework, an innovative system designed to augment language\nagent capabilities through adaptive weight updates based on environmental\nfeedback. This framework fosters collaborative learning and real-time\nadaptation among multiple intelligent agents, enhancing their context-awareness\nand long-term memory. In this research, we propose a new communication agent\nframework that integrates multi-agent systems with environmental feedback\nmechanisms, offering a scalable method to explore cooperative behaviors.\nNotably, our TinyAgent-7B model exhibits performance on par with GPT-3.5,\ndespite having fewer parameters, signifying a substantial improvement in the\nefficiency and effectiveness of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open large language models (LLMs) have significantly advanced the field of\nnatural language processing, showcasing impressive performance across various\ntasks.Despite the significant advancements in LLMs, their effective operation\nstill relies heavily on human input to accurately guide the dialogue flow, with\nagent tuning being a crucial optimization technique that involves human\nadjustments to the model for better response to such guidance.Addressing this\ndependency, our work introduces the TinyAgent model, trained on a meticulously\ncurated high-quality dataset. We also present the Collaborative Multi-Agent\nTuning (CMAT) framework, an innovative system designed to augment language\nagent capabilities through adaptive weight updates based on environmental\nfeedback. This framework fosters collaborative learning and real-time\nadaptation among multiple intelligent agents, enhancing their context-awareness\nand long-term memory. In this research, we propose a new communication agent\nframework that integrates multi-agent systems with environmental feedback\nmechanisms, offering a scalable method to explore cooperative behaviors.\nNotably, our TinyAgent-7B model exhibits performance on par with GPT-3.5,\ndespite having fewer parameters, signifying a substantial improvement in the\nefficiency and effectiveness of LLMs."
                },
                "authors": [
                    {
                        "name": "Xuechen Liang"
                    },
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Yinghui Xia"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "JingSong Yang"
                    }
                ],
                "author_detail": {
                    "name": "JingSong Yang"
                },
                "author": "JingSong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01663v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01663v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06494v2",
                "updated": "2024-08-26T20:10:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    20,
                    10,
                    52,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-12T21:04:16Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    21,
                    4,
                    16,
                    0,
                    225,
                    0
                ],
                "title": "What Color Scheme is More Effective in Assisting Readers to Locate\n  Information in a Color-Coded Article?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Color Scheme is More Effective in Assisting Readers to Locate\n  Information in a Color-Coded Article?"
                },
                "summary": "Color coding, a technique assigning specific colors to cluster information\ntypes, has proven advantages in aiding human cognitive activities, especially\nreading and comprehension. The rise of Large Language Models (LLMs) has\nstreamlined document coding, enabling simple automatic text labeling with\nvarious schemes. This has the potential to make color-coding more accessible\nand benefit more users. However, the impact of color choice on information\nseeking is understudied. We conducted a user study assessing various color\nschemes' effectiveness in LLM-coded text documents, standardizing contrast\nratios to approximately 5.55:1 across schemes. Participants performed timed\ninformation-seeking tasks in color-coded scholarly abstracts. Results showed\nnon-analogous and yellow-inclusive color schemes improved performance, with the\nlatter also being more preferred by participants. These findings can inform\nbetter color scheme choices for text annotation. As LLMs advance document\ncoding, we advocate for more research focusing on the \"color\" aspect of\ncolor-coding techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Color coding, a technique assigning specific colors to cluster information\ntypes, has proven advantages in aiding human cognitive activities, especially\nreading and comprehension. The rise of Large Language Models (LLMs) has\nstreamlined document coding, enabling simple automatic text labeling with\nvarious schemes. This has the potential to make color-coding more accessible\nand benefit more users. However, the impact of color choice on information\nseeking is understudied. We conducted a user study assessing various color\nschemes' effectiveness in LLM-coded text documents, standardizing contrast\nratios to approximately 5.55:1 across schemes. Participants performed timed\ninformation-seeking tasks in color-coded scholarly abstracts. Results showed\nnon-analogous and yellow-inclusive color schemes improved performance, with the\nlatter also being more preferred by participants. These findings can inform\nbetter color scheme choices for text annotation. As LLMs advance document\ncoding, we advocate for more research focusing on the \"color\" aspect of\ncolor-coding techniques."
                },
                "authors": [
                    {
                        "name": "Ho Yin Ng"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Ting-Hao 'Kenneth' Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ting-Hao 'Kenneth' Huang"
                },
                "author": "Ting-Hao 'Kenneth' Huang",
                "arxiv_comment": "This paper will appear at IEEE VIS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11083v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11083v2",
                "updated": "2024-08-26T20:02:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    20,
                    2,
                    16,
                    0,
                    239,
                    0
                ],
                "published": "2024-05-17T20:30:49Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    20,
                    30,
                    49,
                    4,
                    138,
                    0
                ],
                "title": "Prompt Exploration with Prompt Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Exploration with Prompt Regression"
                },
                "summary": "In the advent of democratized usage of large language models (LLMs), there is\na growing desire to systematize LLM prompt creation and selection processes\nbeyond iterative trial-and-error. Prior works majorly focus on searching the\nspace of prompts without accounting for relations between prompt variations.\nHere we propose a framework, Prompt Exploration with Prompt Regression (PEPR),\nto predict the effect of prompt combinations given results for individual\nprompt elements as well as a simple method to select an effective prompt for a\ngiven use-case. We evaluate our approach with open-source LLMs of different\nsizes on several different tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the advent of democratized usage of large language models (LLMs), there is\na growing desire to systematize LLM prompt creation and selection processes\nbeyond iterative trial-and-error. Prior works majorly focus on searching the\nspace of prompts without accounting for relations between prompt variations.\nHere we propose a framework, Prompt Exploration with Prompt Regression (PEPR),\nto predict the effect of prompt combinations given results for individual\nprompt elements as well as a simple method to select an effective prompt for a\ngiven use-case. We evaluate our approach with open-source LLMs of different\nsizes on several different tasks."
                },
                "authors": [
                    {
                        "name": "Michael Feffer"
                    },
                    {
                        "name": "Ronald Xu"
                    },
                    {
                        "name": "Yuekai Sun"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Yurochkin"
                },
                "author": "Mikhail Yurochkin",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11083v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09568v2",
                "updated": "2024-08-26T19:27:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    19,
                    27,
                    46,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-18T18:45:48Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    18,
                    45,
                    48,
                    6,
                    231,
                    0
                ],
                "title": "MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in\n  Code LLMs for Automated Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in\n  Code LLMs for Automated Program Repair"
                },
                "summary": "[Context] Large Language Models (LLMs) have shown good performance in several\nsoftware development-related tasks such as program repair, documentation, code\nrefactoring, debugging, and testing. Adapters are specialized, small modules\ndesigned for parameter efficient fine-tuning of LLMs for specific tasks,\ndomains, or applications without requiring extensive retraining of the entire\nmodel. These adapters offer a more efficient way to customize LLMs for\nparticular needs, leveraging the pre-existing capabilities of the large model.\nMerging LLMs and adapters has shown promising results for various natural\nlanguage domains and tasks, enabling the use of the learned models and adapters\nwithout additional training for a new task. [Objective] This research proposes\ncontinual merging and empirically studies the capabilities of merged adapters\nin Code LLMs, specially for the Automated Program Repair (APR) task. The goal\nis to gain insights into whether and how merging task-specific adapters can\naffect the performance of APR. [Method] In our framework, MergeRepair, we plan\nto merge multiple task-specific adapters using three different merging methods\nand evaluate the performance of the merged adapter for the APR task.\nParticularly, we will employ two main merging scenarios for all three\ntechniques, (i) merging using equal-weight averaging applied on parameters of\ndifferent adapters, where all adapters are of equal importance; and (ii) our\nproposed approach, continual merging, in which we sequentially merge the\ntask-specific adapters and the order and weight of merged adapters matter. By\nexploratory study of merging techniques, we will investigate the improvement\nand generalizability of merged adapters for APR. Through continual merging, we\nwill explore the capability of merged adapters and the effect of task order, as\nit occurs in real-world software projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Context] Large Language Models (LLMs) have shown good performance in several\nsoftware development-related tasks such as program repair, documentation, code\nrefactoring, debugging, and testing. Adapters are specialized, small modules\ndesigned for parameter efficient fine-tuning of LLMs for specific tasks,\ndomains, or applications without requiring extensive retraining of the entire\nmodel. These adapters offer a more efficient way to customize LLMs for\nparticular needs, leveraging the pre-existing capabilities of the large model.\nMerging LLMs and adapters has shown promising results for various natural\nlanguage domains and tasks, enabling the use of the learned models and adapters\nwithout additional training for a new task. [Objective] This research proposes\ncontinual merging and empirically studies the capabilities of merged adapters\nin Code LLMs, specially for the Automated Program Repair (APR) task. The goal\nis to gain insights into whether and how merging task-specific adapters can\naffect the performance of APR. [Method] In our framework, MergeRepair, we plan\nto merge multiple task-specific adapters using three different merging methods\nand evaluate the performance of the merged adapter for the APR task.\nParticularly, we will employ two main merging scenarios for all three\ntechniques, (i) merging using equal-weight averaging applied on parameters of\ndifferent adapters, where all adapters are of equal importance; and (ii) our\nproposed approach, continual merging, in which we sequentially merge the\ntask-specific adapters and the order and weight of merged adapters matter. By\nexploratory study of merging techniques, we will investigate the improvement\nand generalizability of merged adapters for APR. Through continual merging, we\nwill explore the capability of merged adapters and the effect of task order, as\nit occurs in real-world software projects."
                },
                "authors": [
                    {
                        "name": "Meghdad Dehghan"
                    },
                    {
                        "name": "Jie JW Wu"
                    },
                    {
                        "name": "Fatemeh H. Fard"
                    },
                    {
                        "name": "Ali Ouni"
                    }
                ],
                "author_detail": {
                    "name": "Ali Ouni"
                },
                "author": "Ali Ouni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07892v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07892v3",
                "updated": "2024-08-26T19:02:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    19,
                    2,
                    34,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-15T02:41:25Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    41,
                    25,
                    3,
                    228,
                    0
                ],
                "title": "Personhood credentials: Artificial intelligence and the value of\n  privacy-preserving tools to distinguish who is real online",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personhood credentials: Artificial intelligence and the value of\n  privacy-preserving tools to distinguish who is real online"
                },
                "summary": "Anonymity is an important principle online. However, malicious actors have\nlong used misleading identities to conduct fraud, spread disinformation, and\ncarry out other deceptive schemes. With the advent of increasingly capable AI,\nbad actors can amplify the potential scale and effectiveness of their\noperations, intensifying the challenge of balancing anonymity and\ntrustworthiness online. In this paper, we analyze the value of a new tool to\naddress this challenge: \"personhood credentials\" (PHCs), digital credentials\nthat empower users to demonstrate that they are real people -- not AIs -- to\nonline services, without disclosing any personal information. Such credentials\ncan be issued by a range of trusted institutions -- governments or otherwise. A\nPHC system, according to our definition, could be local or global, and does not\nneed to be biometrics-based. Two trends in AI contribute to the urgency of the\nchallenge: AI's increasing indistinguishability from people online (i.e.,\nlifelike content and avatars, agentic activity), and AI's increasing\nscalability (i.e., cost-effectiveness, accessibility). Drawing on a long\nhistory of research into anonymous credentials and \"proof-of-personhood\"\nsystems, personhood credentials give people a way to signal their\ntrustworthiness on online platforms, and offer service providers new tools for\nreducing misuse by bad actors. In contrast, existing countermeasures to\nautomated deception -- such as CAPTCHAs -- are inadequate against sophisticated\nAI, while stringent identity verification solutions are insufficiently private\nfor many use-cases. After surveying the benefits of personhood credentials, we\nalso examine deployment risks and design challenges. We conclude with\nactionable next steps for policymakers, technologists, and standards bodies to\nconsider in consultation with the public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anonymity is an important principle online. However, malicious actors have\nlong used misleading identities to conduct fraud, spread disinformation, and\ncarry out other deceptive schemes. With the advent of increasingly capable AI,\nbad actors can amplify the potential scale and effectiveness of their\noperations, intensifying the challenge of balancing anonymity and\ntrustworthiness online. In this paper, we analyze the value of a new tool to\naddress this challenge: \"personhood credentials\" (PHCs), digital credentials\nthat empower users to demonstrate that they are real people -- not AIs -- to\nonline services, without disclosing any personal information. Such credentials\ncan be issued by a range of trusted institutions -- governments or otherwise. A\nPHC system, according to our definition, could be local or global, and does not\nneed to be biometrics-based. Two trends in AI contribute to the urgency of the\nchallenge: AI's increasing indistinguishability from people online (i.e.,\nlifelike content and avatars, agentic activity), and AI's increasing\nscalability (i.e., cost-effectiveness, accessibility). Drawing on a long\nhistory of research into anonymous credentials and \"proof-of-personhood\"\nsystems, personhood credentials give people a way to signal their\ntrustworthiness on online platforms, and offer service providers new tools for\nreducing misuse by bad actors. In contrast, existing countermeasures to\nautomated deception -- such as CAPTCHAs -- are inadequate against sophisticated\nAI, while stringent identity verification solutions are insufficiently private\nfor many use-cases. After surveying the benefits of personhood credentials, we\nalso examine deployment risks and design challenges. We conclude with\nactionable next steps for policymakers, technologists, and standards bodies to\nconsider in consultation with the public."
                },
                "authors": [
                    {
                        "name": "Steven Adler"
                    },
                    {
                        "name": "Zoë Hitzig"
                    },
                    {
                        "name": "Shrey Jain"
                    },
                    {
                        "name": "Catherine Brewer"
                    },
                    {
                        "name": "Wayne Chang"
                    },
                    {
                        "name": "Renée DiResta"
                    },
                    {
                        "name": "Eddy Lazzarin"
                    },
                    {
                        "name": "Sean McGregor"
                    },
                    {
                        "name": "Wendy Seltzer"
                    },
                    {
                        "name": "Divya Siddarth"
                    },
                    {
                        "name": "Nouran Soliman"
                    },
                    {
                        "name": "Tobin South"
                    },
                    {
                        "name": "Connor Spelliscy"
                    },
                    {
                        "name": "Manu Sporny"
                    },
                    {
                        "name": "Varya Srivastava"
                    },
                    {
                        "name": "John Bailey"
                    },
                    {
                        "name": "Brian Christian"
                    },
                    {
                        "name": "Andrew Critch"
                    },
                    {
                        "name": "Ronnie Falcon"
                    },
                    {
                        "name": "Heather Flanagan"
                    },
                    {
                        "name": "Kim Hamilton Duffy"
                    },
                    {
                        "name": "Eric Ho"
                    },
                    {
                        "name": "Claire R. Leibowicz"
                    },
                    {
                        "name": "Srikanth Nadhamuni"
                    },
                    {
                        "name": "Alan Z. Rozenshtein"
                    },
                    {
                        "name": "David Schnurr"
                    },
                    {
                        "name": "Evan Shapiro"
                    },
                    {
                        "name": "Lacey Strahm"
                    },
                    {
                        "name": "Andrew Trask"
                    },
                    {
                        "name": "Zoe Weinberg"
                    },
                    {
                        "name": "Cedric Whitney"
                    },
                    {
                        "name": "Tom Zick"
                    }
                ],
                "author_detail": {
                    "name": "Tom Zick"
                },
                "author": "Tom Zick",
                "arxiv_comment": "63 pages, 7 figures, 5 tables; minor additions to acknowledgments and\n  wording changes for clarity; corrected typo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07892v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07892v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14575v1",
                "updated": "2024-08-26T18:48:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    18,
                    48,
                    51,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T18:48:51Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    18,
                    48,
                    51,
                    0,
                    239,
                    0
                ],
                "title": "EVINCE: Optimizing Adversarial LLM Dialogues via Conditional Statistics\n  and Information Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVINCE: Optimizing Adversarial LLM Dialogues via Conditional Statistics\n  and Information Theory"
                },
                "summary": "This paper introduces EVINCE (Entropy and Variation IN Conditional\nExchanges), a dialogue framework advancing Artificial General Intelligence\n(AGI) by enhancing versatility, adaptivity, and reasoning in large language\nmodels (LLMs). Leveraging adversarial debate and a novel dual entropy theory,\nEVINCE improves prediction accuracy, robustness, and stability in LLMs by\nintegrating statistical modeling, information theory, and machine learning to\nbalance diverse perspective exploration with strong prior exploitation. The\nframework's effectiveness is demonstrated through consistent convergence of\ninformation-theoretic metrics, particularly improved mutual information,\nfostering productive LLM collaboration. We apply EVINCE to healthcare, showing\nimproved disease diagnosis, and discuss its broader implications for\ndecision-making across domains. This work provides theoretical foundations and\nempirical validation for EVINCE, paving the way for advancements in LLM\ncollaboration and AGI development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces EVINCE (Entropy and Variation IN Conditional\nExchanges), a dialogue framework advancing Artificial General Intelligence\n(AGI) by enhancing versatility, adaptivity, and reasoning in large language\nmodels (LLMs). Leveraging adversarial debate and a novel dual entropy theory,\nEVINCE improves prediction accuracy, robustness, and stability in LLMs by\nintegrating statistical modeling, information theory, and machine learning to\nbalance diverse perspective exploration with strong prior exploitation. The\nframework's effectiveness is demonstrated through consistent convergence of\ninformation-theoretic metrics, particularly improved mutual information,\nfostering productive LLM collaboration. We apply EVINCE to healthcare, showing\nimproved disease diagnosis, and discuss its broader implications for\ndecision-making across domains. This work provides theoretical foundations and\nempirical validation for EVINCE, paving the way for advancements in LLM\ncollaboration and AGI development."
                },
                "authors": [
                    {
                        "name": "Edward Y. Chang"
                    }
                ],
                "author_detail": {
                    "name": "Edward Y. Chang"
                },
                "author": "Edward Y. Chang",
                "arxiv_comment": "19 pages, 7 figures, four tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14572v1",
                "updated": "2024-08-26T18:42:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    18,
                    42,
                    59,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T18:42:59Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    18,
                    42,
                    59,
                    0,
                    239,
                    0
                ],
                "title": "CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting\n  Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting\n  Mitigation"
                },
                "summary": "This paper introduces CURLoRA, a novel approach to fine-tuning large language\nmodels (LLMs) that leverages CUR matrix decomposition in the context of\nLow-Rank Adaptation (LoRA). Our method addresses two critical challenges in LLM\nfine-tuning: mitigating catastrophic forgetting during continual learning and\nreducing the number of trainable parameters. We propose a unique modification\nto the CUR decomposition process, utilizing inverted probabilities for column\nand row selection which acts as an implicit regularization, and initializing\nthe $U$ matrix as a zero matrix, and only fine-tuning it. We demonstrate\nthrough experiments on multiple datasets that CURLoRA outperforms standard LoRA\nin mitigating catastrophic forgetting. It maintains model stability and\nperformance across tasks while significantly reducing the number of trainable\nparameters. Our results show that CURLoRA achieves very good and stable task\naccuracy while maintaining base model's perplexity scores fixed compared to\nLoRA upon continual fine-tuning, particularly in scenarios with limited data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces CURLoRA, a novel approach to fine-tuning large language\nmodels (LLMs) that leverages CUR matrix decomposition in the context of\nLow-Rank Adaptation (LoRA). Our method addresses two critical challenges in LLM\nfine-tuning: mitigating catastrophic forgetting during continual learning and\nreducing the number of trainable parameters. We propose a unique modification\nto the CUR decomposition process, utilizing inverted probabilities for column\nand row selection which acts as an implicit regularization, and initializing\nthe $U$ matrix as a zero matrix, and only fine-tuning it. We demonstrate\nthrough experiments on multiple datasets that CURLoRA outperforms standard LoRA\nin mitigating catastrophic forgetting. It maintains model stability and\nperformance across tasks while significantly reducing the number of trainable\nparameters. Our results show that CURLoRA achieves very good and stable task\naccuracy while maintaining base model's perplexity scores fixed compared to\nLoRA upon continual fine-tuning, particularly in scenarios with limited data."
                },
                "authors": [
                    {
                        "name": "Muhammad Fawi"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Fawi"
                },
                "author": "Muhammad Fawi",
                "arxiv_doi": "10.5281/zenodo.12730055",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5281/zenodo.12730055",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.14572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Code available at https://github.com/MNoorFawi/curlora",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14568v1",
                "updated": "2024-08-26T18:39:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    18,
                    39,
                    31,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T18:39:31Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    18,
                    39,
                    31,
                    0,
                    239,
                    0
                ],
                "title": "Improving Clinical Note Generation from Complex Doctor-Patient\n  Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Clinical Note Generation from Complex Doctor-Patient\n  Conversation"
                },
                "summary": "Writing clinical notes and documenting medical exams is a critical task for\nhealthcare professionals, serving as a vital component of patient care\ndocumentation. However, manually writing these notes is time-consuming and can\nimpact the amount of time clinicians can spend on direct patient interaction\nand other tasks. Consequently, the development of automated clinical note\ngeneration systems has emerged as a clinically meaningful area of research\nwithin AI for health. In this paper, we present three key contributions to the\nfield of clinical note generation using large language models (LLMs). First, we\nintroduce CliniKnote, a comprehensive dataset consisting of 1,200 complex\ndoctor-patient conversations paired with their full clinical notes. This\ndataset, created and curated by medical experts with the help of modern neural\nnetworks, provides a valuable resource for training and evaluating models in\nclinical note generation tasks. Second, we propose the K-SOAP (Keyword,\nSubjective, Objective, Assessment, and Plan) note format, which enhances\ntraditional SOAP~\\cite{podder2023soap} (Subjective, Objective, Assessment, and\nPlan) notes by adding a keyword section at the top, allowing for quick\nidentification of essential information. Third, we develop an automatic\npipeline to generate K-SOAP notes from doctor-patient conversations and\nbenchmark various modern LLMs using various metrics. Our results demonstrate\nsignificant improvements in efficiency and performance compared to standard LLM\nfinetuning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing clinical notes and documenting medical exams is a critical task for\nhealthcare professionals, serving as a vital component of patient care\ndocumentation. However, manually writing these notes is time-consuming and can\nimpact the amount of time clinicians can spend on direct patient interaction\nand other tasks. Consequently, the development of automated clinical note\ngeneration systems has emerged as a clinically meaningful area of research\nwithin AI for health. In this paper, we present three key contributions to the\nfield of clinical note generation using large language models (LLMs). First, we\nintroduce CliniKnote, a comprehensive dataset consisting of 1,200 complex\ndoctor-patient conversations paired with their full clinical notes. This\ndataset, created and curated by medical experts with the help of modern neural\nnetworks, provides a valuable resource for training and evaluating models in\nclinical note generation tasks. Second, we propose the K-SOAP (Keyword,\nSubjective, Objective, Assessment, and Plan) note format, which enhances\ntraditional SOAP~\\cite{podder2023soap} (Subjective, Objective, Assessment, and\nPlan) notes by adding a keyword section at the top, allowing for quick\nidentification of essential information. Third, we develop an automatic\npipeline to generate K-SOAP notes from doctor-patient conversations and\nbenchmark various modern LLMs using various metrics. Our results demonstrate\nsignificant improvements in efficiency and performance compared to standard LLM\nfinetuning methods."
                },
                "authors": [
                    {
                        "name": "Yizhan Li"
                    },
                    {
                        "name": "Sifan Wu"
                    },
                    {
                        "name": "Christopher Smith"
                    },
                    {
                        "name": "Thomas Lo"
                    },
                    {
                        "name": "Bang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bang Liu"
                },
                "author": "Bang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14471v1",
                "updated": "2024-08-26T17:59:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    59,
                    1,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:59:01Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    59,
                    1,
                    0,
                    239,
                    0
                ],
                "title": "A Practitioner's Guide to Continual Multimodal Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practitioner's Guide to Continual Multimodal Pretraining"
                },
                "summary": "Multimodal foundation models serve numerous applications at the intersection\nof vision and language. Still, despite being pretrained on extensive data, they\nbecome outdated over time. To keep models updated, research into continual\npretraining mainly explores scenarios with either (1) infrequent,\nindiscriminate updates on large-scale new data, or (2) frequent, sample-level\nupdates. However, practical model deployment often operates in the gap between\nthese two limit cases, as real-world applications often demand adaptation to\nspecific subdomains, tasks or concepts -- spread over the entire, varying life\ncycle of a model. In this work, we complement current perspectives on continual\npretraining through a research test bed as well as provide comprehensive\nguidance for effective continual model updates in such scenarios. We first\nintroduce FoMo-in-Flux, a continual multimodal pretraining benchmark with\nrealistic compute constraints and practical deployment requirements,\nconstructed over 63 datasets with diverse visual and semantic coverage. Using\nFoMo-in-Flux, we explore the complex landscape of practical continual\npretraining through multiple perspectives: (1) A data-centric investigation of\ndata mixtures and stream orderings that emulate real-world deployment\nsituations, (2) a method-centric investigation ranging from simple fine-tuning\nand traditional continual learning strategies to parameter-efficient updates\nand model merging, (3) meta learning rate schedules and mechanistic design\nchoices, and (4) the influence of model and compute scaling. Together, our\ninsights provide a practitioner's guide to continual multimodal pretraining for\nreal-world deployment. Our benchmark and code is here:\nhttps://github.com/ExplainableML/fomo_in_flux.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal foundation models serve numerous applications at the intersection\nof vision and language. Still, despite being pretrained on extensive data, they\nbecome outdated over time. To keep models updated, research into continual\npretraining mainly explores scenarios with either (1) infrequent,\nindiscriminate updates on large-scale new data, or (2) frequent, sample-level\nupdates. However, practical model deployment often operates in the gap between\nthese two limit cases, as real-world applications often demand adaptation to\nspecific subdomains, tasks or concepts -- spread over the entire, varying life\ncycle of a model. In this work, we complement current perspectives on continual\npretraining through a research test bed as well as provide comprehensive\nguidance for effective continual model updates in such scenarios. We first\nintroduce FoMo-in-Flux, a continual multimodal pretraining benchmark with\nrealistic compute constraints and practical deployment requirements,\nconstructed over 63 datasets with diverse visual and semantic coverage. Using\nFoMo-in-Flux, we explore the complex landscape of practical continual\npretraining through multiple perspectives: (1) A data-centric investigation of\ndata mixtures and stream orderings that emulate real-world deployment\nsituations, (2) a method-centric investigation ranging from simple fine-tuning\nand traditional continual learning strategies to parameter-efficient updates\nand model merging, (3) meta learning rate schedules and mechanistic design\nchoices, and (4) the influence of model and compute scaling. Together, our\ninsights provide a practitioner's guide to continual multimodal pretraining for\nreal-world deployment. Our benchmark and code is here:\nhttps://github.com/ExplainableML/fomo_in_flux."
                },
                "authors": [
                    {
                        "name": "Karsten Roth"
                    },
                    {
                        "name": "Vishaal Udandarao"
                    },
                    {
                        "name": "Sebastian Dziadzio"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Mehdi Cherti"
                    },
                    {
                        "name": "Oriol Vinyals"
                    },
                    {
                        "name": "Olivier Hénaff"
                    },
                    {
                        "name": "Samuel Albanie"
                    },
                    {
                        "name": "Matthias Bethge"
                    },
                    {
                        "name": "Zeynep Akata"
                    }
                ],
                "author_detail": {
                    "name": "Zeynep Akata"
                },
                "author": "Zeynep Akata",
                "arxiv_comment": "Technical Report. 52 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14467v1",
                "updated": "2024-08-26T17:58:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    58,
                    17,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:58:17Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    58,
                    17,
                    0,
                    239,
                    0
                ],
                "title": "Explicit Inductive Inference using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit Inductive Inference using Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are reported to hold undesirable attestation\nbias on inference tasks: when asked to predict if a premise P entails a\nhypothesis H, instead of considering H's conditional truthfulness entailed by\nP, LLMs tend to use the out-of-context truth label of H as a fragile proxy. In\nthis paper, we propose a pipeline that exploits this bias to do explicit\ninductive inference. Our pipeline uses an LLM to transform a premise into a set\nof attested alternatives, and then aggregate answers of the derived new\nentailment inquiries to support the original inference prediction. On a\ndirectional predicate entailment benchmark, we demonstrate that by applying\nthis simple pipeline, we can improve the overall performance of LLMs on\ninference and substantially alleviate the impact of their attestation bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are reported to hold undesirable attestation\nbias on inference tasks: when asked to predict if a premise P entails a\nhypothesis H, instead of considering H's conditional truthfulness entailed by\nP, LLMs tend to use the out-of-context truth label of H as a fragile proxy. In\nthis paper, we propose a pipeline that exploits this bias to do explicit\ninductive inference. Our pipeline uses an LLM to transform a premise into a set\nof attested alternatives, and then aggregate answers of the derived new\nentailment inquiries to support the original inference prediction. On a\ndirectional predicate entailment benchmark, we demonstrate that by applying\nthis simple pipeline, we can improve the overall performance of LLMs on\ninference and substantially alleviate the impact of their attestation bias."
                },
                "authors": [
                    {
                        "name": "Tianyang Liu"
                    },
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Liang Cheng"
                    },
                    {
                        "name": "Mark Steedman"
                    }
                ],
                "author_detail": {
                    "name": "Mark Steedman"
                },
                "author": "Mark Steedman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11796v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11796v2",
                "updated": "2024-08-26T17:50:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    50,
                    46,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-21T17:38:48Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    38,
                    48,
                    2,
                    234,
                    0
                ],
                "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Pruning and Distillation in Practice: The Minitron Approach"
                },
                "summary": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license."
                },
                "authors": [
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "v2: Added missing references. Cleaned up runtime performance section",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11796v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11796v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.13840v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.13840v3",
                "updated": "2024-08-26T17:34:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    34,
                    44,
                    0,
                    239,
                    0
                ],
                "published": "2023-06-24T02:25:56Z",
                "published_parsed": [
                    2023,
                    6,
                    24,
                    2,
                    25,
                    56,
                    5,
                    175,
                    0
                ],
                "title": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for\n  Variability in Natural Language Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for\n  Variability in Natural Language Data"
                },
                "summary": "Current trends in pre-training Large Language Models (LLMs) primarily focus\non the scaling of model and dataset size. While the quality of pre-training\ndata is considered an important factor for training powerful LLMs, it remains a\nnebulous concept that has not been rigorously characterized. To this end, we\npropose a formalization of one key aspect of data quality -- measuring the\nvariability of natural language data -- specifically via a measure we call the\ndiversity coefficient. Our empirical analysis shows that the proposed diversity\ncoefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. Then, we measure\nthe diversity coefficient of publicly available pre-training datasets and\ndemonstrate that their formal diversity is high compared to theoretical lower\nand upper bounds. Finally, we conduct a comprehensive set of controlled\ninterventional experiments with GPT-2 and LLaMAv2 that demonstrate the\ndiversity coefficient of pre-training data characterizes useful aspects of\ndownstream model evaluation performance -- totaling 44 models of various sizes\n(51M to 7B parameters). We conclude that our formal notion of diversity is an\nimportant aspect of data quality that captures variability and causally leads\nto improved evaluation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current trends in pre-training Large Language Models (LLMs) primarily focus\non the scaling of model and dataset size. While the quality of pre-training\ndata is considered an important factor for training powerful LLMs, it remains a\nnebulous concept that has not been rigorously characterized. To this end, we\npropose a formalization of one key aspect of data quality -- measuring the\nvariability of natural language data -- specifically via a measure we call the\ndiversity coefficient. Our empirical analysis shows that the proposed diversity\ncoefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. Then, we measure\nthe diversity coefficient of publicly available pre-training datasets and\ndemonstrate that their formal diversity is high compared to theoretical lower\nand upper bounds. Finally, we conduct a comprehensive set of controlled\ninterventional experiments with GPT-2 and LLaMAv2 that demonstrate the\ndiversity coefficient of pre-training data characterizes useful aspects of\ndownstream model evaluation performance -- totaling 44 models of various sizes\n(51M to 7B parameters). We conclude that our formal notion of diversity is an\nimportant aspect of data quality that captures variability and causally leads\nto improved evaluation performance."
                },
                "authors": [
                    {
                        "name": "Brando Miranda"
                    },
                    {
                        "name": "Alycia Lee"
                    },
                    {
                        "name": "Sudharsan Sundar"
                    },
                    {
                        "name": "Allison Casasola"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    }
                ],
                "author_detail": {
                    "name": "Sanmi Koyejo"
                },
                "author": "Sanmi Koyejo",
                "arxiv_journal_ref": "Published as workshop paper in the Data-centric Machine Learning\n  Research (DMLR) Workshop, ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.13840v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.13840v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10468v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10468v3",
                "updated": "2024-08-26T17:28:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    28,
                    23,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-20T00:40:49Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    0,
                    40,
                    49,
                    1,
                    233,
                    0
                ],
                "title": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions"
                },
                "summary": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths."
                },
                "authors": [
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Zao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zao Yang"
                },
                "author": "Zao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10468v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10468v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14418v1",
                "updated": "2024-08-26T17:04:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    4,
                    0,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:04:00Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    4,
                    0,
                    0,
                    239,
                    0
                ],
                "title": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR\n  Errors with LLM-generated Synthetic Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR\n  Errors with LLM-generated Synthetic Dialogues"
                },
                "summary": "Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization."
                },
                "authors": [
                    {
                        "name": "Kuluhan Binici"
                    },
                    {
                        "name": "Abhinav Ramesh Kashyap"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Andy T. Liu"
                    },
                    {
                        "name": "Vijay Prakash Dwivedi"
                    },
                    {
                        "name": "Thanh-Tung Nguyen"
                    },
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Stefan Winkler"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Winkler"
                },
                "author": "Stefan Winkler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05720v2",
                "updated": "2024-08-26T16:48:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    48,
                    8,
                    0,
                    239,
                    0
                ],
                "published": "2024-03-08T23:17:55Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    23,
                    17,
                    55,
                    4,
                    68,
                    0
                ],
                "title": "A Dataset and Benchmark for Hospital Course Summarization with Adapted\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dataset and Benchmark for Hospital Course Summarization with Adapted\n  Large Language Models"
                },
                "summary": "Brief hospital course (BHC) summaries are clinical documents that summarize a\npatient's hospital stay. While large language models (LLMs) depict remarkable\ncapabilities in automating real-world tasks, their capabilities for healthcare\napplications such as synthesizing BHCs from clinical notes have not been shown.\nWe introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating\nclinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC\nsynthesis. Furthermore, we introduce a benchmark of the summarization\nperformance of two general-purpose LLMs and three healthcare-adapted LLMs.\n  Using clinical notes as input, we apply prompting-based (using in-context\nlearning) and fine-tuning-based adaptation strategies to three open-source LLMs\n(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,\nGPT-4). We evaluate these LLMs across multiple context-length inputs using\nnatural language similarity metrics. We further conduct a clinical study with\nfive clinicians, comparing clinician-written and LLM-generated BHCs across 30\nsamples, focusing on their potential to enhance clinical decision-making\nthrough improved summary quality. We observe that the Llama2-13B fine-tuned LLM\noutperforms other domain-adapted models given quantitative evaluation metrics\nof BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to\nincreasing context lengths of clinical note inputs than fine-tuned Llama2-13B.\nDespite comparable quantitative metrics, the reader study depicts a significant\npreference for summaries generated by GPT-4 with in-context learning compared\nto both Llama2-13B fine-tuned summaries and the original summaries,\nhighlighting the need for qualitative clinical evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brief hospital course (BHC) summaries are clinical documents that summarize a\npatient's hospital stay. While large language models (LLMs) depict remarkable\ncapabilities in automating real-world tasks, their capabilities for healthcare\napplications such as synthesizing BHCs from clinical notes have not been shown.\nWe introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating\nclinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC\nsynthesis. Furthermore, we introduce a benchmark of the summarization\nperformance of two general-purpose LLMs and three healthcare-adapted LLMs.\n  Using clinical notes as input, we apply prompting-based (using in-context\nlearning) and fine-tuning-based adaptation strategies to three open-source LLMs\n(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,\nGPT-4). We evaluate these LLMs across multiple context-length inputs using\nnatural language similarity metrics. We further conduct a clinical study with\nfive clinicians, comparing clinician-written and LLM-generated BHCs across 30\nsamples, focusing on their potential to enhance clinical decision-making\nthrough improved summary quality. We observe that the Llama2-13B fine-tuned LLM\noutperforms other domain-adapted models given quantitative evaluation metrics\nof BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to\nincreasing context lengths of clinical note inputs than fine-tuned Llama2-13B.\nDespite comparable quantitative metrics, the reader study depicts a significant\npreference for summaries generated by GPT-4 with in-context learning compared\nto both Llama2-13B fine-tuned summaries and the original summaries,\nhighlighting the need for qualitative clinical evaluation."
                },
                "authors": [
                    {
                        "name": "Asad Aali"
                    },
                    {
                        "name": "Dave Van Veen"
                    },
                    {
                        "name": "Yamin Ishraq Arefeen"
                    },
                    {
                        "name": "Jason Hom"
                    },
                    {
                        "name": "Christian Bluethgen"
                    },
                    {
                        "name": "Eduardo Pontes Reis"
                    },
                    {
                        "name": "Sergios Gatidis"
                    },
                    {
                        "name": "Namuun Clifford"
                    },
                    {
                        "name": "Joseph Daws"
                    },
                    {
                        "name": "Arash S. Tehrani"
                    },
                    {
                        "name": "Jangwon Kim"
                    },
                    {
                        "name": "Akshay S. Chaudhari"
                    }
                ],
                "author_detail": {
                    "name": "Akshay S. Chaudhari"
                },
                "author": "Akshay S. Chaudhari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.00042v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.00042v4",
                "updated": "2024-08-26T16:47:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    47,
                    4,
                    0,
                    239,
                    0
                ],
                "published": "2023-02-28T19:31:11Z",
                "published_parsed": [
                    2023,
                    2,
                    28,
                    19,
                    31,
                    11,
                    1,
                    59,
                    0
                ],
                "title": "Design, Kinematics, and Deployment of a Continuum Underwater\n  Vehicle-Manipulator System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design, Kinematics, and Deployment of a Continuum Underwater\n  Vehicle-Manipulator System"
                },
                "summary": "Underwater vehicle-manipulator systems (UVMSs) are underwater robots equipped\nwith one or more manipulators to perform intervention missions. This paper\nprovides the mechanical, electrical, and software design of a novel UVMS\nequipped with a continuum manipulator, referred to as a continuum-UVMS. A\nkinematic model for the continuum-UVMS is derived in order to build an\nalgorithm to resolve the robot's redundancy and generate joint space commands.\nDifferent methods to optimize the trajectory for specific tasks are proposed\nusing both the weighted least norm solution and the gradient projection method.\nKinematic simulation results are analyzed to assess the performance of the\nproposed algorithm. Finally, the continuum-UVMS is deployed in an experimental\ndemonstration in which both teleoperation and autonomous control are tested for\na given reference trajectory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater vehicle-manipulator systems (UVMSs) are underwater robots equipped\nwith one or more manipulators to perform intervention missions. This paper\nprovides the mechanical, electrical, and software design of a novel UVMS\nequipped with a continuum manipulator, referred to as a continuum-UVMS. A\nkinematic model for the continuum-UVMS is derived in order to build an\nalgorithm to resolve the robot's redundancy and generate joint space commands.\nDifferent methods to optimize the trajectory for specific tasks are proposed\nusing both the weighted least norm solution and the gradient projection method.\nKinematic simulation results are analyzed to assess the performance of the\nproposed algorithm. Finally, the continuum-UVMS is deployed in an experimental\ndemonstration in which both teleoperation and autonomous control are tested for\na given reference trajectory."
                },
                "authors": [
                    {
                        "name": "Justin L. Sitler"
                    },
                    {
                        "name": "Long Wang"
                    }
                ],
                "author_detail": {
                    "name": "Long Wang"
                },
                "author": "Long Wang",
                "arxiv_comment": "14 pages, ASME Journal of Mechanisms and Robotics, accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.00042v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.00042v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14398v2",
                "updated": "2024-08-28T12:03:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    3,
                    54,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-26T16:29:13Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    29,
                    13,
                    0,
                    239,
                    0
                ],
                "title": "Language-specific Calibration for Pruning Multilingual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-specific Calibration for Pruning Multilingual Language Models"
                },
                "summary": "Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art compression results in post-training and retraining-free\nsettings while maintaining high predictive performance. However, such research\nmainly considers calibrating pruning using English text, despite the\nmultilingual nature of modern LLMs and their frequent uses in non-English\nlanguages. In this paper, we set out to explore effective strategies for\ncalibrating the pruning of multilingual language models. We present the first\ncomprehensive empirical study, comparing different calibration languages for\npruning multilingual models across diverse tasks, models, and state-of-the-art\npruning techniques. Our results present practical suggestions, for example,\ncalibrating in the target language can efficiently yield lower perplexity, but\ndoes not necessarily benefit downstream tasks. Our further analysis experiments\nunveil that calibration in the target language mainly contributes to preserving\nlanguage-specific features related to fluency and coherence, but might not\ncontribute to capturing language-agnostic features such as language\nunderstanding and reasoning. Last, we provide practical recommendations for\nfuture practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art compression results in post-training and retraining-free\nsettings while maintaining high predictive performance. However, such research\nmainly considers calibrating pruning using English text, despite the\nmultilingual nature of modern LLMs and their frequent uses in non-English\nlanguages. In this paper, we set out to explore effective strategies for\ncalibrating the pruning of multilingual language models. We present the first\ncomprehensive empirical study, comparing different calibration languages for\npruning multilingual models across diverse tasks, models, and state-of-the-art\npruning techniques. Our results present practical suggestions, for example,\ncalibrating in the target language can efficiently yield lower perplexity, but\ndoes not necessarily benefit downstream tasks. Our further analysis experiments\nunveil that calibration in the target language mainly contributes to preserving\nlanguage-specific features related to fluency and coherence, but might not\ncontribute to capturing language-agnostic features such as language\nunderstanding and reasoning. Last, we provide practical recommendations for\nfuture practitioners."
                },
                "authors": [
                    {
                        "name": "Simon Kurz"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Lucie Flek"
                    },
                    {
                        "name": "Zhixue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhixue Zhao"
                },
                "author": "Zhixue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14387v1",
                "updated": "2024-08-26T16:11:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    11,
                    53,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T16:11:53Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    11,
                    53,
                    0,
                    239,
                    0
                ],
                "title": "Reprogramming Foundational Large Language Models(LLMs) for Enterprise\n  Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in\n  Copilot-Guided Cross-Modal Time Series Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reprogramming Foundational Large Language Models(LLMs) for Enterprise\n  Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in\n  Copilot-Guided Cross-Modal Time Series Representation Learning"
                },
                "summary": "Spatio-temporal forecasting plays a crucial role in various sectors such as\ntransportation systems, logistics, and supply chain management. However,\nexisting methods are limited by their ability to handle large, complex\ndatasets. To overcome this limitation, we introduce a hybrid approach that\ncombines the strengths of open-source large and small-scale language models\n(LLMs and LMs) with traditional forecasting methods. We augment traditional\nmethods with dynamic prompting and a grouped-query, multi-head attention\nmechanism to more effectively capture both intra-series and inter-series\ndependencies in evolving nonlinear time series data. In addition, we facilitate\non-premises customization by fine-tuning smaller open-source LMs for time\nseries trend analysis utilizing descriptions generated by open-source large LMs\non consumer-grade hardware using Low-Rank Adaptation with Activation Memory\nReduction (LoRA-AMR) technique to reduce computational overhead and activation\nstorage memory demands while preserving inference latency. We combine language\nmodel processing for time series trend analysis with traditional time series\nrepresentation learning method for cross-modal integration, achieving robust\nand accurate forecasts. The framework effectiveness is demonstrated through\nextensive experiments on various real-world datasets, outperforming existing\nmethods by significant margins in terms of forecast accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal forecasting plays a crucial role in various sectors such as\ntransportation systems, logistics, and supply chain management. However,\nexisting methods are limited by their ability to handle large, complex\ndatasets. To overcome this limitation, we introduce a hybrid approach that\ncombines the strengths of open-source large and small-scale language models\n(LLMs and LMs) with traditional forecasting methods. We augment traditional\nmethods with dynamic prompting and a grouped-query, multi-head attention\nmechanism to more effectively capture both intra-series and inter-series\ndependencies in evolving nonlinear time series data. In addition, we facilitate\non-premises customization by fine-tuning smaller open-source LMs for time\nseries trend analysis utilizing descriptions generated by open-source large LMs\non consumer-grade hardware using Low-Rank Adaptation with Activation Memory\nReduction (LoRA-AMR) technique to reduce computational overhead and activation\nstorage memory demands while preserving inference latency. We combine language\nmodel processing for time series trend analysis with traditional time series\nrepresentation learning method for cross-modal integration, achieving robust\nand accurate forecasts. The framework effectiveness is demonstrated through\nextensive experiments on various real-world datasets, outperforming existing\nmethods by significant margins in terms of forecast accuracy."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Chidaksh Ravuru"
                    },
                    {
                        "name": "Geethan Sannidhi"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "arxiv_comment": "Paper published at the Deployable AI (DAI) workshop at AAAI-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14380v1",
                "updated": "2024-08-26T16:00:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    0,
                    41,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T16:00:41Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    0,
                    41,
                    0,
                    239,
                    0
                ],
                "title": "Probing Causality Manipulation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Causality Manipulation of Large Language Models"
                },
                "summary": "Large language models (LLMs) have shown various ability on natural language\nprocessing, including problems about causality. It is not intuitive for LLMs to\ncommand causality, since pretrained models usually work on statistical\nassociations, and do not focus on causes and effects in sentences. So that\nprobing internal manipulation of causality is necessary for LLMs. This paper\nproposes a novel approach to probe causality manipulation hierarchically, by\nproviding different shortcuts to models and observe behaviors. We exploit\nretrieval augmented generation (RAG) and in-context learning (ICL) for models\non a designed causality classification task. We conduct experiments on\nmainstream LLMs, including GPT-4 and some smaller and domain-specific models.\nOur results suggest that LLMs can detect entities related to causality and\nrecognize direct causal relationships. However, LLMs lack specialized cognition\nfor causality, merely treating them as part of the global semantic of the\nsentence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown various ability on natural language\nprocessing, including problems about causality. It is not intuitive for LLMs to\ncommand causality, since pretrained models usually work on statistical\nassociations, and do not focus on causes and effects in sentences. So that\nprobing internal manipulation of causality is necessary for LLMs. This paper\nproposes a novel approach to probe causality manipulation hierarchically, by\nproviding different shortcuts to models and observe behaviors. We exploit\nretrieval augmented generation (RAG) and in-context learning (ICL) for models\non a designed causality classification task. We conduct experiments on\nmainstream LLMs, including GPT-4 and some smaller and domain-specific models.\nOur results suggest that LLMs can detect entities related to causality and\nrecognize direct causal relationships. However, LLMs lack specialized cognition\nfor causality, merely treating them as part of the global semantic of the\nsentence."
                },
                "authors": [
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Haibo Tong"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Dongyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyu Zhang"
                },
                "author": "Dongyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14357v1",
                "updated": "2024-08-26T15:31:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    31,
                    58,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T15:31:58Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    31,
                    58,
                    0,
                    239,
                    0
                ],
                "title": "Exploring ChatGPT App Ecosystem: Distribution, Deployment and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring ChatGPT App Ecosystem: Distribution, Deployment and Security"
                },
                "summary": "ChatGPT has enabled third-party developers to create plugins to expand\nChatGPT's capabilities.These plugins are distributed through OpenAI's plugin\nstore, making them easily accessible to users. With ChatGPT as the backbone,\nthis app ecosystem has illustrated great business potential by offering users\npersonalized services in a conversational manner. Nonetheless, many crucial\naspects regarding app development, deployment, and security of this ecosystem\nhave yet to be thoroughly studied in the research community, potentially\nhindering a broader adoption by both developers and users. In this work, we\nconduct the first comprehensive study of the ChatGPT app ecosystem, aiming to\nilluminate its landscape for our research community. Our study examines the\ndistribution and deployment models in the integration of LLMs and third-party\napps, and assesses their security and privacy implications. We uncover an\nuneven distribution of functionality among ChatGPT plugins, highlighting\nprevalent and emerging topics. We also identify severe flaws in the\nauthentication and user data protection for third-party app APIs integrated\nwithin LLMs, revealing a concerning status quo of security and privacy in this\napp ecosystem. Our work provides insights for the secure and sustainable\ndevelopment of this rapidly evolving ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT has enabled third-party developers to create plugins to expand\nChatGPT's capabilities.These plugins are distributed through OpenAI's plugin\nstore, making them easily accessible to users. With ChatGPT as the backbone,\nthis app ecosystem has illustrated great business potential by offering users\npersonalized services in a conversational manner. Nonetheless, many crucial\naspects regarding app development, deployment, and security of this ecosystem\nhave yet to be thoroughly studied in the research community, potentially\nhindering a broader adoption by both developers and users. In this work, we\nconduct the first comprehensive study of the ChatGPT app ecosystem, aiming to\nilluminate its landscape for our research community. Our study examines the\ndistribution and deployment models in the integration of LLMs and third-party\napps, and assesses their security and privacy implications. We uncover an\nuneven distribution of functionality among ChatGPT plugins, highlighting\nprevalent and emerging topics. We also identify severe flaws in the\nauthentication and user data protection for third-party app APIs integrated\nwithin LLMs, revealing a concerning status quo of security and privacy in this\napp ecosystem. Our work provides insights for the secure and sustainable\ndevelopment of this rapidly evolving ecosystem."
                },
                "authors": [
                    {
                        "name": "Chuan Yan"
                    },
                    {
                        "name": "Ruomai Ren"
                    },
                    {
                        "name": "Mark Huasong Meng"
                    },
                    {
                        "name": "Liuhuo Wan"
                    },
                    {
                        "name": "Tian Yang Ooi"
                    },
                    {
                        "name": "Guangdong Bai"
                    }
                ],
                "author_detail": {
                    "name": "Guangdong Bai"
                },
                "author": "Guangdong Bai",
                "arxiv_comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14354v1",
                "updated": "2024-08-26T15:30:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    30,
                    5,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T15:30:05Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    30,
                    5,
                    0,
                    239,
                    0
                ],
                "title": "SWE-bench-java: A GitHub Issue Resolving Benchmark for Java",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-bench-java: A GitHub Issue Resolving Benchmark for Java"
                },
                "summary": "GitHub issue resolving is a critical task in software engineering, recently\ngaining significant attention in both industry and academia. Within this task,\nSWE-bench has been released to evaluate issue resolving capabilities of large\nlanguage models (LLMs), but has so far only focused on Python version. However,\nsupporting more programming languages is also important, as there is a strong\ndemand in industry. As a first step toward multilingual support, we have\ndeveloped a Java version of SWE-bench, called SWE-bench-java. We have publicly\nreleased the dataset, along with the corresponding Docker-based evaluation\nenvironment and leaderboard, which will be continuously maintained and updated\nin the coming months. To verify the reliability of SWE-bench-java, we implement\na classic method SWE-agent and test several powerful LLMs on it. As is well\nknown, developing a high-quality multi-lingual benchmark is time-consuming and\nlabor-intensive, so we welcome contributions through pull requests or\ncollaboration to accelerate its iteration and refinement, paving the way for\nfully automated programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GitHub issue resolving is a critical task in software engineering, recently\ngaining significant attention in both industry and academia. Within this task,\nSWE-bench has been released to evaluate issue resolving capabilities of large\nlanguage models (LLMs), but has so far only focused on Python version. However,\nsupporting more programming languages is also important, as there is a strong\ndemand in industry. As a first step toward multilingual support, we have\ndeveloped a Java version of SWE-bench, called SWE-bench-java. We have publicly\nreleased the dataset, along with the corresponding Docker-based evaluation\nenvironment and leaderboard, which will be continuously maintained and updated\nin the coming months. To verify the reliability of SWE-bench-java, we implement\na classic method SWE-agent and test several powerful LLMs on it. As is well\nknown, developing a high-quality multi-lingual benchmark is time-consuming and\nlabor-intensive, so we welcome contributions through pull requests or\ncollaboration to accelerate its iteration and refinement, paving the way for\nfully automated programming."
                },
                "authors": [
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Zhirong Huang"
                    },
                    {
                        "name": "Ailun Yu"
                    },
                    {
                        "name": "Shaoxin Lin"
                    },
                    {
                        "name": "Yifan Shi"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Zongshuai Qi"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Dezhi Ran"
                    },
                    {
                        "name": "Muhan Zeng"
                    },
                    {
                        "name": "Bo Shen"
                    },
                    {
                        "name": "Pan Bian"
                    },
                    {
                        "name": "Guangtai Liang"
                    },
                    {
                        "name": "Bei Guan"
                    },
                    {
                        "name": "Pengjie Huang"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yongji Wang"
                    },
                    {
                        "name": "Qianxiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qianxiang Wang"
                },
                "author": "Qianxiang Wang",
                "arxiv_comment": "This work is in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14352v1",
                "updated": "2024-08-26T15:29:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    29,
                    34,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T15:29:34Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    29,
                    34,
                    0,
                    239,
                    0
                ],
                "title": "Assessing Contamination in Large Language Models: Introducing the\n  LogProber method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Contamination in Large Language Models: Introducing the\n  LogProber method"
                },
                "summary": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. Most recent works in the field are not tailored to\nquantify contamination on short sequences of text like we find in psychology\nquestionnaires. In the present paper we introduce LogProber, a novel,\nefficient, algorithm that we show able to detect contamination using token\nprobability in given sentences. In the second part we investigate the\nlimitations of the method and discuss how different training methods can\ncontaminate models without leaving traces in the token probabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. Most recent works in the field are not tailored to\nquantify contamination on short sequences of text like we find in psychology\nquestionnaires. In the present paper we introduce LogProber, a novel,\nefficient, algorithm that we show able to detect contamination using token\nprobability in given sentences. In the second part we investigate the\nlimitations of the method and discuss how different training methods can\ncontaminate models without leaving traces in the token probabilities."
                },
                "authors": [
                    {
                        "name": "Nicolas Yax"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Stefano Palminteri"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Palminteri"
                },
                "author": "Stefano Palminteri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14527v1",
                "updated": "2024-08-26T15:13:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    13,
                    38,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T15:13:38Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    13,
                    38,
                    0,
                    239,
                    0
                ],
                "title": "Multi-Agent Path Finding with Real Robot Dynamics and Interdependent\n  Tasks for Automated Warehouses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding with Real Robot Dynamics and Interdependent\n  Tasks for Automated Warehouses"
                },
                "summary": "Multi-Agent Path Finding (MAPF) is an important optimization problem\nunderlying the deployment of robots in automated warehouses and factories.\nDespite the large body of work on this topic, most approaches make heavy\nsimplifications, both on the environment and the agents, which make the\nresulting algorithms impractical for real-life scenarios. In this paper, we\nconsider a realistic problem of online order delivery in a warehouse, where a\nfleet of robots bring the products belonging to each order from shelves to\nworkstations. This creates a stream of inter-dependent pickup and delivery\ntasks and the associated MAPF problem consists of computing realistic\ncollision-free robot trajectories fulfilling these tasks. To solve this MAPF\nproblem, we propose an extension of the standard Prioritized Planning algorithm\nto deal with the inter-dependent tasks (Interleaved Prioritized Planning) and a\nnovel Via-Point Star (VP*) algorithm to compute an optimal dynamics-compliant\nrobot trajectory to visit a sequence of goal locations while avoiding moving\nobstacles. We prove the completeness of our approach and evaluate it in\nsimulation as well as in a real warehouse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF) is an important optimization problem\nunderlying the deployment of robots in automated warehouses and factories.\nDespite the large body of work on this topic, most approaches make heavy\nsimplifications, both on the environment and the agents, which make the\nresulting algorithms impractical for real-life scenarios. In this paper, we\nconsider a realistic problem of online order delivery in a warehouse, where a\nfleet of robots bring the products belonging to each order from shelves to\nworkstations. This creates a stream of inter-dependent pickup and delivery\ntasks and the associated MAPF problem consists of computing realistic\ncollision-free robot trajectories fulfilling these tasks. To solve this MAPF\nproblem, we propose an extension of the standard Prioritized Planning algorithm\nto deal with the inter-dependent tasks (Interleaved Prioritized Planning) and a\nnovel Via-Point Star (VP*) algorithm to compute an optimal dynamics-compliant\nrobot trajectory to visit a sequence of goal locations while avoiding moving\nobstacles. We prove the completeness of our approach and evaluate it in\nsimulation as well as in a real warehouse."
                },
                "authors": [
                    {
                        "name": "Vassilissa Lehoux-Lebacque"
                    },
                    {
                        "name": "Tomi Silander"
                    },
                    {
                        "name": "Christelle Loiodice"
                    },
                    {
                        "name": "Seungjoon Lee"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Sofia Michel"
                    }
                ],
                "author_detail": {
                    "name": "Sofia Michel"
                },
                "author": "Sofia Michel",
                "arxiv_comment": "Accepted to ECAI-2024. For related videos, see\n  https://europe.naverlabs.com/research/publications/MAPF_IPP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]