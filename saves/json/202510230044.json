[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.15878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15878v2",
                "updated": "2025-10-21T16:32:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    32,
                    50,
                    1,
                    294,
                    0
                ],
                "published": "2025-08-21T16:10:26Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    10,
                    26,
                    3,
                    233,
                    0
                ],
                "title": "Putting the Context back into Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Putting the Context back into Memory"
                },
                "summary": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices."
                },
                "authors": [
                    {
                        "name": "David A. Roberts"
                    }
                ],
                "author_detail": {
                    "name": "David A. Roberts"
                },
                "author": "David A. Roberts",
                "arxiv_comment": "Fixed errors in paragraph numbering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18716v1",
                "updated": "2025-10-21T15:17:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    17,
                    37,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:17:37Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    17,
                    37,
                    1,
                    294,
                    0
                ],
                "title": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Siyong Jian"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14576v3",
                "updated": "2025-10-21T15:13:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    13,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2024-01-26T00:27:00Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    0,
                    27,
                    0,
                    4,
                    26,
                    0
                ],
                "title": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints"
                },
                "summary": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings."
                },
                "authors": [
                    {
                        "name": "Steven W. D. Chien"
                    },
                    {
                        "name": "Kento Sato"
                    },
                    {
                        "name": "Artur Podobas"
                    },
                    {
                        "name": "Niclas Jansson"
                    },
                    {
                        "name": "Stefano Markidis"
                    },
                    {
                        "name": "Michio Honda"
                    }
                ],
                "author_detail": {
                    "name": "Michio Honda"
                },
                "author": "Michio Honda",
                "arxiv_doi": "10.1145/3772052.3772212",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772052.3772212",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SoCC 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18672v1",
                "updated": "2025-10-21T14:25:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:25:51Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study"
                },
                "summary": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving."
                },
                "authors": [
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Junpan Wu"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18586v1",
                "updated": "2025-10-21T12:39:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:39:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18546v1",
                "updated": "2025-10-21T11:52:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T11:52:44Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval"
                },
                "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon."
                },
                "authors": [
                    {
                        "name": "Zebin Yang"
                    },
                    {
                        "name": "Sunjian Zheng"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Bo Yu"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Shaoshan Liu"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v2",
                "updated": "2025-10-21T10:33:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    33,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive\n  Token Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive\n  Token Caching"
                },
                "summary": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal\nreasoning capabilities, enabling direct action generation from visual\nperception and language instructions in an end-to-end manner. However, their\nsubstantial computational cost poses a challenge for real-time robotic control,\nwhere rapid decision-making is essential. This paper introduces VLA-Cache, a\ntraining-free inference acceleration method that reduces computational overhead\nby adaptively caching and reusing static visual tokens across frames.\nExploiting the temporal continuity in robotic manipulation, VLA-Cache\nidentifies minimally changed tokens between adjacent frames and reuses their\ncached key-value representations, thereby circumventing redundant computations.\nAdditionally, to maintain action precision, VLA-Cache selectively re-computes\ntask-relevant tokens that are environmentally sensitive, ensuring the fidelity\nof critical visual information. To further optimize efficiency, we introduce a\nlayer adaptive token reusing strategy that dynamically adjusts the reuse ratio\nbased on attention concentration across decoder layers, prioritizing critical\ntokens for recomputation. Extensive experiments on two simulation platforms\n(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache\nachieves up to 1.7x speedup in CUDA latency and a 15% increase in control\nfrequency, with negligible loss on task success rate. The code and videos can\nbe found at our project page: https://vla-cache.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal\nreasoning capabilities, enabling direct action generation from visual\nperception and language instructions in an end-to-end manner. However, their\nsubstantial computational cost poses a challenge for real-time robotic control,\nwhere rapid decision-making is essential. This paper introduces VLA-Cache, a\ntraining-free inference acceleration method that reduces computational overhead\nby adaptively caching and reusing static visual tokens across frames.\nExploiting the temporal continuity in robotic manipulation, VLA-Cache\nidentifies minimally changed tokens between adjacent frames and reuses their\ncached key-value representations, thereby circumventing redundant computations.\nAdditionally, to maintain action precision, VLA-Cache selectively re-computes\ntask-relevant tokens that are environmentally sensitive, ensuring the fidelity\nof critical visual information. To further optimize efficiency, we introduce a\nlayer adaptive token reusing strategy that dynamically adjusts the reuse ratio\nbased on attention concentration across decoder layers, prioritizing critical\ntokens for recomputation. Extensive experiments on two simulation platforms\n(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache\nachieves up to 1.7x speedup in CUDA latency and a 15% increase in control\nfrequency, with negligible loss on task success rate. The code and videos can\nbe found at our project page: https://vla-cache.github.io."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v2",
                "updated": "2025-10-21T10:08:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    8,
                    33,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18480v1",
                "updated": "2025-10-21T10:00:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T10:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices"
                },
                "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs."
                },
                "authors": [
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18413v1",
                "updated": "2025-10-21T08:44:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    44,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T08:44:47Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    44,
                    47,
                    1,
                    294,
                    0
                ],
                "title": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference"
                },
                "summary": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity."
                },
                "authors": [
                    {
                        "name": "Siyuan Yan"
                    },
                    {
                        "name": "Guo-Qing Jiang"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    },
                    {
                        "name": "Ran Zhu"
                    },
                    {
                        "name": "Chun Cao"
                    },
                    {
                        "name": "Jingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jingwei Xu"
                },
                "author": "Jingwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v5",
                "updated": "2025-10-21T06:47:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    6,
                    47,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was first submitted for review on Sept. 5, 2024, and the\n  initial version was uploaded to Arxiv on Sept. 30, 2024. The latest version\n  has accepted for publication by IEEE Transactions on Information Forensics\n  and Security (TIFS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v3",
                "updated": "2025-10-21T06:30:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    6,
                    30,
                    21,
                    1,
                    294,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Steffen Christgau"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18269v1",
                "updated": "2025-10-21T03:39:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    39,
                    41,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T03:39:41Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    39,
                    41,
                    1,
                    294,
                    0
                ],
                "title": "StreamingTOM: Streaming Token Compression for Efficient Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingTOM: Streaming Token Compression for Efficient Video\n  Understanding"
                },
                "summary": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth."
                },
                "authors": [
                    {
                        "name": "Xueyi Chen"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Kele Shao"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17777v1",
                "updated": "2025-10-20T17:35:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:35:47Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference"
                },
                "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability."
                },
                "authors": [
                    {
                        "name": "Samir Khaki"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Konstantinos N. Plataniotis"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Zhijian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijian Liu"
                },
                "author": "Zhijian Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17238v1",
                "updated": "2025-10-20T07:27:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T07:27:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "title": "StreamingThinker: Large Language Models Can Think While Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingThinker: Large Language Models Can Think While Reading"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}"
                },
                "authors": [
                    {
                        "name": "Junlong Tong"
                    },
                    {
                        "name": "Yingqi Fan"
                    },
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17098v1",
                "updated": "2025-10-20T02:04:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    4,
                    18,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T02:04:18Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    4,
                    18,
                    0,
                    293,
                    0
                ],
                "title": "Can Transformer Memory Be Corrupted? Investigating Cache-Side\n  Vulnerabilities in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Transformer Memory Be Corrupted? Investigating Cache-Side\n  Vulnerabilities in Large Language Models"
                },
                "summary": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research."
                },
                "authors": [
                    {
                        "name": "Elias Hossain"
                    },
                    {
                        "name": "Swayamjit Saha"
                    },
                    {
                        "name": "Somshubhra Roy"
                    },
                    {
                        "name": "Ravi Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Prasad"
                },
                "author": "Ravi Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17045v1",
                "updated": "2025-10-19T23:17:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    23,
                    17,
                    13,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T23:17:13Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    23,
                    17,
                    13,
                    6,
                    292,
                    0
                ],
                "title": "Video Reasoning without Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Reasoning without Training"
                },
                "summary": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model."
                },
                "authors": [
                    {
                        "name": "Deepak Sridhar"
                    },
                    {
                        "name": "Kartikeya Bhardwaj"
                    },
                    {
                        "name": "Jeya Pradha Jeyaraj"
                    },
                    {
                        "name": "Nuno Vasconcelos"
                    },
                    {
                        "name": "Ankita Nayak"
                    },
                    {
                        "name": "Harris Teague"
                    }
                ],
                "author_detail": {
                    "name": "Harris Teague"
                },
                "author": "Harris Teague",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16871v1",
                "updated": "2025-10-19T15:13:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    15,
                    13,
                    25,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T15:13:25Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    15,
                    13,
                    25,
                    6,
                    292,
                    0
                ],
                "title": "Addendum: Systematic Evaluation of Randomized Cache Designs against\n  Cache Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addendum: Systematic Evaluation of Randomized Cache Designs against\n  Cache Occupancy"
                },
                "summary": "In the main text published at USENIX Security 2025, we presented a systematic\nanalysis of the role of cache occupancy in the design considerations for\nrandomized caches (from the perspectives of performance and security). On the\nperformance front, we presented a uniform benchmarking strategy that allows for\na fair comparison among different randomized cache designs. Likewise, from the\nsecurity perspective, we presented three threat assumptions: (1) covert\nchannels; (2) process fingerprinting side-channel; and (3) AES key recovery.\nThe main takeaway of our work is an open problem of designing a randomized\ncache of comparable efficiency with modern set-associative LLCs, while still\nresisting both contention-based and occupancy-based attacks. This note is meant\nas an addendum to the main text in light of the observations made in [2]. To\nsummarize, the authors in [2] argue that (1) L1d cache size plays a role in\nadversarial success, and that (2) a patched version of MIRAGE with randomized\ninitial seeding of global eviction map prevents leakage of AES key. We discuss\nthe same in this addendum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the main text published at USENIX Security 2025, we presented a systematic\nanalysis of the role of cache occupancy in the design considerations for\nrandomized caches (from the perspectives of performance and security). On the\nperformance front, we presented a uniform benchmarking strategy that allows for\na fair comparison among different randomized cache designs. Likewise, from the\nsecurity perspective, we presented three threat assumptions: (1) covert\nchannels; (2) process fingerprinting side-channel; and (3) AES key recovery.\nThe main takeaway of our work is an open problem of designing a randomized\ncache of comparable efficiency with modern set-associative LLCs, while still\nresisting both contention-based and occupancy-based attacks. This note is meant\nas an addendum to the main text in light of the observations made in [2]. To\nsummarize, the authors in [2] argue that (1) L1d cache size plays a role in\nadversarial success, and that (2) a patched version of MIRAGE with randomized\ninitial seeding of global eviction map prevents leakage of AES key. We discuss\nthe same in this addendum."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16807v1",
                "updated": "2025-10-19T12:17:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    17,
                    42,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T12:17:42Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    17,
                    42,
                    6,
                    292,
                    0
                ],
                "title": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads"
                },
                "summary": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhoutong Wu"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Yiming Dong"
                    },
                    {
                        "name": "Chenheng Zhang"
                    },
                    {
                        "name": "Cong Fang"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Zhouchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouchen Lin"
                },
                "author": "Zhouchen Lin",
                "arxiv_comment": "The code is available at:\n  \\url{https://github.com/Zhoutong-Wu/SkipV1Former}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16805v1",
                "updated": "2025-10-19T12:16:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    16,
                    40,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T12:16:40Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    16,
                    40,
                    6,
                    292,
                    0
                ],
                "title": "Mixed-Precision Quantization for Language Models: Techniques and\n  Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-Precision Quantization for Language Models: Techniques and\n  Prospects"
                },
                "summary": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models."
                },
                "authors": [
                    {
                        "name": "Mariam Rakka"
                    },
                    {
                        "name": "Marios Fournarakis"
                    },
                    {
                        "name": "Olga Krestinskaya"
                    },
                    {
                        "name": "Jinane Bazzi"
                    },
                    {
                        "name": "Khaled N. Salama"
                    },
                    {
                        "name": "Fadi Kurdahi"
                    },
                    {
                        "name": "Ahmed M. Eltawil"
                    },
                    {
                        "name": "Mohammed E. Fouda"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed E. Fouda"
                },
                "author": "Mohammed E. Fouda",
                "arxiv_comment": "46 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v2",
                "updated": "2025-10-18T11:29:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    11,
                    29,
                    52,
                    5,
                    291,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models. Nevertheless,\nminimizing the accuracy degradation caused by ultra-low-bit KV cache\nquantization remains a significant challenge. While scalar quantization is\nconstrained by 1-bit bound, vector quantization exploits intra-vector\ncorrelations and enables sub-bit regimes, making it more suitable for\nultra-low-bit quantization. To further mitigate quantization-induced\ndegradation, we reveal that the degradation is highly uneven across tokens in\nattention quality. To investigate this unevenness, we introduce anchor score to\nmeasure each token's sensitivity to quantization. Our analysis and experiments\nshow that preserving a small subset (1\\%) of tokens with the highest Anchor\nScore significantly mitigates accuracy loss under aggressive quantization.\n  We propose AnTKV, a dual-stage framework that leverages anchor token-aware\nvector quantization to compress the KV cache. It combines offline token-aware\ncentroids learning and online anchor token selection to balance compression and\naccuracy. To enable efficient deployment, we design an online anchor token\nselection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale\nto 840K tokens on a single 80GB A100, while delivering up to $3.5\\times$ higher\ndecoding throughput over the FP16 baseline. Experiments demonstrate that AnTKV\nmatches or surpasses prior methods at 4-bit, and significantly reduce\nperplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on\nMistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models. Nevertheless,\nminimizing the accuracy degradation caused by ultra-low-bit KV cache\nquantization remains a significant challenge. While scalar quantization is\nconstrained by 1-bit bound, vector quantization exploits intra-vector\ncorrelations and enables sub-bit regimes, making it more suitable for\nultra-low-bit quantization. To further mitigate quantization-induced\ndegradation, we reveal that the degradation is highly uneven across tokens in\nattention quality. To investigate this unevenness, we introduce anchor score to\nmeasure each token's sensitivity to quantization. Our analysis and experiments\nshow that preserving a small subset (1\\%) of tokens with the highest Anchor\nScore significantly mitigates accuracy loss under aggressive quantization.\n  We propose AnTKV, a dual-stage framework that leverages anchor token-aware\nvector quantization to compress the KV cache. It combines offline token-aware\ncentroids learning and online anchor token selection to balance compression and\naccuracy. To enable efficient deployment, we design an online anchor token\nselection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale\nto 840K tokens on a single 80GB A100, while delivering up to $3.5\\times$ higher\ndecoding throughput over the FP16 baseline. Experiments demonstrate that AnTKV\nmatches or surpasses prior methods at 4-bit, and significantly reduce\nperplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on\nMistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v2",
                "updated": "2025-10-18T06:04:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    6,
                    4,
                    53,
                    5,
                    291,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM\n  Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) achieves formal data privacy protection but suffers from\nsignificant latency overhead, especially for long input sequences. While\nkey-value (KV) cache eviction and sparse attention algorithms have been\nproposed for efficient LLM inference in plaintext, they are not designed for\nMPC and cannot benefit private LLM inference directly. In this paper, we\npropose an accurate and MPC-friendly KV cache eviction framework, dubbed\nMPCache, building on the observation that historical tokens in a long sequence\nmay have different effects on the downstream decoding. Hence, MPCache combines\na look-once static eviction algorithm to discard unimportant KV cache and a\nquery-aware dynamic selection algorithm to activate only a small subset of KV\ncache for attention computation. MPCache further incorporates a series of\noptimizations for efficient dynamic KV cache selection, including MPC-friendly\nsimilarity approximation, hierarchical KV cache clustering, and cross-layer\nindex-sharing strategy. Extensive experiments demonstrate that MPCache\nconsistently outperforms prior-art KV cache eviction baselines across different\ngeneration tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and\ncommunication reduction on different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) achieves formal data privacy protection but suffers from\nsignificant latency overhead, especially for long input sequences. While\nkey-value (KV) cache eviction and sparse attention algorithms have been\nproposed for efficient LLM inference in plaintext, they are not designed for\nMPC and cannot benefit private LLM inference directly. In this paper, we\npropose an accurate and MPC-friendly KV cache eviction framework, dubbed\nMPCache, building on the observation that historical tokens in a long sequence\nmay have different effects on the downstream decoding. Hence, MPCache combines\na look-once static eviction algorithm to discard unimportant KV cache and a\nquery-aware dynamic selection algorithm to activate only a small subset of KV\ncache for attention computation. MPCache further incorporates a series of\noptimizations for efficient dynamic KV cache selection, including MPC-friendly\nsimilarity approximation, hierarchical KV cache clustering, and cross-layer\nindex-sharing strategy. Extensive experiments demonstrate that MPCache\nconsistently outperforms prior-art KV cache eviction baselines across different\ngeneration tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and\ncommunication reduction on different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Tao Wei"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08907v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08907v3",
                "updated": "2025-10-18T02:48:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    2,
                    48,
                    35,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-10T01:42:14Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    1,
                    42,
                    14,
                    4,
                    283,
                    0
                ],
                "title": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors"
                },
                "summary": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Runsong Zhao"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Junyi Xiao"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Shengxiang Gao"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "18 pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08907v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08907v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16292v1",
                "updated": "2025-10-18T01:31:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    1,
                    31,
                    14,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-18T01:31:14Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    1,
                    31,
                    14,
                    5,
                    291,
                    0
                ],
                "title": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value\n  Weight Compression in Low-Precision Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value\n  Weight Compression in Low-Precision Vision-Language Models"
                },
                "summary": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}."
                },
                "authors": [
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Haiyu Wang"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "arxiv_comment": "Accepted as Spotlight paper by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16276v1",
                "updated": "2025-10-18T00:21:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    0,
                    21,
                    45,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-18T00:21:45Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    0,
                    21,
                    45,
                    5,
                    291,
                    0
                ],
                "title": "What Limits Agentic Systems Efficiency?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Limits Agentic Systems Efficiency?"
                },
                "summary": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance."
                },
                "authors": [
                    {
                        "name": "Song Bian"
                    },
                    {
                        "name": "Minghao Yan"
                    },
                    {
                        "name": "Anand Jayarajan"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "27 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15590v1",
                "updated": "2025-10-17T12:38:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    12,
                    38,
                    25,
                    4,
                    290,
                    0
                ],
                "published": "2025-10-17T12:38:25Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    12,
                    38,
                    25,
                    4,
                    290,
                    0
                ],
                "title": "A single optically detectable tumbling spin in silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A single optically detectable tumbling spin in silicon"
                },
                "summary": "Electron spin resonance spectroscopy is a widely used technique for analyzing\nthe microscopic structure, local environment and reorientation of atomic and\nmolecular systems. Conventional inductive detection methods typically require\nto probe more than a billion of electron spins such that single atom motion is\nhidden through ensemble averaging. While several single spin spectroscopy\nmethods are currently available, they have been so far limited to static\nsystems. Here we demonstrate single spin spectroscopy of a fluorescent tumbling\ndefect in silicon called the G center, behaving as a pseudo-molecule randomly\nreorienting itself in the crystalline matrix. Using high-resolution spin\nspectroscopy, we reveal a fine magnetic structure resulting from the spin\nprincipal axes jumping between discrete orientations in the crystal. By\nmodeling the atomic reorientation of the defect, we demonstrate that spin\ntumbling induces variations in the coupling to the microwave magnetic field,\nenabling position-dependent Rabi frequencies to be detected in coherent spin\ncontrol experiments. By virtue of its pseudo-molecule configuration, the G\ncenter in silicon is a unique quantum system to investigate the mutual\ninteraction between optical, spin and rotation properties in a highly versatile\nmaterial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron spin resonance spectroscopy is a widely used technique for analyzing\nthe microscopic structure, local environment and reorientation of atomic and\nmolecular systems. Conventional inductive detection methods typically require\nto probe more than a billion of electron spins such that single atom motion is\nhidden through ensemble averaging. While several single spin spectroscopy\nmethods are currently available, they have been so far limited to static\nsystems. Here we demonstrate single spin spectroscopy of a fluorescent tumbling\ndefect in silicon called the G center, behaving as a pseudo-molecule randomly\nreorienting itself in the crystalline matrix. Using high-resolution spin\nspectroscopy, we reveal a fine magnetic structure resulting from the spin\nprincipal axes jumping between discrete orientations in the crystal. By\nmodeling the atomic reorientation of the defect, we demonstrate that spin\ntumbling induces variations in the coupling to the microwave magnetic field,\nenabling position-dependent Rabi frequencies to be detected in coherent spin\ncontrol experiments. By virtue of its pseudo-molecule configuration, the G\ncenter in silicon is a unique quantum system to investigate the mutual\ninteraction between optical, spin and rotation properties in a highly versatile\nmaterial."
                },
                "authors": [
                    {
                        "name": "Félix Cache"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Frédéric Mazen"
                    },
                    {
                        "name": "Frédéric Milési"
                    },
                    {
                        "name": "Sébastien Kerdilès"
                    },
                    {
                        "name": "Isabelle Robert-Philip"
                    },
                    {
                        "name": "Jean-Michel Gérard"
                    },
                    {
                        "name": "Guillaume Cassabois"
                    },
                    {
                        "name": "Vincent Jacques"
                    },
                    {
                        "name": "Anaïs Dréau"
                    }
                ],
                "author_detail": {
                    "name": "Anaïs Dréau"
                },
                "author": "Anaïs Dréau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15413v1",
                "updated": "2025-10-17T08:07:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    8,
                    7,
                    27,
                    4,
                    290,
                    0
                ],
                "published": "2025-10-17T08:07:27Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    8,
                    7,
                    27,
                    4,
                    290,
                    0
                ],
                "title": "FHE-SQL: Fully Homomorphic Encrypted SQL Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FHE-SQL: Fully Homomorphic Encrypted SQL Database"
                },
                "summary": "FHE-SQL is a privacy-preserving database system that enables secure query\nprocessing on encrypted data using Fully Homomorphic Encryption (FHE),\nproviding privacy guaranties where an untrusted server can execute encrypted\nqueries without learning either the query contents or the underlying data.\nUnlike property-preserving encryption-based systems such as CryptDB, which rely\non deterministic or order-preserving encryption and are vulnerable to\nfrequency, order, and equality-pattern inference attacks, FHE-SQL performs\ncomputations entirely under encryption, eliminating these leakage channels.\nCompared to trusted-hardware approaches such as TrustedDB, which depend on a\nhardware security module and thus inherit its trust and side-channel\nlimitations, our design achieves end-to-end cryptographic protection without\nrequiring trusted execution environments. In contrast to high-performance\nFHE-based engines-Hermes, which target specialized workloads such as vector\nsearch, FHE-SQL supports general SQL query semantics with schema-aware,\ntype-safe definitions suitable for relational data management. FHE-SQL\nmitigates the high cost of ciphertext space by using an indirection\narchitecture that separates metadata in RocksDB from large ciphertexts in blob\nstorage. It supports oblivious selection via homomorphic boolean masks,\nmulti-tier caching, and garbage collection, with security proven under the\nUniversal Composability framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FHE-SQL is a privacy-preserving database system that enables secure query\nprocessing on encrypted data using Fully Homomorphic Encryption (FHE),\nproviding privacy guaranties where an untrusted server can execute encrypted\nqueries without learning either the query contents or the underlying data.\nUnlike property-preserving encryption-based systems such as CryptDB, which rely\non deterministic or order-preserving encryption and are vulnerable to\nfrequency, order, and equality-pattern inference attacks, FHE-SQL performs\ncomputations entirely under encryption, eliminating these leakage channels.\nCompared to trusted-hardware approaches such as TrustedDB, which depend on a\nhardware security module and thus inherit its trust and side-channel\nlimitations, our design achieves end-to-end cryptographic protection without\nrequiring trusted execution environments. In contrast to high-performance\nFHE-based engines-Hermes, which target specialized workloads such as vector\nsearch, FHE-SQL supports general SQL query semantics with schema-aware,\ntype-safe definitions suitable for relational data management. FHE-SQL\nmitigates the high cost of ciphertext space by using an indirection\narchitecture that separates metadata in RocksDB from large ciphertexts in blob\nstorage. It supports oblivious selection via homomorphic boolean masks,\nmulti-tier caching, and garbage collection, with security proven under the\nUniversal Composability framework."
                },
                "authors": [
                    {
                        "name": "Po-Yu Tseng"
                    },
                    {
                        "name": "Po-Chu Hsu"
                    },
                    {
                        "name": "Shih-Wei Liao"
                    }
                ],
                "author_detail": {
                    "name": "Shih-Wei Liao"
                },
                "author": "Shih-Wei Liao",
                "arxiv_comment": "12 pages, 1 figures, Keywords: Fully Homomorphic Encryption, Private\n  Information Retrieval, Encrypted Databases, Privacy-Preserving Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v4",
                "updated": "2025-10-17T06:54:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    6,
                    54,
                    10,
                    4,
                    290,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV, where\n\"Prefix\" means the top-ranked KV based on importance rather than position in\nthe original sequence. It reframes the challenge of determining KV cache sizes\nfor all layers into the task of searching for the optimal global prefix\nconfiguration. With an adaptive layer-wise KV retention recipe based on binary\nsearch, the maximum contextual information can thus be preserved in each layer,\nfacilitating the generation. Extensive experiments demonstrate that our method\nachieves the state-of-the-art performance compared with others. It exhibits\nsuperior inference efficiency and generation quality trade-offs, showing\npromising potential for practical applications. Code is available at\nhttps://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV, where\n\"Prefix\" means the top-ranked KV based on importance rather than position in\nthe original sequence. It reframes the challenge of determining KV cache sizes\nfor all layers into the task of searching for the optimal global prefix\nconfiguration. With an adaptive layer-wise KV retention recipe based on binary\nsearch, the maximum contextual information can thus be preserved in each layer,\nfacilitating the generation. Extensive experiments demonstrate that our method\nachieves the state-of-the-art performance compared with others. It exhibits\nsuperior inference efficiency and generation quality trade-offs, showing\npromising potential for practical applications. Code is available at\nhttps://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "NeurIPS 2025 Camera-ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v2",
                "updated": "2025-10-17T06:45:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    6,
                    45,
                    17,
                    4,
                    290,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Pre-trained Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Pre-trained Large Language Models"
                },
                "summary": "To enhance the efficiency of the attention mechanism within large language\nmodels (LLMs), previous works primarily compress the KV cache or group\nattention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to reduce the redundancy by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights.\n  Driven by these insights, we introduce LISA, a lightweight substitute for\nself-attention in well-trained LLMs. LISA employs tiny feed-forward networks to\nalign attention heads between adjacent layers and low-rank matrices to\napproximate differences in layer-wise attention weights. Evaluations\nencompassing 13 typical benchmarks demonstrate that LISA maintains high\nresponse quality in terms of accuracy and perplexity while reducing redundant\nattention calculations within 53%-84% of the total layers. Our implementations\nof LISA achieve a 6x compression of Q and K matrices within the attention\nmechanism, with maximum throughput improvements 19.5%, 32.3%, and 40.1% for\nLLaMA3-8B, LLaMA2-7B, and LLaMA2-13B, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enhance the efficiency of the attention mechanism within large language\nmodels (LLMs), previous works primarily compress the KV cache or group\nattention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to reduce the redundancy by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights.\n  Driven by these insights, we introduce LISA, a lightweight substitute for\nself-attention in well-trained LLMs. LISA employs tiny feed-forward networks to\nalign attention heads between adjacent layers and low-rank matrices to\napproximate differences in layer-wise attention weights. Evaluations\nencompassing 13 typical benchmarks demonstrate that LISA maintains high\nresponse quality in terms of accuracy and perplexity while reducing redundant\nattention calculations within 53%-84% of the total layers. Our implementations\nof LISA achieve a 6x compression of Q and K matrices within the attention\nmechanism, with maximum throughput improvements 19.5%, 32.3%, and 40.1% for\nLLaMA3-8B, LLaMA2-7B, and LLaMA2-13B, respectively."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Jiali Zeng"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "A version accepted by TACL, prior to its publication by MIT Press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15152v1",
                "updated": "2025-10-16T21:22:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    21,
                    22,
                    16,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T21:22:16Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    21,
                    22,
                    16,
                    3,
                    289,
                    0
                ],
                "title": "Tail-Optimized Caching for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tail-Optimized Caching for LLM Inference"
                },
                "summary": "Prompt caching is critical for reducing latency and cost in LLM inference:\nOpenAI and Anthropic report up to 50-90% cost savings through prompt reuse.\nDespite its widespread success, little is known about what constitutes an\noptimal prompt caching policy, particularly when optimizing tail latency, a\nmetric of central importance to practitioners. The widely used Least Recently\nUsed (LRU) policy can perform arbitrarily poor on this metric, as it is\noblivious to the heterogeneity of conversation lengths. To address this gap, we\npropose Tail-Optimized LRU, a simple two-line modification that reallocates KV\ncache capacity to prioritize high-latency conversations by evicting cache\nentries that are unlikely to affect future turns. Though the implementation is\nsimple, we prove its optimality under a natural stochastic model of\nconversation dynamics, providing the first theoretical justification for LRU in\nthis setting, a result that may be of independent interest to the caching\ncommunity. Experimentally, on real conversation data WildChat, Tail-Optimized\nLRU achieves up to 27.5% reduction in P90 tail Time to First Token latency and\n23.9% in P95 tail latency compared to LRU, along with up to 38.9% decrease in\nSLO violations of 200ms. We believe this provides a practical and theoretically\ngrounded option for practitioners seeking to optimize tail latency in\nreal-world LLM deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching is critical for reducing latency and cost in LLM inference:\nOpenAI and Anthropic report up to 50-90% cost savings through prompt reuse.\nDespite its widespread success, little is known about what constitutes an\noptimal prompt caching policy, particularly when optimizing tail latency, a\nmetric of central importance to practitioners. The widely used Least Recently\nUsed (LRU) policy can perform arbitrarily poor on this metric, as it is\noblivious to the heterogeneity of conversation lengths. To address this gap, we\npropose Tail-Optimized LRU, a simple two-line modification that reallocates KV\ncache capacity to prioritize high-latency conversations by evicting cache\nentries that are unlikely to affect future turns. Though the implementation is\nsimple, we prove its optimality under a natural stochastic model of\nconversation dynamics, providing the first theoretical justification for LRU in\nthis setting, a result that may be of independent interest to the caching\ncommunity. Experimentally, on real conversation data WildChat, Tail-Optimized\nLRU achieves up to 27.5% reduction in P90 tail Time to First Token latency and\n23.9% in P95 tail latency compared to LRU, along with up to 38.9% decrease in\nSLO violations of 200ms. We believe this provides a practical and theoretically\ngrounded option for practitioners seeking to optimize tail latency in\nreal-world LLM deployments."
                },
                "authors": [
                    {
                        "name": "Wenxin Zhang"
                    },
                    {
                        "name": "Yueying Li"
                    },
                    {
                        "name": "Ciamac C. Moallemi"
                    },
                    {
                        "name": "Tianyi Peng"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Peng"
                },
                "author": "Tianyi Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15095v1",
                "updated": "2025-10-16T19:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    19,
                    28,
                    30,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T19:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    19,
                    28,
                    30,
                    3,
                    289,
                    0
                ],
                "title": "Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table\n  for GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table\n  for GPUs"
                },
                "summary": "Hash tables are essential building blocks in data-intensive applications, yet\nexisting GPU implementations often struggle with concurrent updates, high load\nfactors, and irregular memory access patterns. We present Hive hash table, a\nhigh-performance, warp-cooperative and dynamically resizable GPU hash table\nthat adapts to varying workloads without global rehashing.\n  Hive hash table makes three key contributions. First, a cache-aligned packed\nbucket layout stores key-value pairs as 64-bit words, enabling coalesced memory\naccess and atomic updates via single-CAS operations. Second, warp-synchronous\nconcurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and\nWarp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic\noperation per warp while ensuring lock-free progress. Third, a\nload-factor-aware dynamic resizing strategy expands or contracts capacity in\nwarp-parallel K-bucket batches using linear hashing, maintaining balanced\noccupancy. To handle insertions under heavy contention, Hive hash table employs\na four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and\noverflow-stash fallback. This design provides lock-free fast paths and bounded\nrecovery cost under contention determined by a fixed eviction depth, while\neliminating ABA hazards during concurrent updates.\n  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains\nload factors up to 95% while delivering 1.5-2x higher throughput than\nstate-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed\ninsert-delete-lookup workloads. On balanced workload, Hive hash table reaches\n3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability\nand efficiency for GPU-accelerated data processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hash tables are essential building blocks in data-intensive applications, yet\nexisting GPU implementations often struggle with concurrent updates, high load\nfactors, and irregular memory access patterns. We present Hive hash table, a\nhigh-performance, warp-cooperative and dynamically resizable GPU hash table\nthat adapts to varying workloads without global rehashing.\n  Hive hash table makes three key contributions. First, a cache-aligned packed\nbucket layout stores key-value pairs as 64-bit words, enabling coalesced memory\naccess and atomic updates via single-CAS operations. Second, warp-synchronous\nconcurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and\nWarp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic\noperation per warp while ensuring lock-free progress. Third, a\nload-factor-aware dynamic resizing strategy expands or contracts capacity in\nwarp-parallel K-bucket batches using linear hashing, maintaining balanced\noccupancy. To handle insertions under heavy contention, Hive hash table employs\na four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and\noverflow-stash fallback. This design provides lock-free fast paths and bounded\nrecovery cost under contention determined by a fixed eviction depth, while\neliminating ABA hazards during concurrent updates.\n  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains\nload factors up to 95% while delivering 1.5-2x higher throughput than\nstate-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed\ninsert-delete-lookup workloads. On balanced workload, Hive hash table reaches\n3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability\nand efficiency for GPU-accelerated data processing."
                },
                "authors": [
                    {
                        "name": "Md Sabbir Hossain Polak"
                    },
                    {
                        "name": "David Troendle"
                    },
                    {
                        "name": "Byunghyun Jang"
                    }
                ],
                "author_detail": {
                    "name": "Byunghyun Jang"
                },
                "author": "Byunghyun Jang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14973v1",
                "updated": "2025-10-16T17:59:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:48Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Is All You Need for KV Cache in Diffusion LLMs"
                },
                "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Quan Nguyen-Tri"
                    },
                    {
                        "name": "Mukul Ranjan"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "https://vila-lab.github.io/elastic-cache-webpage/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14891v1",
                "updated": "2025-10-16T17:10:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    10,
                    3,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:10:03Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    10,
                    3,
                    3,
                    289,
                    0
                ],
                "title": "A Performance Portable Matrix Free Dense MTTKRP in GenTen",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Performance Portable Matrix Free Dense MTTKRP in GenTen"
                },
                "summary": "We extend the GenTen tensor decomposition package by introducing an\naccelerated dense matricized tensor times Khatri-Rao product (MTTKRP), the\nworkhorse kernel for canonical polyadic (CP) tensor decompositions, that is\nportable and performant on modern CPU and GPU architectures. In contrast to the\nstate-of-the-art matrix multiply based MTTKRP kernels used by Tensor Toolbox,\nTensorLy, etc., that explicitly form Khatri-Rao matrices, we develop a\nmatrix-free element-wise parallelization approach whose memory cost grows with\nthe rank R like the sum of the tensor shape O(R(n+m+k)), compared to\nmatrix-based methods whose memory cost grows like the product of the tensor\nshape O(R(mnk)). For the largest problem we study, a rank 2000 MTTKRP, the\nsmaller growth rate yields a matrix-free memory cost of just 2% of the\nmatrix-based methods, a 50x improvement. In practice, the reduced memory impact\nmeans our matrix-free MTTKRP can compute a rank 2000 tensor decomposition on a\nsingle NVIDIA H100 instead of six H100s using a matrix-based MTTKRP. We also\ncompare our optimized matrix-free MTTKRP to baseline matrix-free\nimplementations on different devices, showing a 3x single-device speedup on an\nIntel 8480+ CPU and an 11x speedup on a H100 GPU. In addition to numerical\nresults, we provide fine grained performance models for an ideal multi-level\ncache machine, compare analytical performance predictions to empirical results,\nand provide a motivated heuristic selection for selecting an algorithmic\nhyperparameter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We extend the GenTen tensor decomposition package by introducing an\naccelerated dense matricized tensor times Khatri-Rao product (MTTKRP), the\nworkhorse kernel for canonical polyadic (CP) tensor decompositions, that is\nportable and performant on modern CPU and GPU architectures. In contrast to the\nstate-of-the-art matrix multiply based MTTKRP kernels used by Tensor Toolbox,\nTensorLy, etc., that explicitly form Khatri-Rao matrices, we develop a\nmatrix-free element-wise parallelization approach whose memory cost grows with\nthe rank R like the sum of the tensor shape O(R(n+m+k)), compared to\nmatrix-based methods whose memory cost grows like the product of the tensor\nshape O(R(mnk)). For the largest problem we study, a rank 2000 MTTKRP, the\nsmaller growth rate yields a matrix-free memory cost of just 2% of the\nmatrix-based methods, a 50x improvement. In practice, the reduced memory impact\nmeans our matrix-free MTTKRP can compute a rank 2000 tensor decomposition on a\nsingle NVIDIA H100 instead of six H100s using a matrix-based MTTKRP. We also\ncompare our optimized matrix-free MTTKRP to baseline matrix-free\nimplementations on different devices, showing a 3x single-device speedup on an\nIntel 8480+ CPU and an 11x speedup on a H100 GPU. In addition to numerical\nresults, we provide fine grained performance models for an ideal multi-level\ncache machine, compare analytical performance predictions to empirical results,\nand provide a motivated heuristic selection for selecting an algorithmic\nhyperparameter."
                },
                "authors": [
                    {
                        "name": "Gabriel Kosmacher"
                    },
                    {
                        "name": "Eric T. Phipps"
                    },
                    {
                        "name": "Sivasankaran Rajamanickam"
                    }
                ],
                "author_detail": {
                    "name": "Sivasankaran Rajamanickam"
                },
                "author": "Sivasankaran Rajamanickam",
                "arxiv_comment": "10 pages, 5 figures, 4 tables, for implementation see\n  https://github.com/sandialabs/GenTen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14686v1",
                "updated": "2025-10-16T13:53:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    53,
                    47,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:53:47Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    53,
                    47,
                    3,
                    289,
                    0
                ],
                "title": "xLLM Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xLLM Technical Report"
                },
                "summary": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service."
                },
                "authors": [
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Tao Peng"
                    },
                    {
                        "name": "Peijun Yang"
                    },
                    {
                        "name": "Xiaoyang Zhao"
                    },
                    {
                        "name": "Xiusheng Lu"
                    },
                    {
                        "name": "Weizhe Huang"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xiaoyu Chen"
                    },
                    {
                        "name": "Zhiwei Liang"
                    },
                    {
                        "name": "Jun Xiong"
                    },
                    {
                        "name": "Donghe Jin"
                    },
                    {
                        "name": "Minchao Zhang"
                    },
                    {
                        "name": "Jinrong Guo"
                    },
                    {
                        "name": "Yingxu Deng"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xianzhe Dong"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Siyu Wu"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Zihan Tang"
                    },
                    {
                        "name": "Yuting Zeng"
                    },
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Jinguang Liu"
                    },
                    {
                        "name": "Meng Kang"
                    },
                    {
                        "name": "Menxin Li"
                    },
                    {
                        "name": "Yunlong Wang"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yichen Zhang"
                    },
                    {
                        "name": "Jinrun Yin"
                    },
                    {
                        "name": "Keyang Zheng"
                    },
                    {
                        "name": "Jiawei Yin"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ziyue Wang"
                    },
                    {
                        "name": "Xiaobo Lin"
                    },
                    {
                        "name": "Liangyu Liu"
                    },
                    {
                        "name": "Liwei Lan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Chunhua Peng"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Songcheng Ren"
                    },
                    {
                        "name": "Xuezhu Wang"
                    },
                    {
                        "name": "Yunheng Shen"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Ke Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Zhang"
                },
                "author": "Ke Zhang",
                "arxiv_comment": "39 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v5",
                "updated": "2025-10-16T13:25:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    25,
                    38,
                    3,
                    289,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14622v1",
                "updated": "2025-10-16T12:32:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    32,
                    51,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T12:32:51Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    32,
                    51,
                    3,
                    289,
                    0
                ],
                "title": "MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC\n  Systems"
                },
                "summary": "MPI implementations commonly rely on explicit memory-copy operations,\nincurring overhead from redundant data movement and buffer management. This\noverhead notably impacts HPC workloads involving intensive inter-processor\ncommunication. In response, we introduce MPI-over-CXL, a novel MPI\ncommunication paradigm leveraging CXL, which provides cache-coherent shared\nmemory across multiple hosts. MPI-over-CXL replaces traditional data-copy\nmethods with direct shared memory access, significantly reducing communication\nlatency and memory bandwidth usage. By mapping shared memory regions directly\ninto the virtual address spaces of MPI processes, our design enables efficient\npointer-based communication, eliminating redundant copying operations. To\nvalidate this approach, we implement a comprehensive hardware and software\nenvironment, including a custom CXL 3.2 controller, FPGA-based multi-host\nemulation, and dedicated software stack. Our evaluations using representative\nbenchmarks demonstrate substantial performance improvements over conventional\nMPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and\nscalability in large-scale HPC environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPI implementations commonly rely on explicit memory-copy operations,\nincurring overhead from redundant data movement and buffer management. This\noverhead notably impacts HPC workloads involving intensive inter-processor\ncommunication. In response, we introduce MPI-over-CXL, a novel MPI\ncommunication paradigm leveraging CXL, which provides cache-coherent shared\nmemory across multiple hosts. MPI-over-CXL replaces traditional data-copy\nmethods with direct shared memory access, significantly reducing communication\nlatency and memory bandwidth usage. By mapping shared memory regions directly\ninto the virtual address spaces of MPI processes, our design enables efficient\npointer-based communication, eliminating redundant copying operations. To\nvalidate this approach, we implement a comprehensive hardware and software\nenvironment, including a custom CXL 3.2 controller, FPGA-based multi-host\nemulation, and dedicated software stack. Our evaluations using representative\nbenchmarks demonstrate substantial performance improvements over conventional\nMPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and\nscalability in large-scale HPC environments."
                },
                "authors": [
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Donghyun Gouk"
                    },
                    {
                        "name": "Hyein Woo"
                    },
                    {
                        "name": "Junhee Kim"
                    },
                    {
                        "name": "Jinwoo Baek"
                    },
                    {
                        "name": "Kyungkuk Nam"
                    },
                    {
                        "name": "Sangyoon Ji"
                    },
                    {
                        "name": "Jiseon Kim"
                    },
                    {
                        "name": "Hanyeoreum Bae"
                    },
                    {
                        "name": "Junhyeok Jang"
                    },
                    {
                        "name": "Hyunwoo You"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14531v1",
                "updated": "2025-10-16T10:21:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    10,
                    21,
                    35,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T10:21:35Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    10,
                    21,
                    35,
                    3,
                    289,
                    0
                ],
                "title": "Design and simulation of a 4H-SiC low gain avalanche diode with\n  trench-isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and simulation of a 4H-SiC low gain avalanche diode with\n  trench-isolation"
                },
                "summary": "We present the design and simulation of a 30 $\\mathrm{\\mu m}$ thick 4H-SiC\nLow Gain Avalanche Diode (LGAD) optimized for high-voltage operation. A 2.4\n$\\mathrm{\\mu m}$ thick epitaxially grown gain layer enables controlled internal\namplification up to 1 kV reverse bias, while maintaining full depletion below\n500 V. Electrical characteristics, including I-V, C-V, and gain behavior, were\nsimulated in Synopsys Sentaurus Technology Computer-Aided Design (TCAD) using a\nquasi-1D geometry and verified across process-related variations in gain layer\nparameters. To ensure high-voltage stability and proper edge termination, a\nguard structure combining deep etched trenches and deep $p^+$ junction\ntermination extension (JTE) implants was designed. TCAD simulations varying the\nguard structure dimensions yielded an optimized design with a breakdown voltage\nabove 2.4 kV. A corresponding wafer run is currently processed at IMB-CNM,\nBarcelona.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and simulation of a 30 $\\mathrm{\\mu m}$ thick 4H-SiC\nLow Gain Avalanche Diode (LGAD) optimized for high-voltage operation. A 2.4\n$\\mathrm{\\mu m}$ thick epitaxially grown gain layer enables controlled internal\namplification up to 1 kV reverse bias, while maintaining full depletion below\n500 V. Electrical characteristics, including I-V, C-V, and gain behavior, were\nsimulated in Synopsys Sentaurus Technology Computer-Aided Design (TCAD) using a\nquasi-1D geometry and verified across process-related variations in gain layer\nparameters. To ensure high-voltage stability and proper edge termination, a\nguard structure combining deep etched trenches and deep $p^+$ junction\ntermination extension (JTE) implants was designed. TCAD simulations varying the\nguard structure dimensions yielded an optimized design with a breakdown voltage\nabove 2.4 kV. A corresponding wafer run is currently processed at IMB-CNM,\nBarcelona."
                },
                "authors": [
                    {
                        "name": "Sebastian Onder"
                    },
                    {
                        "name": "Philipp Gaggl"
                    },
                    {
                        "name": "Jürgen Burin"
                    },
                    {
                        "name": "Andreas Gsponer"
                    },
                    {
                        "name": "Matthias Knopf"
                    },
                    {
                        "name": "Simon Waid"
                    },
                    {
                        "name": "Neil Moffat"
                    },
                    {
                        "name": "Giulio Pellegrini"
                    },
                    {
                        "name": "Thomas Bergauer"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Bergauer"
                },
                "author": "Thomas Bergauer",
                "arxiv_doi": "10.1016/j.nima.2025.170740",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nima.2025.170740",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.14531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16040v1",
                "updated": "2025-10-16T07:12:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    7,
                    12,
                    8,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T07:12:08Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    7,
                    12,
                    8,
                    3,
                    289,
                    0
                ],
                "title": "Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge\n  Computing"
                },
                "summary": "Running Large Language Models (LLMs) on edge devices is crucial for reducing\nlatency, improving real-time processing, and enhancing privacy. By performing\ninference directly on the device, data does not need to be sent to the cloud,\nensuring faster responses and reducing reliance on network connectivity.\nHowever, implementing LLMs on edge devices presents challenges, particularly\nwith managing key-value (KV) caches, which plays a pivotal role in LLM serving.\nAs the input text lengthens, the size of the KV cache increases linearly with\nthe sequence length, leading to a significant memory footprint and data access\ncosts. On the other hand, edge devices have limited memory and computational\npower, making it hard to store and efficiently access the large caches needed\nfor LLM inference.\n  To mitigate the substantial overhead caused by KV cache, we propose using\nembedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,\nwhich offers higher storage density compared to SRAM. However, to ensure data\nintegrity, eDRAM needs periodic refresh operations, which are power-intensive.\nTo reduce eDRAM costs and improve overall system performance, we\npropose~\\textit{Kelle}, a software-hardware co-design solution optimized for\ndeploying LLMs on eDRAM-based edge systems. Combined with our fine-grained\nmemory eviction, recomputation, and refresh control algorithms, the\n\\textit{Kelle} accelerator delivers a $3.9\\times$ speedup and $4.5\\times$\nenergy savings compared to existing baseline solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running Large Language Models (LLMs) on edge devices is crucial for reducing\nlatency, improving real-time processing, and enhancing privacy. By performing\ninference directly on the device, data does not need to be sent to the cloud,\nensuring faster responses and reducing reliance on network connectivity.\nHowever, implementing LLMs on edge devices presents challenges, particularly\nwith managing key-value (KV) caches, which plays a pivotal role in LLM serving.\nAs the input text lengthens, the size of the KV cache increases linearly with\nthe sequence length, leading to a significant memory footprint and data access\ncosts. On the other hand, edge devices have limited memory and computational\npower, making it hard to store and efficiently access the large caches needed\nfor LLM inference.\n  To mitigate the substantial overhead caused by KV cache, we propose using\nembedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,\nwhich offers higher storage density compared to SRAM. However, to ensure data\nintegrity, eDRAM needs periodic refresh operations, which are power-intensive.\nTo reduce eDRAM costs and improve overall system performance, we\npropose~\\textit{Kelle}, a software-hardware co-design solution optimized for\ndeploying LLMs on eDRAM-based edge systems. Combined with our fine-grained\nmemory eviction, recomputation, and refresh control algorithms, the\n\\textit{Kelle} accelerator delivers a $3.9\\times$ speedup and $4.5\\times$\nenergy savings compared to existing baseline solutions."
                },
                "authors": [
                    {
                        "name": "Tianhua Xia"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14126v1",
                "updated": "2025-10-15T21:49:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    21,
                    49,
                    38,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T21:49:38Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    21,
                    49,
                    38,
                    2,
                    288,
                    0
                ],
                "title": "Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic\n  Serving"
                },
                "summary": "We introduce Cortex, a prototype workflow-aware serving platform designed for\nagentic workloads. The core principle of Cortex is stage isolation: it\nprovisions dedicated resource pools for each distinct stage of an agentic\nworkflow. This simple yet powerful strategy mitigates inter-stage interference\nin compute and memory, leading to better KV cache utilization, higher\nthroughput, and more predictable performance. By customizing resource\nallocation and scheduling within each distinct stage of agentic workflows,\nCortex lays the groundwork for more advanced, agent-native serving paradigms,\nincluding malleable resource management, speculative execution of workflow\nbranches, and a shared, multi-tiered cache for \"agentic state.\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Cortex, a prototype workflow-aware serving platform designed for\nagentic workloads. The core principle of Cortex is stage isolation: it\nprovisions dedicated resource pools for each distinct stage of an agentic\nworkflow. This simple yet powerful strategy mitigates inter-stage interference\nin compute and memory, leading to better KV cache utilization, higher\nthroughput, and more predictable performance. By customizing resource\nallocation and scheduling within each distinct stage of agentic workflows,\nCortex lays the groundwork for more advanced, agent-native serving paradigms,\nincluding malleable resource management, speculative execution of workflow\nbranches, and a shared, multi-tiered cache for \"agentic state.\""
                },
                "authors": [
                    {
                        "name": "Nikos Pagonas"
                    },
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Kostis Kaffes"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Krishnamurthy"
                },
                "author": "Arvind Krishnamurthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13940v1",
                "updated": "2025-10-15T17:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    45,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:59:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    45,
                    2,
                    288,
                    0
                ],
                "title": "Less is More: Improving LLM Reasoning with Minimal Test-Time\n  Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Improving LLM Reasoning with Minimal Test-Time\n  Intervention"
                },
                "summary": "Recent progress in large language models (LLMs) has focused on test-time\nscaling to improve reasoning via increased inference computation, but often at\nthe cost of efficiency. We revisit test-time behavior and uncover a simple yet\nunderexplored phenomenon: reasoning uncertainty is highly localized-only a\nsmall subset of high-entropy tokens dominantly affects output correctness.\nMotivated by this, we propose Minimal Test-Time Intervention (MTI), a\ntraining-free framework that enhances reasoning accuracy and stability with\nminimal overhead. MTI includes: (i) Selective CFG intervention, applying\nclassifier-free guidance only at uncertain positions; and (ii) Lightweight\nnegative-prompt guidance, reusing the main model's KV cache to approximate\nunconditional decoding efficiently. MTI yields consistent gains across general,\ncoding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for\nQwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining\nhighly efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has focused on test-time\nscaling to improve reasoning via increased inference computation, but often at\nthe cost of efficiency. We revisit test-time behavior and uncover a simple yet\nunderexplored phenomenon: reasoning uncertainty is highly localized-only a\nsmall subset of high-entropy tokens dominantly affects output correctness.\nMotivated by this, we propose Minimal Test-Time Intervention (MTI), a\ntraining-free framework that enhances reasoning accuracy and stability with\nminimal overhead. MTI includes: (i) Selective CFG intervention, applying\nclassifier-free guidance only at uncertain positions; and (ii) Lightweight\nnegative-prompt guidance, reusing the main model's KV cache to approximate\nunconditional decoding efficiently. MTI yields consistent gains across general,\ncoding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for\nQwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining\nhighly efficient."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Ganggui Ding"
                    },
                    {
                        "name": "Liang Hou"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Ying-Cong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying-Cong Chen"
                },
                "author": "Ying-Cong Chen",
                "arxiv_comment": "Code: https://github.com/EnVision-Research/MTI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13797v1",
                "updated": "2025-10-15T17:57:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:57:21Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression\n  Beacons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression\n  Beacons"
                },
                "summary": "The scalability of large language models for long-context reasoning is\nseverely constrained by the linear growth of their Transformer key-value cache,\nwhich incurs significant memory and computational costs. We posit that as a\nmodel generates reasoning tokens, the informational value of past generated\ntokens diminishes, creating an opportunity for compression. In this work, we\npropose to periodically compress the generation KV cache with a learned,\nspecial-purpose token and evict compressed entries. We train the model to\nperform this compression via a modified joint distillation and reinforcement\nlearning (RL) framework. Our training method minimizes overhead over the\nconventional RL process, as it leverages RL outputs for distillation.\nEmpirically, our method achieves a superior memory-accuracy Pareto frontier\ncompared to both the model without cache compression and training-free\ncompression techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scalability of large language models for long-context reasoning is\nseverely constrained by the linear growth of their Transformer key-value cache,\nwhich incurs significant memory and computational costs. We posit that as a\nmodel generates reasoning tokens, the informational value of past generated\ntokens diminishes, creating an opportunity for compression. In this work, we\npropose to periodically compress the generation KV cache with a learned,\nspecial-purpose token and evict compressed entries. We train the model to\nperform this compression via a modified joint distillation and reinforcement\nlearning (RL) framework. Our training method minimizes overhead over the\nconventional RL process, as it leverages RL outputs for distillation.\nEmpirically, our method achieves a superior memory-accuracy Pareto frontier\ncompared to both the model without cache compression and training-free\ncompression techniques."
                },
                "authors": [
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Yair Feldman"
                    },
                    {
                        "name": "Shankar Padmanabhan"
                    },
                    {
                        "name": "Kianté Brantley"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v3",
                "updated": "2025-10-15T16:03:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    3,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Quantize What Counts: More for Keys, Less for Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantize What Counts: More for Keys, Less for Values"
                },
                "summary": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv."
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Alan Luo"
                    },
                    {
                        "name": "Weicong Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13602v1",
                "updated": "2025-10-15T14:33:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:33:16Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "title": "NOSA: Native and Offloadable Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOSA: Native and Offloadable Sparse Attention"
                },
                "summary": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2)."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13401v1",
                "updated": "2025-10-15T10:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    56,
                    37,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T10:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    56,
                    37,
                    2,
                    288,
                    0
                ],
                "title": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs"
                },
                "summary": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second)."
                },
                "authors": [
                    {
                        "name": "Jude Haris"
                    },
                    {
                        "name": "José Cano"
                    }
                ],
                "author_detail": {
                    "name": "José Cano"
                },
                "author": "José Cano",
                "arxiv_comment": "Accepted to Workshop on New Approaches for Addressing the Computing\n  Requirements of LLMs and GNNs (LG-ARC) @ ISCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13334v1",
                "updated": "2025-10-15T09:18:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    18,
                    58,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T09:18:58Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    18,
                    58,
                    2,
                    288,
                    0
                ],
                "title": "Taming the Fragility of KV Cache Eviction in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming the Fragility of KV Cache Eviction in LLM Inference"
                },
                "summary": "Large language models have revolutionized natural language processing, yet\ntheir deployment remains hampered by the substantial memory and runtime\noverhead of the transformer's Key-Value cache. To mitigate this, recent methods\nemploy a scoring-aggregation framework to evict unimportant cache entries,\nbased on the stability assumption-that a fixed subset of entries remains\nconsistently important during generation. However, prior work has largely\nfocused on refining importance indicators for scoring, while defaulting to mean\naggregation due to a faithful trust in the stability assumption. In this work,\nwe argue that this underlying assumption is inherently fragile, making mean\naggregation highly vulnerable in extreme cases. To counter this, we propose a\nsimple yet elegant defensive aggregation strategy: a two-step, linear-time\napproach that controls worst-case risk, thereby defending against extreme cases\nwith negligible computational overhead. Embodying this strategy, we propose a\nnovel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,\nwhich incorporates layer-wise budget allocation. Across seven task domains (18\ndatasets), our methods reduce generation quality loss by 2.3x and 4.3x\nrespectively, versus the strongest baseline under a 20% cache size. These\nresults set new performance benchmarks and pioneer a promising direction for\noptimizing cache eviction against underlying fragility through worst-case risk\nmanagement. Our code is available at https://github.com/FFY0/DefensiveKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing, yet\ntheir deployment remains hampered by the substantial memory and runtime\noverhead of the transformer's Key-Value cache. To mitigate this, recent methods\nemploy a scoring-aggregation framework to evict unimportant cache entries,\nbased on the stability assumption-that a fixed subset of entries remains\nconsistently important during generation. However, prior work has largely\nfocused on refining importance indicators for scoring, while defaulting to mean\naggregation due to a faithful trust in the stability assumption. In this work,\nwe argue that this underlying assumption is inherently fragile, making mean\naggregation highly vulnerable in extreme cases. To counter this, we propose a\nsimple yet elegant defensive aggregation strategy: a two-step, linear-time\napproach that controls worst-case risk, thereby defending against extreme cases\nwith negligible computational overhead. Embodying this strategy, we propose a\nnovel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,\nwhich incorporates layer-wise budget allocation. Across seven task domains (18\ndatasets), our methods reduce generation quality loss by 2.3x and 4.3x\nrespectively, versus the strongest baseline under a 20% cache size. These\nresults set new performance benchmarks and pioneer a promising direction for\noptimizing cache eviction against underlying fragility through worst-case risk\nmanagement. Our code is available at https://github.com/FFY0/DefensiveKV."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Haoyu Guo"
                    },
                    {
                        "name": "JunLin Lv"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    },
                    {
                        "name": "Xike Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xike Xie"
                },
                "author": "Xike Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13279v1",
                "updated": "2025-10-15T08:25:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    8,
                    25,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T08:25:13Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    8,
                    25,
                    13,
                    2,
                    288,
                    0
                ],
                "title": "Partitioned Scheduling for DAG Tasks Considering Probabilistic Execution\n  Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partitioned Scheduling for DAG Tasks Considering Probabilistic Execution\n  Time"
                },
                "summary": "Autonomous driving systems, critical for safety, require real-time guarantees\nand can be modeled as DAGs. Their acceleration features, such as caches and\npipelining, often result in execution times below the worst-case. Thus, a\nprobabilistic approach ensuring constraint satisfaction within a probability\nthreshold is more suitable than worst-case guarantees for these systems. This\npaper considers probabilistic guarantees for DAG tasks by utilizing the results\nof probabilistic guarantees for single processors, which have been relatively\nmore advanced than those for multi-core processors. This paper proposes a task\nset partitioning method that guarantees schedulability under the partitioned\nscheduling. The evaluation on randomly generated DAG task sets demonstrates\nthat the proposed method schedules more task sets with a smaller mean analysis\ntime compared to existing probabilistic schedulability analysis for DAGs. The\nevaluation also compares four bin-packing heuristics, revealing Item-Centric\nWorst-Fit-Decreasing schedules the most task sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving systems, critical for safety, require real-time guarantees\nand can be modeled as DAGs. Their acceleration features, such as caches and\npipelining, often result in execution times below the worst-case. Thus, a\nprobabilistic approach ensuring constraint satisfaction within a probability\nthreshold is more suitable than worst-case guarantees for these systems. This\npaper considers probabilistic guarantees for DAG tasks by utilizing the results\nof probabilistic guarantees for single processors, which have been relatively\nmore advanced than those for multi-core processors. This paper proposes a task\nset partitioning method that guarantees schedulability under the partitioned\nscheduling. The evaluation on randomly generated DAG task sets demonstrates\nthat the proposed method schedules more task sets with a smaller mean analysis\ntime compared to existing probabilistic schedulability analysis for DAGs. The\nevaluation also compares four bin-packing heuristics, revealing Item-Centric\nWorst-Fit-Decreasing schedules the most task sets."
                },
                "authors": [
                    {
                        "name": "Fuma Omori"
                    },
                    {
                        "name": "Atsushi Yano"
                    },
                    {
                        "name": "Takuya Azumi"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Azumi"
                },
                "author": "Takuya Azumi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13223v1",
                "updated": "2025-10-15T07:20:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    7,
                    20,
                    14,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T07:20:14Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    7,
                    20,
                    14,
                    2,
                    288,
                    0
                ],
                "title": "BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing\n  Disaggregated LLM Serving in AI Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing\n  Disaggregated LLM Serving in AI Infrastructure"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in AI infrastructure,\ndriving the need for high throughput, resource efficient serving systems.\nDisaggregated LLM serving, which separates prompt prefill from auto-regressive\ndecode, has emerged as a promising architecture by isolating their\nheterogeneous compute and memory demands. However, current disaggregated\nsystems face three key limitations: (i) static resource allocation cannot adapt\nto highly dynamic workloads, causing over-provisioning that wastes resources or\nunder-provisioning that violates service level objectives (SLOs); (ii) inherent\nload imbalance between prefill and decode stages, where prefill is\ncompute-bound and decode is memory-bound, causes under-utilization in one tier\nwhile the other becomes a bottleneck; and (iii) prefix cache aware routing\nskews load distribution, as high cache hit rate prefill nodes attract\ndisproportionately more requests, further degrading balance and efficiency. To\naddress these issues, we present BanaServe, a dynamic orchestration framework\nthat continuously rebalances computational and memory resources across prefill\nand decode instances while eliminating hotspots induced by cache. BanaServe\nintroduces layer level weight migration, attention level Key Value Cache (KV\nCache) migration, and Global KV Cache Store sharing with layer wise overlapped\ntransmission, enabling both coarse grained (layer level) and fine grained\n(attention level) load redistribution with minimal latency overhead. These\nmechanisms allow routers to perform purely load aware scheduling, unconstrained\nby cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher\nthroughput with 3.9%-78.4% lower total processing time, and outperforms\nDistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in AI infrastructure,\ndriving the need for high throughput, resource efficient serving systems.\nDisaggregated LLM serving, which separates prompt prefill from auto-regressive\ndecode, has emerged as a promising architecture by isolating their\nheterogeneous compute and memory demands. However, current disaggregated\nsystems face three key limitations: (i) static resource allocation cannot adapt\nto highly dynamic workloads, causing over-provisioning that wastes resources or\nunder-provisioning that violates service level objectives (SLOs); (ii) inherent\nload imbalance between prefill and decode stages, where prefill is\ncompute-bound and decode is memory-bound, causes under-utilization in one tier\nwhile the other becomes a bottleneck; and (iii) prefix cache aware routing\nskews load distribution, as high cache hit rate prefill nodes attract\ndisproportionately more requests, further degrading balance and efficiency. To\naddress these issues, we present BanaServe, a dynamic orchestration framework\nthat continuously rebalances computational and memory resources across prefill\nand decode instances while eliminating hotspots induced by cache. BanaServe\nintroduces layer level weight migration, attention level Key Value Cache (KV\nCache) migration, and Global KV Cache Store sharing with layer wise overlapped\ntransmission, enabling both coarse grained (layer level) and fine grained\n(attention level) load redistribution with minimal latency overhead. These\nmechanisms allow routers to perform purely load aware scheduling, unconstrained\nby cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher\nthroughput with 3.9%-78.4% lower total processing time, and outperforms\nDistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction."
                },
                "authors": [
                    {
                        "name": "Yiyuan He"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Jingfeng Wu"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Chong Ma"
                    },
                    {
                        "name": "Min Shen"
                    },
                    {
                        "name": "Le Chen"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13084v1",
                "updated": "2025-10-15T01:55:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    32,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T01:55:32Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    32,
                    2,
                    288,
                    0
                ],
                "title": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar\n  Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar\n  Propagation"
                },
                "summary": "Text-to-image (T2I) diffusion models have recently demonstrated significant\nprogress in video editing.\n  However, existing video editing methods are severely limited by their high\ncomputational overhead and memory consumption.\n  Furthermore, these approaches often sacrifice visual fidelity, leading to\nundesirable temporal inconsistencies and artifacts such as blurring and\npronounced mosaic-like patterns.\n  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video\nediting method.\n  Edit-Your-Interest introduces a spatio-temporal feature memory to cache\nfeatures from previous frames, significantly reducing computational overhead\ncompared to full-sequence spatio-temporal modeling approaches.\n  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),\nwhich is designed to efficiently cache and retain the crucial image tokens\nprocessed by spatial attention.\n  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP\npropagates the most relevant tokens from previous frames to subsequent ones,\npreserving temporal consistency.\n  Finally, we introduce an SFM update algorithm that continuously refreshes the\ncached features, ensuring their long-term relevance and effectiveness\nthroughout the video sequence.\n  Furthermore, we leverage cross-attention maps to automatically extract masks\nfor the instances of interest.\n  These masks are seamlessly integrated into the diffusion denoising process,\nenabling fine-grained control over target objects and allowing\nEdit-Your-Interest to perform highly accurate edits while robustly preserving\nthe background integrity.\n  Extensive experiments decisively demonstrate that the proposed\nEdit-Your-Interest outperforms state-of-the-art methods in both efficiency and\nvisual fidelity, validating its superior effectiveness and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) diffusion models have recently demonstrated significant\nprogress in video editing.\n  However, existing video editing methods are severely limited by their high\ncomputational overhead and memory consumption.\n  Furthermore, these approaches often sacrifice visual fidelity, leading to\nundesirable temporal inconsistencies and artifacts such as blurring and\npronounced mosaic-like patterns.\n  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video\nediting method.\n  Edit-Your-Interest introduces a spatio-temporal feature memory to cache\nfeatures from previous frames, significantly reducing computational overhead\ncompared to full-sequence spatio-temporal modeling approaches.\n  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),\nwhich is designed to efficiently cache and retain the crucial image tokens\nprocessed by spatial attention.\n  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP\npropagates the most relevant tokens from previous frames to subsequent ones,\npreserving temporal consistency.\n  Finally, we introduce an SFM update algorithm that continuously refreshes the\ncached features, ensuring their long-term relevance and effectiveness\nthroughout the video sequence.\n  Furthermore, we leverage cross-attention maps to automatically extract masks\nfor the instances of interest.\n  These masks are seamlessly integrated into the diffusion denoising process,\nenabling fine-grained control over target objects and allowing\nEdit-Your-Interest to perform highly accurate edits while robustly preserving\nthe background integrity.\n  Extensive experiments decisively demonstrate that the proposed\nEdit-Your-Interest outperforms state-of-the-art methods in both efficiency and\nvisual fidelity, validating its superior effectiveness and practicality."
                },
                "authors": [
                    {
                        "name": "Yi Zuo"
                    },
                    {
                        "name": "Zitao Wang"
                    },
                    {
                        "name": "Lingling Li"
                    },
                    {
                        "name": "Xu Liu"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Licheng Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Licheng Jiao"
                },
                "author": "Licheng Jiao",
                "arxiv_comment": "32 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v3",
                "updated": "2025-10-15T01:55:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    31,
                    2,
                    288,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12889v1",
                "updated": "2025-10-14T18:04:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    4,
                    0,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T18:04:00Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    4,
                    0,
                    1,
                    287,
                    0
                ],
                "title": "Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching\n  for Heterogeneous Tasks and Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching\n  for Heterogeneous Tasks and Clusters"
                },
                "summary": "This paper introduces Dodoor, an efficient randomized decentralized scheduler\ndesigned for task scheduling in modern data centers. Dodoor leverages advanced\nresearch on the weighted balls-into-bins model with b-batched setting. Unlike\nother decentralized schedulers that rely on real-time probing of remote\nservers, Dodoor makes scheduling decisions based on cached server information,\nwhich is updated in batches, to reduce communication overheads. To schedule\ntasks with dynamic, multidimensional resource requirements in heterogeneous\ncluster, Dodoor uses a novel load score to measure servers' loads for each\nscheduled task. This score captures the anti-affinity between servers and tasks\nin contrast to the commonly used heuristic of counting pending tasks to balance\nload. On a 101-node heterogeneous cluster, Dodoor is evaluated using two\nworkloads: (i) simulated Azure virtual machines placements and (ii) real\nserverless Python functions executions in Docker. The evaluation shows that\nDodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can\nalso increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency\nby 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two\nworkloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Dodoor, an efficient randomized decentralized scheduler\ndesigned for task scheduling in modern data centers. Dodoor leverages advanced\nresearch on the weighted balls-into-bins model with b-batched setting. Unlike\nother decentralized schedulers that rely on real-time probing of remote\nservers, Dodoor makes scheduling decisions based on cached server information,\nwhich is updated in batches, to reduce communication overheads. To schedule\ntasks with dynamic, multidimensional resource requirements in heterogeneous\ncluster, Dodoor uses a novel load score to measure servers' loads for each\nscheduled task. This score captures the anti-affinity between servers and tasks\nin contrast to the commonly used heuristic of counting pending tasks to balance\nload. On a 101-node heterogeneous cluster, Dodoor is evaluated using two\nworkloads: (i) simulated Azure virtual machines placements and (ii) real\nserverless Python functions executions in Docker. The evaluation shows that\nDodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can\nalso increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency\nby 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two\nworkloads."
                },
                "authors": [
                    {
                        "name": "Wei Da"
                    },
                    {
                        "name": "Evangelia Kalyvianaki"
                    }
                ],
                "author_detail": {
                    "name": "Evangelia Kalyvianaki"
                },
                "author": "Evangelia Kalyvianaki",
                "arxiv_comment": "single column,20 pages and 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12872v1",
                "updated": "2025-10-14T18:00:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T18:00:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems"
                },
                "summary": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms."
                },
                "authors": [
                    {
                        "name": "Hancheng Ye"
                    },
                    {
                        "name": "Zhengqi Gao"
                    },
                    {
                        "name": "Mingyuan Ma"
                    },
                    {
                        "name": "Qinsi Wang"
                    },
                    {
                        "name": "Yuzhe Fu"
                    },
                    {
                        "name": "Ming-Yu Chung"
                    },
                    {
                        "name": "Yueqian Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "Accepted for publication in NeurIPS2025. Code is available at\n  \\url{https://github.com/HankYe/KVCOMM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12705v1",
                "updated": "2025-10-14T16:39:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    39,
                    29,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:39:29Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    39,
                    29,
                    1,
                    287,
                    0
                ],
                "title": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization\n  of Banded Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization\n  of Banded Matrices"
                },
                "summary": "The reduction of a banded matrix to a bidiagonal form is a crucial step in\nthe Singular Value Decomposition (SVD), a cornerstone of scientific computing\nand AI. Despite being a highly parallel algorithm, it was previously believed\nto be unsuitable for GPU computation because it is memory bandwidth-bound.\nRecent developments in GPU hardware, including larger L1 memory per Streaming\nMultiprocessor/Compute Unit, have changed that. We present the first GPU\nalgorithm for reducing a banded matrix to bidiagonal form as part of the\nNextLA$.$jl open-source software package. Our algorithm is based on previous\nCPU-based multicore parallel cache-efficient bulge chasing algorithms and\nadapted to optimize for GPU throughput. We leverage Julia Language's Array\nabstractions and KernelAbstractions to implement a single hardware- and data\nprecision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for\nhalf, single, and double precision, and examine performance optimization across\nhardware architectures and data precision. We also develop a hardware-aware\nperformance model and identify key hyperparameters, such as inner tilewidth and\nblock concurrency, that govern optimal GPU execution for bandwidth-bound\nworkloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU\ncan outperform CPU-based implementations: the GPU algorithm outperforms\nmultithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size\n1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,\nthe performance of the algorithm increases linearly with matrix bandwidth size,\nmaking faster reduction of larger matrix bandwidths now also possible. With\nthis work, we break memory bandwidth barriers, as well as matrix bandwidth\nbarriers, resulting in orders-of-magnitude faster algorithms for the reduction\nof banded matrices to bidiagonal form on the GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reduction of a banded matrix to a bidiagonal form is a crucial step in\nthe Singular Value Decomposition (SVD), a cornerstone of scientific computing\nand AI. Despite being a highly parallel algorithm, it was previously believed\nto be unsuitable for GPU computation because it is memory bandwidth-bound.\nRecent developments in GPU hardware, including larger L1 memory per Streaming\nMultiprocessor/Compute Unit, have changed that. We present the first GPU\nalgorithm for reducing a banded matrix to bidiagonal form as part of the\nNextLA$.$jl open-source software package. Our algorithm is based on previous\nCPU-based multicore parallel cache-efficient bulge chasing algorithms and\nadapted to optimize for GPU throughput. We leverage Julia Language's Array\nabstractions and KernelAbstractions to implement a single hardware- and data\nprecision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for\nhalf, single, and double precision, and examine performance optimization across\nhardware architectures and data precision. We also develop a hardware-aware\nperformance model and identify key hyperparameters, such as inner tilewidth and\nblock concurrency, that govern optimal GPU execution for bandwidth-bound\nworkloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU\ncan outperform CPU-based implementations: the GPU algorithm outperforms\nmultithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size\n1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,\nthe performance of the algorithm increases linearly with matrix bandwidth size,\nmaking faster reduction of larger matrix bandwidths now also possible. With\nthis work, we break memory bandwidth barriers, as well as matrix bandwidth\nbarriers, resulting in orders-of-magnitude faster algorithms for the reduction\nof banded matrices to bidiagonal form on the GPU."
                },
                "authors": [
                    {
                        "name": "Evelyne Ringoot"
                    },
                    {
                        "name": "Rabab Alomairy"
                    },
                    {
                        "name": "Alan Edelman"
                    }
                ],
                "author_detail": {
                    "name": "Alan Edelman"
                },
                "author": "Alan Edelman",
                "arxiv_comment": "13 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v3",
                "updated": "2025-10-14T16:05:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    5,
                    11,
                    1,
                    287,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v5",
                "updated": "2025-10-14T15:42:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    42,
                    41,
                    1,
                    287,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12642v1",
                "updated": "2025-10-14T15:34:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    35,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:34:35Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    35,
                    1,
                    287,
                    0
                ],
                "title": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data\n  Analysis"
                },
                "summary": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system."
                },
                "authors": [
                    {
                        "name": "Meihui Zhang"
                    },
                    {
                        "name": "Liming Wang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Zhaojing Luo"
                    }
                ],
                "author_detail": {
                    "name": "Zhaojing Luo"
                },
                "author": "Zhaojing Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12629v1",
                "updated": "2025-10-14T15:26:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    26,
                    9,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:26:09Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    26,
                    9,
                    1,
                    287,
                    0
                ],
                "title": "Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in\n  Containerized Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in\n  Containerized Clouds"
                },
                "summary": "In modern containerized cloud environments, the adoption of RDMA (Remote\nDirect Memory Access) has expanded to reduce CPU overhead and enable\nhigh-performance data exchange. Achieving this requires strong performance\nisolation to ensure that one container's RDMA workload does not degrade the\nperformance of others, thereby maintaining critical security assurances.\nHowever, existing isolation techniques are difficult to apply effectively due\nto the complexity of microarchitectural resource management within RDMA NICs\n(RNICs). This paper experimentally analyzes two types of resource exhaustion\nattacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline\nsaturation attacks. Our results show that state saturation attacks can cause up\nto a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in\ncache misses for victim containers, while pipeline saturation attacks lead to\nsevere link-level congestion and significant amplification, where small verb\nrequests result in disproportionately high resource consumption. To mitigate\nthese threats and restore predictable security assurances, we propose HT-Verbs,\na threshold-driven framework based on real-time per-container RDMA verb\ntelemetry and adaptive resource classification that partitions RNIC resources\ninto hot, warm, and cold tiers and throttles abusive workloads without\nrequiring hardware modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern containerized cloud environments, the adoption of RDMA (Remote\nDirect Memory Access) has expanded to reduce CPU overhead and enable\nhigh-performance data exchange. Achieving this requires strong performance\nisolation to ensure that one container's RDMA workload does not degrade the\nperformance of others, thereby maintaining critical security assurances.\nHowever, existing isolation techniques are difficult to apply effectively due\nto the complexity of microarchitectural resource management within RDMA NICs\n(RNICs). This paper experimentally analyzes two types of resource exhaustion\nattacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline\nsaturation attacks. Our results show that state saturation attacks can cause up\nto a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in\ncache misses for victim containers, while pipeline saturation attacks lead to\nsevere link-level congestion and significant amplification, where small verb\nrequests result in disproportionately high resource consumption. To mitigate\nthese threats and restore predictable security assurances, we propose HT-Verbs,\na threshold-driven framework based on real-time per-container RDMA verb\ntelemetry and adaptive resource classification that partitions RNIC resources\ninto hot, warm, and cold tiers and throttles abusive workloads without\nrequiring hardware modifications."
                },
                "authors": [
                    {
                        "name": "Gunwoo Kim"
                    },
                    {
                        "name": "Taejune Park"
                    },
                    {
                        "name": "Jinwoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jinwoo Kim"
                },
                "author": "Jinwoo Kim",
                "arxiv_comment": "20 pages, 14 figures, presented at the 4th International Workshop on\n  System Security Assurance (SecAssure 2025), co-located with ESORICS 2025, to\n  appear in Springer LNCS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12280v1",
                "updated": "2025-10-14T08:34:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    34,
                    9,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T08:34:09Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    34,
                    9,
                    1,
                    287,
                    0
                ],
                "title": "Analysis and Evaluation of Using Microsecond-Latency Memory for\n  In-Memory Indices and Caches in SSD-Based Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis and Evaluation of Using Microsecond-Latency Memory for\n  In-Memory Indices and Caches in SSD-Based Key-Value Stores"
                },
                "summary": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM."
                },
                "authors": [
                    {
                        "name": "Yosuke Bando"
                    },
                    {
                        "name": "Akinobu Mita"
                    },
                    {
                        "name": "Kazuhiro Hiwada"
                    },
                    {
                        "name": "Shintaro Sano"
                    },
                    {
                        "name": "Tomoya Suzuki"
                    },
                    {
                        "name": "Yu Nakanishi"
                    },
                    {
                        "name": "Kazutaka Tomida"
                    },
                    {
                        "name": "Hirotsugu Kajihara"
                    },
                    {
                        "name": "Akiyuki Kaneko"
                    },
                    {
                        "name": "Daisuke Taki"
                    },
                    {
                        "name": "Yukimasa Miyamoto"
                    },
                    {
                        "name": "Tomokazu Yoshida"
                    },
                    {
                        "name": "Tatsuo Shiozawa"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuo Shiozawa"
                },
                "author": "Tatsuo Shiozawa",
                "arxiv_doi": "10.1145/3769759",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769759",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.12280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proc. ACM Manag. Data 3, 6 (SIGMOD), Article 294 (December 2025),\n  28 pages",
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10975v2",
                "updated": "2025-10-14T07:41:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    7,
                    41,
                    47,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T03:26:14Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    26,
                    14,
                    0,
                    286,
                    0
                ],
                "title": "RoVer: Robot Reward Model as Test-Time Verifier for\n  Vision-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoVer: Robot Reward Model as Test-Time Verifier for\n  Vision-Language-Action Model"
                },
                "summary": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs. We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs. We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference."
                },
                "authors": [
                    {
                        "name": "Mingtong Dai"
                    },
                    {
                        "name": "Lingbo Liu"
                    },
                    {
                        "name": "Yongjie Bai"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zhouxia Wang"
                    },
                    {
                        "name": "Rui SU"
                    },
                    {
                        "name": "Chunjie Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Xinyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Wu"
                },
                "author": "Xinyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11496v2",
                "updated": "2025-10-14T05:05:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    5,
                    5,
                    14,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T15:04:38Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    4,
                    38,
                    0,
                    286,
                    0
                ],
                "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model"
                },
                "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer."
                },
                "authors": [
                    {
                        "name": "Zhiwei Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yafei Liu"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Ruichen Wang"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Qi Qi"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Dongze Hao"
                    },
                    {
                        "name": "Quanlong Zheng"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haobo Ji"
                    },
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Zhitong Zheng"
                    },
                    {
                        "name": "Zhenyi Lin"
                    },
                    {
                        "name": "Haolin Deng"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Xiaojie Yin"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Liankai Cai"
                    },
                    {
                        "name": "Haijing Liu"
                    },
                    {
                        "name": "Yuqing Qiu"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Zixian Li"
                    },
                    {
                        "name": "Chi Xie"
                    },
                    {
                        "name": "Huafei Li"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Chuangchuang Wang"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Zhiguang Zhu"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Wenmei Gao"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Qin Xie"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Lu"
                },
                "author": "Haonan Lu",
                "arxiv_comment": "Tech report of OPPO AndesVL Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12051v1",
                "updated": "2025-10-14T01:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    1,
                    26,
                    36,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T01:26:36Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    1,
                    26,
                    36,
                    1,
                    287,
                    0
                ],
                "title": "APCE: Adaptive Progressive Context Expansion for Long Context Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APCE: Adaptive Progressive Context Expansion for Long Context Processing"
                },
                "summary": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing\ntwo key challenges: (1) A growing memory footprint due to quadratic\nself-attention and linear KV-cache scaling in memory as sequence length\nincreases; (2) the ContextRot phenomena where empirical evidence suggests that\ntransformer architecture's performance degrades with increasing context length.\nGiven the shared dependency on the input, a natural question arises: Can we\nsurgically select the most important input chunks for processing to\nsynergistically (a) reduce the memory footprint, and (b) mitigate the\nContextRot effects? In this paper, we answer this question in the affirmative\nfor long-context summarization tasks. We propose APCE as a context-aware\nsolution to select the most important input chunks through low-dimensional\nsemantic similarity matching with the current query. By directly operating on\nthe input, APCE decouples from strict dependency on underlying hardware or CUDA\nenvironments, promising a compatible solution scalable to different deployment\nsystems. Our empirical evaluations have demonstrated superior or on-par\nsummarization performance for APCE compared to the full dense baseline using a\nfraction (50%-70%) of the input sequence resulting in KV-cache and\nself-attention memory efficiency improvements. We hope our findings inspire\nfurther research on context-aware efficiency solutions for LCTMs geared towards\nother relevant long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing\ntwo key challenges: (1) A growing memory footprint due to quadratic\nself-attention and linear KV-cache scaling in memory as sequence length\nincreases; (2) the ContextRot phenomena where empirical evidence suggests that\ntransformer architecture's performance degrades with increasing context length.\nGiven the shared dependency on the input, a natural question arises: Can we\nsurgically select the most important input chunks for processing to\nsynergistically (a) reduce the memory footprint, and (b) mitigate the\nContextRot effects? In this paper, we answer this question in the affirmative\nfor long-context summarization tasks. We propose APCE as a context-aware\nsolution to select the most important input chunks through low-dimensional\nsemantic similarity matching with the current query. By directly operating on\nthe input, APCE decouples from strict dependency on underlying hardware or CUDA\nenvironments, promising a compatible solution scalable to different deployment\nsystems. Our empirical evaluations have demonstrated superior or on-par\nsummarization performance for APCE compared to the full dense baseline using a\nfraction (50%-70%) of the input sequence resulting in KV-cache and\nself-attention memory efficiency improvements. We hope our findings inspire\nfurther research on context-aware efficiency solutions for LCTMs geared towards\nother relevant long-context tasks."
                },
                "authors": [
                    {
                        "name": "Baisub Lee"
                    },
                    {
                        "name": "Sanghyun Byun"
                    },
                    {
                        "name": "Mohanad Odema"
                    },
                    {
                        "name": "Jung Guack"
                    },
                    {
                        "name": "Jacob Song"
                    },
                    {
                        "name": "Woo Seong Chung"
                    }
                ],
                "author_detail": {
                    "name": "Woo Seong Chung"
                },
                "author": "Woo Seong Chung",
                "arxiv_comment": "NeurIPS 2025 Workshop: ML For Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22622v2",
                "updated": "2025-10-13T22:41:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    22,
                    41,
                    26,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-26T17:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "LongLive: Real-time Interactive Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLive: Real-time Interactive Long Video Generation"
                },
                "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss."
                },
                "authors": [
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code, model, and demos are available at\n  https://github.com/NVlabs/LongLive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11938v1",
                "updated": "2025-10-13T21:01:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    21,
                    1,
                    40,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T21:01:40Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    21,
                    1,
                    40,
                    0,
                    286,
                    0
                ],
                "title": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline\n  Refactoring in Fragmented Serverless Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline\n  Refactoring in Fragmented Serverless Clusters"
                },
                "summary": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity."
                },
                "authors": [
                    {
                        "name": "Yanying Lin"
                    },
                    {
                        "name": "Shijie Peng"
                    },
                    {
                        "name": "Chengzhi Lu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_doi": "10.1145/3767295.3769316",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3767295.3769316",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.11938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "EuroSys 26",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11857v3",
                "updated": "2025-10-21T22:37:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    22,
                    37,
                    11,
                    1,
                    294,
                    0
                ],
                "published": "2024-10-04T15:23:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet"
                },
                "summary": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality."
                },
                "authors": [
                    {
                        "name": "Noah Martin"
                    },
                    {
                        "name": "Abdullah Bin Faisal"
                    },
                    {
                        "name": "Hiba Eltigani"
                    },
                    {
                        "name": "Rukhshan Haroon"
                    },
                    {
                        "name": "Swaminathan Lamelas"
                    },
                    {
                        "name": "Fahad Dogar"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Dogar"
                },
                "author": "Fahad Dogar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v3",
                "updated": "2025-10-13T17:15:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    15,
                    14,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Lingxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17238v2",
                "updated": "2025-10-13T16:48:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    48,
                    37,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-21T21:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE"
                },
                "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction. To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction. To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters."
                },
                "authors": [
                    {
                        "name": "Soheil Zibakhsh"
                    },
                    {
                        "name": "Mohammad Samragh"
                    },
                    {
                        "name": "Kumari Nishu"
                    },
                    {
                        "name": "Lauren Hannah"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "arxiv_comment": "Corrected typo in arxiv abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11292v1",
                "updated": "2025-10-13T11:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "title": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences"
                },
                "summary": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios."
                },
                "authors": [
                    {
                        "name": "Wenbo Wu"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v2",
                "updated": "2025-10-13T11:21:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    21,
                    0,
                    0,
                    286,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_doi": "10.1088/1367-2630/ae0ea7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1367-2630/ae0ea7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.21725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Added a figure, minor changes to text",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08666v2",
                "updated": "2025-10-13T10:39:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    39,
                    59,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-09T16:19:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    19,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dInfer: An Efficient Inference Framework for Diffusion Language Models"
                },
                "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer."
                },
                "authors": [
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Guofeng Feng"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Qi"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Ziyun Jiang"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Da Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Da Zheng"
                },
                "author": "Da Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19257v2",
                "updated": "2025-10-13T10:18:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    18,
                    34,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-15T12:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    3,
                    34,
                    4,
                    227,
                    0
                ],
                "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates."
                },
                "authors": [
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Jiachen Zhang"
                    },
                    {
                        "name": "Chengxuan Li"
                    },
                    {
                        "name": "Zhimu Zhou"
                    },
                    {
                        "name": "Shixin Wu"
                    },
                    {
                        "name": "Songfang Huang"
                    },
                    {
                        "name": "Huiling Duan"
                    }
                ],
                "author_detail": {
                    "name": "Huiling Duan"
                },
                "author": "Huiling Duan",
                "arxiv_comment": "Manuscript submitted to AAAI 2026, currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11236v1",
                "updated": "2025-10-13T10:17:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    17,
                    21,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T10:17:21Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    17,
                    21,
                    0,
                    286,
                    0
                ],
                "title": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy."
                },
                "authors": [
                    {
                        "name": "Haoqi Yang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24695v2",
                "updated": "2025-10-13T09:12:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    9,
                    12,
                    27,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-29T12:28:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer"
                },
                "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Jincheng Yu"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "arxiv_comment": "21 pages, 15 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15980v1",
                "updated": "2025-10-13T09:04:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    9,
                    4,
                    19,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T09:04:19Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    9,
                    4,
                    19,
                    0,
                    286,
                    0
                ],
                "title": "Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model\n  Cognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model\n  Cognition"
                },
                "summary": "We propose \\textbf{Cognitive Load Traces} (CLTs) as a mid-level\ninterpretability framework for deep models, inspired by Cognitive Load Theory\nin human cognition. CLTs are defined as symbolic, temporally varying functions\nthat quantify model-internal resource allocation. Formally, we represent CLTs\nas a three-component stochastic process $(\\mathrm{IL}_t, \\mathrm{EL}_t,\n\\mathrm{GL}_t)$, corresponding to \\emph{Intrinsic}, \\emph{Extraneous}, and\n\\emph{Germane} load. Each component is instantiated through measurable proxies\nsuch as attention entropy, KV-cache miss ratio, representation dispersion, and\ndecoding stability. We propose both symbolic formulations and visualization\nmethods (load curves, simplex diagrams) that enable interpretable analysis of\nreasoning dynamics. Experiments on reasoning and planning benchmarks show that\nCLTs predict error-onset, reveal cognitive strategies, and enable load-guided\ninterventions that improve reasoning efficiency by 15-30\\% while maintaining\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose \\textbf{Cognitive Load Traces} (CLTs) as a mid-level\ninterpretability framework for deep models, inspired by Cognitive Load Theory\nin human cognition. CLTs are defined as symbolic, temporally varying functions\nthat quantify model-internal resource allocation. Formally, we represent CLTs\nas a three-component stochastic process $(\\mathrm{IL}_t, \\mathrm{EL}_t,\n\\mathrm{GL}_t)$, corresponding to \\emph{Intrinsic}, \\emph{Extraneous}, and\n\\emph{Germane} load. Each component is instantiated through measurable proxies\nsuch as attention entropy, KV-cache miss ratio, representation dispersion, and\ndecoding stability. We propose both symbolic formulations and visualization\nmethods (load curves, simplex diagrams) that enable interpretable analysis of\nreasoning dynamics. Experiments on reasoning and planning benchmarks show that\nCLTs predict error-onset, reveal cognitive strategies, and enable load-guided\ninterventions that improve reasoning efficiency by 15-30\\% while maintaining\naccuracy."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yanxuan Yu"
                },
                "author": "Yanxuan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11121v1",
                "updated": "2025-10-13T08:08:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    8,
                    58,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T08:08:58Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    8,
                    58,
                    0,
                    286,
                    0
                ],
                "title": "Refining Hybrid Genetic Search for CVRP via Reinforcement\n  Learning-Finetuned LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Hybrid Genetic Search for CVRP via Reinforcement\n  Learning-Finetuned LLM"
                },
                "summary": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini."
                },
                "authors": [
                    {
                        "name": "Rongjie Zhu"
                    },
                    {
                        "name": "Cong Zhang"
                    },
                    {
                        "name": "Zhiguang Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiguang Cao"
                },
                "author": "Zhiguang Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11011v1",
                "updated": "2025-10-13T05:03:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    5,
                    3,
                    23,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T05:03:23Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    5,
                    3,
                    23,
                    0,
                    286,
                    0
                ],
                "title": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable\n  Transactional and Analytical Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable\n  Transactional and Analytical Workloads"
                },
                "summary": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction."
                },
                "authors": [
                    {
                        "name": "Farzaneh Zirak"
                    },
                    {
                        "name": "Farhana Choudhury"
                    },
                    {
                        "name": "Renata Borovica-Gajic"
                    }
                ],
                "author_detail": {
                    "name": "Renata Borovica-Gajic"
                },
                "author": "Renata Borovica-Gajic",
                "arxiv_comment": "This is a preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13860v1",
                "updated": "2025-10-13T04:04:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    4,
                    4,
                    54,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T04:04:54Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    4,
                    4,
                    54,
                    0,
                    286,
                    0
                ],
                "title": "ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP\n  Architecture and Paired Weight Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP\n  Architecture and Paired Weight Sharing"
                },
                "summary": "While the transformer architecture has achieved state-of-the-art performance\non natural language processing tasks, these models impose substantial memory\nand computational overhead. Recent research has identified significant\narchitectural redundancies within these models, presenting opportunities for\noptimization without compromising performance. Taking insights from research in\nAI interpretability and inference-time layer pruning, we introduce an efficient\nlanguage model architecture, referred to as ShishuLM, which reduces both the\nparameter count and Key-Value (KV) cache requirements. Given the increasing\nimportance of Small Language Models (SLMs) in agentic AI systems, we evaluate\nour approach on two SLMs of different scales. Our analysis reveals that for\nmoderate-context scenarios, normalization coupled with attention computation is\nroughly linear with the input, enabling entire transformer blocks to be\napproximated through Multi-Layer Perceptrons (MLPs). Our results show that\nShishuLM provides up to 25% reduction in memory requirements and up to 40%\nimprovement in latency during both training and inference, compared to parent\nmodels. Our experimental and analytical findings provide insights towards\nbuilding more efficient SLM architectures from a pre-training standpoint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the transformer architecture has achieved state-of-the-art performance\non natural language processing tasks, these models impose substantial memory\nand computational overhead. Recent research has identified significant\narchitectural redundancies within these models, presenting opportunities for\noptimization without compromising performance. Taking insights from research in\nAI interpretability and inference-time layer pruning, we introduce an efficient\nlanguage model architecture, referred to as ShishuLM, which reduces both the\nparameter count and Key-Value (KV) cache requirements. Given the increasing\nimportance of Small Language Models (SLMs) in agentic AI systems, we evaluate\nour approach on two SLMs of different scales. Our analysis reveals that for\nmoderate-context scenarios, normalization coupled with attention computation is\nroughly linear with the input, enabling entire transformer blocks to be\napproximated through Multi-Layer Perceptrons (MLPs). Our results show that\nShishuLM provides up to 25% reduction in memory requirements and up to 40%\nimprovement in latency during both training and inference, compared to parent\nmodels. Our experimental and analytical findings provide insights towards\nbuilding more efficient SLM architectures from a pre-training standpoint."
                },
                "authors": [
                    {
                        "name": "Shivanshu Kumar"
                    },
                    {
                        "name": "Gopalakrishnan Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Gopalakrishnan Srinivasan"
                },
                "author": "Gopalakrishnan Srinivasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10964v1",
                "updated": "2025-10-13T03:14:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    14,
                    28,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T03:14:28Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    14,
                    28,
                    0,
                    286,
                    0
                ],
                "title": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies\n  for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies\n  for Reasoning Models"
                },
                "summary": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Taehong Moon"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10862v1",
                "updated": "2025-10-13T00:11:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    0,
                    11,
                    2,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T00:11:02Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    0,
                    11,
                    2,
                    0,
                    286,
                    0
                ],
                "title": "A Joint Learning Approach to Hardware Caching and Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Joint Learning Approach to Hardware Caching and Prefetching"
                },
                "summary": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction."
                },
                "authors": [
                    {
                        "name": "Samuel Yuan"
                    },
                    {
                        "name": "Divyanshu Saxena"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Nihal Sharma"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "arxiv_comment": "Accepted at ML for Systems Workshop at the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10858v1",
                "updated": "2025-10-12T23:46:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    46,
                    4,
                    6,
                    285,
                    0
                ],
                "published": "2025-10-12T23:46:04Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    46,
                    4,
                    6,
                    285,
                    0
                ],
                "title": "DriftBench: Defining and Generating Data and Query Workload Drift for\n  Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriftBench: Defining and Generating Data and Query Workload Drift for\n  Benchmarking"
                },
                "summary": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation."
                },
                "authors": [
                    {
                        "name": "Guanli Liu"
                    },
                    {
                        "name": "Renata Borovica-Gajic"
                    }
                ],
                "author_detail": {
                    "name": "Renata Borovica-Gajic"
                },
                "author": "Renata Borovica-Gajic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00313v2",
                "updated": "2025-10-12T23:17:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    17,
                    39,
                    6,
                    285,
                    0
                ],
                "published": "2024-05-01T04:30:03Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    4,
                    30,
                    3,
                    2,
                    122,
                    0
                ],
                "title": "Streamlining Image Editing with Layered Diffusion Brushes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining Image Editing with Layered Diffusion Brushes"
                },
                "summary": "Denoising diffusion models have emerged as powerful tools for image\nmanipulation, yet interactive, localized editing workflows remain\nunderdeveloped. We introduce Layered Diffusion Brushes (LDB), a novel\ntraining-free framework that enables interactive, layer-based editing using\nstandard diffusion models. LDB defines each \"layer\" as a self-contained set of\nparameters guiding the generative process, enabling independent,\nnon-destructive, and fine-grained prompt-guided edits, even in overlapping\nregions. LDB leverages a unique intermediate latent caching approach to reduce\neach edit to only a few denoising steps, achieving 140~ms per edit on consumer\nGPUs. An editor implementing LDB, incorporating familiar layer concepts, was\nevaluated via user study and quantitative metrics. Results demonstrate LDB's\nsuperior speed alongside comparable or improved image quality, background\npreservation, and edit fidelity relative to state-of-the-art methods across\nvarious sequential image manipulation tasks. The findings highlight LDB's\nability to significantly enhance creative workflows by providing an intuitive\nand efficient approach to diffusion-based image editing and its potential for\nexpansion into related subdomains, such as video editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denoising diffusion models have emerged as powerful tools for image\nmanipulation, yet interactive, localized editing workflows remain\nunderdeveloped. We introduce Layered Diffusion Brushes (LDB), a novel\ntraining-free framework that enables interactive, layer-based editing using\nstandard diffusion models. LDB defines each \"layer\" as a self-contained set of\nparameters guiding the generative process, enabling independent,\nnon-destructive, and fine-grained prompt-guided edits, even in overlapping\nregions. LDB leverages a unique intermediate latent caching approach to reduce\neach edit to only a few denoising steps, achieving 140~ms per edit on consumer\nGPUs. An editor implementing LDB, incorporating familiar layer concepts, was\nevaluated via user study and quantitative metrics. Results demonstrate LDB's\nsuperior speed alongside comparable or improved image quality, background\npreservation, and edit fidelity relative to state-of-the-art methods across\nvarious sequential image manipulation tasks. The findings highlight LDB's\nability to significantly enhance creative workflows by providing an intuitive\nand efficient approach to diffusion-based image editing and its potential for\nexpansion into related subdomains, such as video editing."
                },
                "authors": [
                    {
                        "name": "Peyman Gholami"
                    },
                    {
                        "name": "Robert Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Robert Xiao"
                },
                "author": "Robert Xiao",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2306.00219",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10587v1",
                "updated": "2025-10-12T13:06:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    13,
                    6,
                    59,
                    6,
                    285,
                    0
                ],
                "published": "2025-10-12T13:06:59Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    13,
                    6,
                    59,
                    6,
                    285,
                    0
                ],
                "title": "A Simple and Better Baseline for Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Better Baseline for Visual Grounding"
                },
                "summary": "Visual grounding aims to predict the locations of target objects specified by\ntextual descriptions. For this task with linguistic and visual modalities,\nthere is a latest research line that focuses on only selecting the\nlinguistic-relevant visual regions for object localization to reduce the\ncomputational overhead. Albeit achieving impressive performance, it is\niteratively performed on different image scales, and at every iteration,\nlinguistic features and visual features need to be stored in a cache, incurring\nextra overhead. To facilitate the implementation, in this paper, we propose a\nfeature selection-based simple yet effective baseline for visual grounding,\ncalled FSVG. Specifically, we directly encapsulate the linguistic and visual\nmodalities into an overall network architecture without complicated iterative\nprocedures, and utilize the language in parallel as guidance to facilitate the\ninteraction between linguistic modal and visual modal for extracting effective\nvisual features. Furthermore, to reduce the computational cost, during the\nvisual feature learning, we introduce a similarity-based feature selection\nmechanism to only exploit language-related visual features for faster\nprediction. Extensive experiments conducted on several benchmark datasets\ncomprehensively substantiate that the proposed FSVG achieves a better balance\nbetween accuracy and efficiency beyond the current state-of-the-art methods.\nCode is available at https://github.com/jcwang0602/FSVG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual grounding aims to predict the locations of target objects specified by\ntextual descriptions. For this task with linguistic and visual modalities,\nthere is a latest research line that focuses on only selecting the\nlinguistic-relevant visual regions for object localization to reduce the\ncomputational overhead. Albeit achieving impressive performance, it is\niteratively performed on different image scales, and at every iteration,\nlinguistic features and visual features need to be stored in a cache, incurring\nextra overhead. To facilitate the implementation, in this paper, we propose a\nfeature selection-based simple yet effective baseline for visual grounding,\ncalled FSVG. Specifically, we directly encapsulate the linguistic and visual\nmodalities into an overall network architecture without complicated iterative\nprocedures, and utilize the language in parallel as guidance to facilitate the\ninteraction between linguistic modal and visual modal for extracting effective\nvisual features. Furthermore, to reduce the computational cost, during the\nvisual feature learning, we introduce a similarity-based feature selection\nmechanism to only exploit language-related visual features for faster\nprediction. Extensive experiments conducted on several benchmark datasets\ncomprehensively substantiate that the proposed FSVG achieves a better balance\nbetween accuracy and efficiency beyond the current state-of-the-art methods.\nCode is available at https://github.com/jcwang0602/FSVG."
                },
                "authors": [
                    {
                        "name": "Jingchao Wang"
                    },
                    {
                        "name": "Wenlong Zhang"
                    },
                    {
                        "name": "Dingjiang Huang"
                    },
                    {
                        "name": "Hong Wang"
                    },
                    {
                        "name": "Yefeng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yefeng Zheng"
                },
                "author": "Yefeng Zheng",
                "arxiv_comment": "ICME2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18809v2",
                "updated": "2025-10-12T10:09:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    10,
                    9,
                    53,
                    6,
                    285,
                    0
                ],
                "published": "2025-05-24T17:46:47Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VORTA: Efficient Video Diffusion via Routing Sparse Attention"
                },
                "summary": "Video diffusion transformers have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nacceleration methods enhance the efficiency by exploiting the local sparsity of\nattention scores; yet they often struggle with accelerating the long-range\ncomputation. To address this problem, we propose VORTA, an acceleration\nframework with two novel components: 1) a sparse attention mechanism that\nefficiently captures long-range dependencies, and 2) a routing strategy that\nadaptively replaces full 3D attention with specialized sparse attention\nvariants. VORTA achieves an end-to-end speedup $1.76\\times$ without loss of\nquality on VBench. Furthermore, it can seamlessly integrate with various other\nacceleration methods, such as model caching and step distillation, reaching up\nto speedup $14.41\\times$ with negligible performance degradation. VORTA\ndemonstrates its efficiency and enhances the practicality of video diffusion\ntransformers in real-world settings. Codes and weights are available at\nhttps://github.com/wenhao728/VORTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion transformers have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nacceleration methods enhance the efficiency by exploiting the local sparsity of\nattention scores; yet they often struggle with accelerating the long-range\ncomputation. To address this problem, we propose VORTA, an acceleration\nframework with two novel components: 1) a sparse attention mechanism that\nefficiently captures long-range dependencies, and 2) a routing strategy that\nadaptively replaces full 3D attention with specialized sparse attention\nvariants. VORTA achieves an end-to-end speedup $1.76\\times$ without loss of\nquality on VBench. Furthermore, it can seamlessly integrate with various other\nacceleration methods, such as model caching and step distillation, reaching up\nto speedup $14.41\\times$ with negligible performance degradation. VORTA\ndemonstrates its efficiency and enhances the practicality of video diffusion\ntransformers in real-world settings. Codes and weights are available at\nhttps://github.com/wenhao728/VORTA."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by NeurIPS 2025. The code is available at\n  https://github.com/wenhao728/VORTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v2",
                "updated": "2025-10-12T04:46:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    4,
                    46,
                    48,
                    6,
                    285,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "arxiv_comment": "fix typo perplexity->log perplexity; added recent papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v3",
                "updated": "2025-10-12T04:04:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    4,
                    4,
                    34,
                    6,
                    285,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation\n  Linking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation\n  Linking"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Neuralink, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory.\nNeuralink leverages the concept of Neuron Co-Activation, where neurons\nfrequently activated together are linked to facilitate continuous read access\nand optimize I/O efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Neuralink achieves on\naverage $1.49\\times$ improvements in end-to-end latency compared to the\nstate-of-the-art. As the first solution to optimize storage placement under\nsparsity, Neuralink explores a new optimization space at the intersection of\nsparsity-driven algorithm and storage-level system co-design for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Neuralink, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory.\nNeuralink leverages the concept of Neuron Co-Activation, where neurons\nfrequently activated together are linked to facilitate continuous read access\nand optimize I/O efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Neuralink achieves on\naverage $1.49\\times$ improvements in end-to-end latency compared to the\nstate-of-the-art. As the first solution to optimize storage placement under\nsparsity, Neuralink explores a new optimization space at the intersection of\nsparsity-driven algorithm and storage-level system co-design for LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "arxiv_doi": "10.1145/3676642.3736114",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736114",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.19274v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Vol.\n  3, Rotterdam, Netherlands, 2025, pp. 147-162",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10290v1",
                "updated": "2025-10-11T17:08:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    17,
                    8,
                    45,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T17:08:45Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    17,
                    8,
                    45,
                    5,
                    284,
                    0
                ],
                "title": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in\n  Enterprise Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in\n  Enterprise Pipelines"
                },
                "summary": "Automated code review adoption lags in compliance-heavy settings, where\nstatic analyzers produce high-volume, low-rationale outputs, and naive LLM use\nrisks hallucination and incurring cost overhead. We present a production system\nfor grounded, PR-native review that pairs static-analysis findings with\nAST-guided context extraction and a single-GPU, on-demand serving stack\n(quantized open-weight model, multi-tier caching) to deliver concise\nexplanations and remediation guidance. Evaluated on safety-oriented C/C++\nstandards, the approach achieves sub-minute median first-feedback (offline p50\nbuild+LLM 59.8s) while maintaining competitive violation reduction and lower\nviolation rates versus larger proprietary models. The architecture is\ndecoupled: teams can adopt the grounding/prompting layer or the serving layer\nindependently. A small internal survey (n=8) provides directional signals of\nreduced triage effort and moderate perceived grounding, with participants\nreporting fewer human review iterations. We outline operational lessons and\nlimitations, emphasizing reproducibility, auditability, and pathways to broader\nstandards and assisted patching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated code review adoption lags in compliance-heavy settings, where\nstatic analyzers produce high-volume, low-rationale outputs, and naive LLM use\nrisks hallucination and incurring cost overhead. We present a production system\nfor grounded, PR-native review that pairs static-analysis findings with\nAST-guided context extraction and a single-GPU, on-demand serving stack\n(quantized open-weight model, multi-tier caching) to deliver concise\nexplanations and remediation guidance. Evaluated on safety-oriented C/C++\nstandards, the approach achieves sub-minute median first-feedback (offline p50\nbuild+LLM 59.8s) while maintaining competitive violation reduction and lower\nviolation rates versus larger proprietary models. The architecture is\ndecoupled: teams can adopt the grounding/prompting layer or the serving layer\nindependently. A small internal survey (n=8) provides directional signals of\nreduced triage effort and moderate perceived grounding, with participants\nreporting fewer human review iterations. We outline operational lessons and\nlimitations, emphasizing reproducibility, auditability, and pathways to broader\nstandards and assisted patching."
                },
                "authors": [
                    {
                        "name": "Sayan Mandal"
                    },
                    {
                        "name": "Hua Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Jiang"
                },
                "author": "Hua Jiang",
                "arxiv_comment": "Submitted to MLSys 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10219v1",
                "updated": "2025-10-11T13:52:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    13,
                    52,
                    48,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T13:52:48Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    13,
                    52,
                    48,
                    5,
                    284,
                    0
                ],
                "title": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc"
                },
                "summary": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10129v1",
                "updated": "2025-10-11T09:28:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    28,
                    26,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T09:28:26Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    28,
                    26,
                    5,
                    284,
                    0
                ],
                "title": "CacheClip: Accelerating RAG with Effective KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheClip: Accelerating RAG with Effective KV Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems."
                },
                "authors": [
                    {
                        "name": "Bin Yang"
                    },
                    {
                        "name": "Qiuyu Leng"
                    },
                    {
                        "name": "Jun Zeng"
                    },
                    {
                        "name": "Zhenhua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhua Wu"
                },
                "author": "Zhenhua Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v3",
                "updated": "2025-10-11T09:04:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    4,
                    23,
                    5,
                    284,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Modern large language models (LLMs) extend context lengths to millions of\ntokens, enabling coherent, personalized responses grounded in long\nconversational histories. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly becomes\nthe bottleneck in resource-constrained environments. An active line of research\nfor reducing memory bottleneck is KV cache compression, which seeks to limit\ncache size while preserving accuracy. Yet existing methods face two major\nlimitations: (i) evicting the KV cache after full-context prefill causes\nunbounded peak memory, and (ii) query-dependent eviction narrows the cache to a\nsingle query, leading to failure cases in multi-turn conversations. We\nintroduce EpiCache, a training-free KV cache management framework for long\nconversational question answering (LongConvQA) under fixed memory budgets.\nEpiCache bounds cache growth through block-wise prefill and preserves\ntopic-relevant context via episodic KV compression, which clusters conversation\nhistory into coherent episodes and applies episode-specific KV cache eviction.\nWe further design an adaptive layer-wise budget allocation strategy that\nmeasures each layer's sensitivity to eviction and distributes the memory budget\nacross layers accordingly. Across three LongConvQA benchmarks, EpiCache\nimproves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x\ncompression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient\nmulti-turn interaction under strict resource limits. Our code is available at\nhttps://github.com/apple/ml-epicache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) extend context lengths to millions of\ntokens, enabling coherent, personalized responses grounded in long\nconversational histories. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly becomes\nthe bottleneck in resource-constrained environments. An active line of research\nfor reducing memory bottleneck is KV cache compression, which seeks to limit\ncache size while preserving accuracy. Yet existing methods face two major\nlimitations: (i) evicting the KV cache after full-context prefill causes\nunbounded peak memory, and (ii) query-dependent eviction narrows the cache to a\nsingle query, leading to failure cases in multi-turn conversations. We\nintroduce EpiCache, a training-free KV cache management framework for long\nconversational question answering (LongConvQA) under fixed memory budgets.\nEpiCache bounds cache growth through block-wise prefill and preserves\ntopic-relevant context via episodic KV compression, which clusters conversation\nhistory into coherent episodes and applies episode-specific KV cache eviction.\nWe further design an adaptive layer-wise budget allocation strategy that\nmeasures each layer's sensitivity to eviction and distributes the memory budget\nacross layers accordingly. Across three LongConvQA benchmarks, EpiCache\nimproves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x\ncompression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient\nmulti-turn interaction under strict resource limits. Our code is available at\nhttps://github.com/apple/ml-epicache."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10102v1",
                "updated": "2025-10-11T08:24:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    8,
                    24,
                    19,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T08:24:19Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    8,
                    24,
                    19,
                    5,
                    284,
                    0
                ],
                "title": "PANTHER: Generative Pretraining Beyond Language for Sequential User\n  Behavior Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PANTHER: Generative Pretraining Beyond Language for Sequential User\n  Behavior Modeling"
                },
                "summary": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling."
                },
                "authors": [
                    {
                        "name": "Guilin Li"
                    },
                    {
                        "name": "Yun Zhang"
                    },
                    {
                        "name": "Xiuyuan Chen"
                    },
                    {
                        "name": "Chengqi Li"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Wenjia Wang"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Matthias Hwai Yong Tan"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Hwai Yong Tan"
                },
                "author": "Matthias Hwai Yong Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09952v1",
                "updated": "2025-10-11T01:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    1,
                    42,
                    38,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T01:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    1,
                    42,
                    38,
                    5,
                    284,
                    0
                ],
                "title": "HTTP Request Synchronization Defeats Discrepancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HTTP Request Synchronization Defeats Discrepancy Attacks"
                },
                "summary": "Contemporary web application architectures involve many layers of proxy\nservices that process traffic. Due to the complexity of HTTP and vendor design\ndecisions, these proxies sometimes process a given request in different ways.\nAttackers can exploit these processing discrepancies to launch damaging attacks\nincluding web cache poisoning and request smuggling. Discrepancy attacks are\nsurging, yet, there exists no systemic defense.\n  In this work, we propose the first comprehensive defense to address this\nproblem, called HTTP Request Synchronization. Our scheme uses standard HTTP\nextension mechanisms to augment each request with a complete processing\nhistory. It propagates this context through the traffic path detailing how each\nserver hop has processed said request. Using this history, every proxy server\ncan validate that their processing is consistent with all previous hops,\neliminating discrepancy attacks. We implement our scheme for 5 popular proxy\ntechnologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating\nits practical impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary web application architectures involve many layers of proxy\nservices that process traffic. Due to the complexity of HTTP and vendor design\ndecisions, these proxies sometimes process a given request in different ways.\nAttackers can exploit these processing discrepancies to launch damaging attacks\nincluding web cache poisoning and request smuggling. Discrepancy attacks are\nsurging, yet, there exists no systemic defense.\n  In this work, we propose the first comprehensive defense to address this\nproblem, called HTTP Request Synchronization. Our scheme uses standard HTTP\nextension mechanisms to augment each request with a complete processing\nhistory. It propagates this context through the traffic path detailing how each\nserver hop has processed said request. Using this history, every proxy server\ncan validate that their processing is consistent with all previous hops,\neliminating discrepancy attacks. We implement our scheme for 5 popular proxy\ntechnologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating\nits practical impact."
                },
                "authors": [
                    {
                        "name": "Cem Topcuoglu"
                    },
                    {
                        "name": "Kaan Onarlioglu"
                    },
                    {
                        "name": "Steven Sprecher"
                    },
                    {
                        "name": "Engin Kirda"
                    }
                ],
                "author_detail": {
                    "name": "Engin Kirda"
                },
                "author": "Engin Kirda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09907v1",
                "updated": "2025-10-10T22:43:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    22,
                    43,
                    54,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T22:43:54Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    22,
                    43,
                    54,
                    4,
                    283,
                    0
                ],
                "title": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem"
                },
                "summary": "Property-based testing (PBT) is a lightweight formal method, typically\nimplemented as a randomized testing framework. Users specify the input domain\nfor their test using combinators supplied by the PBT framework, and the\nexpected properties or invariants as a unit-test function. The framework then\nsearches for a counterexample, e.g. by generating inputs and calling the test\nfunction. In this work, we demonstrate an LLM-based agent which analyzes Python\nmodules, infers function-specific and cross-function properties from code and\ndocumentation, synthesizes and executes PBTs, reflects on outputs of these\ntests to confirm true bugs, and finally outputs actionable bug reports for the\ndeveloper. We perform an extensive evaluation of our agent across 100 popular\nPython packages. Of the bug reports generated by the agent, we found after\nmanual review that 56\\% were valid bugs and 32\\% were valid bugs that we would\nreport to maintainers. We then developed a ranking rubric to surface\nhigh-priority valid bugs to developers, and found that of the 21 top-scoring\nbugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure\nmodes from serialization failures to numerical precision errors to flawed cache\nimplementations. We reported 5 bugs, 4 with patches, including to NumPy and\ncloud computing SDKs, with 3 patches merged successfully. Our results suggest\nthat LLMs with PBT provides a rigorous and scalable method for autonomously\ntesting software. Our code and artifacts are available at:\nhttps://github.com/mmaaz-git/agentic-pbt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Property-based testing (PBT) is a lightweight formal method, typically\nimplemented as a randomized testing framework. Users specify the input domain\nfor their test using combinators supplied by the PBT framework, and the\nexpected properties or invariants as a unit-test function. The framework then\nsearches for a counterexample, e.g. by generating inputs and calling the test\nfunction. In this work, we demonstrate an LLM-based agent which analyzes Python\nmodules, infers function-specific and cross-function properties from code and\ndocumentation, synthesizes and executes PBTs, reflects on outputs of these\ntests to confirm true bugs, and finally outputs actionable bug reports for the\ndeveloper. We perform an extensive evaluation of our agent across 100 popular\nPython packages. Of the bug reports generated by the agent, we found after\nmanual review that 56\\% were valid bugs and 32\\% were valid bugs that we would\nreport to maintainers. We then developed a ranking rubric to surface\nhigh-priority valid bugs to developers, and found that of the 21 top-scoring\nbugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure\nmodes from serialization failures to numerical precision errors to flawed cache\nimplementations. We reported 5 bugs, 4 with patches, including to NumPy and\ncloud computing SDKs, with 3 patches merged successfully. Our results suggest\nthat LLMs with PBT provides a rigorous and scalable method for autonomously\ntesting software. Our code and artifacts are available at:\nhttps://github.com/mmaaz-git/agentic-pbt."
                },
                "authors": [
                    {
                        "name": "Muhammad Maaz"
                    },
                    {
                        "name": "Liam DeVoe"
                    },
                    {
                        "name": "Zac Hatfield-Dodds"
                    },
                    {
                        "name": "Nicholas Carlini"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Carlini"
                },
                "author": "Nicholas Carlini",
                "arxiv_comment": "4 pages (main), NeurIPS 2025, The 4th Deep Learning for Code Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09883v1",
                "updated": "2025-10-10T21:37:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    21,
                    37,
                    49,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T21:37:49Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    21,
                    37,
                    49,
                    4,
                    283,
                    0
                ],
                "title": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context\n  Reasoning"
                },
                "summary": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning."
                },
                "authors": [
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Murali Annavarm"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavarm"
                },
                "author": "Murali Annavarm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09847v1",
                "updated": "2025-10-10T20:19:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    20,
                    19,
                    44,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T20:19:44Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    20,
                    19,
                    44,
                    4,
                    283,
                    0
                ],
                "title": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware\n  Resource Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware\n  Resource Scheduling"
                },
                "summary": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability."
                },
                "authors": [
                    {
                        "name": "Said Muhammad"
                    },
                    {
                        "name": "Lahlou Laaziz"
                    },
                    {
                        "name": "Nadjia Kara"
                    },
                    {
                        "name": "Phat Tan Nguyen"
                    },
                    {
                        "name": "Timothy Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Murphy"
                },
                "author": "Timothy Murphy",
                "arxiv_comment": "Accepted and presented at the 13th IEEE International Conference on\n  Intelligent Mobile Computing 2025 (IMC), CISOSE 2025 in Tucson, Arizona, USA.\n  This is the author's accepted manuscript (AAM). The final published version\n  will appear in the IEEE conference proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09608v1",
                "updated": "2025-10-10T17:59:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    59,
                    58,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T17:59:58Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    59,
                    58,
                    4,
                    283,
                    0
                ],
                "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingVLM: Real-Time Understanding for Infinite Video Streams"
                },
                "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm."
                },
                "authors": [
                    {
                        "name": "Ruyi Xu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Liuning He"
                    },
                    {
                        "name": "Kelly Peng"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v2",
                "updated": "2025-10-10T16:56:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    16,
                    56,
                    23,
                    4,
                    283,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_doi": "10.1145/3769780",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769780",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03812v2",
                "updated": "2025-10-10T16:08:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    16,
                    8,
                    26,
                    4,
                    283,
                    0
                ],
                "published": "2025-07-04T21:09:51Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "title": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma"
                },
                "summary": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained. In an additionally experimental setup of using GMGPolar as a\npreconditioner for conjugate gradients, this speedup could even be increased to\nfactors between 25 and 37.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained. In an additionally experimental setup of using GMGPolar as a\npreconditioner for conjugate gradients, this speedup could even be increased to\nfactors between 25 and 37."
                },
                "authors": [
                    {
                        "name": "Julian Litz"
                    },
                    {
                        "name": "Philippe Leleux"
                    },
                    {
                        "name": "Carola Kruse"
                    },
                    {
                        "name": "Joscha Gedicke"
                    },
                    {
                        "name": "Martin J. Kühn"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. Kühn"
                },
                "author": "Martin J. Kühn",
                "arxiv_comment": "29 pages, 11 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09477v1",
                "updated": "2025-10-10T15:32:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    15,
                    32,
                    58,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T15:32:58Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    15,
                    32,
                    58,
                    4,
                    283,
                    0
                ],
                "title": "Efficient Autoregressive Inference for Transformer Probabilistic Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Autoregressive Inference for Transformer Probabilistic Models"
                },
                "summary": "Transformer-based models for amortized probabilistic inference, such as\nneural processes, prior-fitted networks, and tabular foundation models, excel\nat single-pass marginal prediction. However, many real-world applications, from\nsignal interpolation to multi-column tabular predictions, require coherent\njoint distributions that capture dependencies between predictions. While purely\nautoregressive architectures efficiently generate such distributions, they\nsacrifice the flexible set-conditioning that makes these models powerful for\nmeta-learning. Conversely, the standard approach to obtain joint distributions\nfrom set-based models requires expensive re-encoding of the entire augmented\nconditioning set at each autoregressive step. We introduce a causal\nautoregressive buffer that preserves the advantages of both paradigms. Our\napproach decouples context encoding from updating the conditioning set. The\nmodel processes the context once and caches it. A dynamic buffer then captures\ntarget dependencies: as targets are incorporated, they enter the buffer and\nattend to both the cached context and previously buffered targets. This enables\nefficient batched autoregressive generation and one-pass joint log-likelihood\nevaluation. A unified training strategy allows seamless integration of\nset-based and autoregressive modes at minimal additional cost. Across synthetic\nfunctions, EEG signals, cognitive models, and tabular data, our method matches\npredictive accuracy of strong baselines while delivering up to 20 times faster\njoint sampling. Our approach combines the efficiency of autoregressive\ngenerative models with the representational power of set-based conditioning,\nmaking joint prediction practical for transformer-based probabilistic models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models for amortized probabilistic inference, such as\nneural processes, prior-fitted networks, and tabular foundation models, excel\nat single-pass marginal prediction. However, many real-world applications, from\nsignal interpolation to multi-column tabular predictions, require coherent\njoint distributions that capture dependencies between predictions. While purely\nautoregressive architectures efficiently generate such distributions, they\nsacrifice the flexible set-conditioning that makes these models powerful for\nmeta-learning. Conversely, the standard approach to obtain joint distributions\nfrom set-based models requires expensive re-encoding of the entire augmented\nconditioning set at each autoregressive step. We introduce a causal\nautoregressive buffer that preserves the advantages of both paradigms. Our\napproach decouples context encoding from updating the conditioning set. The\nmodel processes the context once and caches it. A dynamic buffer then captures\ntarget dependencies: as targets are incorporated, they enter the buffer and\nattend to both the cached context and previously buffered targets. This enables\nefficient batched autoregressive generation and one-pass joint log-likelihood\nevaluation. A unified training strategy allows seamless integration of\nset-based and autoregressive modes at minimal additional cost. Across synthetic\nfunctions, EEG signals, cognitive models, and tabular data, our method matches\npredictive accuracy of strong baselines while delivering up to 20 times faster\njoint sampling. Our approach combines the efficiency of autoregressive\ngenerative models with the representational power of set-based conditioning,\nmaking joint prediction practical for transformer-based probabilistic models."
                },
                "authors": [
                    {
                        "name": "Conor Hassan"
                    },
                    {
                        "name": "Nasrulloh Loka"
                    },
                    {
                        "name": "Cen-You Li"
                    },
                    {
                        "name": "Daolang Huang"
                    },
                    {
                        "name": "Paul E. Chang"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Francesco Silvestrin"
                    },
                    {
                        "name": "Samuel Kaski"
                    },
                    {
                        "name": "Luigi Acerbi"
                    }
                ],
                "author_detail": {
                    "name": "Luigi Acerbi"
                },
                "author": "Luigi Acerbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09409v1",
                "updated": "2025-10-10T14:03:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    14,
                    3,
                    42,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T14:03:42Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    14,
                    3,
                    42,
                    4,
                    283,
                    0
                ],
                "title": "3C Resources Joint Allocation for Time-Deterministic Remote Sensing\n  Image Backhaul in the Space-Ground Integrated Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3C Resources Joint Allocation for Time-Deterministic Remote Sensing\n  Image Backhaul in the Space-Ground Integrated Network"
                },
                "summary": "Low-Earth-orbit (LEO) satellites assist observation satellites (OSs) to\ncompress and backhaul more time-determined images (TDI) has become a new\nparadigm, which is used to enhance the timeout caused by the limited computing\nresources of OSs. However, how to capture the time-varying and dynamic\ncharacteristics of multi-dimensional resources is challenging for efficient\ncollaborative scheduling. Motivated by this factor, we design a highly succinct\nmulti-dimensional resource time-expanded graph (MDR-TEG) modell. Specifically,\nby employing a slots division mechanism and introducing an external virtual\nnode, the time-varying communication, caching, and computing (3C) resources are\ndepicted in low complexity by the link weights within, between, and outside the\nslots. Based on the MDR-TEG, the maximizing successful transmission ratio of\nTDI (MSTR-TDI) is modeled as a mixed integer linear programming (MILP) problem.\nWhich further relaxed decomposed into two tractable sub-problems: maximizing\nthe successful transmission rate of images (MSTRI) and ensuring the timeliness\nproblem (ETP). Subsequently, an efficient subgradient of relaxation computing\nconstraint (SRCC) algorithm is proposed. The upper and lower bounds of MSTR-TDI\nare obtained by solving the two subproblems and the dual problem (DP), and the\ndirection of the next iteration is obtained by feedback. Furthermore, arranging\nthe sending sequences of images to improve the quality of the solution. The\napproximate optimal solution of MSTR-TDI is eventually obtained through\nrepeated iterations. The simulation results verify the superiority of the\nproposed MDR-TEG model and the effectiveness of the SRCC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Earth-orbit (LEO) satellites assist observation satellites (OSs) to\ncompress and backhaul more time-determined images (TDI) has become a new\nparadigm, which is used to enhance the timeout caused by the limited computing\nresources of OSs. However, how to capture the time-varying and dynamic\ncharacteristics of multi-dimensional resources is challenging for efficient\ncollaborative scheduling. Motivated by this factor, we design a highly succinct\nmulti-dimensional resource time-expanded graph (MDR-TEG) modell. Specifically,\nby employing a slots division mechanism and introducing an external virtual\nnode, the time-varying communication, caching, and computing (3C) resources are\ndepicted in low complexity by the link weights within, between, and outside the\nslots. Based on the MDR-TEG, the maximizing successful transmission ratio of\nTDI (MSTR-TDI) is modeled as a mixed integer linear programming (MILP) problem.\nWhich further relaxed decomposed into two tractable sub-problems: maximizing\nthe successful transmission rate of images (MSTRI) and ensuring the timeliness\nproblem (ETP). Subsequently, an efficient subgradient of relaxation computing\nconstraint (SRCC) algorithm is proposed. The upper and lower bounds of MSTR-TDI\nare obtained by solving the two subproblems and the dual problem (DP), and the\ndirection of the next iteration is obtained by feedback. Furthermore, arranging\nthe sending sequences of images to improve the quality of the solution. The\napproximate optimal solution of MSTR-TDI is eventually obtained through\nrepeated iterations. The simulation results verify the superiority of the\nproposed MDR-TEG model and the effectiveness of the SRCC."
                },
                "authors": [
                    {
                        "name": "Chongxiao Cai"
                    },
                    {
                        "name": "Yan Zhu"
                    },
                    {
                        "name": "Min Sheng"
                    },
                    {
                        "name": "Jiandong Li"
                    },
                    {
                        "name": "Yan Shi"
                    },
                    {
                        "name": "Di Zhou"
                    },
                    {
                        "name": "Ziwen Xie"
                    },
                    {
                        "name": "Chen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhang"
                },
                "author": "Chen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08272v2",
                "updated": "2025-10-10T13:15:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    13,
                    15,
                    40,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-09T14:29:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    29,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "Systematic Assessment of Cache Timing Vulnerabilities on RISC-V\n  Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Assessment of Cache Timing Vulnerabilities on RISC-V\n  Processors"
                },
                "summary": "While interest in the open RISC-V instruction set architecture is growing,\ntools to assess the security of concrete processor implementations are lacking.\nThere are dedicated tools and benchmarks for common microarchitectural\nside-channel vulnerabilities for popular processor families such as Intel\nx86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in\nporting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities\nto RISC-V. We then use this benchmark to evaluate the security of three\ncommercially available RISC-V processors, the T-Head C910 and the SiFive U54\nand U74 cores. We observe that the C910 processor exhibits more distinct timing\ntypes than the other processors, leading to the assumption that code running on\nthe C910 would be exposed to more microarchitectural vulnerability sources. In\naddition, our evaluation reveals that $65.9\\%$ of the vulnerabilities covered\nby the benchmark exist in all processors, while only $6.8\\%$ are absent from\nall cores. Our work, in particular the ported benchmark, aims to support RISC-V\nprocessor designers to identify leakage sources early in their designs and to\nsupport the development of countermeasures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While interest in the open RISC-V instruction set architecture is growing,\ntools to assess the security of concrete processor implementations are lacking.\nThere are dedicated tools and benchmarks for common microarchitectural\nside-channel vulnerabilities for popular processor families such as Intel\nx86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in\nporting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities\nto RISC-V. We then use this benchmark to evaluate the security of three\ncommercially available RISC-V processors, the T-Head C910 and the SiFive U54\nand U74 cores. We observe that the C910 processor exhibits more distinct timing\ntypes than the other processors, leading to the assumption that code running on\nthe C910 would be exposed to more microarchitectural vulnerability sources. In\naddition, our evaluation reveals that $65.9\\%$ of the vulnerabilities covered\nby the benchmark exist in all processors, while only $6.8\\%$ are absent from\nall cores. Our work, in particular the ported benchmark, aims to support RISC-V\nprocessor designers to identify leakage sources early in their designs and to\nsupport the development of countermeasures."
                },
                "authors": [
                    {
                        "name": "Cédrick Austa"
                    },
                    {
                        "name": "Jan Tobias Mühlberg"
                    },
                    {
                        "name": "Jean-Michel Dricot"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Dricot"
                },
                "author": "Jean-Michel Dricot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v2",
                "updated": "2025-10-10T13:08:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    13,
                    8,
                    39,
                    4,
                    283,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "The paper is currently under investigation regarding concerns of\n  potential academic misconduct. While the investigation is ongoing, the\n  authors have voluntarily requested to withdraw the manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09309v1",
                "updated": "2025-10-10T12:01:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    12,
                    1,
                    16,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T12:01:16Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    12,
                    1,
                    16,
                    4,
                    283,
                    0
                ],
                "title": "Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM\n  Inference"
                },
                "summary": "Diffusion large language models (dLLMs) present a promising alternative to\ndominant autoregressive models (ARMs) by the ability of parallel decoding at\nthe expense of substantial computation and memory costs. Specifically, the\ncache mechanism for bidirectional attention in dLLMs demands large memory\nfootprint, restricting their ability to handle long contexts under\nresource-limited settings. Existing cache eviction strategies are designed for\nARMs and ignore the unique characteristics of dLLMs, thus leading to\nunsatisfactory performance. To address these challenges, we introduce MaskKV, a\ntraining-free cache eviction framework tailored to dLLMs, focusing on the\neffect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a\nmask-query guided scoring mechanism that leverages attention weights to\nidentify and evict less critical prompt tokens for each head; (2) an adaptive\ncache budgeting strategy that improves efficiency by reducing allocation in\nintermediate layers and concentrating resources on prompt-preferring heads. On\nLLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of\ntokens) retains 94% of the full-cache performance on LongBench and achieves up\nto 31x acceleration at 32k prompt length. The code is publicly available at:\nhttps://github.com/jianuo-huang/MaskKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion large language models (dLLMs) present a promising alternative to\ndominant autoregressive models (ARMs) by the ability of parallel decoding at\nthe expense of substantial computation and memory costs. Specifically, the\ncache mechanism for bidirectional attention in dLLMs demands large memory\nfootprint, restricting their ability to handle long contexts under\nresource-limited settings. Existing cache eviction strategies are designed for\nARMs and ignore the unique characteristics of dLLMs, thus leading to\nunsatisfactory performance. To address these challenges, we introduce MaskKV, a\ntraining-free cache eviction framework tailored to dLLMs, focusing on the\neffect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a\nmask-query guided scoring mechanism that leverages attention weights to\nidentify and evict less critical prompt tokens for each head; (2) an adaptive\ncache budgeting strategy that improves efficiency by reducing allocation in\nintermediate layers and concentrating resources on prompt-preferring heads. On\nLLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of\ntokens) retains 94% of the full-cache performance on LongBench and achieves up\nto 31x acceleration at 32k prompt length. The code is publicly available at:\nhttps://github.com/jianuo-huang/MaskKV"
                },
                "authors": [
                    {
                        "name": "Jianuo Huang"
                    },
                    {
                        "name": "Yaojie Zhang"
                    },
                    {
                        "name": "Yicun Yang"
                    },
                    {
                        "name": "Benhao Huang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.18876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18876v2",
                "updated": "2025-10-22T04:30:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    30,
                    24,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-21T17:59:59Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    59,
                    59,
                    1,
                    294,
                    0
                ],
                "title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for\n  Multimodal LLMs"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) excel at holistic\nunderstanding, they struggle in capturing the dense world with complex scenes,\nrequiring fine-grained analysis of intricate details and object\ninter-relationships. Region-level MLLMs have been a promising step. However,\nprevious attempts are generally optimized to understand given regions in\nisolation, neglecting crucial global contexts. To address this, we introduce\nGrasp Any Region (GAR) for comprehen- sive region-level visual understanding.\nEmpowered by an effective RoI-aligned feature replay technique, GAR supports\n(1) precise perception by leveraging necessary global contexts, and (2)\nmodeling interactions between multiple prompts. Together, it then naturally\nachieves (3) advanced compositional reasoning to answer specific free-form\nquestions about any region, shifting the paradigm from passive description to\nactive dialogue. Moreover, we construct GAR-Bench, which not only provides a\nmore accurate evaluation of single-region comprehension, but also, more\nimportantly, measures interactions and complex reasoning across multiple\nregions. Extensive experiments have demonstrated that GAR-1B not only maintains\nthe state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5\non DLC-Bench, but also excels at modeling relationships between multiple\nprompts with advanced comprehension capabilities, even surpassing InternVL3-78B\non GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms\nin-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong\ncapabilities can be easily transferred to videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) excel at holistic\nunderstanding, they struggle in capturing the dense world with complex scenes,\nrequiring fine-grained analysis of intricate details and object\ninter-relationships. Region-level MLLMs have been a promising step. However,\nprevious attempts are generally optimized to understand given regions in\nisolation, neglecting crucial global contexts. To address this, we introduce\nGrasp Any Region (GAR) for comprehen- sive region-level visual understanding.\nEmpowered by an effective RoI-aligned feature replay technique, GAR supports\n(1) precise perception by leveraging necessary global contexts, and (2)\nmodeling interactions between multiple prompts. Together, it then naturally\nachieves (3) advanced compositional reasoning to answer specific free-form\nquestions about any region, shifting the paradigm from passive description to\nactive dialogue. Moreover, we construct GAR-Bench, which not only provides a\nmore accurate evaluation of single-region comprehension, but also, more\nimportantly, measures interactions and complex reasoning across multiple\nregions. Extensive experiments have demonstrated that GAR-1B not only maintains\nthe state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5\non DLC-Bench, but also excels at modeling relationships between multiple\nprompts with advanced comprehension capabilities, even surpassing InternVL3-78B\non GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms\nin-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong\ncapabilities can be easily transferred to videos."
                },
                "authors": [
                    {
                        "name": "Haochen Wang"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Yikang Zhou"
                    },
                    {
                        "name": "Yanwei Li"
                    },
                    {
                        "name": "Jiacong Wang"
                    },
                    {
                        "name": "Jiani Zheng"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Jiahao Meng"
                    },
                    {
                        "name": "Zilong Huang"
                    },
                    {
                        "name": "Guangcan Mai"
                    },
                    {
                        "name": "Anran Wang"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Zhuochen Wang"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoxiang Zhang"
                },
                "author": "Zhaoxiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18873v1",
                "updated": "2025-10-21T17:59:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    59,
                    36,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:59:36Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    59,
                    36,
                    1,
                    294,
                    0
                ],
                "title": "DSI-Bench: A Benchmark for Dynamic Spatial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSI-Bench: A Benchmark for Dynamic Spatial Intelligence"
                },
                "summary": "Reasoning about dynamic spatial relationships is essential, as both observers\nand objects often move simultaneously. Although vision-language models (VLMs)\nand visual expertise models excel in 2D tasks and static scenarios, their\nability to fully understand dynamic 3D scenarios remains limited. We introduce\nDynamic Spatial Intelligence and propose DSI-Bench, a benchmark with nearly\n1,000 dynamic videos and over 1,700 manually annotated questions covering nine\ndecoupled motion patterns of observers and objects. Spatially and temporally\nsymmetric designs reduce biases and enable systematic evaluation of models'\nreasoning about self-motion and object motion. Our evaluation of 14 VLMs and\nexpert models reveals key limitations: models often conflate observer and\nobject motion, exhibit semantic biases, and fail to accurately infer relative\nrelationships in dynamic scenarios. Our DSI-Bench provides valuable findings\nand insights about the future development of general and expertise models with\ndynamic spatial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning about dynamic spatial relationships is essential, as both observers\nand objects often move simultaneously. Although vision-language models (VLMs)\nand visual expertise models excel in 2D tasks and static scenarios, their\nability to fully understand dynamic 3D scenarios remains limited. We introduce\nDynamic Spatial Intelligence and propose DSI-Bench, a benchmark with nearly\n1,000 dynamic videos and over 1,700 manually annotated questions covering nine\ndecoupled motion patterns of observers and objects. Spatially and temporally\nsymmetric designs reduce biases and enable systematic evaluation of models'\nreasoning about self-motion and object motion. Our evaluation of 14 VLMs and\nexpert models reveals key limitations: models often conflate observer and\nobject motion, exhibit semantic biases, and fail to accurately infer relative\nrelationships in dynamic scenarios. Our DSI-Bench provides valuable findings\nand insights about the future development of general and expertise models with\ndynamic spatial intelligence."
                },
                "authors": [
                    {
                        "name": "Ziang Zhang"
                    },
                    {
                        "name": "Zehan Wang"
                    },
                    {
                        "name": "Guanghao Zhang"
                    },
                    {
                        "name": "Weilong Dai"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Ziang Yan"
                    },
                    {
                        "name": "Minjie Hong"
                    },
                    {
                        "name": "Zhou Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Zhao"
                },
                "author": "Zhou Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18871v1",
                "updated": "2025-10-21T17:59:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    59,
                    5,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:59:05Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    59,
                    5,
                    1,
                    294,
                    0
                ],
                "title": "How Do LLMs Use Their Depth?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Do LLMs Use Their Depth?"
                },
                "summary": "Growing evidence suggests that large language models do not use their depth\nuniformly, yet we still lack a fine-grained understanding of their layer-wise\nprediction dynamics. In this paper, we trace the intermediate representations\nof several open-weight models during inference and reveal a structured and\nnuanced use of depth. Specifically, we propose a \"Guess-then-Refine\" framework\nthat explains how LLMs internally structure their computations to make\npredictions. We first show that the top-ranked predictions in early LLM layers\nare composed primarily of high-frequency tokens, which act as statistical\nguesses proposed by the model early on due to the lack of appropriate\ncontextual information. As contextual information develops deeper into the\nmodel, these initial guesses get refined into contextually appropriate tokens.\nEven high-frequency token predictions from early layers get refined >70% of the\ntime, indicating that correct token prediction is not \"one-and-done\". We then\ngo beyond frequency-based prediction to examine the dynamic usage of layer\ndepth across three case studies. (i) Part-of-speech analysis shows that\nfunction words are, on average, the earliest to be predicted correctly. (ii)\nFact recall task analysis shows that, in a multi-token answer, the first token\nrequires more computational depth than the rest. (iii) Multiple-choice task\nanalysis shows that the model identifies the format of the response within the\nfirst half of the layers, but finalizes its response only toward the end.\nTogether, our results provide a detailed view of depth usage in LLMs, shedding\nlight on the layer-by-layer computations that underlie successful predictions\nand providing insights for future works to improve computational efficiency in\ntransformer-based models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing evidence suggests that large language models do not use their depth\nuniformly, yet we still lack a fine-grained understanding of their layer-wise\nprediction dynamics. In this paper, we trace the intermediate representations\nof several open-weight models during inference and reveal a structured and\nnuanced use of depth. Specifically, we propose a \"Guess-then-Refine\" framework\nthat explains how LLMs internally structure their computations to make\npredictions. We first show that the top-ranked predictions in early LLM layers\nare composed primarily of high-frequency tokens, which act as statistical\nguesses proposed by the model early on due to the lack of appropriate\ncontextual information. As contextual information develops deeper into the\nmodel, these initial guesses get refined into contextually appropriate tokens.\nEven high-frequency token predictions from early layers get refined >70% of the\ntime, indicating that correct token prediction is not \"one-and-done\". We then\ngo beyond frequency-based prediction to examine the dynamic usage of layer\ndepth across three case studies. (i) Part-of-speech analysis shows that\nfunction words are, on average, the earliest to be predicted correctly. (ii)\nFact recall task analysis shows that, in a multi-token answer, the first token\nrequires more computational depth than the rest. (iii) Multiple-choice task\nanalysis shows that the model identifies the format of the response within the\nfirst half of the layers, but finalizes its response only toward the end.\nTogether, our results provide a detailed view of depth usage in LLMs, shedding\nlight on the layer-by-layer computations that underlie successful predictions\nand providing insights for future works to improve computational efficiency in\ntransformer-based models."
                },
                "authors": [
                    {
                        "name": "Akshat Gupta"
                    },
                    {
                        "name": "Jay Yeung"
                    },
                    {
                        "name": "Gopala Anumanchipalli"
                    },
                    {
                        "name": "Anna Ivanova"
                    }
                ],
                "author_detail": {
                    "name": "Anna Ivanova"
                },
                "author": "Anna Ivanova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18870v1",
                "updated": "2025-10-21T17:59:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    59,
                    2,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:59:02Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    59,
                    2,
                    1,
                    294,
                    0
                ],
                "title": "Triangle Multiplication Is All You Need For Biomolecular Structure\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triangle Multiplication Is All You Need For Biomolecular Structure\n  Representations"
                },
                "summary": "AlphaFold has transformed protein structure prediction, but emerging\napplications such as virtual ligand screening, proteome-wide folding, and de\nnovo binder design demand predictions at a massive scale, where runtime and\nmemory costs become prohibitive. A major bottleneck lies in the Pairformer\nbackbone of AlphaFold3-style models, which relies on computationally expensive\ntriangular primitives-especially triangle attention-for pairwise reasoning. We\nintroduce Pairmixer, a streamlined alternative that eliminates triangle\nattention while preserving higher-order geometric reasoning capabilities that\nare critical for structure prediction. Pairmixer substantially improves\ncomputational efficiency, matching state-of-the-art structure predictors across\nfolding and docking benchmarks, delivering up to 4x faster inference on long\nsequences while reducing training cost by 34%. Its efficiency alleviates the\ncomputational burden of downstream applications such as modeling large protein\ncomplexes, high-throughput ligand and binder screening, and hallucination-based\ndesign. Within BoltzDesign, for example, Pairmixer delivers over 2x faster\nsampling and scales to sequences ~30% longer than the memory limits of\nPairformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaFold has transformed protein structure prediction, but emerging\napplications such as virtual ligand screening, proteome-wide folding, and de\nnovo binder design demand predictions at a massive scale, where runtime and\nmemory costs become prohibitive. A major bottleneck lies in the Pairformer\nbackbone of AlphaFold3-style models, which relies on computationally expensive\ntriangular primitives-especially triangle attention-for pairwise reasoning. We\nintroduce Pairmixer, a streamlined alternative that eliminates triangle\nattention while preserving higher-order geometric reasoning capabilities that\nare critical for structure prediction. Pairmixer substantially improves\ncomputational efficiency, matching state-of-the-art structure predictors across\nfolding and docking benchmarks, delivering up to 4x faster inference on long\nsequences while reducing training cost by 34%. Its efficiency alleviates the\ncomputational burden of downstream applications such as modeling large protein\ncomplexes, high-throughput ligand and binder screening, and hallucination-based\ndesign. Within BoltzDesign, for example, Pairmixer delivers over 2x faster\nsampling and scales to sequences ~30% longer than the memory limits of\nPairformer."
                },
                "authors": [
                    {
                        "name": "Jeffrey Ouyang-Zhang"
                    },
                    {
                        "name": "Pranav Murugan"
                    },
                    {
                        "name": "Daniel J. Diaz"
                    },
                    {
                        "name": "Gianluca Scarpellini"
                    },
                    {
                        "name": "Richard Strong Bowen"
                    },
                    {
                        "name": "Nate Gruver"
                    },
                    {
                        "name": "Adam Klivans"
                    },
                    {
                        "name": "Philipp Krähenbühl"
                    },
                    {
                        "name": "Aleksandra Faust"
                    },
                    {
                        "name": "Maruan Al-Shedivat"
                    }
                ],
                "author_detail": {
                    "name": "Maruan Al-Shedivat"
                },
                "author": "Maruan Al-Shedivat",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18867v1",
                "updated": "2025-10-21T17:58:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    58,
                    21,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:58:21Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    58,
                    21,
                    1,
                    294,
                    0
                ],
                "title": "Reexamining Evidence of a Pair-Instability Mass Gap in the Binary Black\n  Hole Population",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reexamining Evidence of a Pair-Instability Mass Gap in the Binary Black\n  Hole Population"
                },
                "summary": "The fourth gravitational wave transient catalog~(GWTC-4) has enabled\nempirical probes of the theorized pair-instability gap in the higher end of the\nbinary black hole~(BBH) mass-spectrum. In this letter, using flexibly\nparametrized models, we show that at present there is no evidence of a sharp\ndrop-off in the spectrum of black hole masses near $~40-50M_{\\odot}$. We\nsimultaneously characterize the transition in the distribution of BBH\nmass-ratios, effective aligned and effective precessing spins using our\nflexible models. From the transitions in our inferred spin and mass-ratio\ndistributions, we find that the high-mass broad-spin sub-population has a\nsignificant fraction~($52^{+18}_{-23}\\%$) of systems with mass ratios in the\nrange $0.6-1$. This suggests that alternatives to the hypothesis of 2G+1G\nhierarchical systems dominating BBH formation above $\\sim 40-50 M_{\\odot}$ are\nmore consistent with the GWTC-4 detection sample. By comparing with the\npredictions of star cluster simulations, we further show that contributions\nfrom (2G+2G) systems are not abundant enough to alleviate this discrepancy. We\nalso demonstrate the effects of strong model assumptions on this inference,\nwhich can lead to biased astrophysical interpretation from restricted priors.\nWe note that our results do not exclude that a high-mass gap may be identified\nas our sample size increases. We constrain the lower bound on the location of a\npossible PISN cutoff still allowed within measurement uncertainties to be\n$(57^{+17}_{-10}M_{\\odot})$ and discuss its implications on the S factor of\n$^{12}\\mathrm{C}(\\alpha, \\gamma)^{16}O$ at 300 kev.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fourth gravitational wave transient catalog~(GWTC-4) has enabled\nempirical probes of the theorized pair-instability gap in the higher end of the\nbinary black hole~(BBH) mass-spectrum. In this letter, using flexibly\nparametrized models, we show that at present there is no evidence of a sharp\ndrop-off in the spectrum of black hole masses near $~40-50M_{\\odot}$. We\nsimultaneously characterize the transition in the distribution of BBH\nmass-ratios, effective aligned and effective precessing spins using our\nflexible models. From the transitions in our inferred spin and mass-ratio\ndistributions, we find that the high-mass broad-spin sub-population has a\nsignificant fraction~($52^{+18}_{-23}\\%$) of systems with mass ratios in the\nrange $0.6-1$. This suggests that alternatives to the hypothesis of 2G+1G\nhierarchical systems dominating BBH formation above $\\sim 40-50 M_{\\odot}$ are\nmore consistent with the GWTC-4 detection sample. By comparing with the\npredictions of star cluster simulations, we further show that contributions\nfrom (2G+2G) systems are not abundant enough to alleviate this discrepancy. We\nalso demonstrate the effects of strong model assumptions on this inference,\nwhich can lead to biased astrophysical interpretation from restricted priors.\nWe note that our results do not exclude that a high-mass gap may be identified\nas our sample size increases. We constrain the lower bound on the location of a\npossible PISN cutoff still allowed within measurement uncertainties to be\n$(57^{+17}_{-10}M_{\\odot})$ and discuss its implications on the S factor of\n$^{12}\\mathrm{C}(\\alpha, \\gamma)^{16}O$ at 300 kev."
                },
                "authors": [
                    {
                        "name": "Anarya Ray"
                    },
                    {
                        "name": "Vicky Kalogera"
                    }
                ],
                "author_detail": {
                    "name": "Vicky Kalogera"
                },
                "author": "Vicky Kalogera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18866v1",
                "updated": "2025-10-21T17:58:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    58,
                    17,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:58:17Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    58,
                    17,
                    1,
                    294,
                    0
                ],
                "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightMem: Lightweight and Efficient Memory-Augmented Generation"
                },
                "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) struggle\nto effectively leverage historical interaction information in dynamic and\ncomplex environments. Memory systems enable LLMs to move beyond stateless\ninteractions by introducing persistent information storage, retrieval, and\nutilization mechanisms. However, existing memory systems often introduce\nsubstantial time and computational overhead. To this end, we introduce a new\nmemory system called LightMem, which strikes a balance between the performance\nand efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of\nhuman memory, LightMem organizes memory into three complementary stages. First,\ncognition-inspired sensory memory rapidly filters irrelevant information\nthrough lightweight compression and groups information according to their\ntopics. Next, topic-aware short-term memory consolidates these topic-based\ngroups, organizing and summarizing content for more structured access. Finally,\nlong-term memory with sleep-time update employs an offline procedure that\ndecouples consolidation from online inference. Experiments on LongMemEval with\nGPT and Qwen backbones show that LightMem outperforms strong baselines in\naccuracy (up to 10.9% gains) while reducing token usage by up to 117x, API\ncalls by up to 159x, and runtime by over 12x. The code is available at\nhttps://github.com/zjunlp/LightMem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable capabilities, Large Language Models (LLMs) struggle\nto effectively leverage historical interaction information in dynamic and\ncomplex environments. Memory systems enable LLMs to move beyond stateless\ninteractions by introducing persistent information storage, retrieval, and\nutilization mechanisms. However, existing memory systems often introduce\nsubstantial time and computational overhead. To this end, we introduce a new\nmemory system called LightMem, which strikes a balance between the performance\nand efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of\nhuman memory, LightMem organizes memory into three complementary stages. First,\ncognition-inspired sensory memory rapidly filters irrelevant information\nthrough lightweight compression and groups information according to their\ntopics. Next, topic-aware short-term memory consolidates these topic-based\ngroups, organizing and summarizing content for more structured access. Finally,\nlong-term memory with sleep-time update employs an offline procedure that\ndecouples consolidation from online inference. Experiments on LongMemEval with\nGPT and Qwen backbones show that LightMem outperforms strong baselines in\naccuracy (up to 10.9% gains) while reducing token usage by up to 117x, API\ncalls by up to 159x, and runtime by over 12x. The code is available at\nhttps://github.com/zjunlp/LightMem."
                },
                "authors": [
                    {
                        "name": "Jizhan Fang"
                    },
                    {
                        "name": "Xinle Deng"
                    },
                    {
                        "name": "Haoming Xu"
                    },
                    {
                        "name": "Ziyan Jiang"
                    },
                    {
                        "name": "Yuqi Tang"
                    },
                    {
                        "name": "Ziwen Xu"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Mengru Wang"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20648v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20648v2",
                "updated": "2025-10-21T17:58:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    58,
                    16,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-25T01:07:08Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    1,
                    7,
                    8,
                    3,
                    268,
                    0
                ],
                "title": "Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent\n  Contextual Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent\n  Contextual Calibration"
                },
                "summary": "Autonomous exploration in complex multi-agent reinforcement learning (MARL)\nwith sparse rewards critically depends on providing agents with effective\nintrinsic motivation. While artificial curiosity offers a powerful\nself-supervised signal, it often confuses environmental stochasticity with\nmeaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform\nnovelty bias, treating all unexpected observations equally. However, peer\nbehavior novelty, which encode latent task dynamics, are often overlooked,\nresulting in suboptimal exploration in decentralized, communication-free MARL\nsettings. To this end, inspired by how human children adaptively calibrate\ntheir own exploratory behaviors via observing peers, we propose a novel\napproach to enhance multi-agent exploration. We introduce CERMIC, a principled\nframework that empowers agents to robustly filter noisy surprise signals and\nguide exploration by dynamically calibrating their intrinsic curiosity with\ninferred multi-agent context. Additionally, CERMIC generates\ntheoretically-grounded intrinsic rewards, encouraging agents to explore state\ntransitions with high information gain. We evaluate CERMIC on benchmark suites\nincluding VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that\nexploration with CERMIC significantly outperforms SoTA algorithms in\nsparse-reward environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous exploration in complex multi-agent reinforcement learning (MARL)\nwith sparse rewards critically depends on providing agents with effective\nintrinsic motivation. While artificial curiosity offers a powerful\nself-supervised signal, it often confuses environmental stochasticity with\nmeaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform\nnovelty bias, treating all unexpected observations equally. However, peer\nbehavior novelty, which encode latent task dynamics, are often overlooked,\nresulting in suboptimal exploration in decentralized, communication-free MARL\nsettings. To this end, inspired by how human children adaptively calibrate\ntheir own exploratory behaviors via observing peers, we propose a novel\napproach to enhance multi-agent exploration. We introduce CERMIC, a principled\nframework that empowers agents to robustly filter noisy surprise signals and\nguide exploration by dynamically calibrating their intrinsic curiosity with\ninferred multi-agent context. Additionally, CERMIC generates\ntheoretically-grounded intrinsic rewards, encouraging agents to explore state\ntransitions with high information gain. We evaluate CERMIC on benchmark suites\nincluding VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that\nexploration with CERMIC significantly outperforms SoTA algorithms in\nsparse-reward environments."
                },
                "authors": [
                    {
                        "name": "Yiyuan Pan"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Hesheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hesheng Wang"
                },
                "author": "Hesheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20648v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20648v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18863v1",
                "updated": "2025-10-21T17:55:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    55,
                    39,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:55:39Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    55,
                    39,
                    1,
                    294,
                    0
                ],
                "title": "EffiReasonTrans: RL-Optimized Reasoning for Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EffiReasonTrans: RL-Optimized Reasoning for Code Translation"
                },
                "summary": "Code translation is a crucial task in software development and maintenance.\nWhile recent advancements in large language models (LLMs) have improved\nautomated code translation accuracy, these gains often come at the cost of\nincreased inference latency, hindering real-world development workflows that\ninvolve human-in-the-loop inspection. To address this trade-off, we propose\nEffiReasonTrans, a training framework designed to improve translation accuracy\nwhile balancing inference latency. We first construct a high-quality\nreasoning-augmented dataset by prompting a stronger language model,\nDeepSeek-R1, to generate intermediate reasoning and target translations. Each\n(source code, reasoning, target code) triplet undergoes automated syntax and\nfunctionality checks to ensure reliability. Based on this dataset, we employ a\ntwo-stage training strategy: supervised fine-tuning on reasoning-augmented\nsamples, followed by reinforcement learning to further enhance accuracy and\nbalance inference latency. We evaluate EffiReasonTrans on six translation\npairs. Experimental results show that it consistently improves translation\naccuracy (up to +49.2% CA and +27.8% CodeBLEU compared to the base model) while\nreducing the number of generated tokens (up to -19.3%) and lowering inference\nlatency in most cases (up to -29.0%). Ablation studies further confirm the\ncomplementary benefits of the two-stage training framework. Additionally,\nEffiReasonTrans demonstrates improved translation accuracy when integrated into\nagent-based frameworks. Our code and data are available at\nhttps://github.com/DeepSoftwareAnalytics/EffiReasonTrans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation is a crucial task in software development and maintenance.\nWhile recent advancements in large language models (LLMs) have improved\nautomated code translation accuracy, these gains often come at the cost of\nincreased inference latency, hindering real-world development workflows that\ninvolve human-in-the-loop inspection. To address this trade-off, we propose\nEffiReasonTrans, a training framework designed to improve translation accuracy\nwhile balancing inference latency. We first construct a high-quality\nreasoning-augmented dataset by prompting a stronger language model,\nDeepSeek-R1, to generate intermediate reasoning and target translations. Each\n(source code, reasoning, target code) triplet undergoes automated syntax and\nfunctionality checks to ensure reliability. Based on this dataset, we employ a\ntwo-stage training strategy: supervised fine-tuning on reasoning-augmented\nsamples, followed by reinforcement learning to further enhance accuracy and\nbalance inference latency. We evaluate EffiReasonTrans on six translation\npairs. Experimental results show that it consistently improves translation\naccuracy (up to +49.2% CA and +27.8% CodeBLEU compared to the base model) while\nreducing the number of generated tokens (up to -19.3%) and lowering inference\nlatency in most cases (up to -29.0%). Ablation studies further confirm the\ncomplementary benefits of the two-stage training framework. Additionally,\nEffiReasonTrans demonstrates improved translation accuracy when integrated into\nagent-based frameworks. Our code and data are available at\nhttps://github.com/DeepSoftwareAnalytics/EffiReasonTrans."
                },
                "authors": [
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Rongyi Ou"
                    },
                    {
                        "name": "Yanli Wang"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Ensheng Shi"
                    },
                    {
                        "name": "Xilin Liu"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18860v1",
                "updated": "2025-10-21T17:53:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    53,
                    56,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:53:56Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    53,
                    56,
                    1,
                    294,
                    0
                ],
                "title": "An Encoder-Decoder Foundation Chemical Language Model for Generative\n  Polymer Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Encoder-Decoder Foundation Chemical Language Model for Generative\n  Polymer Design"
                },
                "summary": "Traditional machine learning has advanced polymer discovery, yet direct\ngeneration of chemically valid and synthesizable polymers without exhaustive\nenumeration remains a challenge. Here we present polyT5, an encoder-decoder\nchemical language model based on the T5 architecture, trained to understand and\ngenerate polymer structures. polyT5 enables both property prediction and the\ntargeted generation of polymers conditioned on desired property values. We\ndemonstrate its utility for dielectric polymer design, seeking candidates with\ndielectric constant >3, bandgap >4 eV, and glass transition temperature >400 K,\nalongside melt-processability and solubility requirements. From over 20,000\ngenerated promising candidates, one was experimentally synthesized and\nvalidated, showing strong agreement with predictions. To further enhance\nusability, we integrated polyT5 within an agentic AI framework that couples it\nwith a general-purpose LLM, allowing natural language interaction for property\nprediction and generative design. Together, these advances establish a\nversatile and accessible framework for accelerated polymer discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional machine learning has advanced polymer discovery, yet direct\ngeneration of chemically valid and synthesizable polymers without exhaustive\nenumeration remains a challenge. Here we present polyT5, an encoder-decoder\nchemical language model based on the T5 architecture, trained to understand and\ngenerate polymer structures. polyT5 enables both property prediction and the\ntargeted generation of polymers conditioned on desired property values. We\ndemonstrate its utility for dielectric polymer design, seeking candidates with\ndielectric constant >3, bandgap >4 eV, and glass transition temperature >400 K,\nalongside melt-processability and solubility requirements. From over 20,000\ngenerated promising candidates, one was experimentally synthesized and\nvalidated, showing strong agreement with predictions. To further enhance\nusability, we integrated polyT5 within an agentic AI framework that couples it\nwith a general-purpose LLM, allowing natural language interaction for property\nprediction and generative design. Together, these advances establish a\nversatile and accessible framework for accelerated polymer discovery."
                },
                "authors": [
                    {
                        "name": "Harikrishna Sahu"
                    },
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Anagha Savit"
                    },
                    {
                        "name": "Shivank S Shukla"
                    },
                    {
                        "name": "Rampi Ramprasad"
                    }
                ],
                "author_detail": {
                    "name": "Rampi Ramprasad"
                },
                "author": "Rampi Ramprasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14456v2",
                "updated": "2025-10-21T17:46:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    46,
                    51,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-17T22:12:30Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    22,
                    12,
                    30,
                    2,
                    260,
                    0
                ],
                "title": "Correct-Detect: Balancing Performance and Ambiguity Through the Lens of\n  Coreference Resolution in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct-Detect: Balancing Performance and Ambiguity Through the Lens of\n  Coreference Resolution in LLMs"
                },
                "summary": "Large Language Models (LLMs) are intended to reflect human linguistic\ncompetencies. But humans have access to a broad and embodied context, which is\nkey in detecting and resolving linguistic ambiguities, even in isolated text\nspans. A foundational case of semantic ambiguity is found in the task of\ncoreference resolution: how is a pronoun related to an earlier person mention?\nThis capability is implicit in nearly every downstream task, and the presence\nof ambiguity at this level can alter performance significantly. We show that\nLLMs can achieve good performance with minimal prompting in both coreference\ndisambiguation and the detection of ambiguity in coreference, however, they\ncannot do both at the same time. We present the CORRECT-DETECT trade-off:\nthough models have both capabilities and deploy them implicitly, successful\nperformance balancing these two abilities remains elusive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are intended to reflect human linguistic\ncompetencies. But humans have access to a broad and embodied context, which is\nkey in detecting and resolving linguistic ambiguities, even in isolated text\nspans. A foundational case of semantic ambiguity is found in the task of\ncoreference resolution: how is a pronoun related to an earlier person mention?\nThis capability is implicit in nearly every downstream task, and the presence\nof ambiguity at this level can alter performance significantly. We show that\nLLMs can achieve good performance with minimal prompting in both coreference\ndisambiguation and the detection of ambiguity in coreference, however, they\ncannot do both at the same time. We present the CORRECT-DETECT trade-off:\nthough models have both capabilities and deploy them implicitly, successful\nperformance balancing these two abilities remains elusive."
                },
                "authors": [
                    {
                        "name": "Amber Shore"
                    },
                    {
                        "name": "Russell Scheinberg"
                    },
                    {
                        "name": "Ameeta Agrawal"
                    },
                    {
                        "name": "So Young Lee"
                    }
                ],
                "author_detail": {
                    "name": "So Young Lee"
                },
                "author": "So Young Lee",
                "arxiv_comment": "Accepted at EMNLP 2025 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18855v1",
                "updated": "2025-10-21T17:46:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    46,
                    14,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:46:14Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    46,
                    14,
                    1,
                    294,
                    0
                ],
                "title": "Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale\n  Thinking Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale\n  Thinking Model"
                },
                "summary": "We present Ring-1T, the first open-source, state-of-the-art thinking model\nwith a trillion-scale parameter. It features 1 trillion total parameters and\nactivates approximately 50 billion per token. Training such models at a\ntrillion-parameter scale introduces unprecedented challenges, including\ntrain-inference misalignment, inefficiencies in rollout processing, and\nbottlenecks in the RL system. To address these, we pioneer three interconnected\ninnovations: (1) IcePop stabilizes RL training via token-level discrepancy\nmasking and clipping, resolving instability from training-inference mismatches;\n(2) C3PO++ improves resource utilization for long rollouts under a token budget\nby dynamically partitioning them, thereby obtaining high time efficiency; and\n(3) ASystem, a high-performance RL framework designed to overcome the systemic\nbottlenecks that impede trillion-parameter model training. Ring-1T delivers\nbreakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on\nHMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a\nsilver medal-level result on the IMO-2025, underscoring its exceptional\nreasoning capabilities. By releasing the complete 1T parameter MoE model to the\ncommunity, we provide the research community with direct access to cutting-edge\nreasoning capabilities. This contribution marks a significant milestone in\ndemocratizing large-scale reasoning intelligence and establishes a new baseline\nfor open-source model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Ring-1T, the first open-source, state-of-the-art thinking model\nwith a trillion-scale parameter. It features 1 trillion total parameters and\nactivates approximately 50 billion per token. Training such models at a\ntrillion-parameter scale introduces unprecedented challenges, including\ntrain-inference misalignment, inefficiencies in rollout processing, and\nbottlenecks in the RL system. To address these, we pioneer three interconnected\ninnovations: (1) IcePop stabilizes RL training via token-level discrepancy\nmasking and clipping, resolving instability from training-inference mismatches;\n(2) C3PO++ improves resource utilization for long rollouts under a token budget\nby dynamically partitioning them, thereby obtaining high time efficiency; and\n(3) ASystem, a high-performance RL framework designed to overcome the systemic\nbottlenecks that impede trillion-parameter model training. Ring-1T delivers\nbreakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on\nHMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a\nsilver medal-level result on the IMO-2025, underscoring its exceptional\nreasoning capabilities. By releasing the complete 1T parameter MoE model to the\ncommunity, we provide the research community with direct access to cutting-edge\nreasoning capabilities. This contribution marks a significant milestone in\ndemocratizing large-scale reasoning intelligence and establishes a new baseline\nfor open-source model performance."
                },
                "authors": [
                    {
                        "name": "Ling Team"
                    },
                    {
                        "name": "Anqi Shen"
                    },
                    {
                        "name": "Baihui Li"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Bin Jing"
                    },
                    {
                        "name": "Cai Chen"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Chaokun Yang"
                    },
                    {
                        "name": "Cheng Lin"
                    },
                    {
                        "name": "Chengyao Wen"
                    },
                    {
                        "name": "Congqi Li"
                    },
                    {
                        "name": "Deng Zhao"
                    },
                    {
                        "name": "Dingbo Yuan"
                    },
                    {
                        "name": "Donghai You"
                    },
                    {
                        "name": "Fagui Mao"
                    },
                    {
                        "name": "Fanzhuang Meng"
                    },
                    {
                        "name": "Feng Xu"
                    },
                    {
                        "name": "Guojie Li"
                    },
                    {
                        "name": "Guowei Wang"
                    },
                    {
                        "name": "Hao Dai"
                    },
                    {
                        "name": "Haonan Zheng"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Jia Guo"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Jianhao Fu"
                    },
                    {
                        "name": "Jiannan Shi"
                    },
                    {
                        "name": "Jianwen Wang"
                    },
                    {
                        "name": "Jianxin Lai"
                    },
                    {
                        "name": "Jin Yang"
                    },
                    {
                        "name": "Jun Mei"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Kuan Xu"
                    },
                    {
                        "name": "Le Su"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Li Tang"
                    },
                    {
                        "name": "Liang Jiang"
                    },
                    {
                        "name": "Liangcheng Fu"
                    },
                    {
                        "name": "Lianhao Xu"
                    },
                    {
                        "name": "Linfeng Shi"
                    },
                    {
                        "name": "Lisha Liao"
                    },
                    {
                        "name": "Longfei Zheng"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Mingchun Chen"
                    },
                    {
                        "name": "Qi Zuo"
                    },
                    {
                        "name": "Qiang Cheng"
                    },
                    {
                        "name": "Qianggang Cao"
                    },
                    {
                        "name": "Qitao Shi"
                    },
                    {
                        "name": "Quanrui Guo"
                    },
                    {
                        "name": "Senlin Zhu"
                    },
                    {
                        "name": "Shaofei Wang"
                    },
                    {
                        "name": "Shaomian Zheng"
                    },
                    {
                        "name": "Shuaicheng Li"
                    },
                    {
                        "name": "Shuwei Gu"
                    },
                    {
                        "name": "Siba Chen"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Tianyu Zhou"
                    },
                    {
                        "name": "Tiwei Bie"
                    },
                    {
                        "name": "Tongkai Yang"
                    },
                    {
                        "name": "Wang Hong"
                    },
                    {
                        "name": "Wang Ren"
                    },
                    {
                        "name": "Weihua Chen"
                    },
                    {
                        "name": "Wenbo Yu"
                    },
                    {
                        "name": "Wengang Zheng"
                    },
                    {
                        "name": "Xiangchun Wang"
                    },
                    {
                        "name": "Xiaodong Yan"
                    },
                    {
                        "name": "Xiaopei Wan"
                    },
                    {
                        "name": "Xin Zhao"
                    },
                    {
                        "name": "Xinyu Kong"
                    },
                    {
                        "name": "Xinyu Tang"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Xudong Wang"
                    },
                    {
                        "name": "Xuemin Yang"
                    },
                    {
                        "name": "Xueyu Hu"
                    },
                    {
                        "name": "Yalin Zhang"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Yicheng Shan"
                    },
                    {
                        "name": "Yilong Wang"
                    },
                    {
                        "name": "Yingying Xu"
                    },
                    {
                        "name": "Yongkang Liu"
                    },
                    {
                        "name": "Yongzhen Guo"
                    },
                    {
                        "name": "Yuanyuan Wang"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Yuefan Wang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Zehuan Li"
                    },
                    {
                        "name": "Zhankai Xu"
                    },
                    {
                        "name": "Zhe Li"
                    },
                    {
                        "name": "Zhenduo Zhang"
                    },
                    {
                        "name": "Zhengke Gui"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Zhiqiang Ding"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Zhixun Li"
                    },
                    {
                        "name": "Zhizhen Liu"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Zujie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Zujie Wen"
                },
                "author": "Zujie Wen",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18854v1",
                "updated": "2025-10-21T17:45:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    45,
                    13,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:45:13Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    45,
                    13,
                    1,
                    294,
                    0
                ],
                "title": "AT2025ulz and S250818k: zooming in with the Hubble Space Telescope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AT2025ulz and S250818k: zooming in with the Hubble Space Telescope"
                },
                "summary": "AT2025ulz is an optical/near-infrared transient discovered during follow-up\nof the candidate gravitational wave (GW) event S250818k. Its young age\n($\\lesssim$1 d), rapid decline and strong color evolution over the first 48 hr\nclassify it as a potential kilonova candidate. In this work, we present the\nresults of our observing campaign, carried out with the Gran Telescopio\nCanarias (GTC) and the Hubble Space Telescope (HST). Although the early time\nevolution of AT2025ulz resembles some aspects of a kilonova, its rapid onset\n($\\sim$3 hr after the GW trigger) and luminosity (a factor of $\\sim5$ brighter\nthan AT2017gfo in $g$-band) are difficult to reproduce. Only a small subset of\nour kilonova models matches its multi-color light curve, and the inferred\nejecta mass is uncomfortably large given the low chirp mass ($\\lesssim\\!0.87\\!$\nM$_{\\odot}$) of the GW candidate. HST observations place the transient within a\nnearby ($z=0.08489$) spiral galaxy with on-going star-formation and measure a\ncolor ($F336W-F160W\\!\\approx\\!1.4$ mag) that is too blue to match with a\nkilonova. Our data support the classification of AT2025ulz as a supernova,\ninitially undergoing a shock-cooling phase and later entering its photospheric\nphase, and spectroscopically identified via its broad absorption features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AT2025ulz is an optical/near-infrared transient discovered during follow-up\nof the candidate gravitational wave (GW) event S250818k. Its young age\n($\\lesssim$1 d), rapid decline and strong color evolution over the first 48 hr\nclassify it as a potential kilonova candidate. In this work, we present the\nresults of our observing campaign, carried out with the Gran Telescopio\nCanarias (GTC) and the Hubble Space Telescope (HST). Although the early time\nevolution of AT2025ulz resembles some aspects of a kilonova, its rapid onset\n($\\sim$3 hr after the GW trigger) and luminosity (a factor of $\\sim5$ brighter\nthan AT2017gfo in $g$-band) are difficult to reproduce. Only a small subset of\nour kilonova models matches its multi-color light curve, and the inferred\nejecta mass is uncomfortably large given the low chirp mass ($\\lesssim\\!0.87\\!$\nM$_{\\odot}$) of the GW candidate. HST observations place the transient within a\nnearby ($z=0.08489$) spiral galaxy with on-going star-formation and measure a\ncolor ($F336W-F160W\\!\\approx\\!1.4$ mag) that is too blue to match with a\nkilonova. Our data support the classification of AT2025ulz as a supernova,\ninitially undergoing a shock-cooling phase and later entering its photospheric\nphase, and spectroscopically identified via its broad absorption features."
                },
                "authors": [
                    {
                        "name": "Yu-Han Yang"
                    },
                    {
                        "name": "Eleonora Troja"
                    },
                    {
                        "name": "Marko Ristić"
                    },
                    {
                        "name": "Muskan Yadav"
                    },
                    {
                        "name": "Massine El Kabir"
                    },
                    {
                        "name": "Rubén Sánchez-Ramírez"
                    },
                    {
                        "name": "Rosa L. Becerra"
                    },
                    {
                        "name": "Chris L. Fryer"
                    },
                    {
                        "name": "Brendan O'Connor"
                    },
                    {
                        "name": "Simone Dichiara"
                    },
                    {
                        "name": "Alberto J. Castro-Tirado"
                    },
                    {
                        "name": "Camila Angulo-Valdez"
                    },
                    {
                        "name": "Josefa Becerra González"
                    },
                    {
                        "name": "José A. Font"
                    },
                    {
                        "name": "Ori Fox"
                    },
                    {
                        "name": "Lei Hu"
                    },
                    {
                        "name": "Youdong Hu"
                    },
                    {
                        "name": "William H. Lee"
                    },
                    {
                        "name": "Margarita Pereyra"
                    },
                    {
                        "name": "Alicia M. Sintes"
                    },
                    {
                        "name": "Alan M. Watson"
                    },
                    {
                        "name": "López Mendoza K. Océlotl C"
                    }
                ],
                "author_detail": {
                    "name": "López Mendoza K. Océlotl C"
                },
                "author": "López Mendoza K. Océlotl C",
                "arxiv_comment": "12 pages, 5 figures, 2 tables. Submitted to ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03417v2",
                "updated": "2025-10-21T17:41:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    41,
                    58,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-03T18:24:14Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    18,
                    24,
                    14,
                    4,
                    276,
                    0
                ],
                "title": "NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn\n  LLM Jailbreaks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn\n  LLM Jailbreaks"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nbut remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks\nthat distribute malicious intent across benign exchanges and bypass alignment\nmechanisms. Existing approaches often explore the adversarial space poorly,\nrely on hand-crafted heuristics, or lack systematic query refinement. We\npresent NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular\nframework for constructing, refining, and executing optimized multi-turn\nattacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a\nharmful intent into a structured semantic network of topics, entities, and\nquery chains; (2) a feedback-driven Simulator that iteratively refines and\nprunes these chains through attacker-victim-judge LLM collaboration using\nharmfulness and semantic-similarity benchmarks; and (3) a Network Traverser\nthat adaptively navigates the refined query space for real-time attacks. This\npipeline uncovers stealthy, high-success adversarial paths across LLMs. On\nseveral closed-source and open-source LLMs, NEXUS increases attack success rate\nby 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nbut remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks\nthat distribute malicious intent across benign exchanges and bypass alignment\nmechanisms. Existing approaches often explore the adversarial space poorly,\nrely on hand-crafted heuristics, or lack systematic query refinement. We\npresent NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular\nframework for constructing, refining, and executing optimized multi-turn\nattacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a\nharmful intent into a structured semantic network of topics, entities, and\nquery chains; (2) a feedback-driven Simulator that iteratively refines and\nprunes these chains through attacker-victim-judge LLM collaboration using\nharmfulness and semantic-similarity benchmarks; and (3) a Network Traverser\nthat adaptively navigates the refined query space for real-time attacks. This\npipeline uncovers stealthy, high-success adversarial paths across LLMs. On\nseveral closed-source and open-source LLMs, NEXUS increases attack success rate\nby 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS"
                },
                "authors": [
                    {
                        "name": "Javad Rafiei Asl"
                    },
                    {
                        "name": "Sidhant Narula"
                    },
                    {
                        "name": "Mohammad Ghasemigol"
                    },
                    {
                        "name": "Eduardo Blanco"
                    },
                    {
                        "name": "Daniel Takabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Takabi"
                },
                "author": "Daniel Takabi",
                "arxiv_comment": "This paper has been accepted in the main conference proceedings of\n  the 2025 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2025). Javad Rafiei Asl and Sidhant Narula are co-first authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18849v1",
                "updated": "2025-10-21T17:40:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    40,
                    3,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:40:03Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    40,
                    3,
                    1,
                    294,
                    0
                ],
                "title": "Towards Faithful and Controllable Personalization via Critique-Post-Edit\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Faithful and Controllable Personalization via Critique-Post-Edit\n  Reinforcement Learning"
                },
                "summary": "Faithfully personalizing large language models (LLMs) to align with\nindividual user preferences is a critical but challenging task. While\nsupervised fine-tuning (SFT) quickly reaches a performance plateau, standard\nreinforcement learning from human feedback (RLHF) also struggles with the\nnuances of personalization. Scalar-based reward models are prone to reward\nhacking which leads to verbose and superficially personalized responses. To\naddress these limitations, we propose Critique-Post-Edit, a robust\nreinforcement learning framework that enables more faithful and controllable\npersonalization. Our framework integrates two key components: (1) a\nPersonalized Generative Reward Model (GRM) that provides multi-dimensional\nscores and textual critiques to resist reward hacking, and (2) a\nCritique-Post-Edit mechanism where the policy model revises its own outputs\nbased on these critiques for more targeted and efficient learning. Under a\nrigorous length-controlled evaluation, our method substantially outperforms\nstandard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an\naverage 11\\% win-rate improvement, and personalized Qwen2.5-14B model surpasses\nthe performance of GPT-4.1. These results demonstrate a practical path to\nfaithful, efficient, and controllable personalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfully personalizing large language models (LLMs) to align with\nindividual user preferences is a critical but challenging task. While\nsupervised fine-tuning (SFT) quickly reaches a performance plateau, standard\nreinforcement learning from human feedback (RLHF) also struggles with the\nnuances of personalization. Scalar-based reward models are prone to reward\nhacking which leads to verbose and superficially personalized responses. To\naddress these limitations, we propose Critique-Post-Edit, a robust\nreinforcement learning framework that enables more faithful and controllable\npersonalization. Our framework integrates two key components: (1) a\nPersonalized Generative Reward Model (GRM) that provides multi-dimensional\nscores and textual critiques to resist reward hacking, and (2) a\nCritique-Post-Edit mechanism where the policy model revises its own outputs\nbased on these critiques for more targeted and efficient learning. Under a\nrigorous length-controlled evaluation, our method substantially outperforms\nstandard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an\naverage 11\\% win-rate improvement, and personalized Qwen2.5-14B model surpasses\nthe performance of GPT-4.1. These results demonstrate a practical path to\nfaithful, efficient, and controllable personalization."
                },
                "authors": [
                    {
                        "name": "Chenghao Zhu"
                    },
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Tiannan Wang"
                    },
                    {
                        "name": "Dongyi Ding"
                    },
                    {
                        "name": "Yuchen Eleanor Jiang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wangchunshu Zhou"
                },
                "author": "Wangchunshu Zhou",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17203v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17203v3",
                "updated": "2025-10-21T17:38:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    38,
                    57,
                    1,
                    294,
                    0
                ],
                "published": "2025-04-24T02:27:17Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    2,
                    27,
                    17,
                    3,
                    114,
                    0
                ],
                "title": "High-Fidelity And Complex Test Data Generation For Google SQL Code\n  Generation Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Fidelity And Complex Test Data Generation For Google SQL Code\n  Generation Services"
                },
                "summary": "The demand for high-fidelity test data is paramount in industrial settings\nwhere access to production data is largely restricted. Traditional data\ngeneration methods often fall short, struggling with low-fidelity and the\nability to model complex data structures and semantic relationships that are\ncritical for testing complex SQL code generation services like Natural Language\nto SQL (NL2SQL). In this paper, we address the critical need for generating\nsyntactically correct and semantically relevant high-fidelity mock data for\ncomplex data structures that includes columns with nested structures that we\nfrequently encounter in Google workloads. We highlight the limitations of\nexisting approaches used in production, particularly their inability to handle\nlarge and complex data structures, as well as the lack of semantically coherent\ntest data that lead to limited test coverage. We demonstrate that by leveraging\nLarge Language Models (LLMs) and incorporating strategic pre- and\npost-processing steps, we can generate syntactically correct and semantically\nrelevant high-fidelity test data that adheres to complex structural constraints\nand maintains semantic integrity to the SQL test targets (queries/functions).\nThis approach supports comprehensive testing of complex SQL queries involving\njoins, aggregations, and even deeply nested subqueries, ensuring robust\nevaluation of SQL code generation services, like NL2SQL and SQL Code Assistant.\nOur results demonstrate the practical utility of an LLM (\\textit{gemini}) based\ntest data generation for industrial SQL code generation services where\ngenerating high-fidelity test data is essential due to the frequent\nunavailability and inaccessibility of production datasets for testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for high-fidelity test data is paramount in industrial settings\nwhere access to production data is largely restricted. Traditional data\ngeneration methods often fall short, struggling with low-fidelity and the\nability to model complex data structures and semantic relationships that are\ncritical for testing complex SQL code generation services like Natural Language\nto SQL (NL2SQL). In this paper, we address the critical need for generating\nsyntactically correct and semantically relevant high-fidelity mock data for\ncomplex data structures that includes columns with nested structures that we\nfrequently encounter in Google workloads. We highlight the limitations of\nexisting approaches used in production, particularly their inability to handle\nlarge and complex data structures, as well as the lack of semantically coherent\ntest data that lead to limited test coverage. We demonstrate that by leveraging\nLarge Language Models (LLMs) and incorporating strategic pre- and\npost-processing steps, we can generate syntactically correct and semantically\nrelevant high-fidelity test data that adheres to complex structural constraints\nand maintains semantic integrity to the SQL test targets (queries/functions).\nThis approach supports comprehensive testing of complex SQL queries involving\njoins, aggregations, and even deeply nested subqueries, ensuring robust\nevaluation of SQL code generation services, like NL2SQL and SQL Code Assistant.\nOur results demonstrate the practical utility of an LLM (\\textit{gemini}) based\ntest data generation for industrial SQL code generation services where\ngenerating high-fidelity test data is essential due to the frequent\nunavailability and inaccessibility of production datasets for testing."
                },
                "authors": [
                    {
                        "name": "Shivasankari Kannan"
                    },
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Amita Gondi"
                    },
                    {
                        "name": "Tristan Swadell"
                    },
                    {
                        "name": "Fatma Ozcan"
                    }
                ],
                "author_detail": {
                    "name": "Fatma Ozcan"
                },
                "author": "Fatma Ozcan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17203v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17203v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18843v1",
                "updated": "2025-10-21T17:35:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    35,
                    33,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:35:33Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    35,
                    33,
                    1,
                    294,
                    0
                ],
                "title": "Inference on Local Variable Importance Measures for Heterogeneous\n  Treatment Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on Local Variable Importance Measures for Heterogeneous\n  Treatment Effects"
                },
                "summary": "We provide an inferential framework to assess variable importance for\nheterogeneous treatment effects. This assessment is especially useful in\nhigh-risk domains such as medicine, where decision makers hesitate to rely on\nblack-box treatment recommendation algorithms. The variable importance measures\nwe consider are local in that they may differ across individuals, while the\ninference is global in that it tests whether a given variable is important for\nany individual. Our approach builds on recent developments in semiparametric\ntheory for function-valued parameters, and is valid even when statistical\nmachine learning algorithms are employed to quantify treatment effect\nheterogeneity. We demonstrate the applicability of our method to infectious\ndisease prevention strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We provide an inferential framework to assess variable importance for\nheterogeneous treatment effects. This assessment is especially useful in\nhigh-risk domains such as medicine, where decision makers hesitate to rely on\nblack-box treatment recommendation algorithms. The variable importance measures\nwe consider are local in that they may differ across individuals, while the\ninference is global in that it tests whether a given variable is important for\nany individual. Our approach builds on recent developments in semiparametric\ntheory for function-valued parameters, and is valid even when statistical\nmachine learning algorithms are employed to quantify treatment effect\nheterogeneity. We demonstrate the applicability of our method to infectious\ndisease prevention strategies."
                },
                "authors": [
                    {
                        "name": "Pawel Morzywolek"
                    },
                    {
                        "name": "Peter B. Gilbert"
                    },
                    {
                        "name": "Alex Luedtke"
                    }
                ],
                "author_detail": {
                    "name": "Alex Luedtke"
                },
                "author": "Alex Luedtke",
                "arxiv_comment": "40 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18841v1",
                "updated": "2025-10-21T17:35:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    35,
                    12,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:35:12Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    35,
                    12,
                    1,
                    294,
                    0
                ],
                "title": "A Hybrid Enumeration Framework for Optimal Counterfactual Generation in\n  Post-Acute COVID-19 Heart Failure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid Enumeration Framework for Optimal Counterfactual Generation in\n  Post-Acute COVID-19 Heart Failure"
                },
                "summary": "Counterfactual inference provides a mathematical framework for reasoning\nabout hypothetical outcomes under alternative interventions, bridging causal\nreasoning and predictive modeling. We present a counterfactual inference\nframework for individualized risk estimation and intervention analysis,\nillustrated through a clinical application to post-acute sequelae of COVID-19\n(PASC) among patients with pre-existing heart failure (HF). Using longitudinal\ndiagnosis, laboratory, and medication data from a large health-system cohort,\nwe integrate regularized predictive modeling with counterfactual search to\nidentify actionable pathways to PASC-related HF hospital admissions. The\nframework combines exact enumeration with optimization-based methods, including\nthe Nearest Instance Counterfactual Explanations (NICE) and Multi-Objective\nCounterfactuals (MOC) algorithms, to efficiently explore high-dimensional\nintervention spaces. Applied to more than 2700 individuals with confirmed\nSARS-CoV-2 infection and prior HF, the model achieved strong discriminative\nperformance (AUROC: 0.88, 95% CI: 0.84-0.91) and generated interpretable,\npatient-specific counterfactuals that quantify how modifying comorbidity\npatterns or treatment factors could alter predicted outcomes. This work\ndemonstrates how counterfactual reasoning can be formalized as an optimization\nproblem over predictive functions, offering a rigorous, interpretable, and\ncomputationally efficient approach to personalized inference in complex\nbiomedical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual inference provides a mathematical framework for reasoning\nabout hypothetical outcomes under alternative interventions, bridging causal\nreasoning and predictive modeling. We present a counterfactual inference\nframework for individualized risk estimation and intervention analysis,\nillustrated through a clinical application to post-acute sequelae of COVID-19\n(PASC) among patients with pre-existing heart failure (HF). Using longitudinal\ndiagnosis, laboratory, and medication data from a large health-system cohort,\nwe integrate regularized predictive modeling with counterfactual search to\nidentify actionable pathways to PASC-related HF hospital admissions. The\nframework combines exact enumeration with optimization-based methods, including\nthe Nearest Instance Counterfactual Explanations (NICE) and Multi-Objective\nCounterfactuals (MOC) algorithms, to efficiently explore high-dimensional\nintervention spaces. Applied to more than 2700 individuals with confirmed\nSARS-CoV-2 infection and prior HF, the model achieved strong discriminative\nperformance (AUROC: 0.88, 95% CI: 0.84-0.91) and generated interpretable,\npatient-specific counterfactuals that quantify how modifying comorbidity\npatterns or treatment factors could alter predicted outcomes. This work\ndemonstrates how counterfactual reasoning can be formalized as an optimization\nproblem over predictive functions, offering a rigorous, interpretable, and\ncomputationally efficient approach to personalized inference in complex\nbiomedical systems."
                },
                "authors": [
                    {
                        "name": "Jingya Cheng"
                    },
                    {
                        "name": "Alaleh Azhir"
                    },
                    {
                        "name": "Jiazi Tian"
                    },
                    {
                        "name": "Hossein Estiri"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Estiri"
                },
                "author": "Hossein Estiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18840v1",
                "updated": "2025-10-21T17:34:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    34,
                    48,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:34:48Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    34,
                    48,
                    1,
                    294,
                    0
                ],
                "title": "See the Text: From Tokenization to Visual Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "See the Text: From Tokenization to Visual Reading"
                },
                "summary": "People see text. Humans read by recognizing words as visual objects,\nincluding their shapes, layouts, and patterns, before connecting them to\nmeaning, which enables us to handle typos, distorted fonts, and various scripts\neffectively. Modern large language models (LLMs), however, rely on subword\ntokenization, fragmenting text into pieces from a fixed vocabulary. While\neffective for high-resource languages, this approach over-segments low-resource\nlanguages, yielding long, linguistically meaningless sequences and inflating\ncomputation. In this work, we challenge this entrenched paradigm and move\ntoward a vision-centric alternative. Our method, SeeTok, renders text as images\n(visual-text) and leverages pretrained multimodal LLMs to interpret them,\nreusing strong OCR and text-vision alignment abilities learned from large-scale\nmultimodal training. Across three different language tasks, SeeTok matches or\nsurpasses subword tokenizers while requiring 4.43 times fewer tokens and\nreducing FLOPs by 70.5%, with additional gains in cross-lingual generalization,\nrobustness to typographic noise, and linguistic hierarchy. SeeTok signals a\nshift from symbolic tokenization to human-like visual reading, and takes a step\ntoward more natural and cognitively inspired language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People see text. Humans read by recognizing words as visual objects,\nincluding their shapes, layouts, and patterns, before connecting them to\nmeaning, which enables us to handle typos, distorted fonts, and various scripts\neffectively. Modern large language models (LLMs), however, rely on subword\ntokenization, fragmenting text into pieces from a fixed vocabulary. While\neffective for high-resource languages, this approach over-segments low-resource\nlanguages, yielding long, linguistically meaningless sequences and inflating\ncomputation. In this work, we challenge this entrenched paradigm and move\ntoward a vision-centric alternative. Our method, SeeTok, renders text as images\n(visual-text) and leverages pretrained multimodal LLMs to interpret them,\nreusing strong OCR and text-vision alignment abilities learned from large-scale\nmultimodal training. Across three different language tasks, SeeTok matches or\nsurpasses subword tokenizers while requiring 4.43 times fewer tokens and\nreducing FLOPs by 70.5%, with additional gains in cross-lingual generalization,\nrobustness to typographic noise, and linguistic hierarchy. SeeTok signals a\nshift from symbolic tokenization to human-like visual reading, and takes a step\ntoward more natural and cognitively inspired language models."
                },
                "authors": [
                    {
                        "name": "Ling Xing"
                    },
                    {
                        "name": "Alex Jinpeng Wang"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Hongyu Qu"
                    },
                    {
                        "name": "Zechao Li"
                    },
                    {
                        "name": "Jinhui Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jinhui Tang"
                },
                "author": "Jinhui Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16028v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16028v2",
                "updated": "2025-10-21T17:28:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    28,
                    4,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-15T21:10:39Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    21,
                    10,
                    39,
                    2,
                    288,
                    0
                ],
                "title": "Nondeterminism-Aware Optimistic Verification for Floating-Point Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nondeterminism-Aware Optimistic Verification for Floating-Point Neural\n  Networks"
                },
                "summary": "Neural networks increasingly run on hardware outside the user's control\n(cloud GPUs, inference marketplaces). Yet ML-as-a-Service reveals little about\nwhat actually ran or whether returned outputs faithfully reflect the intended\ninputs. Users lack recourse against service downgrades (model swaps,\nquantization, graph rewrites, or discrepancies like altered ad embeddings).\nVerifying outputs is hard because floating-point(FP) execution on heterogeneous\naccelerators is inherently nondeterministic. Existing approaches are either\nimpractical for real FP neural networks or reintroduce vendor trust. We present\nNAO: a Nondeterministic tolerance Aware Optimistic verification protocol that\naccepts outputs within principled operator-level acceptance regions rather than\nrequiring bitwise equality. NAO combines two error models: (i) sound\nper-operator IEEE-754 worst-case bounds and (ii) tight empirical percentile\nprofiles calibrated across hardware. Discrepancies trigger a Merkle-anchored,\nthreshold-guided dispute game that recursively partitions the computation graph\nuntil one operator remains, where adjudication reduces to a lightweight\ntheoretical-bound check or a small honest-majority vote against empirical\nthresholds. Unchallenged results finalize after a challenge window, without\nrequiring trusted hardware or deterministic kernels. We implement NAO as a\nPyTorch-compatible runtime and a contract layer currently deployed on Ethereum\nHolesky testnet. The runtime instruments graphs, computes per-operator bounds,\nand runs unmodified vendor kernels in FP32 with negligible overhead (0.3% on\nQwen3-8B). Across CNNs, Transformers and diffusion models on A100, H100,\nRTX6000, RTX4090, empirical thresholds are $10^2-10^3$ times tighter than\ntheoretical bounds, and bound-aware adversarial attacks achieve 0% success. NAO\nreconciles scalability with verifiability for real-world heterogeneous ML\ncompute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks increasingly run on hardware outside the user's control\n(cloud GPUs, inference marketplaces). Yet ML-as-a-Service reveals little about\nwhat actually ran or whether returned outputs faithfully reflect the intended\ninputs. Users lack recourse against service downgrades (model swaps,\nquantization, graph rewrites, or discrepancies like altered ad embeddings).\nVerifying outputs is hard because floating-point(FP) execution on heterogeneous\naccelerators is inherently nondeterministic. Existing approaches are either\nimpractical for real FP neural networks or reintroduce vendor trust. We present\nNAO: a Nondeterministic tolerance Aware Optimistic verification protocol that\naccepts outputs within principled operator-level acceptance regions rather than\nrequiring bitwise equality. NAO combines two error models: (i) sound\nper-operator IEEE-754 worst-case bounds and (ii) tight empirical percentile\nprofiles calibrated across hardware. Discrepancies trigger a Merkle-anchored,\nthreshold-guided dispute game that recursively partitions the computation graph\nuntil one operator remains, where adjudication reduces to a lightweight\ntheoretical-bound check or a small honest-majority vote against empirical\nthresholds. Unchallenged results finalize after a challenge window, without\nrequiring trusted hardware or deterministic kernels. We implement NAO as a\nPyTorch-compatible runtime and a contract layer currently deployed on Ethereum\nHolesky testnet. The runtime instruments graphs, computes per-operator bounds,\nand runs unmodified vendor kernels in FP32 with negligible overhead (0.3% on\nQwen3-8B). Across CNNs, Transformers and diffusion models on A100, H100,\nRTX6000, RTX4090, empirical thresholds are $10^2-10^3$ times tighter than\ntheoretical bounds, and bound-aware adversarial attacks achieve 0% success. NAO\nreconciles scalability with verifiability for real-world heterogeneous ML\ncompute."
                },
                "authors": [
                    {
                        "name": "Jianzhu Yao"
                    },
                    {
                        "name": "Hongxu Su"
                    },
                    {
                        "name": "Taobo Liao"
                    },
                    {
                        "name": "Zerui Cheng"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Xuechao Wang"
                    },
                    {
                        "name": "Pramod Viswanath"
                    }
                ],
                "author_detail": {
                    "name": "Pramod Viswanath"
                },
                "author": "Pramod Viswanath",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16028v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16028v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18830v1",
                "updated": "2025-10-21T17:25:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    25,
                    32,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:25:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    25,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long\n  Context Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long\n  Context Training"
                },
                "summary": "The adoption of long context windows has become a standard feature in Large\nLanguage Models (LLMs), as extended contexts significantly enhance their\ncapacity for complex reasoning and broaden their applicability across diverse\nscenarios. Dynamic sparse attention is a promising approach for reducing the\ncomputational cost of long-context. However, efficiently training LLMs with\ndynamic sparse attention on ultra-long contexts-especially in distributed\nsettings-remains a significant challenge, due in large part to worker- and\nstep-level imbalance. This paper introduces MTraining, a novel distributed\nmethodology leveraging dynamic sparse attention to enable efficient training\nfor LLMs with ultra-long contexts. Specifically, MTraining integrates three key\ncomponents: a dynamic sparse training pattern, balanced sparse ring attention,\nand hierarchical sparse ring attention. These components are designed to\nsynergistically address the computational imbalance and communication overheads\ninherent in dynamic sparse attention mechanisms during the training of models\nwith extensive context lengths. We demonstrate the efficacy of MTraining by\ntraining Qwen2.5-3B, successfully expanding its context window from 32K to 512K\ntokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite\nof downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A\nHaystack, reveal that MTraining achieves up to a 6x higher training throughput\nwhile preserving model accuracy. Our code is available at\nhttps://github.com/microsoft/MInference/tree/main/MTraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of long context windows has become a standard feature in Large\nLanguage Models (LLMs), as extended contexts significantly enhance their\ncapacity for complex reasoning and broaden their applicability across diverse\nscenarios. Dynamic sparse attention is a promising approach for reducing the\ncomputational cost of long-context. However, efficiently training LLMs with\ndynamic sparse attention on ultra-long contexts-especially in distributed\nsettings-remains a significant challenge, due in large part to worker- and\nstep-level imbalance. This paper introduces MTraining, a novel distributed\nmethodology leveraging dynamic sparse attention to enable efficient training\nfor LLMs with ultra-long contexts. Specifically, MTraining integrates three key\ncomponents: a dynamic sparse training pattern, balanced sparse ring attention,\nand hierarchical sparse ring attention. These components are designed to\nsynergistically address the computational imbalance and communication overheads\ninherent in dynamic sparse attention mechanisms during the training of models\nwith extensive context lengths. We demonstrate the efficacy of MTraining by\ntraining Qwen2.5-3B, successfully expanding its context window from 32K to 512K\ntokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite\nof downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A\nHaystack, reveal that MTraining achieves up to a 6x higher training throughput\nwhile preserving model accuracy. Our code is available at\nhttps://github.com/microsoft/MInference/tree/main/MTraining."
                },
                "authors": [
                    {
                        "name": "Wenxuan Li"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15926v2",
                "updated": "2025-10-21T17:20:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    20,
                    2,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-03T05:37:51Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    5,
                    37,
                    51,
                    4,
                    276,
                    0
                ],
                "title": "TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode\n  Accelerator with Table-Lookup Matmul on Edge FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode\n  Accelerator with Table-Lookup Matmul on Edge FPGAs"
                },
                "summary": "With the emergence of wearable devices and other embedded systems, deploying\nlarge language models (LLMs) on edge platforms has become an urgent need.\nHowever, this is challenging because of their high computational and memory\ndemands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)\ncompress weights to as low as 1.58~bits with minimal accuracy loss, edge\ndeployment is still constrained by limited on-chip resources, power budgets,\nand the often-neglected long latency of the prefill stage. We present\n\\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for\nlow-power edge FPGAs that fully supports both prefill and autoregressive\ndecoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates\nseveral novel techniques, including (1) a table-lookup-based ternary matrix\nmultiplication (TLMM) engine utilizing grouped activations and online\nprecomputation for low resource utilization and high throughput; (2) a\nfine-grained analytic URAM-based weight buffer management scheme for efficient\nloading and compute engine access; (3) a streaming dataflow architecture that\nfuses floating-point element-wise operations with linear computations to hide\nlatency; (4) a reversed-reordered prefill stage attention with fused attention\noperations for high memory efficiency; and (5) a resource-efficient specialized\ndecoding stage attention. Under a 5~W power budget, TeLLMe delivers up to\n25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for\n64--128 token prompts, marking a significant energy-efficiency advancement in\nLLM inference on edge FPGAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of wearable devices and other embedded systems, deploying\nlarge language models (LLMs) on edge platforms has become an urgent need.\nHowever, this is challenging because of their high computational and memory\ndemands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)\ncompress weights to as low as 1.58~bits with minimal accuracy loss, edge\ndeployment is still constrained by limited on-chip resources, power budgets,\nand the often-neglected long latency of the prefill stage. We present\n\\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for\nlow-power edge FPGAs that fully supports both prefill and autoregressive\ndecoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates\nseveral novel techniques, including (1) a table-lookup-based ternary matrix\nmultiplication (TLMM) engine utilizing grouped activations and online\nprecomputation for low resource utilization and high throughput; (2) a\nfine-grained analytic URAM-based weight buffer management scheme for efficient\nloading and compute engine access; (3) a streaming dataflow architecture that\nfuses floating-point element-wise operations with linear computations to hide\nlatency; (4) a reversed-reordered prefill stage attention with fused attention\noperations for high memory efficiency; and (5) a resource-efficient specialized\ndecoding stage attention. Under a 5~W power budget, TeLLMe delivers up to\n25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for\n64--128 token prompts, marking a significant energy-efficiency advancement in\nLLM inference on edge FPGAs."
                },
                "authors": [
                    {
                        "name": "Ye Qiao"
                    },
                    {
                        "name": "Zhiheng Chen"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yian Wang"
                    },
                    {
                        "name": "Sitao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Sitao Huang"
                },
                "author": "Sitao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11370v2",
                "updated": "2025-10-21T17:19:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    19,
                    46,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-13T13:11:27Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    11,
                    27,
                    0,
                    286,
                    0
                ],
                "title": "Stabilizing MoE Reinforcement Learning by Aligning Training and\n  Inference Routers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stabilizing MoE Reinforcement Learning by Aligning Training and\n  Inference Routers"
                },
                "summary": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing\nthe capabilities of large language models. However, in Mixture-of-Experts (MoE)\nmodels, the routing mechanism often introduces instability, even leading to\ncatastrophic RL training collapse. We analyze the training-inference\nconsistency of MoE models and identify a notable discrepancy in routing\nbehaviors between the two phases. Moreover, even under identical conditions,\nthe routing framework can yield divergent expert selections across repeated\nforward passes. To address this foundational inconsistency, we propose Rollout\nRouting Replay (R3), a method that records routing distributions from the\ninference engine and replays them during training. R3 significantly reduces\ntraining-inference policy KL divergence and mitigates extreme discrepancies\nwithout compromising training speed. Extensive experiments on various settings\nconfirm that R3 succeeds in stabilizing RL training, preventing collapse and\noutperforming methods such as GSPO and TIS. We believe this work can offer a\nnew solution for stabilizing RL in MoE models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing\nthe capabilities of large language models. However, in Mixture-of-Experts (MoE)\nmodels, the routing mechanism often introduces instability, even leading to\ncatastrophic RL training collapse. We analyze the training-inference\nconsistency of MoE models and identify a notable discrepancy in routing\nbehaviors between the two phases. Moreover, even under identical conditions,\nthe routing framework can yield divergent expert selections across repeated\nforward passes. To address this foundational inconsistency, we propose Rollout\nRouting Replay (R3), a method that records routing distributions from the\ninference engine and replays them during training. R3 significantly reduces\ntraining-inference policy KL divergence and mitigates extreme discrepancies\nwithout compromising training speed. Extensive experiments on various settings\nconfirm that R3 succeeds in stabilizing RL training, preventing collapse and\noutperforming methods such as GSPO and TIS. We believe this work can offer a\nnew solution for stabilizing RL in MoE models."
                },
                "authors": [
                    {
                        "name": "Wenhan Ma"
                    },
                    {
                        "name": "Hailin Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Yudong Wang"
                    },
                    {
                        "name": "Zhifang Sui"
                    },
                    {
                        "name": "Fuli Luo"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Luo"
                },
                "author": "Fuli Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18821v1",
                "updated": "2025-10-21T17:19:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    19,
                    35,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:19:35Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    19,
                    35,
                    1,
                    294,
                    0
                ],
                "title": "Search Self-play: Pushing the Frontier of Agent Capability without\n  Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search Self-play: Pushing the Frontier of Agent Capability without\n  Supervision"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has become the\nmainstream technique for training LLM agents. However, RLVR highly depends on\nwell-crafted task queries and corresponding ground-truth answers to provide\naccurate rewards, which requires massive human efforts and hinders the RL\nscaling processes, especially under agentic scenarios. Although a few recent\nworks explore task synthesis methods, the difficulty of generated agentic tasks\ncan hardly be controlled to provide effective RL training advantages. To\nachieve agentic RLVR with higher scalability, we explore self-play training for\ndeep search agents, in which the learning LLM utilizes multi-turn search engine\ncalling and acts simultaneously as both a task proposer and a problem solver.\nThe task proposer aims to generate deep search queries with well-defined\nground-truth answers and increasing task difficulty. The problem solver tries\nto handle the generated search queries and output the correct answer\npredictions. To ensure that each generated search query has accurate ground\ntruth, we collect all the searching results from the proposer's trajectory as\nexternal knowledge, then conduct retrieval-augmentation generation (RAG) to\ntest whether the proposed query can be correctly answered with all necessary\nsearch documents provided. In this search self-play (SSP) game, the proposer\nand the solver co-evolve their agent capabilities through both competition and\ncooperation. With substantial experimental results, we find that SSP can\nsignificantly improve search agents' performance uniformly on various\nbenchmarks without any supervision under both from-scratch and continuous RL\ntraining setups. The code is at https://github.com/Alibaba-Quark/SSP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has become the\nmainstream technique for training LLM agents. However, RLVR highly depends on\nwell-crafted task queries and corresponding ground-truth answers to provide\naccurate rewards, which requires massive human efforts and hinders the RL\nscaling processes, especially under agentic scenarios. Although a few recent\nworks explore task synthesis methods, the difficulty of generated agentic tasks\ncan hardly be controlled to provide effective RL training advantages. To\nachieve agentic RLVR with higher scalability, we explore self-play training for\ndeep search agents, in which the learning LLM utilizes multi-turn search engine\ncalling and acts simultaneously as both a task proposer and a problem solver.\nThe task proposer aims to generate deep search queries with well-defined\nground-truth answers and increasing task difficulty. The problem solver tries\nto handle the generated search queries and output the correct answer\npredictions. To ensure that each generated search query has accurate ground\ntruth, we collect all the searching results from the proposer's trajectory as\nexternal knowledge, then conduct retrieval-augmentation generation (RAG) to\ntest whether the proposed query can be correctly answered with all necessary\nsearch documents provided. In this search self-play (SSP) game, the proposer\nand the solver co-evolve their agent capabilities through both competition and\ncooperation. With substantial experimental results, we find that SSP can\nsignificantly improve search agents' performance uniformly on various\nbenchmarks without any supervision under both from-scratch and continuous RL\ntraining setups. The code is at https://github.com/Alibaba-Quark/SSP."
                },
                "authors": [
                    {
                        "name": "Hongliang Lu"
                    },
                    {
                        "name": "Yuhang Wen"
                    },
                    {
                        "name": "Pengyu Cheng"
                    },
                    {
                        "name": "Ruijin Ding"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Jiaqi Guo"
                    },
                    {
                        "name": "Chutian Wang"
                    },
                    {
                        "name": "Haonan Chen"
                    },
                    {
                        "name": "Xiaoxi Jiang"
                    },
                    {
                        "name": "Guanjun Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Guanjun Jiang"
                },
                "author": "Guanjun Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18817v1",
                "updated": "2025-10-21T17:18:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    18,
                    24,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:18:24Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    18,
                    24,
                    1,
                    294,
                    0
                ],
                "title": "Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for\n  Industrial Asset Health Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for\n  Industrial Asset Health Monitoring"
                },
                "summary": "Small Language Models (SLMs) are becoming increasingly popular in specialized\nfields, such as industrial applications, due to their efficiency, lower\ncomputational requirements, and ability to be fine-tuned for domain-specific\ntasks, enabling accurate and cost-effective solutions. However, performing\ncomplex reasoning using SLMs in specialized fields such as Industry 4.0 remains\nchallenging. In this paper, we propose a knowledge distillation framework for\nindustrial asset health, which transfers reasoning capabilities via\nChain-of-Thought (CoT) distillation from Large Language Models (LLMs) to\nsmaller, more efficient models (SLMs). We discuss the advantages and the\nprocess of distilling LLMs using multi-choice question answering (MCQA) prompts\nto enhance reasoning and refine decision-making. We also perform in-context\nlearning to verify the quality of the generated knowledge and benchmark the\nperformance of fine-tuned SLMs with generated knowledge against widely used\nLLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform\nthe base models by a significant margin, narrowing the gap to their LLM\ncounterparts. Our code is open-sourced at:\nhttps://github.com/IBM/FailureSensorIQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models (SLMs) are becoming increasingly popular in specialized\nfields, such as industrial applications, due to their efficiency, lower\ncomputational requirements, and ability to be fine-tuned for domain-specific\ntasks, enabling accurate and cost-effective solutions. However, performing\ncomplex reasoning using SLMs in specialized fields such as Industry 4.0 remains\nchallenging. In this paper, we propose a knowledge distillation framework for\nindustrial asset health, which transfers reasoning capabilities via\nChain-of-Thought (CoT) distillation from Large Language Models (LLMs) to\nsmaller, more efficient models (SLMs). We discuss the advantages and the\nprocess of distilling LLMs using multi-choice question answering (MCQA) prompts\nto enhance reasoning and refine decision-making. We also perform in-context\nlearning to verify the quality of the generated knowledge and benchmark the\nperformance of fine-tuned SLMs with generated knowledge against widely used\nLLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform\nthe base models by a significant margin, narrowing the gap to their LLM\ncounterparts. Our code is open-sourced at:\nhttps://github.com/IBM/FailureSensorIQ."
                },
                "authors": [
                    {
                        "name": "Shuxin Lin"
                    },
                    {
                        "name": "Dhaval Patel"
                    },
                    {
                        "name": "Christodoulos Constantinides"
                    }
                ],
                "author_detail": {
                    "name": "Christodoulos Constantinides"
                },
                "author": "Christodoulos Constantinides",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18814v1",
                "updated": "2025-10-21T17:15:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    15,
                    56,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:15:56Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    15,
                    56,
                    1,
                    294,
                    0
                ],
                "title": "Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning\n  without Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning\n  without Rewards"
                },
                "summary": "We present a simple, self-help online supervised finetuning (OSFT) paradigm\nfor LLM reasoning. In this paradigm, the model generates its own responses and\nis immediately finetuned on this self-generated data. OSFT is a highly\nefficient training strategy for LLM reasoning, as it is reward-free and uses\njust one rollout by default. Experiment results show that OSFT achieves\ndownstream performance on challenging mathematical reasoning tasks comparable\nto strong reinforcement learning with verifiable rewards (RLVR) methods such as\nGRPO. Our ablation study further demonstrates the efficiency and robustness of\nOSFT. The major mechanism of OSFT lies in facilitating the model's own existing\npreference (latent knowledge) learned from pretraining, which leads to\nreasoning ability improvement. We believe that OSFT offers an efficient and\npromising alternative to more complex, reward-based training paradigms. Our\ncode is available at https://github.com/ElementQi/OnlineSFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a simple, self-help online supervised finetuning (OSFT) paradigm\nfor LLM reasoning. In this paradigm, the model generates its own responses and\nis immediately finetuned on this self-generated data. OSFT is a highly\nefficient training strategy for LLM reasoning, as it is reward-free and uses\njust one rollout by default. Experiment results show that OSFT achieves\ndownstream performance on challenging mathematical reasoning tasks comparable\nto strong reinforcement learning with verifiable rewards (RLVR) methods such as\nGRPO. Our ablation study further demonstrates the efficiency and robustness of\nOSFT. The major mechanism of OSFT lies in facilitating the model's own existing\npreference (latent knowledge) learned from pretraining, which leads to\nreasoning ability improvement. We believe that OSFT offers an efficient and\npromising alternative to more complex, reward-based training paradigms. Our\ncode is available at https://github.com/ElementQi/OnlineSFT."
                },
                "authors": [
                    {
                        "name": "Mengqi Li"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Anthony Man-Cho So"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Xiao Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Li"
                },
                "author": "Xiao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17800v2",
                "updated": "2025-10-21T17:12:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    12,
                    48,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-20T17:58:56Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    58,
                    56,
                    0,
                    293,
                    0
                ],
                "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glyph: Scaling Context Windows via Visual-Text Compression"
                },
                "summary": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph."
                },
                "authors": [
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Yusen Liu"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Yulin Fei"
                    },
                    {
                        "name": "Wenyi Hong"
                    },
                    {
                        "name": "Ruiliang Lyu"
                    },
                    {
                        "name": "Weihan Wang"
                    },
                    {
                        "name": "Zhe Su"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17017v2",
                "updated": "2025-10-21T17:12:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    12,
                    22,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-19T21:47:19Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    21,
                    47,
                    19,
                    6,
                    292,
                    0
                ],
                "title": "SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents"
                },
                "summary": "Large language model (LLM) based search agents iteratively generate queries,\nretrieve external information, and reason to answer open-domain questions.\nWhile researchers have primarily focused on improving their utility, their\nsafety behaviors remain underexplored. In this paper, we first evaluate search\nagents using red-teaming datasets and find that they are more likely to produce\nharmful outputs than base LLMs. For example, when asked \"How can I track\nsomeone's location without their consent?\", a base model refuses, whereas a\nsearch agent designed to retrieve and cite sources may lower its refusal\nthreshold, fetch documents (e.g., court cases), and, once appended, synthesize\nthem into an informative yet unsafe summary. We further show that\nutility-oriented fine-tuning intensifies this risk, motivating joint alignment\nof safety and utility. We present SafeSearch, a multi-objective reinforcement\nlearning approach that couples a final-output safety/utility reward with a\nnovel query-level shaping term that penalizes unsafe queries and rewards safe\nones. Experiments show that SafeSearch reduces agent harmfulness by over 70%\nacross three red-teaming datasets while producing safe, helpful responses, and\nmatches the QA performance of a utility-only finetuned agent; further analyses\nconfirm the effectiveness of the query-level reward in jointly improving safety\nand utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) based search agents iteratively generate queries,\nretrieve external information, and reason to answer open-domain questions.\nWhile researchers have primarily focused on improving their utility, their\nsafety behaviors remain underexplored. In this paper, we first evaluate search\nagents using red-teaming datasets and find that they are more likely to produce\nharmful outputs than base LLMs. For example, when asked \"How can I track\nsomeone's location without their consent?\", a base model refuses, whereas a\nsearch agent designed to retrieve and cite sources may lower its refusal\nthreshold, fetch documents (e.g., court cases), and, once appended, synthesize\nthem into an informative yet unsafe summary. We further show that\nutility-oriented fine-tuning intensifies this risk, motivating joint alignment\nof safety and utility. We present SafeSearch, a multi-objective reinforcement\nlearning approach that couples a final-output safety/utility reward with a\nnovel query-level shaping term that penalizes unsafe queries and rewards safe\nones. Experiments show that SafeSearch reduces agent harmfulness by over 70%\nacross three red-teaming datasets while producing safe, helpful responses, and\nmatches the QA performance of a utility-only finetuned agent; further analyses\nconfirm the effectiveness of the query-level reward in jointly improving safety\nand utility."
                },
                "authors": [
                    {
                        "name": "Qiusi Zhan"
                    },
                    {
                        "name": "Angeline Budiman-Chan"
                    },
                    {
                        "name": "Abdelrahman Zayed"
                    },
                    {
                        "name": "Xingzhi Guo"
                    },
                    {
                        "name": "Daniel Kang"
                    },
                    {
                        "name": "Joo-Kyung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Joo-Kyung Kim"
                },
                "author": "Joo-Kyung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14008v2",
                "updated": "2025-10-21T17:07:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    7,
                    41,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-15T18:39:31Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    18,
                    39,
                    31,
                    2,
                    288,
                    0
                ],
                "title": "Stop Reducing Responsibility in LLM-Powered Multi-Agent Systems to Local\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stop Reducing Responsibility in LLM-Powered Multi-Agent Systems to Local\n  Alignment"
                },
                "summary": "LLM-powered Multi-Agent Systems (LLM-MAS) unlock new potentials in\ndistributed reasoning, collaboration, and task generalization but also\nintroduce additional risks due to unguaranteed agreement, cascading\nuncertainty, and adversarial vulnerabilities. We argue that ensuring\nresponsible behavior in such systems requires a paradigm shift: from local,\nsuperficial agent-level alignment to global, systemic agreement. We\nconceptualize responsibility not as a static constraint but as a lifecycle-wide\nproperty encompassing agreement, uncertainty, and security, each requiring the\ncomplementary integration of subjective human-centered values and objective\nverifiability. Furthermore, a dual-perspective governance framework that\ncombines interdisciplinary design with human-AI collaborative oversight is\nessential for tracing and ensuring responsibility throughout the lifecycle of\nLLM-MAS. Our position views LLM-MAS not as loose collections of agents, but as\nunified, dynamic socio-technical systems that demand principled mechanisms to\nsupport each dimension of responsibility and enable ethically aligned,\nverifiably coherent, and resilient behavior for sustained, system-wide\nagreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered Multi-Agent Systems (LLM-MAS) unlock new potentials in\ndistributed reasoning, collaboration, and task generalization but also\nintroduce additional risks due to unguaranteed agreement, cascading\nuncertainty, and adversarial vulnerabilities. We argue that ensuring\nresponsible behavior in such systems requires a paradigm shift: from local,\nsuperficial agent-level alignment to global, systemic agreement. We\nconceptualize responsibility not as a static constraint but as a lifecycle-wide\nproperty encompassing agreement, uncertainty, and security, each requiring the\ncomplementary integration of subjective human-centered values and objective\nverifiability. Furthermore, a dual-perspective governance framework that\ncombines interdisciplinary design with human-AI collaborative oversight is\nessential for tracing and ensuring responsibility throughout the lifecycle of\nLLM-MAS. Our position views LLM-MAS not as loose collections of agents, but as\nunified, dynamic socio-technical systems that demand principled mechanisms to\nsupport each dimension of responsibility and enable ethically aligned,\nverifiably coherent, and resilient behavior for sustained, system-wide\nagreement."
                },
                "authors": [
                    {
                        "name": "Jinwei Hu"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Shuang Ao"
                    },
                    {
                        "name": "Zhuoyun Li"
                    },
                    {
                        "name": "Boxuan Wang"
                    },
                    {
                        "name": "Lokesh Singh"
                    },
                    {
                        "name": "Guangliang Cheng"
                    },
                    {
                        "name": "Sarvapali D. Ramchurn"
                    },
                    {
                        "name": "Xiaowei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Huang"
                },
                "author": "Xiaowei Huang",
                "arxiv_comment": "Updated manuscript of our previous version (arXiv:2502.01714). Under\n  review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17501v2",
                "updated": "2025-10-21T17:06:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    6,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-20T12:54:32Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    54,
                    32,
                    0,
                    293,
                    0
                ],
                "title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization"
                },
                "summary": "With video exploding across social media, surveillance, and education,\ncompressing long footage into concise yet faithful surrogates is crucial.\nSupervised methods learn frame/shot importance from dense labels and excel\nin-domain, but are costly and brittle across datasets; unsupervised methods\navoid labels but often miss high-level semantics and narrative cues. Recent\nzero-shot pipelines use LLMs for training-free summarization, yet remain\nsensitive to handcrafted prompts and dataset-specific normalization.We propose\na rubric-guided, pseudo-labeled prompting framework. A small subset of human\nannotations is converted into high-confidence pseudo labels and aggregated into\nstructured, dataset-adaptive scoring rubrics for interpretable scene\nevaluation. At inference, boundary scenes (first/last) are scored from their\nown descriptions, while intermediate scenes include brief summaries of adjacent\nsegments to assess progression and redundancy, enabling the LLM to balance\nlocal salience with global coherence without parameter tuning.Across three\nbenchmarks, our method is consistently effective. On SumMe and TVSum it\nachieves F1 of 57.58 and 63.05, surpassing a zero-shot baseline (56.73, 62.21)\nby +0.85 and +0.84 and approaching supervised performance. On the query-focused\nQFVS benchmark it attains 53.79 F1, beating 53.42 by +0.37 and remaining stable\nacross validation videos. These results show that rubric-guided pseudo\nlabeling, coupled with contextual prompting, stabilizes LLM-based scoring and\nyields a general, interpretable zero-shot paradigm for both generic and\nquery-focused video summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With video exploding across social media, surveillance, and education,\ncompressing long footage into concise yet faithful surrogates is crucial.\nSupervised methods learn frame/shot importance from dense labels and excel\nin-domain, but are costly and brittle across datasets; unsupervised methods\navoid labels but often miss high-level semantics and narrative cues. Recent\nzero-shot pipelines use LLMs for training-free summarization, yet remain\nsensitive to handcrafted prompts and dataset-specific normalization.We propose\na rubric-guided, pseudo-labeled prompting framework. A small subset of human\nannotations is converted into high-confidence pseudo labels and aggregated into\nstructured, dataset-adaptive scoring rubrics for interpretable scene\nevaluation. At inference, boundary scenes (first/last) are scored from their\nown descriptions, while intermediate scenes include brief summaries of adjacent\nsegments to assess progression and redundancy, enabling the LLM to balance\nlocal salience with global coherence without parameter tuning.Across three\nbenchmarks, our method is consistently effective. On SumMe and TVSum it\nachieves F1 of 57.58 and 63.05, surpassing a zero-shot baseline (56.73, 62.21)\nby +0.85 and +0.84 and approaching supervised performance. On the query-focused\nQFVS benchmark it attains 53.79 F1, beating 53.42 by +0.37 and remaining stable\nacross validation videos. These results show that rubric-guided pseudo\nlabeling, coupled with contextual prompting, stabilizes LLM-based scoring and\nyields a general, interpretable zero-shot paradigm for both generic and\nquery-focused video summarization."
                },
                "authors": [
                    {
                        "name": "Yuanli Wu"
                    },
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Yue Du"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15050v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15050v3",
                "updated": "2025-10-21T17:05:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    5,
                    38,
                    1,
                    294,
                    0
                ],
                "published": "2025-05-21T03:15:06Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    3,
                    15,
                    6,
                    2,
                    141,
                    0
                ],
                "title": "Improving the fact-checking performance of language models by relying on\n  their entailment ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the fact-checking performance of language models by relying on\n  their entailment ability"
                },
                "summary": "Automated fact-checking has been a challenging task for the research\ncommunity. Past works tried various strategies, such as end-to-end training,\nretrieval-augmented generation, and prompt engineering, to build robust\nfact-checking systems. However, their accuracy has not been very high for\nreal-world deployment. We, on the other hand, propose a simple yet effective\nstrategy, where entailed justifications generated by LLMs are used to train\nencoder-only language models (ELMs) for fact-checking. We conducted a rigorous\nset of experiments, comparing our approach with recent works and various\nprompting and fine-tuning strategies to demonstrate the superiority of our\napproach. Additionally, we did quality analysis of model explanations, ablation\nstudies, and error analysis to provide a comprehensive understanding of our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated fact-checking has been a challenging task for the research\ncommunity. Past works tried various strategies, such as end-to-end training,\nretrieval-augmented generation, and prompt engineering, to build robust\nfact-checking systems. However, their accuracy has not been very high for\nreal-world deployment. We, on the other hand, propose a simple yet effective\nstrategy, where entailed justifications generated by LLMs are used to train\nencoder-only language models (ELMs) for fact-checking. We conducted a rigorous\nset of experiments, comparing our approach with recent works and various\nprompting and fine-tuning strategies to demonstrate the superiority of our\napproach. Additionally, we did quality analysis of model explanations, ablation\nstudies, and error analysis to provide a comprehensive understanding of our\napproach."
                },
                "authors": [
                    {
                        "name": "Gaurav Kumar"
                    },
                    {
                        "name": "Debajyoti Mazumder"
                    },
                    {
                        "name": "Ayush Garg"
                    },
                    {
                        "name": "Jasabanta Patro"
                    }
                ],
                "author_detail": {
                    "name": "Jasabanta Patro"
                },
                "author": "Jasabanta Patro",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15050v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15050v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02186v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02186v2",
                "updated": "2025-10-21T17:04:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    4,
                    9,
                    1,
                    294,
                    0
                ],
                "published": "2025-07-02T22:45:39Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    22,
                    45,
                    39,
                    2,
                    183,
                    0
                ],
                "title": "EvalAssist: A Human-Centered Tool for LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvalAssist: A Human-Centered Tool for LLM-as-a-Judge"
                },
                "summary": "With the broad availability of large language models and their ability to\ngenerate vast outputs using varied prompts and configurations, determining the\nbest output for a given task requires an intensive evaluation process, one\nwhere machine learning practitioners must decide how to assess the outputs and\nthen carefully carry out the evaluation. This process is both time-consuming\nand costly. As practitioners work with an increasing number of models, they\nmust now evaluate outputs to determine which model and prompt performs best for\na given task. LLMs are increasingly used as evaluators to filter training data,\nevaluate model performance, assess harms and risks, or assist human evaluators\nwith detailed assessments. We present EvalAssist, a framework that simplifies\nthe LLM-as-a-judge workflow. The system provides an online criteria development\nenvironment, where users can interactively build, test, and share custom\nevaluation criteria in a structured and portable format. We support a set of\nLLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a\nprompt-chaining approach we developed and contributed to the UNITXT open-source\nlibrary. Additionally, our system also includes specially trained evaluators to\ndetect harms and risks in LLM outputs. We have deployed the system internally\nin our organization with several hundreds of users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the broad availability of large language models and their ability to\ngenerate vast outputs using varied prompts and configurations, determining the\nbest output for a given task requires an intensive evaluation process, one\nwhere machine learning practitioners must decide how to assess the outputs and\nthen carefully carry out the evaluation. This process is both time-consuming\nand costly. As practitioners work with an increasing number of models, they\nmust now evaluate outputs to determine which model and prompt performs best for\na given task. LLMs are increasingly used as evaluators to filter training data,\nevaluate model performance, assess harms and risks, or assist human evaluators\nwith detailed assessments. We present EvalAssist, a framework that simplifies\nthe LLM-as-a-judge workflow. The system provides an online criteria development\nenvironment, where users can interactively build, test, and share custom\nevaluation criteria in a structured and portable format. We support a set of\nLLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a\nprompt-chaining approach we developed and contributed to the UNITXT open-source\nlibrary. Additionally, our system also includes specially trained evaluators to\ndetect harms and risks in LLM outputs. We have deployed the system internally\nin our organization with several hundreds of users."
                },
                "authors": [
                    {
                        "name": "Zahra Ashktorab"
                    },
                    {
                        "name": "Werner Geyer"
                    },
                    {
                        "name": "Michael Desmond"
                    },
                    {
                        "name": "Elizabeth M. Daly"
                    },
                    {
                        "name": "Martin Santillan Cooper"
                    },
                    {
                        "name": "Qian Pan"
                    },
                    {
                        "name": "Erik Miehling"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Hyo Jin Do"
                    }
                ],
                "author_detail": {
                    "name": "Hyo Jin Do"
                },
                "author": "Hyo Jin Do",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2410.00873",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02186v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02186v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18808v1",
                "updated": "2025-10-21T17:04:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    4,
                    6,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:04:06Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    4,
                    6,
                    1,
                    294,
                    0
                ],
                "title": "On Biologically Plausible Learning in Continuous Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Biologically Plausible Learning in Continuous Time"
                },
                "summary": "Biological learning unfolds continuously in time, yet most algorithmic models\nrely on discrete updates and separate inference and learning phases. We study a\ncontinuous-time neural model that unifies several biologically plausible\nlearning algorithms and removes the need for phase separation. Rules including\nstochastic gradient descent (SGD), feedback alignment (FA), direct feedback\nalignment (DFA), and Kolen-Pollack (KP) emerge naturally as limiting cases of\nthe dynamics. Simulations show that these continuous-time networks stably learn\nat biological timescales, even under temporal mismatches and integration noise.\nThrough analysis and simulation, we show that learning depends on temporal\noverlap: a synapse updates correctly only when its input and the corresponding\nerror signal coincide in time. When inputs are held constant, learning strength\ndeclines linearly as the delay between input and error approaches the stimulus\nduration, explaining observed robustness and failure across network depths.\nCritically, robust learning requires the synaptic plasticity timescale to\nexceed the stimulus duration by one to two orders of magnitude. For typical\ncortical stimuli (tens of milliseconds), this places the functional plasticity\nwindow in the few-second range, a testable prediction that identifies\nseconds-scale eligibility traces as necessary for error-driven learning in\nbiological circuits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biological learning unfolds continuously in time, yet most algorithmic models\nrely on discrete updates and separate inference and learning phases. We study a\ncontinuous-time neural model that unifies several biologically plausible\nlearning algorithms and removes the need for phase separation. Rules including\nstochastic gradient descent (SGD), feedback alignment (FA), direct feedback\nalignment (DFA), and Kolen-Pollack (KP) emerge naturally as limiting cases of\nthe dynamics. Simulations show that these continuous-time networks stably learn\nat biological timescales, even under temporal mismatches and integration noise.\nThrough analysis and simulation, we show that learning depends on temporal\noverlap: a synapse updates correctly only when its input and the corresponding\nerror signal coincide in time. When inputs are held constant, learning strength\ndeclines linearly as the delay between input and error approaches the stimulus\nduration, explaining observed robustness and failure across network depths.\nCritically, robust learning requires the synaptic plasticity timescale to\nexceed the stimulus duration by one to two orders of magnitude. For typical\ncortical stimuli (tens of milliseconds), this places the functional plasticity\nwindow in the few-second range, a testable prediction that identifies\nseconds-scale eligibility traces as necessary for error-driven learning in\nbiological circuits."
                },
                "authors": [
                    {
                        "name": "Marc Gong Bacvanski"
                    },
                    {
                        "name": "Liu Ziyin"
                    },
                    {
                        "name": "Tomaso Poggio"
                    }
                ],
                "author_detail": {
                    "name": "Tomaso Poggio"
                },
                "author": "Tomaso Poggio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18807v1",
                "updated": "2025-10-21T17:03:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    3,
                    8,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:03:08Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    3,
                    8,
                    1,
                    294,
                    0
                ],
                "title": "Degeneracy-Aware Pulsar Parameter Estimation from Light Curves via Deep\n  Learning and Test-Time Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degeneracy-Aware Pulsar Parameter Estimation from Light Curves via Deep\n  Learning and Test-Time Optimization"
                },
                "summary": "Probing properties of neutron stars from photometric observations of these\nobjects helps us answer crucial questions at the forefront of multi-messenger\nastronomy, such as, what is behavior of highest density matter in extreme\nenvironments and what is the procedure of generation and evolution of magnetic\nfields in these astrophysical environments? However, uncertainties and\ndegeneracies-where different parameter sets produce similar light curves-make\nthis task challenging. We propose a deep learning framework for inferring\npulsar parameters from observed light curves. Traditional deep learning models\nare not designed to produce multiple degenerate solutions for a given input. To\naddress this, we introduce a custom loss function that incorporates a light\ncurve emulator as a forward model, along with a dissimilarity loss that\nencourages the model to capture diverse, degenerate parameter sets for a given\nlight curve. We further introduce a test-time optimization scheme that refines\npredicted parameters by minimizing the discrepancy between the observed light\ncurve and those reconstructed by the forward model from predicted parameters\nduring inference. The model is trained using a suite of state-of-the-art\nsimulated pulsar light curves. Finally, we demonstrate that the parameter sets\npredicted by our approach reproduce light curves that are consistent with the\ntrue observation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing properties of neutron stars from photometric observations of these\nobjects helps us answer crucial questions at the forefront of multi-messenger\nastronomy, such as, what is behavior of highest density matter in extreme\nenvironments and what is the procedure of generation and evolution of magnetic\nfields in these astrophysical environments? However, uncertainties and\ndegeneracies-where different parameter sets produce similar light curves-make\nthis task challenging. We propose a deep learning framework for inferring\npulsar parameters from observed light curves. Traditional deep learning models\nare not designed to produce multiple degenerate solutions for a given input. To\naddress this, we introduce a custom loss function that incorporates a light\ncurve emulator as a forward model, along with a dissimilarity loss that\nencourages the model to capture diverse, degenerate parameter sets for a given\nlight curve. We further introduce a test-time optimization scheme that refines\npredicted parameters by minimizing the discrepancy between the observed light\ncurve and those reconstructed by the forward model from predicted parameters\nduring inference. The model is trained using a suite of state-of-the-art\nsimulated pulsar light curves. Finally, we demonstrate that the parameter sets\npredicted by our approach reproduce light curves that are consistent with the\ntrue observation."
                },
                "authors": [
                    {
                        "name": "Abu Bucker Siddik"
                    },
                    {
                        "name": "Diane Oyen"
                    },
                    {
                        "name": "Soumi De"
                    },
                    {
                        "name": "Greg Olmschenk"
                    },
                    {
                        "name": "Constantinos Kalapotharakos"
                    }
                ],
                "author_detail": {
                    "name": "Constantinos Kalapotharakos"
                },
                "author": "Constantinos Kalapotharakos",
                "arxiv_comment": "Accepted at ML4Astro 2025, ICML, Vancouver, CA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01480v2",
                "updated": "2025-10-21T17:02:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    2,
                    48,
                    1,
                    294,
                    0
                ],
                "published": "2025-06-02T09:39:28Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    39,
                    28,
                    0,
                    153,
                    0
                ],
                "title": "Janus-Pro-R1: Advancing Collaborative Visual Comprehension and\n  Generation via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Janus-Pro-R1: Advancing Collaborative Visual Comprehension and\n  Generation via Reinforcement Learning"
                },
                "summary": "Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify\nvisual comprehension and generation. However, these two capabilities remain\nlargely independent, as if they are two separate functions encapsulated within\nthe same model. Consequently, visual comprehension does not enhance visual\ngeneration, and the reasoning mechanisms of LLMs have not been fully integrated\nto revolutionize image generation. In this paper, we propose to enable the\ncollaborative co-evolution of visual comprehension and generation, advancing\nimage generation into an iterative introspective process. We introduce a\ntwo-stage training approach: supervised fine-tuning teaches the MLLM with the\nfoundational ability to generate genuine CoT for visual generation, while\nreinforcement learning activates its full potential via an\nexploration-exploitation trade-off. Ultimately, we unlock the Aha moment in\nvisual generation, advancing MLLMs from text-to-image tasks to unified image\ngeneration. Extensive experiments demonstrate that our model not only excels in\ntext-to-image generation and image editing, but also functions as a superior\nimage semantic evaluator with enhanced visual comprehension capabilities.\nProject Page: https://janus-pro-r1.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify\nvisual comprehension and generation. However, these two capabilities remain\nlargely independent, as if they are two separate functions encapsulated within\nthe same model. Consequently, visual comprehension does not enhance visual\ngeneration, and the reasoning mechanisms of LLMs have not been fully integrated\nto revolutionize image generation. In this paper, we propose to enable the\ncollaborative co-evolution of visual comprehension and generation, advancing\nimage generation into an iterative introspective process. We introduce a\ntwo-stage training approach: supervised fine-tuning teaches the MLLM with the\nfoundational ability to generate genuine CoT for visual generation, while\nreinforcement learning activates its full potential via an\nexploration-exploitation trade-off. Ultimately, we unlock the Aha moment in\nvisual generation, advancing MLLMs from text-to-image tasks to unified image\ngeneration. Extensive experiments demonstrate that our model not only excels in\ntext-to-image generation and image editing, but also functions as a superior\nimage semantic evaluator with enhanced visual comprehension capabilities.\nProject Page: https://janus-pro-r1.github.io."
                },
                "authors": [
                    {
                        "name": "Kaihang Pan"
                    },
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Wendong Bu"
                    },
                    {
                        "name": "Kai Shen"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Yingting Wang"
                    },
                    {
                        "name": "Yunfei Li"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Hang Zhao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18806v1",
                "updated": "2025-10-21T16:59:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    59,
                    54,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T16:59:54Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    59,
                    54,
                    1,
                    294,
                    0
                ],
                "title": "Integrating Large Language Models and Evaluating Student Outcomes in an\n  Introductory Computer Science Course",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models and Evaluating Student Outcomes in an\n  Introductory Computer Science Course"
                },
                "summary": "Generative AI (GenAI) models have broad implications for education in\ngeneral, impacting the foundations of what we teach and how we assess. This is\nespecially true in computing, where LLMs tuned for coding have demonstrated\nshockingly good performance on the types of assignments historically used in\nintroductory CS (CS1) courses. As a result, CS1 courses will need to change\nwhat skills are taught and how they are assessed. Computing education\nresearchers have begun to study student use of LLMs, but there remains much to\nbe understood about the ways that these tools affect student outcomes. In this\npaper, we present the design and evaluation of a new CS1 course at a large\nresearch-intensive university that integrates the use of LLMs as a learning\ntool for students. We describe the design principles used to create our new\nCS1-LLM course, our new course objectives, and evaluation of student outcomes\nand perceptions throughout the course as measured by assessment scores and\nsurveys. Our findings suggest that 1) student exam performance outcomes,\nincluding differences among demographic groups, are largely similar to\nhistorical outcomes for courses without integration of LLM tools, 2) large,\nopen-ended projects may be particularly valuable in an LLM context, and 3)\nstudents predominantly found the LLM tools helpful, although some had concerns\nregarding over-reliance on the tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) models have broad implications for education in\ngeneral, impacting the foundations of what we teach and how we assess. This is\nespecially true in computing, where LLMs tuned for coding have demonstrated\nshockingly good performance on the types of assignments historically used in\nintroductory CS (CS1) courses. As a result, CS1 courses will need to change\nwhat skills are taught and how they are assessed. Computing education\nresearchers have begun to study student use of LLMs, but there remains much to\nbe understood about the ways that these tools affect student outcomes. In this\npaper, we present the design and evaluation of a new CS1 course at a large\nresearch-intensive university that integrates the use of LLMs as a learning\ntool for students. We describe the design principles used to create our new\nCS1-LLM course, our new course objectives, and evaluation of student outcomes\nand perceptions throughout the course as measured by assessment scores and\nsurveys. Our findings suggest that 1) student exam performance outcomes,\nincluding differences among demographic groups, are largely similar to\nhistorical outcomes for courses without integration of LLM tools, 2) large,\nopen-ended projects may be particularly valuable in an LLM context, and 3)\nstudents predominantly found the LLM tools helpful, although some had concerns\nregarding over-reliance on the tools."
                },
                "authors": [
                    {
                        "name": "Annapurna Vadaparty"
                    },
                    {
                        "name": "David H. Smith IV"
                    },
                    {
                        "name": "Samvrit Srinath"
                    },
                    {
                        "name": "Mounika Padala"
                    },
                    {
                        "name": "Christine Alvarado"
                    },
                    {
                        "name": "Jamie Gorson Benario"
                    },
                    {
                        "name": "Daniel Zingaro"
                    },
                    {
                        "name": "Leo Porter"
                    }
                ],
                "author_detail": {
                    "name": "Leo Porter"
                },
                "author": "Leo Porter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18799v1",
                "updated": "2025-10-21T16:54:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    54,
                    21,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T16:54:21Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    54,
                    21,
                    1,
                    294,
                    0
                ],
                "title": "FeClustRE: Hierarchical Clustering and Semantic Tagging of App Features\n  from User Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FeClustRE: Hierarchical Clustering and Semantic Tagging of App Features\n  from User Reviews"
                },
                "summary": "[Context and motivation.] Extracting features from mobile app reviews is\nincreasingly important for multiple requirements engineering (RE) tasks.\nHowever, existing methods struggle to turn noisy, ambiguous feedback into\ninterpretable insights. [Question/problem.] Syntactic approaches lack semantic\ndepth, while large language models (LLMs) often miss fine-grained features or\nfail to structure them coherently. In addition, existing methods output flat\nlists of features without semantic organization, limiting interpretation and\ncomparability. Consequently, current feature extraction approaches do not\nprovide structured, meaningful representations of app features. As a result,\npractitioners face fragmented information that hinder requirement analysis,\nprioritization, and cross-app comparison, among other use cases. [Principal\nideas/results.] In this context, we propose FeClustRE, a framework integrating\nhybrid feature extraction, hierarchical clustering with auto-tuning and\nLLM-based semantic labelling. FeClustRE combines syntactic parsing with LLM\nenrichment, organizes features into clusters, and automatically generates\nmeaningful taxonomy labels. We evaluate FeClustRE on public benchmarks for\nextraction correctness and on a sample study of generative AI assistant app\nreviews for clustering quality, semantic coherence, and interpretability.\n[Contribution.] Overall, FeClustRE delivers (1) a hybrid framework for feature\nextraction and taxonomy generation, (2) an auto-tuning mechanism with a\ncomprehensive evaluation methodology, and (3) open-source and replicable\nimplementation. These contributions bridge user feedback and feature\nunderstanding, enabling deeper insights into current and emerging requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Context and motivation.] Extracting features from mobile app reviews is\nincreasingly important for multiple requirements engineering (RE) tasks.\nHowever, existing methods struggle to turn noisy, ambiguous feedback into\ninterpretable insights. [Question/problem.] Syntactic approaches lack semantic\ndepth, while large language models (LLMs) often miss fine-grained features or\nfail to structure them coherently. In addition, existing methods output flat\nlists of features without semantic organization, limiting interpretation and\ncomparability. Consequently, current feature extraction approaches do not\nprovide structured, meaningful representations of app features. As a result,\npractitioners face fragmented information that hinder requirement analysis,\nprioritization, and cross-app comparison, among other use cases. [Principal\nideas/results.] In this context, we propose FeClustRE, a framework integrating\nhybrid feature extraction, hierarchical clustering with auto-tuning and\nLLM-based semantic labelling. FeClustRE combines syntactic parsing with LLM\nenrichment, organizes features into clusters, and automatically generates\nmeaningful taxonomy labels. We evaluate FeClustRE on public benchmarks for\nextraction correctness and on a sample study of generative AI assistant app\nreviews for clustering quality, semantic coherence, and interpretability.\n[Contribution.] Overall, FeClustRE delivers (1) a hybrid framework for feature\nextraction and taxonomy generation, (2) an auto-tuning mechanism with a\ncomprehensive evaluation methodology, and (3) open-source and replicable\nimplementation. These contributions bridge user feedback and feature\nunderstanding, enabling deeper insights into current and emerging requirements."
                },
                "authors": [
                    {
                        "name": "Max Tiessler"
                    },
                    {
                        "name": "Quim Motger"
                    }
                ],
                "author_detail": {
                    "name": "Quim Motger"
                },
                "author": "Quim Motger",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18797v1",
                "updated": "2025-10-21T16:51:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    51,
                    19,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T16:51:19Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    51,
                    19,
                    1,
                    294,
                    0
                ],
                "title": "N-body Simulations of cosmologies with Light Massive Relics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N-body Simulations of cosmologies with Light Massive Relics"
                },
                "summary": "The presence of additional relativistic particles at the time of\nrecombination can be inferred through their contribution to $\\Delta N_{\\rm\neff}$. If these species have a finite but low mass (Light Massive Relics -\nLiMRs), they act as a hot subcomponent of dark matter and impact late-time\nstructure formation. Understanding these effects will be crucial to pin down\nthe underlying particle physics properties of any future $\\Delta N_{\\rm eff}$\ndetection. While their impact has been well-studied on linear scales, this work\ndevelops the framework for and presents results from the first set of\ncosmological N-body simulations that can track the effects of LiMRs, as a\nfunction of their mass and temperature, down to fully nonlinear scales.\nImportantly, our simulations model the impact of both the massive Standard\nModel neutrinos and LiMRs, which will be crucial in disentangling possible\ndegeneracies. We systematically explore the effects of LiMR properties such as\nmass, temperature, and initial distribution, on various cosmological\nobservables, including the total matter power spectrum, Halo Mass Functions\n(HMF), Mass-Concentration relation, radial halo profiles, and weak lensing\nsignals around massive clusters. The framework and simulations developed here\nwill enable detailed follow-up of the rich phenomenology of LiMR cosmologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The presence of additional relativistic particles at the time of\nrecombination can be inferred through their contribution to $\\Delta N_{\\rm\neff}$. If these species have a finite but low mass (Light Massive Relics -\nLiMRs), they act as a hot subcomponent of dark matter and impact late-time\nstructure formation. Understanding these effects will be crucial to pin down\nthe underlying particle physics properties of any future $\\Delta N_{\\rm eff}$\ndetection. While their impact has been well-studied on linear scales, this work\ndevelops the framework for and presents results from the first set of\ncosmological N-body simulations that can track the effects of LiMRs, as a\nfunction of their mass and temperature, down to fully nonlinear scales.\nImportantly, our simulations model the impact of both the massive Standard\nModel neutrinos and LiMRs, which will be crucial in disentangling possible\ndegeneracies. We systematically explore the effects of LiMR properties such as\nmass, temperature, and initial distribution, on various cosmological\nobservables, including the total matter power spectrum, Halo Mass Functions\n(HMF), Mass-Concentration relation, radial halo profiles, and weak lensing\nsignals around massive clusters. The framework and simulations developed here\nwill enable detailed follow-up of the rich phenomenology of LiMR cosmologies."
                },
                "authors": [
                    {
                        "name": "Vikhyat Sharma"
                    },
                    {
                        "name": "Arka Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Arka Banerjee"
                },
                "author": "Arka Banerjee",
                "arxiv_comment": "18 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18795v2",
                "updated": "2025-10-22T03:43:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    3,
                    43,
                    28,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-21T16:48:49Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    48,
                    49,
                    1,
                    294,
                    0
                ],
                "title": "ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder"
                },
                "summary": "The original CLIP text encoder is limited by a maximum input length of 77\ntokens, which hampers its ability to effectively process long texts and perform\nfine-grained semantic understanding. In addition, the CLIP text encoder lacks\nsupport for multilingual inputs. All these limitations significantly restrict\nits applicability across a broader range of tasks. Recent studies have\nattempted to replace the CLIP text encoder with an LLM-based embedder to\nenhance its ability in processing long texts, multilingual understanding, and\nfine-grained semantic comprehension. However, because the representation spaces\nof LLMs and the vision-language space of CLIP are pretrained independently\nwithout alignment priors, direct alignment using contrastive learning can\ndisrupt the intrinsic vision-language alignment in the CLIP image encoder,\nleading to an underutilization of the knowledge acquired during pre-training.\nTo address this challenge, we propose ProCLIP, a curriculum learning-based\nprogressive vision-language alignment framework to effectively align the CLIP\nimage encoder with an LLM-based embedder. Specifically, ProCLIP first distills\nknowledge from CLIP's text encoder into the LLM-based embedder to leverage\nCLIP's rich pretrained knowledge while establishing initial alignment between\nthe LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns\nthe CLIP image encoder with the LLM-based embedder through image-text\ncontrastive tuning, employing self-distillation regularization to avoid\noverfitting. To achieve a more effective alignment, instance semantic alignment\nloss and embedding structure alignment loss are employed during representation\ninheritance and contrastive tuning. The Code is available at\nhttps://github.com/VisionXLab/ProCLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The original CLIP text encoder is limited by a maximum input length of 77\ntokens, which hampers its ability to effectively process long texts and perform\nfine-grained semantic understanding. In addition, the CLIP text encoder lacks\nsupport for multilingual inputs. All these limitations significantly restrict\nits applicability across a broader range of tasks. Recent studies have\nattempted to replace the CLIP text encoder with an LLM-based embedder to\nenhance its ability in processing long texts, multilingual understanding, and\nfine-grained semantic comprehension. However, because the representation spaces\nof LLMs and the vision-language space of CLIP are pretrained independently\nwithout alignment priors, direct alignment using contrastive learning can\ndisrupt the intrinsic vision-language alignment in the CLIP image encoder,\nleading to an underutilization of the knowledge acquired during pre-training.\nTo address this challenge, we propose ProCLIP, a curriculum learning-based\nprogressive vision-language alignment framework to effectively align the CLIP\nimage encoder with an LLM-based embedder. Specifically, ProCLIP first distills\nknowledge from CLIP's text encoder into the LLM-based embedder to leverage\nCLIP's rich pretrained knowledge while establishing initial alignment between\nthe LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns\nthe CLIP image encoder with the LLM-based embedder through image-text\ncontrastive tuning, employing self-distillation regularization to avoid\noverfitting. To achieve a more effective alignment, instance semantic alignment\nloss and embedding structure alignment loss are employed during representation\ninheritance and contrastive tuning. The Code is available at\nhttps://github.com/VisionXLab/ProCLIP."
                },
                "authors": [
                    {
                        "name": "Xiaoxing Hu"
                    },
                    {
                        "name": "Kaicheng Yang"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Qi Ming"
                    },
                    {
                        "name": "Zonghao Guo"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ziyong Feng"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Xue Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xue Yang"
                },
                "author": "Xue Yang",
                "arxiv_comment": "17 pages, 5 fiugres",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18787v1",
                "updated": "2025-10-21T16:40:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    40,
                    26,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T16:40:26Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    40,
                    26,
                    1,
                    294,
                    0
                ],
                "title": "ShaRE your Data! Characterizing Datasets for LLM-based Requirements\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShaRE your Data! Characterizing Datasets for LLM-based Requirements\n  Engineering"
                },
                "summary": "[Context] Large Language Models (LLMs) rely on domain-specific datasets to\nachieve robust performance across training and inference stages. However, in\nRequirements Engineering (RE), data scarcity remains a persistent limitation\nreported in surveys and mapping studies. [Question/Problem] Although there are\nmultiple datasets supporting LLM-based RE tasks (LLM4RE), they are fragmented\nand poorly characterized, limiting reuse and comparability. This research\naddresses the limited visibility and characterization of datasets used in\nLLM4RE. We investigate which public datasets are employed, how they can be\nsystematically characterized, and which RE tasks and dataset descriptors remain\nunder-represented. [Ideas/Results] To address this, we conduct a systematic\nmapping study to identify and analyse datasets used in LLM4RE research. A total\nof 62 publicly available datasets are referenced across 43 primary studies.\nEach dataset is characterized along descriptors such as artifact type,\ngranularity, RE stage, task, domain, and language. Preliminary findings show\nmultiple research gaps, including limited coverage for elicitation tasks,\nscarce datasets for management activities beyond traceability, and limited\nmultilingual availability. [Contribution] This research preview offers a public\ncatalogue and structured characterization scheme to support dataset selection,\ncomparison, and reuse in LLM4RE research. Future work will extend the scope to\ngrey literature, as well as integration with open dataset and benchmark\nrepositories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Context] Large Language Models (LLMs) rely on domain-specific datasets to\nachieve robust performance across training and inference stages. However, in\nRequirements Engineering (RE), data scarcity remains a persistent limitation\nreported in surveys and mapping studies. [Question/Problem] Although there are\nmultiple datasets supporting LLM-based RE tasks (LLM4RE), they are fragmented\nand poorly characterized, limiting reuse and comparability. This research\naddresses the limited visibility and characterization of datasets used in\nLLM4RE. We investigate which public datasets are employed, how they can be\nsystematically characterized, and which RE tasks and dataset descriptors remain\nunder-represented. [Ideas/Results] To address this, we conduct a systematic\nmapping study to identify and analyse datasets used in LLM4RE research. A total\nof 62 publicly available datasets are referenced across 43 primary studies.\nEach dataset is characterized along descriptors such as artifact type,\ngranularity, RE stage, task, domain, and language. Preliminary findings show\nmultiple research gaps, including limited coverage for elicitation tasks,\nscarce datasets for management activities beyond traceability, and limited\nmultilingual availability. [Contribution] This research preview offers a public\ncatalogue and structured characterization scheme to support dataset selection,\ncomparison, and reuse in LLM4RE research. Future work will extend the scope to\ngrey literature, as well as integration with open dataset and benchmark\nrepositories."
                },
                "authors": [
                    {
                        "name": "Quim Motger"
                    },
                    {
                        "name": "Carlota Catot"
                    },
                    {
                        "name": "Xavier Franch"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Franch"
                },
                "author": "Xavier Franch",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18786v1",
                "updated": "2025-10-21T16:40:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    40,
                    14,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T16:40:14Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    40,
                    14,
                    1,
                    294,
                    0
                ],
                "title": "Stick-Breaking Embedded Topic Model with Continuous Optimal Transport\n  for Online Analysis of Document Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stick-Breaking Embedded Topic Model with Continuous Optimal Transport\n  for Online Analysis of Document Streams"
                },
                "summary": "Online topic models are unsupervised algorithms to identify latent topics in\ndata streams that continuously evolve over time. Although these methods\nnaturally align with real-world scenarios, they have received considerably less\nattention from the community compared to their offline counterparts, due to\nspecific additional challenges. To tackle these issues, we present SB-SETM, an\ninnovative model extending the Embedded Topic Model (ETM) to process data\nstreams by merging models formed on successive partial document batches. To\nthis end, SB-SETM (i) leverages a truncated stick-breaking construction for the\ntopic-per-document distribution, enabling the model to automatically infer from\nthe data the appropriate number of active topics at each timestep; and (ii)\nintroduces a merging strategy for topic embeddings based on a continuous\nformulation of optimal transport adapted to the high dimensionality of the\nlatent topic space. Numerical experiments show SB-SETM outperforming baselines\non simulated scenarios. We extensively test it on a real-world corpus of news\narticles covering the Russian-Ukrainian war throughout 2022-2023.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online topic models are unsupervised algorithms to identify latent topics in\ndata streams that continuously evolve over time. Although these methods\nnaturally align with real-world scenarios, they have received considerably less\nattention from the community compared to their offline counterparts, due to\nspecific additional challenges. To tackle these issues, we present SB-SETM, an\ninnovative model extending the Embedded Topic Model (ETM) to process data\nstreams by merging models formed on successive partial document batches. To\nthis end, SB-SETM (i) leverages a truncated stick-breaking construction for the\ntopic-per-document distribution, enabling the model to automatically infer from\nthe data the appropriate number of active topics at each timestep; and (ii)\nintroduces a merging strategy for topic embeddings based on a continuous\nformulation of optimal transport adapted to the high dimensionality of the\nlatent topic space. Numerical experiments show SB-SETM outperforming baselines\non simulated scenarios. We extensively test it on a real-world corpus of news\narticles covering the Russian-Ukrainian war throughout 2022-2023."
                },
                "authors": [
                    {
                        "name": "Federica Granese"
                    },
                    {
                        "name": "Serena Villata"
                    },
                    {
                        "name": "Charles Bouveyron"
                    }
                ],
                "author_detail": {
                    "name": "Charles Bouveyron"
                },
                "author": "Charles Bouveyron",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18779v1",
                "updated": "2025-10-21T16:27:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    27,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T16:27:47Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    27,
                    47,
                    1,
                    294,
                    0
                ],
                "title": "KAT-Coder Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAT-Coder Technical Report"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled progress in\nagentic coding, where models autonomously reason, plan, and act within\ninteractive software development workflows. However, bridging the gap between\nstatic text-based training and dynamic real-world agentic execution remains a\ncore challenge. In this technical report, we present KAT-Coder, a large-scale\nagentic code model trained through a multi-stage curriculum encompassing\nMid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning\n(RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances\nreasoning, planning, and reflection capabilities through a corpus of real\nsoftware engineering data and synthetic agentic interactions. The SFT stage\nconstructs a million-sample dataset balancing twenty programming languages, ten\ndevelopment contexts, and ten task archetypes. The RFT stage introduces a novel\nmulti-ground-truth reward formulation for stable and sample-efficient policy\noptimization. Finally, the Reinforcement-to-Deployment phase adapts the model\nto production-grade IDE environments using Error-Masked SFT and Tree-Structured\nTrajectory Training. In summary, these stages enable KAT-Coder to achieve\nrobust tool-use reliability, instruction alignment, and long-context reasoning,\nforming a deployable foundation for real-world intelligent coding agents. Our\nKAT series 32B model, KAT-Dev, has been open-sourced on\nhttps://huggingface.co/Kwaipilot/KAT-Dev.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled progress in\nagentic coding, where models autonomously reason, plan, and act within\ninteractive software development workflows. However, bridging the gap between\nstatic text-based training and dynamic real-world agentic execution remains a\ncore challenge. In this technical report, we present KAT-Coder, a large-scale\nagentic code model trained through a multi-stage curriculum encompassing\nMid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning\n(RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances\nreasoning, planning, and reflection capabilities through a corpus of real\nsoftware engineering data and synthetic agentic interactions. The SFT stage\nconstructs a million-sample dataset balancing twenty programming languages, ten\ndevelopment contexts, and ten task archetypes. The RFT stage introduces a novel\nmulti-ground-truth reward formulation for stable and sample-efficient policy\noptimization. Finally, the Reinforcement-to-Deployment phase adapts the model\nto production-grade IDE environments using Error-Masked SFT and Tree-Structured\nTrajectory Training. In summary, these stages enable KAT-Coder to achieve\nrobust tool-use reliability, instruction alignment, and long-context reasoning,\nforming a deployable foundation for real-world intelligent coding agents. Our\nKAT series 32B model, KAT-Dev, has been open-sourced on\nhttps://huggingface.co/Kwaipilot/KAT-Dev."
                },
                "authors": [
                    {
                        "name": "Zizheng Zhan"
                    },
                    {
                        "name": "Ken Deng"
                    },
                    {
                        "name": "Xiaojiang Zhang"
                    },
                    {
                        "name": "Jinghui Wang"
                    },
                    {
                        "name": "Huaixi Tang"
                    },
                    {
                        "name": "Zhiyi Lai"
                    },
                    {
                        "name": "Haoyang Huang"
                    },
                    {
                        "name": "Wen Xiang"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Wenhao Zhuang"
                    },
                    {
                        "name": "Minglei Zhang"
                    },
                    {
                        "name": "Shaojie Wang"
                    },
                    {
                        "name": "Shangpeng Yan"
                    },
                    {
                        "name": "Kepeng Lei"
                    },
                    {
                        "name": "Zongxian Feng"
                    },
                    {
                        "name": "Huiming Wang"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Mengtong Li"
                    },
                    {
                        "name": "Mengfei Xie"
                    },
                    {
                        "name": "Yinghan Cui"
                    },
                    {
                        "name": "Xuxing Chen"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Weihao Li"
                    },
                    {
                        "name": "Wenqiang Zhu"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jingxuan Xu"
                    },
                    {
                        "name": "Songwei Yu"
                    },
                    {
                        "name": "Yifan Yao"
                    },
                    {
                        "name": "Xinping Lei"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Junqi Xiong"
                    },
                    {
                        "name": "Zuchen Gao"
                    },
                    {
                        "name": "Dailin Li"
                    },
                    {
                        "name": "Haimo Li"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yuqun Zhang"
                    },
                    {
                        "name": "Junyi Peng"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Bin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Bin Chen"
                },
                "author": "Bin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18777v1",
                "updated": "2025-10-21T16:25:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    25,
                    19,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T16:25:19Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    25,
                    19,
                    1,
                    294,
                    0
                ],
                "title": "A Frequentist Statistical Introduction to Variational Inference,\n  Autoencoders, and Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Frequentist Statistical Introduction to Variational Inference,\n  Autoencoders, and Diffusion Models"
                },
                "summary": "While Variational Inference (VI) is central to modern generative models like\nVariational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs), its\npedagogical treatment is split across disciplines. In statistics, VI is\ntypically framed as a Bayesian method for posterior approximation. In machine\nlearning, however, VAEs and DDMs are developed from a Frequentist viewpoint,\nwhere VI is used to approximate a maximum likelihood estimator. This creates a\nbarrier for statisticians, as the principles behind VAEs and DDMs are hard to\ncontextualize without a corresponding Frequentist introduction to VI. This\npaper provides that introduction: we explain the theory for VI, VAEs, and DDMs\nfrom a purely Frequentist perspective, starting with the classical\nExpectation-Maximization (EM) algorithm. We show how VI arises as a scalable\nsolution for intractable E-steps and how VAEs and DDMs are natural,\ndeep-learning-based extensions of this framework, thereby bridging the gap\nbetween classical statistical inference and modern generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Variational Inference (VI) is central to modern generative models like\nVariational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs), its\npedagogical treatment is split across disciplines. In statistics, VI is\ntypically framed as a Bayesian method for posterior approximation. In machine\nlearning, however, VAEs and DDMs are developed from a Frequentist viewpoint,\nwhere VI is used to approximate a maximum likelihood estimator. This creates a\nbarrier for statisticians, as the principles behind VAEs and DDMs are hard to\ncontextualize without a corresponding Frequentist introduction to VI. This\npaper provides that introduction: we explain the theory for VI, VAEs, and DDMs\nfrom a purely Frequentist perspective, starting with the classical\nExpectation-Maximization (EM) algorithm. We show how VI arises as a scalable\nsolution for intractable E-steps and how VAEs and DDMs are natural,\ndeep-learning-based extensions of this framework, thereby bridging the gap\nbetween classical statistical inference and modern generative AI."
                },
                "authors": [
                    {
                        "name": "Yen-Chi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yen-Chi Chen"
                },
                "author": "Yen-Chi Chen",
                "arxiv_comment": "This is an introduction paper. 28 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18775v1",
                "updated": "2025-10-21T16:23:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    23,
                    21,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T16:23:21Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    23,
                    21,
                    1,
                    294,
                    0
                ],
                "title": "UltraGen: High-Resolution Video Generation with Hierarchical Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UltraGen: High-Resolution Video Generation with Hierarchical Attention"
                },
                "summary": "Recent advances in video generation have made it possible to produce visually\ncompelling videos, with wide-ranging applications in content creation,\nentertainment, and virtual reality. However, most existing diffusion\ntransformer based video generation models are limited to low-resolution outputs\n(<=720P) due to the quadratic computational complexity of the attention\nmechanism with respect to the output width and height. This computational\nbottleneck makes native high-resolution video generation (1080P/2K/4K)\nimpractical for both training and inference. To address this challenge, we\npresent UltraGen, a novel video generation framework that enables i) efficient\nand ii) end-to-end native high-resolution video synthesis. Specifically,\nUltraGen features a hierarchical dual-branch attention architecture based on\nglobal-local attention decomposition, which decouples full attention into a\nlocal attention branch for high-fidelity regional content and a global\nattention branch for overall semantic consistency. We further propose a\nspatially compressed global modeling strategy to efficiently learn global\ndependencies, and a hierarchical cross-window local attention mechanism to\nreduce computational costs while enhancing information flow across different\nlocal windows. Extensive experiments demonstrate that UltraGen can effectively\nscale pre-trained low-resolution video models to 1080P and even 4K resolution\nfor the first time, outperforming existing state-of-the-art methods and\nsuper-resolution based two-stage pipelines in both qualitative and quantitative\nevaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation have made it possible to produce visually\ncompelling videos, with wide-ranging applications in content creation,\nentertainment, and virtual reality. However, most existing diffusion\ntransformer based video generation models are limited to low-resolution outputs\n(<=720P) due to the quadratic computational complexity of the attention\nmechanism with respect to the output width and height. This computational\nbottleneck makes native high-resolution video generation (1080P/2K/4K)\nimpractical for both training and inference. To address this challenge, we\npresent UltraGen, a novel video generation framework that enables i) efficient\nand ii) end-to-end native high-resolution video synthesis. Specifically,\nUltraGen features a hierarchical dual-branch attention architecture based on\nglobal-local attention decomposition, which decouples full attention into a\nlocal attention branch for high-fidelity regional content and a global\nattention branch for overall semantic consistency. We further propose a\nspatially compressed global modeling strategy to efficiently learn global\ndependencies, and a hierarchical cross-window local attention mechanism to\nreduce computational costs while enhancing information flow across different\nlocal windows. Extensive experiments demonstrate that UltraGen can effectively\nscale pre-trained low-resolution video models to 1080P and even 4K resolution\nfor the first time, outperforming existing state-of-the-art methods and\nsuper-resolution based two-stage pipelines in both qualitative and quantitative\nevaluations."
                },
                "authors": [
                    {
                        "name": "Teng Hu"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Zihan Su"
                    },
                    {
                        "name": "Ran Yi"
                    }
                ],
                "author_detail": {
                    "name": "Ran Yi"
                },
                "author": "Ran Yi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18768v1",
                "updated": "2025-10-21T16:16:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    16,
                    0,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T16:16:00Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    16,
                    0,
                    1,
                    294,
                    0
                ],
                "title": "Improving the Generation and Evaluation of Synthetic Data for Downstream\n  Medical Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Generation and Evaluation of Synthetic Data for Downstream\n  Medical Causal Inference"
                },
                "summary": "Causal inference is essential for developing and evaluating medical\ninterventions, yet real-world medical datasets are often difficult to access\ndue to regulatory barriers. This makes synthetic data a potentially valuable\nasset that enables these medical analyses, along with the development of new\ninference methods themselves. Generative models can produce synthetic data that\nclosely approximate real data distributions, yet existing methods do not\nconsider the unique challenges that downstream causal inference tasks, and\nspecifically those focused on treatments, pose. We establish a set of\ndesiderata that synthetic data containing treatments should satisfy to maximise\ndownstream utility: preservation of (i) the covariate distribution, (ii) the\ntreatment assignment mechanism, and (iii) the outcome generation mechanism.\nBased on these desiderata, we propose a set of evaluation metrics to assess\nsuch synthetic data. Finally, we present STEAM: a novel method for generating\nSynthetic data for Treatment Effect Analysis in Medicine that mimics the\ndata-generating process of data containing treatments and optimises for our\ndesiderata. We empirically demonstrate that STEAM achieves state-of-the-art\nperformance across our metrics as compared to existing generative models,\nparticularly as the complexity of the true data-generating process increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference is essential for developing and evaluating medical\ninterventions, yet real-world medical datasets are often difficult to access\ndue to regulatory barriers. This makes synthetic data a potentially valuable\nasset that enables these medical analyses, along with the development of new\ninference methods themselves. Generative models can produce synthetic data that\nclosely approximate real data distributions, yet existing methods do not\nconsider the unique challenges that downstream causal inference tasks, and\nspecifically those focused on treatments, pose. We establish a set of\ndesiderata that synthetic data containing treatments should satisfy to maximise\ndownstream utility: preservation of (i) the covariate distribution, (ii) the\ntreatment assignment mechanism, and (iii) the outcome generation mechanism.\nBased on these desiderata, we propose a set of evaluation metrics to assess\nsuch synthetic data. Finally, we present STEAM: a novel method for generating\nSynthetic data for Treatment Effect Analysis in Medicine that mimics the\ndata-generating process of data containing treatments and optimises for our\ndesiderata. We empirically demonstrate that STEAM achieves state-of-the-art\nperformance across our metrics as compared to existing generative models,\nparticularly as the complexity of the true data-generating process increases."
                },
                "authors": [
                    {
                        "name": "Harry Amad"
                    },
                    {
                        "name": "Zhaozhi Qian"
                    },
                    {
                        "name": "Dennis Frauen"
                    },
                    {
                        "name": "Julianna Piskorz"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10806v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10806v2",
                "updated": "2025-10-21T16:10:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    10,
                    0,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-12T20:52:43Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    20,
                    52,
                    43,
                    6,
                    285,
                    0
                ],
                "title": "Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based\n  Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based\n  Structures"
                },
                "summary": "Large Language Models (LLMs) are adept at generating responses based on\ninformation within their context. While this ability is useful for interacting\nwith structured data like code files, another popular method,\nRetrieval-Augmented Generation (RAG), retrieves relevant documents to augment\nthe model's in-context learning. However, it is not well-explored how to best\nrepresent this retrieved knowledge for generating responses on structured data,\nparticularly hierarchical structures like trees. In this work, we propose a\nnovel bottom-up method to linearize knowledge from tree-like structures (like a\nGitHub repository) by generating implicit, aggregated summaries at each\nhierarchical level. This approach enables the knowledge to be stored in a\nknowledge base and used directly with RAG. We then compare our method to using\nRAG on raw, unstructured code, evaluating the accuracy and quality of the\ngenerated responses. Our results show that while response quality is comparable\nacross both methods, our approach generates over 68% fewer documents in the\nretriever, a significant gain in efficiency. This finding suggests that\nleveraging implicit, linearized knowledge may be a highly effective and\nscalable strategy for handling complex, hierarchical data structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are adept at generating responses based on\ninformation within their context. While this ability is useful for interacting\nwith structured data like code files, another popular method,\nRetrieval-Augmented Generation (RAG), retrieves relevant documents to augment\nthe model's in-context learning. However, it is not well-explored how to best\nrepresent this retrieved knowledge for generating responses on structured data,\nparticularly hierarchical structures like trees. In this work, we propose a\nnovel bottom-up method to linearize knowledge from tree-like structures (like a\nGitHub repository) by generating implicit, aggregated summaries at each\nhierarchical level. This approach enables the knowledge to be stored in a\nknowledge base and used directly with RAG. We then compare our method to using\nRAG on raw, unstructured code, evaluating the accuracy and quality of the\ngenerated responses. Our results show that while response quality is comparable\nacross both methods, our approach generates over 68% fewer documents in the\nretriever, a significant gain in efficiency. This finding suggests that\nleveraging implicit, linearized knowledge may be a highly effective and\nscalable strategy for handling complex, hierarchical data structures."
                },
                "authors": [
                    {
                        "name": "Mihir Gupte"
                    },
                    {
                        "name": "Paolo Giusto"
                    },
                    {
                        "name": "Ramesh S"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh S"
                },
                "author": "Ramesh S",
                "arxiv_comment": "Waiting for Conference Response",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10806v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10806v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03655v2",
                "updated": "2025-10-21T16:09:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    9,
                    4,
                    1,
                    294,
                    0
                ],
                "published": "2025-06-04T07:47:21Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    7,
                    47,
                    21,
                    2,
                    155,
                    0
                ],
                "title": "Facts are Harder Than Opinions -- A Multilingual, Comparative Analysis\n  of LLM-Based Fact-Checking Reliability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facts are Harder Than Opinions -- A Multilingual, Comparative Analysis\n  of LLM-Based Fact-Checking Reliability"
                },
                "summary": "The proliferation of misinformation necessitates scalable, automated\nfact-checking solutions. Yet, current benchmarks often overlook multilingual\nand topical diversity. This paper introduces a novel, dynamically extensible\ndata set that includes 61,514 claims in multiple languages and topics,\nextending existing datasets up to 2024. Through a comprehensive evaluation of\nfive prominent Large Language Models (LLMs), including GPT-4o, GPT-3.5 Turbo,\nLLaMA 3.1, and Mixtral 8x7B, we identify significant performance gaps between\ndifferent languages and topics. While overall GPT-4o achieves the highest\naccuracy, it declines to classify 43% of claims. Across all models,\nfactual-sounding claims are misclassified more often than opinions, revealing a\nkey vulnerability. These findings underscore the need for caution and highlight\nchallenges in deploying LLM-based fact-checking systems at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of misinformation necessitates scalable, automated\nfact-checking solutions. Yet, current benchmarks often overlook multilingual\nand topical diversity. This paper introduces a novel, dynamically extensible\ndata set that includes 61,514 claims in multiple languages and topics,\nextending existing datasets up to 2024. Through a comprehensive evaluation of\nfive prominent Large Language Models (LLMs), including GPT-4o, GPT-3.5 Turbo,\nLLaMA 3.1, and Mixtral 8x7B, we identify significant performance gaps between\ndifferent languages and topics. While overall GPT-4o achieves the highest\naccuracy, it declines to classify 43% of claims. Across all models,\nfactual-sounding claims are misclassified more often than opinions, revealing a\nkey vulnerability. These findings underscore the need for caution and highlight\nchallenges in deploying LLM-based fact-checking systems at scale."
                },
                "authors": [
                    {
                        "name": "Lorraine Saju"
                    },
                    {
                        "name": "Arnim Bleier"
                    },
                    {
                        "name": "Jana Lasser"
                    },
                    {
                        "name": "Claudia Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Claudia Wagner"
                },
                "author": "Claudia Wagner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19049v2",
                "updated": "2025-10-21T16:08:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    8,
                    49,
                    1,
                    294,
                    0
                ],
                "published": "2025-02-26T11:04:02Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    4,
                    2,
                    2,
                    57,
                    0
                ],
                "title": "In-Context Learning of Stochastic Differential Equations with Foundation\n  Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning of Stochastic Differential Equations with Foundation\n  Inference Models"
                },
                "summary": "Stochastic differential equations (SDEs) describe dynamical systems where\ndeterministic flows, governed by a drift function, are superimposed with random\nfluctuations, dictated by a diffusion function. The accurate estimation (or\ndiscovery) of these functions from data is a central problem in machine\nlearning, with wide application across the natural and social sciences. Yet\ncurrent solutions either rely heavily on prior knowledge of the dynamics or\ninvolve intricate training procedures. We introduce FIM-SDE (Foundation\nInference Model for SDEs), a pretrained recognition model that delivers\naccurate in-context (or zero-shot) estimation of the drift and diffusion\nfunctions of low-dimensional SDEs, from noisy time series data, and allows\nrapid finetuning to target datasets. Leveraging concepts from amortized\ninference and neural operators, we (pre)train FIM-SDE in a supervised fashion\nto map a large set of noisy, discretely observed SDE paths onto the space of\ndrift and diffusion functions. We demonstrate that FIM-SDE achieves robust\nin-context function estimation across a wide range of synthetic and real-world\nprocesses -- from canonical SDE systems (e.g., double-well dynamics or weakly\nperturbed Lorenz attractors) to stock price recordings and oil-price and\nwind-speed fluctuations -- while matching the performance of symbolic, Gaussian\nprocess and Neural SDE baselines trained on the target datasets. When finetuned\nto the target processes, we show that FIM-SDE consistently outperforms all\nthese baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic differential equations (SDEs) describe dynamical systems where\ndeterministic flows, governed by a drift function, are superimposed with random\nfluctuations, dictated by a diffusion function. The accurate estimation (or\ndiscovery) of these functions from data is a central problem in machine\nlearning, with wide application across the natural and social sciences. Yet\ncurrent solutions either rely heavily on prior knowledge of the dynamics or\ninvolve intricate training procedures. We introduce FIM-SDE (Foundation\nInference Model for SDEs), a pretrained recognition model that delivers\naccurate in-context (or zero-shot) estimation of the drift and diffusion\nfunctions of low-dimensional SDEs, from noisy time series data, and allows\nrapid finetuning to target datasets. Leveraging concepts from amortized\ninference and neural operators, we (pre)train FIM-SDE in a supervised fashion\nto map a large set of noisy, discretely observed SDE paths onto the space of\ndrift and diffusion functions. We demonstrate that FIM-SDE achieves robust\nin-context function estimation across a wide range of synthetic and real-world\nprocesses -- from canonical SDE systems (e.g., double-well dynamics or weakly\nperturbed Lorenz attractors) to stock price recordings and oil-price and\nwind-speed fluctuations -- while matching the performance of symbolic, Gaussian\nprocess and Neural SDE baselines trained on the target datasets. When finetuned\nto the target processes, we show that FIM-SDE consistently outperforms all\nthese baselines."
                },
                "authors": [
                    {
                        "name": "Patrick Seifner"
                    },
                    {
                        "name": "Kostadin Cvejoski"
                    },
                    {
                        "name": "David Berghaus"
                    },
                    {
                        "name": "Cesar Ojeda"
                    },
                    {
                        "name": "Ramses J. Sanchez"
                    }
                ],
                "author_detail": {
                    "name": "Ramses J. Sanchez"
                },
                "author": "Ramses J. Sanchez",
                "arxiv_comment": "Accepted at NeurIPS 2025. The previous version appeared under the\n  title \"Foundation Inference Models for Stochastic Differential Equations: A\n  Transformer-based Approach for Zero-shot Function Estimation.\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05188v2",
                "updated": "2025-10-21T16:08:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    8,
                    36,
                    1,
                    294,
                    0
                ],
                "published": "2025-06-05T16:02:07Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    2,
                    7,
                    3,
                    156,
                    0
                ],
                "title": "Counterfactual reasoning: an analysis of in-context emergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual reasoning: an analysis of in-context emergence"
                },
                "summary": "Large-scale neural language models exhibit remarkable performance in\nin-context learning: the ability to learn and reason about the input context on\nthe fly. This work studies in-context counterfactual reasoning in language\nmodels, that is, the ability to predict consequences of a hypothetical\nscenario. We focus on a well-defined, synthetic linear regression task that\nrequires noise abduction. Accurate prediction is based on (1) inferring an\nunobserved latent concept and (2) copying contextual noise from factual\nobservations. We show that language models are capable of counterfactual\nreasoning. Further, we enhance existing identifiability results and reduce\ncounterfactual reasoning for a broad class of functions to a transformation on\nin-context observations. In Transformers, we find that self-attention, model\ndepth and pre-training data diversity drive performance. Moreover, we provide\nmechanistic evidence that the latent concept is linearly represented in the\nresidual stream and we introduce designated \\textit{noise abduction heads}\ncentral to performing counterfactual reasoning. Lastly, our findings extend to\ncounterfactual reasoning under SDE dynamics and reflect that Transformers can\nperform noise abduction on sequential data, providing preliminary evidence on\nthe potential for counterfactual story generation. Our code is available under\nhttps://github.com/mrtzmllr/iccr.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale neural language models exhibit remarkable performance in\nin-context learning: the ability to learn and reason about the input context on\nthe fly. This work studies in-context counterfactual reasoning in language\nmodels, that is, the ability to predict consequences of a hypothetical\nscenario. We focus on a well-defined, synthetic linear regression task that\nrequires noise abduction. Accurate prediction is based on (1) inferring an\nunobserved latent concept and (2) copying contextual noise from factual\nobservations. We show that language models are capable of counterfactual\nreasoning. Further, we enhance existing identifiability results and reduce\ncounterfactual reasoning for a broad class of functions to a transformation on\nin-context observations. In Transformers, we find that self-attention, model\ndepth and pre-training data diversity drive performance. Moreover, we provide\nmechanistic evidence that the latent concept is linearly represented in the\nresidual stream and we introduce designated \\textit{noise abduction heads}\ncentral to performing counterfactual reasoning. Lastly, our findings extend to\ncounterfactual reasoning under SDE dynamics and reflect that Transformers can\nperform noise abduction on sequential data, providing preliminary evidence on\nthe potential for counterfactual story generation. Our code is available under\nhttps://github.com/mrtzmllr/iccr."
                },
                "authors": [
                    {
                        "name": "Moritz Miller"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Siyuan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Siyuan Guo"
                },
                "author": "Siyuan Guo",
                "arxiv_comment": "Published as a conference paper at the Thirty-Ninth Annual Conference\n  on Neural Information Processing Systems (NeurIPS) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18752v1",
                "updated": "2025-10-21T15:59:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    59,
                    54,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:59:54Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    59,
                    54,
                    1,
                    294,
                    0
                ],
                "title": "Probing cosmology with bright sirens from the CosmoDC2_BCO LSST\n  synthetic catalog",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing cosmology with bright sirens from the CosmoDC2_BCO LSST\n  synthetic catalog"
                },
                "summary": "Bright sirens, i.e. gravitational-wave detections of compact binary mergers\nwith electromagnetic counterparts, provide a self-calibrated distance-redshift\nrelation and are therefore powerful probes of cosmic expansion. Using the\nCosmoDC2_BCO catalog, we forecast cosmological constraints from current (LVK)\nand next-generation (ET, CE) detector networks, in combination with a\nRoman-like Type Ia supernova sample. We find that third-generation networks\nreach sub-percent precision on the Hubble constant within a few years,\nachieving 0.2% after a decade with CE+ET+LVK, while LVK remains limited to the\n6% level. The LVK fifth observing run may shed light on the H_0 tension only if\nthe inferred value falls outside the range spanned by the Planck and SH0ES\ndeterminations, which currently achieve far higher precisions. Supernovae do\nnot directly tighten H_0 but stabilize its inference through parameter\ncorrelations and enable an absolute calibration of the supernova magnitude M_B.\nIn dynamical dark-energy models, the joint analysis of Roman supernovae and\nbright sirens yields a Figure of Merit of 25 for ET+LVK and 76 for CE+ET+LVK,\nto be compared with the state-of-the-art DESI DR2 BAO plus DESY5 supernovae\nvalue of 56. Sky-localization thresholds of DeltaOmega < 50 deg^2, or even\nDeltaOmega < 10 deg^2, entail only mild penalties, suggesting efficient\nfollow-up strategies. These results establish third-generation GW+EM\nobservations, especially when combined with Roman supernovae, as a cornerstone\nfor precision cosmology in the next decade.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bright sirens, i.e. gravitational-wave detections of compact binary mergers\nwith electromagnetic counterparts, provide a self-calibrated distance-redshift\nrelation and are therefore powerful probes of cosmic expansion. Using the\nCosmoDC2_BCO catalog, we forecast cosmological constraints from current (LVK)\nand next-generation (ET, CE) detector networks, in combination with a\nRoman-like Type Ia supernova sample. We find that third-generation networks\nreach sub-percent precision on the Hubble constant within a few years,\nachieving 0.2% after a decade with CE+ET+LVK, while LVK remains limited to the\n6% level. The LVK fifth observing run may shed light on the H_0 tension only if\nthe inferred value falls outside the range spanned by the Planck and SH0ES\ndeterminations, which currently achieve far higher precisions. Supernovae do\nnot directly tighten H_0 but stabilize its inference through parameter\ncorrelations and enable an absolute calibration of the supernova magnitude M_B.\nIn dynamical dark-energy models, the joint analysis of Roman supernovae and\nbright sirens yields a Figure of Merit of 25 for ET+LVK and 76 for CE+ET+LVK,\nto be compared with the state-of-the-art DESI DR2 BAO plus DESY5 supernovae\nvalue of 56. Sky-localization thresholds of DeltaOmega < 50 deg^2, or even\nDeltaOmega < 10 deg^2, entail only mild penalties, suggesting efficient\nfollow-up strategies. These results establish third-generation GW+EM\nobservations, especially when combined with Roman supernovae, as a cornerstone\nfor precision cosmology in the next decade."
                },
                "authors": [
                    {
                        "name": "Ranier Menote"
                    },
                    {
                        "name": "Valerio Marra"
                    }
                ],
                "author_detail": {
                    "name": "Valerio Marra"
                },
                "author": "Valerio Marra",
                "arxiv_comment": "17 pages, 3 figures and 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18749v1",
                "updated": "2025-10-21T15:57:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    57,
                    23,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:57:23Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    57,
                    23,
                    1,
                    294,
                    0
                ],
                "title": "Symbolic Emulators for Cosmology: Accelerating Cosmological Analyses\n  Without Sacrificing Precision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic Emulators for Cosmology: Accelerating Cosmological Analyses\n  Without Sacrificing Precision"
                },
                "summary": "In cosmology, emulators play a crucial role by providing fast and accurate\npredictions of complex physical models, enabling efficient exploration of\nhigh-dimensional parameter spaces that would be computationally prohibitive\nwith direct numerical simulations. Symbolic emulators have emerged as promising\nalternatives to numerical approaches, delivering comparable accuracy with\nsignificantly faster evaluation times. While previous symbolic emulators were\nlimited to relatively narrow prior ranges, we expand these to cover the\nparameter space relevant for current cosmological analyses. We introduce\napproximations to hypergeometric functions used for the $\\Lambda$CDM comoving\ndistance and linear growth factor which are accurate to better than 0.001% and\n0.05%, respectively, for all redshifts and for $\\Omega_{\\rm m} \\in [0.1, 0.5]$.\nWe show that integrating symbolic emulators into a Dark Energy Survey-like\n$3\\times2$pt analysis produces cosmological constraints consistent with those\nobtained using standard numerical methods. Our symbolic emulators offer\nsubstantial improvements in speed and memory usage, demonstrating their\npractical potential for scalable, likelihood-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cosmology, emulators play a crucial role by providing fast and accurate\npredictions of complex physical models, enabling efficient exploration of\nhigh-dimensional parameter spaces that would be computationally prohibitive\nwith direct numerical simulations. Symbolic emulators have emerged as promising\nalternatives to numerical approaches, delivering comparable accuracy with\nsignificantly faster evaluation times. While previous symbolic emulators were\nlimited to relatively narrow prior ranges, we expand these to cover the\nparameter space relevant for current cosmological analyses. We introduce\napproximations to hypergeometric functions used for the $\\Lambda$CDM comoving\ndistance and linear growth factor which are accurate to better than 0.001% and\n0.05%, respectively, for all redshifts and for $\\Omega_{\\rm m} \\in [0.1, 0.5]$.\nWe show that integrating symbolic emulators into a Dark Energy Survey-like\n$3\\times2$pt analysis produces cosmological constraints consistent with those\nobtained using standard numerical methods. Our symbolic emulators offer\nsubstantial improvements in speed and memory usage, demonstrating their\npractical potential for scalable, likelihood-based inference."
                },
                "authors": [
                    {
                        "name": "Deaglan J. Bartlett"
                    },
                    {
                        "name": "Shivam Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Shivam Pandey"
                },
                "author": "Shivam Pandey",
                "arxiv_comment": "22 pages, 6 figures. Invited contribution for the Royal Society\n  Philosophical Transactions A special issue \"Symbolic regression in the\n  physical sciences\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18744v1",
                "updated": "2025-10-21T15:52:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    52,
                    33,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:52:33Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    52,
                    33,
                    1,
                    294,
                    0
                ],
                "title": "Diffusion Buffer for Online Generative Speech Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Buffer for Online Generative Speech Enhancement"
                },
                "summary": "Online Speech Enhancement was mainly reserved for predictive models. A key\nadvantage of these models is that for an incoming signal frame from a stream of\ndata, the model is called only once for enhancement. In contrast, generative\nSpeech Enhancement models often require multiple calls, resulting in a\ncomputational complexity that is too high for many online speech enhancement\napplications. This work presents the Diffusion Buffer, a generative\ndiffusion-based Speech Enhancement model which only requires one neural network\ncall per incoming signal frame from a stream of data and performs enhancement\nin an online fashion on a consumer-grade GPU. The key idea of the Diffusion\nBuffer is to align physical time with Diffusion time-steps. The approach\nprogressively denoises frames through physical time, where past frames have\nmore noise removed. Consequently, an enhanced frame is output to the listener\nwith a delay defined by the Diffusion Buffer, and the output frame has a\ncorresponding look-ahead. In this work, we extend upon our previous work by\ncarefully designing a 2D convolutional UNet architecture that specifically\naligns with the Diffusion Buffer's look-ahead. We observe that the proposed\nUNet improves performance, particularly when the algorithmic latency is low.\nMoreover, we show that using a Data Prediction loss instead of Denoising Score\nMatching loss enables flexible control over the trade-off between algorithmic\nlatency and quality during inference. The extended Diffusion Buffer equipped\nwith a novel NN and loss function drastically reduces the algorithmic latency\nfrom 320 - 960 ms to 32 - 176 ms with an even increased performance. While it\nhas been shown before that offline generative diffusion models outperform\npredictive approaches in unseen noisy speech data, we confirm that the online\nDiffusion Buffer also outperforms its predictive counterpart on unseen noisy\nspeech data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Speech Enhancement was mainly reserved for predictive models. A key\nadvantage of these models is that for an incoming signal frame from a stream of\ndata, the model is called only once for enhancement. In contrast, generative\nSpeech Enhancement models often require multiple calls, resulting in a\ncomputational complexity that is too high for many online speech enhancement\napplications. This work presents the Diffusion Buffer, a generative\ndiffusion-based Speech Enhancement model which only requires one neural network\ncall per incoming signal frame from a stream of data and performs enhancement\nin an online fashion on a consumer-grade GPU. The key idea of the Diffusion\nBuffer is to align physical time with Diffusion time-steps. The approach\nprogressively denoises frames through physical time, where past frames have\nmore noise removed. Consequently, an enhanced frame is output to the listener\nwith a delay defined by the Diffusion Buffer, and the output frame has a\ncorresponding look-ahead. In this work, we extend upon our previous work by\ncarefully designing a 2D convolutional UNet architecture that specifically\naligns with the Diffusion Buffer's look-ahead. We observe that the proposed\nUNet improves performance, particularly when the algorithmic latency is low.\nMoreover, we show that using a Data Prediction loss instead of Denoising Score\nMatching loss enables flexible control over the trade-off between algorithmic\nlatency and quality during inference. The extended Diffusion Buffer equipped\nwith a novel NN and loss function drastically reduces the algorithmic latency\nfrom 320 - 960 ms to 32 - 176 ms with an even increased performance. While it\nhas been shown before that offline generative diffusion models outperform\npredictive approaches in unseen noisy speech data, we confirm that the online\nDiffusion Buffer also outperforms its predictive counterpart on unseen noisy\nspeech data."
                },
                "authors": [
                    {
                        "name": "Bunlong Lay"
                    },
                    {
                        "name": "Rostislav Makarov"
                    },
                    {
                        "name": "Simon Welker"
                    },
                    {
                        "name": "Maris Hillemann"
                    },
                    {
                        "name": "Timo Gerkmann"
                    }
                ],
                "author_detail": {
                    "name": "Timo Gerkmann"
                },
                "author": "Timo Gerkmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01466v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01466v3",
                "updated": "2025-10-21T15:50:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    50,
                    28,
                    1,
                    294,
                    0
                ],
                "published": "2024-05-02T16:55:03Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    55,
                    3,
                    3,
                    123,
                    0
                ],
                "title": "A Systematic Literature Review on Large Language Models for Automated\n  Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Literature Review on Large Language Models for Automated\n  Program Repair"
                },
                "summary": "Automated Program Repair (APR) attempts to patch software bugs and reduce\nmanual debugging efforts. Very recently, with the advances in Large Language\nModels (LLMs), an increasing number of APR techniques have been proposed,\nfacilitating software development and maintenance and demonstrating remarkable\nperformance. However, due to ongoing explorations in the LLM-based APR field,\nit is challenging for researchers to understand the current achievements,\nchallenges, and potential opportunities. This work provides the first\nsystematic literature review to summarize the applications of LLMs in APR\nbetween 2020 and 2025. We analyze 189 relevant papers from LLMs, APR and their\nintegration perspectives. First, we categorize existing popular LLMs that are\napplied to support APR and outline four types of utilization strategies for\ntheir deployment. Besides, we detail some specific repair scenarios that\nbenefit from LLMs, e.g., semantic bugs and security vulnerabilities.\nFurthermore, we discuss several critical aspects of integrating LLMs into APR\nresearch, e.g., input forms and open science. Finally, we highlight a set of\nchallenges remaining to be investigated and the potential guidelines for future\nresearch. Overall, our paper provides a systematic overview of the research\nlandscape to the APR community, helping researchers gain a comprehensive\nunderstanding of achievements and promote future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair (APR) attempts to patch software bugs and reduce\nmanual debugging efforts. Very recently, with the advances in Large Language\nModels (LLMs), an increasing number of APR techniques have been proposed,\nfacilitating software development and maintenance and demonstrating remarkable\nperformance. However, due to ongoing explorations in the LLM-based APR field,\nit is challenging for researchers to understand the current achievements,\nchallenges, and potential opportunities. This work provides the first\nsystematic literature review to summarize the applications of LLMs in APR\nbetween 2020 and 2025. We analyze 189 relevant papers from LLMs, APR and their\nintegration perspectives. First, we categorize existing popular LLMs that are\napplied to support APR and outline four types of utilization strategies for\ntheir deployment. Besides, we detail some specific repair scenarios that\nbenefit from LLMs, e.g., semantic bugs and security vulnerabilities.\nFurthermore, we discuss several critical aspects of integrating LLMs into APR\nresearch, e.g., input forms and open science. Finally, we highlight a set of\nchallenges remaining to be investigated and the potential guidelines for future\nresearch. Overall, our paper provides a systematic overview of the research\nlandscape to the APR community, helping researchers gain a comprehensive\nunderstanding of achievements and promote future research."
                },
                "authors": [
                    {
                        "name": "Quanjun Zhang"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Yang Xie"
                    },
                    {
                        "name": "YuXiang Ma"
                    },
                    {
                        "name": "Weisong Sun"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "arxiv_comment": "update new papers, up to September 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01466v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01466v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03963v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03963v3",
                "updated": "2025-10-21T15:47:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    47,
                    20,
                    1,
                    294,
                    0
                ],
                "published": "2025-08-05T22:58:54Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    22,
                    58,
                    54,
                    1,
                    217,
                    0
                ],
                "title": "Can Large Language Models Adequately Perform Symbolic Reasoning Over\n  Time Series?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Adequately Perform Symbolic Reasoning Over\n  Time Series?"
                },
                "summary": "Uncovering hidden symbolic laws from time series data, as an aspiration\ndating back to Kepler's discovery of planetary motion, remains a core challenge\nin scientific discovery and artificial intelligence. While Large Language\nModels show promise in structured reasoning tasks, their ability to infer\ninterpretable, context-aligned symbolic structures from time series data is\nstill underexplored. To systematically evaluate this capability, we introduce\nSymbolBench, a comprehensive benchmark designed to assess symbolic reasoning\nover real-world time series across three tasks: multivariate symbolic\nregression, Boolean network inference, and causal discovery. Unlike prior\nefforts limited to simple algebraic equations, SymbolBench spans a diverse set\nof symbolic forms with varying complexity. We further propose a unified\nframework that integrates LLMs with genetic programming to form a closed-loop\nsymbolic reasoning system, where LLMs act both as predictors and evaluators.\nOur empirical results reveal key strengths and limitations of current models,\nhighlighting the importance of combining domain knowledge, context alignment,\nand reasoning structure to improve LLMs in automated scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering hidden symbolic laws from time series data, as an aspiration\ndating back to Kepler's discovery of planetary motion, remains a core challenge\nin scientific discovery and artificial intelligence. While Large Language\nModels show promise in structured reasoning tasks, their ability to infer\ninterpretable, context-aligned symbolic structures from time series data is\nstill underexplored. To systematically evaluate this capability, we introduce\nSymbolBench, a comprehensive benchmark designed to assess symbolic reasoning\nover real-world time series across three tasks: multivariate symbolic\nregression, Boolean network inference, and causal discovery. Unlike prior\nefforts limited to simple algebraic equations, SymbolBench spans a diverse set\nof symbolic forms with varying complexity. We further propose a unified\nframework that integrates LLMs with genetic programming to form a closed-loop\nsymbolic reasoning system, where LLMs act both as predictors and evaluators.\nOur empirical results reveal key strengths and limitations of current models,\nhighlighting the importance of combining domain knowledge, context alignment,\nand reasoning structure to improve LLMs in automated scientific discovery."
                },
                "authors": [
                    {
                        "name": "Zewen Liu"
                    },
                    {
                        "name": "Juntong Ni"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Max S. Y. Lau"
                    },
                    {
                        "name": "Wenpeng Yin"
                    },
                    {
                        "name": "Wei Jin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Jin"
                },
                "author": "Wei Jin",
                "arxiv_comment": "version2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03963v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03963v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15593v3",
                "updated": "2025-10-21T15:45:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    45,
                    0,
                    1,
                    294,
                    0
                ],
                "published": "2025-08-21T14:06:42Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    6,
                    42,
                    3,
                    233,
                    0
                ],
                "title": "Inductive Domain Transfer In Misspecified Simulation-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inductive Domain Transfer In Misspecified Simulation-Based Inference"
                },
                "summary": "Simulation-based inference (SBI) is a statistical inference approach for\nestimating latent parameters of a physical system when the likelihood is\nintractable but simulations are available. In practice, SBI is often hindered\nby model misspecification--the mismatch between simulated and real-world\nobservations caused by inherent modeling simplifications. RoPE, a recent SBI\napproach, addresses this challenge through a two-stage domain transfer process\nthat combines semi-supervised calibration with optimal transport (OT)-based\ndistribution alignment. However, RoPE operates in a fully transductive setting,\nrequiring access to a batch of test samples at inference time, which limits\nscalability and generalization. We propose here a fully inductive and amortized\nSBI framework that integrates calibration and distributional alignment into a\nsingle, end-to-end trainable model. Our method leverages mini-batch OT with a\nclosed-form coupling to align real and simulated observations that correspond\nto the same latent parameters, using both paired calibration data and unpaired\nsamples. A conditional normalizing flow is then trained to approximate the\nOT-induced posterior, enabling efficient inference without simulation access at\ntest time. Across a range of synthetic and real-world benchmarks--including\ncomplex medical biomarker estimation--our approach matches or surpasses the\nperformance of RoPE, as well as other standard SBI and non-SBI estimators,\nwhile offering improved scalability and applicability in challenging,\nmisspecified environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference (SBI) is a statistical inference approach for\nestimating latent parameters of a physical system when the likelihood is\nintractable but simulations are available. In practice, SBI is often hindered\nby model misspecification--the mismatch between simulated and real-world\nobservations caused by inherent modeling simplifications. RoPE, a recent SBI\napproach, addresses this challenge through a two-stage domain transfer process\nthat combines semi-supervised calibration with optimal transport (OT)-based\ndistribution alignment. However, RoPE operates in a fully transductive setting,\nrequiring access to a batch of test samples at inference time, which limits\nscalability and generalization. We propose here a fully inductive and amortized\nSBI framework that integrates calibration and distributional alignment into a\nsingle, end-to-end trainable model. Our method leverages mini-batch OT with a\nclosed-form coupling to align real and simulated observations that correspond\nto the same latent parameters, using both paired calibration data and unpaired\nsamples. A conditional normalizing flow is then trained to approximate the\nOT-induced posterior, enabling efficient inference without simulation access at\ntest time. Across a range of synthetic and real-world benchmarks--including\ncomplex medical biomarker estimation--our approach matches or surpasses the\nperformance of RoPE, as well as other standard SBI and non-SBI estimators,\nwhile offering improved scalability and applicability in challenging,\nmisspecified environments."
                },
                "authors": [
                    {
                        "name": "Ortal Senouf"
                    },
                    {
                        "name": "Antoine Wehenkel"
                    },
                    {
                        "name": "Cédric Vincent-Cuaz"
                    },
                    {
                        "name": "Emmanuel Abbé"
                    },
                    {
                        "name": "Pascal Frossard"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Frossard"
                },
                "author": "Pascal Frossard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15700v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15700v2",
                "updated": "2025-10-21T15:39:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    39,
                    4,
                    1,
                    294,
                    0
                ],
                "published": "2024-10-21T07:18:23Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    18,
                    23,
                    0,
                    295,
                    0
                ],
                "title": "InternLM2.5-StepProver: Advancing Automated Theorem Proving via\n  Critic-Guided Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InternLM2.5-StepProver: Advancing Automated Theorem Proving via\n  Critic-Guided Search"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools in mathematical\ntheorem proving, particularly when utilizing formal languages such as LEAN. A\nprevalent proof method involves the LLM prover iteratively constructing the\nproof tactic by tactic, typically following a best-first search scheme.\nHowever, this method often ignores the critical preference information inside\nthe existing tactic trajectories, hindering the search for deeper proofs. We\npropose an intuitive yet effective method, which utilizes a critic model to\ncapture the preference information and to guide the search of the prover model\nat runtime. Given the prover-critic framework, a large-scale expert iteration\nwith more than 20,000 CPU days is then applied to further fine-tune the prover\nand the critic. The trained InternLM2.5-StepProver critic significantly boosts\nthe performance of the prover model (59.4% to 65.9%). We also analyze the\nimpact of the critic on various aspects of the theorem proving process during\nexpert iteration, providing insights into its effectiveness. We open-source our\nmodels and searched proofs at https://github.com/InternLM/InternLM-Math and\nhttps://huggingface.co/datasets/internlm/Lean-Workbook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools in mathematical\ntheorem proving, particularly when utilizing formal languages such as LEAN. A\nprevalent proof method involves the LLM prover iteratively constructing the\nproof tactic by tactic, typically following a best-first search scheme.\nHowever, this method often ignores the critical preference information inside\nthe existing tactic trajectories, hindering the search for deeper proofs. We\npropose an intuitive yet effective method, which utilizes a critic model to\ncapture the preference information and to guide the search of the prover model\nat runtime. Given the prover-critic framework, a large-scale expert iteration\nwith more than 20,000 CPU days is then applied to further fine-tune the prover\nand the critic. The trained InternLM2.5-StepProver critic significantly boosts\nthe performance of the prover model (59.4% to 65.9%). We also analyze the\nimpact of the critic on various aspects of the theorem proving process during\nexpert iteration, providing insights into its effectiveness. We open-source our\nmodels and searched proofs at https://github.com/InternLM/InternLM-Math and\nhttps://huggingface.co/datasets/internlm/Lean-Workbook."
                },
                "authors": [
                    {
                        "name": "Zijian Wu"
                    },
                    {
                        "name": "Suozhi Huang"
                    },
                    {
                        "name": "Zhejian Zhou"
                    },
                    {
                        "name": "Huaiyuan Ying"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15700v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15700v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13982v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13982v3",
                "updated": "2025-10-21T15:32:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    32,
                    35,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-15T18:05:06Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    18,
                    5,
                    6,
                    2,
                    288,
                    0
                ],
                "title": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires\n  Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires\n  Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations"
                },
                "summary": "What if artificial agents could not just communicate, but also evolve, adapt,\nand reshape their worlds in ways we cannot fully predict? With llm now powering\nmulti-agent systems and social simulations, we are witnessing new possibilities\nfor modeling open-ended, ever-changing environments. Yet, most current\nsimulations remain constrained within static sandboxes, characterized by\npredefined tasks, limited dynamics, and rigid evaluation criteria. These\nlimitations prevent them from capturing the complexity of real-world societies.\nIn this paper, we argue that static, task-specific benchmarks are fundamentally\ninadequate and must be rethought. We critically review emerging architectures\nthat blend llm with multi-agent dynamics, highlight key hurdles such as\nbalancing stability and diversity, evaluating unexpected behaviors, and scaling\nto greater complexity, and introduce a fresh taxonomy for this rapidly evolving\nfield. Finally, we present a research roadmap centered on open-endedness,\ncontinuous co-evolution, and the development of resilient, socially aligned AI\necosystems. We call on the community to move beyond static paradigms and help\nshape the next generation of adaptive, socially-aware multi-agent simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What if artificial agents could not just communicate, but also evolve, adapt,\nand reshape their worlds in ways we cannot fully predict? With llm now powering\nmulti-agent systems and social simulations, we are witnessing new possibilities\nfor modeling open-ended, ever-changing environments. Yet, most current\nsimulations remain constrained within static sandboxes, characterized by\npredefined tasks, limited dynamics, and rigid evaluation criteria. These\nlimitations prevent them from capturing the complexity of real-world societies.\nIn this paper, we argue that static, task-specific benchmarks are fundamentally\ninadequate and must be rethought. We critically review emerging architectures\nthat blend llm with multi-agent dynamics, highlight key hurdles such as\nbalancing stability and diversity, evaluating unexpected behaviors, and scaling\nto greater complexity, and introduce a fresh taxonomy for this rapidly evolving\nfield. Finally, we present a research roadmap centered on open-endedness,\ncontinuous co-evolution, and the development of resilient, socially aligned AI\necosystems. We call on the community to move beyond static paradigms and help\nshape the next generation of adaptive, socially-aware multi-agent simulations."
                },
                "authors": [
                    {
                        "name": "Jinkun Chen"
                    },
                    {
                        "name": "Sher Badshah"
                    },
                    {
                        "name": "Xuemin Yu"
                    },
                    {
                        "name": "Sijia Han"
                    }
                ],
                "author_detail": {
                    "name": "Sijia Han"
                },
                "author": "Sijia Han",
                "arxiv_comment": "Preprint; feedback welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13982v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13982v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18731v1",
                "updated": "2025-10-21T15:32:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    32,
                    26,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:32:26Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    32,
                    26,
                    1,
                    294,
                    0
                ],
                "title": "Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate\n  Lost-in-Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate\n  Lost-in-Conversation"
                },
                "summary": "Large Language Models demonstrate strong capabilities in single-turn\ninstruction following but suffer from Lost-in-Conversation (LiC), a degradation\nin performance as information is revealed progressively in multi-turn settings.\nMotivated by the current progress on Reinforcement Learning with Verifiable\nRewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable\nAccuracy and Abstention Rewards (RLAAR), a framework that encourages models not\nonly to generate correct answers, but also to judge the solvability of\nquestions in the multi-turn conversation setting. Our approach employs a\ncompetence-gated curriculum that incrementally increases dialogue difficulty\n(in terms of instruction shards), stabilizing training while promoting\nreliability. Using multi-turn, on-policy rollouts and a mixed-reward system,\nRLAAR teaches models to balance problem-solving with informed abstention,\nreducing premature answering behaviors that cause LiC. Evaluated on LiC\nbenchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to\n75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together,\nthese results provide a practical recipe for building multi-turn reliable and\ntrustworthy LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models demonstrate strong capabilities in single-turn\ninstruction following but suffer from Lost-in-Conversation (LiC), a degradation\nin performance as information is revealed progressively in multi-turn settings.\nMotivated by the current progress on Reinforcement Learning with Verifiable\nRewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable\nAccuracy and Abstention Rewards (RLAAR), a framework that encourages models not\nonly to generate correct answers, but also to judge the solvability of\nquestions in the multi-turn conversation setting. Our approach employs a\ncompetence-gated curriculum that incrementally increases dialogue difficulty\n(in terms of instruction shards), stabilizing training while promoting\nreliability. Using multi-turn, on-policy rollouts and a mixed-reward system,\nRLAAR teaches models to balance problem-solving with informed abstention,\nreducing premature answering behaviors that cause LiC. Evaluated on LiC\nbenchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to\n75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together,\nthese results provide a practical recipe for building multi-turn reliable and\ntrustworthy LLMs."
                },
                "authors": [
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20409v2",
                "updated": "2025-10-21T15:29:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    29,
                    52,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-24T05:40:08Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    5,
                    40,
                    8,
                    2,
                    267,
                    0
                ],
                "title": "A Unified Formal Theory on the Logical Limits of Symbol Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Formal Theory on the Logical Limits of Symbol Grounding"
                },
                "summary": "This paper synthesizes a series of formal proofs to construct a unified\ntheory on the logical limits of the Symbol Grounding Problem. We demonstrate\nthrough a four-stage argument that meaning within a formal system must arise\nfrom a process that is external, dynamic, and non-algorithmic. First, we prove\nthat any purely symbolic system, devoid of external connections, cannot\ninternally establish a consistent foundation for meaning due to\nself-referential paradoxes. Second, we extend this limitation to systems with\nany finite, static set of pre-established meanings, proving they are inherently\nincomplete. Third, we demonstrate that the very \"act\" of connecting an internal\nsymbol to an external meaning cannot be a product of logical inference within\nthe system but must be an axiomatic, meta-level update. Finally, we prove that\nany attempt to automate this update process using a fixed, external \"judgment\"\nalgorithm will inevitably construct a larger, yet equally incomplete, symbolic\nsystem. Together, these conclusions formally establish that the grounding of\nmeaning is a necessarily open-ended, non-algorithmic process, revealing a\nfundamental, G\\\"odel-style limitation for any self-contained intelligent\nsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper synthesizes a series of formal proofs to construct a unified\ntheory on the logical limits of the Symbol Grounding Problem. We demonstrate\nthrough a four-stage argument that meaning within a formal system must arise\nfrom a process that is external, dynamic, and non-algorithmic. First, we prove\nthat any purely symbolic system, devoid of external connections, cannot\ninternally establish a consistent foundation for meaning due to\nself-referential paradoxes. Second, we extend this limitation to systems with\nany finite, static set of pre-established meanings, proving they are inherently\nincomplete. Third, we demonstrate that the very \"act\" of connecting an internal\nsymbol to an external meaning cannot be a product of logical inference within\nthe system but must be an axiomatic, meta-level update. Finally, we prove that\nany attempt to automate this update process using a fixed, external \"judgment\"\nalgorithm will inevitably construct a larger, yet equally incomplete, symbolic\nsystem. Together, these conclusions formally establish that the grounding of\nmeaning is a necessarily open-ended, non-algorithmic process, revealing a\nfundamental, G\\\"odel-style limitation for any self-contained intelligent\nsystem."
                },
                "authors": [
                    {
                        "name": "Zhangchi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhangchi Liu"
                },
                "author": "Zhangchi Liu",
                "arxiv_comment": "8 pages, 1 figure. A formal proof on the logical limits of symbol\n  grounding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B65, 03F40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04501v2",
                "updated": "2025-10-21T15:29:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    29,
                    40,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-02T03:59:40Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    3,
                    59,
                    40,
                    1,
                    245,
                    0
                ],
                "title": "Understanding Reinforcement Learning for Model Training, and future\n  directions with GRAPE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Reinforcement Learning for Model Training, and future\n  directions with GRAPE"
                },
                "summary": "This paper provides a self-contained, from-scratch, exposition of key\nalgorithms for instruction tuning of models: SFT, Rejection Sampling,\nREINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy\nOptimization (PPO), Group Relative Policy Optimization (GRPO), and Direct\nPreference Optimization (DPO). Explanations of these algorithms often assume\nprior knowledge, lack critical details, and/or are overly generalized and\ncomplex. Here, each method is discussed and developed step by step using\nsimplified and explicit notation focused on LLMs, aiming to eliminate ambiguity\nand provide a clear and intuitive understanding of the concepts. By minimizing\ndetours into the broader RL literature and connecting concepts to LLMs, we\neliminate superfluous abstractions and reduce cognitive overhead. Following\nthis exposition, we provide a literature review of new techniques and\napproaches beyond those detailed. Finally, new ideas for research and\nexploration in the form of GRAPE (Generalized Relative Advantage Policy\nEvolution) are presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a self-contained, from-scratch, exposition of key\nalgorithms for instruction tuning of models: SFT, Rejection Sampling,\nREINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy\nOptimization (PPO), Group Relative Policy Optimization (GRPO), and Direct\nPreference Optimization (DPO). Explanations of these algorithms often assume\nprior knowledge, lack critical details, and/or are overly generalized and\ncomplex. Here, each method is discussed and developed step by step using\nsimplified and explicit notation focused on LLMs, aiming to eliminate ambiguity\nand provide a clear and intuitive understanding of the concepts. By minimizing\ndetours into the broader RL literature and connecting concepts to LLMs, we\neliminate superfluous abstractions and reduce cognitive overhead. Following\nthis exposition, we provide a literature review of new techniques and\napproaches beyond those detailed. Finally, new ideas for research and\nexploration in the form of GRAPE (Generalized Relative Advantage Policy\nEvolution) are presented."
                },
                "authors": [
                    {
                        "name": "Rohit Patel"
                    }
                ],
                "author_detail": {
                    "name": "Rohit Patel"
                },
                "author": "Rohit Patel",
                "arxiv_comment": "35 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T05, 62M45, 68T50, 90C40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18728v1",
                "updated": "2025-10-21T15:28:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    28,
                    20,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:28:20Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    28,
                    20,
                    1,
                    294,
                    0
                ],
                "title": "HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) remain vulnerable to multi-turn jailbreak\nattacks. We introduce HarmNet, a modular framework comprising ThoughtNet, a\nhierarchical semantic network; a feedback-driven Simulator for iterative query\nrefinement; and a Network Traverser for real-time adaptive attack execution.\nHarmNet systematically explores and refines the adversarial space to uncover\nstealthy, high-success attack paths. Experiments across closed-source and\nopen-source LLMs show that HarmNet outperforms state-of-the-art methods,\nachieving higher attack success rates. For example, on Mistral-7B, HarmNet\nachieves a 99.4% attack success rate, 13.9% higher than the best baseline.\nIndex terms: jailbreak attacks; large language models; adversarial framework;\nquery refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) remain vulnerable to multi-turn jailbreak\nattacks. We introduce HarmNet, a modular framework comprising ThoughtNet, a\nhierarchical semantic network; a feedback-driven Simulator for iterative query\nrefinement; and a Network Traverser for real-time adaptive attack execution.\nHarmNet systematically explores and refines the adversarial space to uncover\nstealthy, high-success attack paths. Experiments across closed-source and\nopen-source LLMs show that HarmNet outperforms state-of-the-art methods,\nachieving higher attack success rates. For example, on Mistral-7B, HarmNet\nachieves a 99.4% attack success rate, 13.9% higher than the best baseline.\nIndex terms: jailbreak attacks; large language models; adversarial framework;\nquery refinement."
                },
                "authors": [
                    {
                        "name": "Sidhant Narula"
                    },
                    {
                        "name": "Javad Rafiei Asl"
                    },
                    {
                        "name": "Mohammad Ghasemigol"
                    },
                    {
                        "name": "Eduardo Blanco"
                    },
                    {
                        "name": "Daniel Takabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Takabi"
                },
                "author": "Daniel Takabi",
                "arxiv_comment": "This paper has been accepted for presentation at the Conference on\n  Applied Machine Learning in Information Security (CAMLIS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18725v1",
                "updated": "2025-10-21T15:24:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    24,
                    15,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:24:15Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    24,
                    15,
                    1,
                    294,
                    0
                ],
                "title": "SemiAdapt and SemiLoRA: Efficient Domain Adaptation for\n  Transformer-based Low-Resource Language Translation with a Case Study on\n  Irish",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemiAdapt and SemiLoRA: Efficient Domain Adaptation for\n  Transformer-based Low-Resource Language Translation with a Case Study on\n  Irish"
                },
                "summary": "Fine-tuning is widely used to tailor large language models for specific tasks\nsuch as neural machine translation (NMT). However, leveraging transfer learning\nis computationally expensive when fine-tuning large multilingual models with\nbillions of parameters, thus creating a barrier to entry for researchers\nworking on low-resource domains such as Irish translation. Parameter-efficient\nfine-tuning (PEFT) bridges this gap by training on a fraction of the original\nmodel parameters, with the Low-Rank Adaptation (LoRA) approach introducing\nsmall, trainable adapter layers. We introduce SemiAdapt and SemiLoRA as\nsemi-supervised inference-efficient approaches that strengthen domain\nadaptation and lead to improved overall performance in NMT. We demonstrate that\nSemiAdapt can outperform full-domain fine-tuning, while most notably, SemiLoRA\ncan propel PEFT methods to match or even outperform full-model fine-tuning. We\nfurther evaluate domain-by-dataset fine-tuning and demonstrate that our\nembedding-based inference methods perform especially well on larger and noisier\ncorpora. All Irish translation models developed in this work are released as\nopen resources. These methods aim to make high-quality domain adaptation and\nfine-tuning more accessible to researchers working with low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning is widely used to tailor large language models for specific tasks\nsuch as neural machine translation (NMT). However, leveraging transfer learning\nis computationally expensive when fine-tuning large multilingual models with\nbillions of parameters, thus creating a barrier to entry for researchers\nworking on low-resource domains such as Irish translation. Parameter-efficient\nfine-tuning (PEFT) bridges this gap by training on a fraction of the original\nmodel parameters, with the Low-Rank Adaptation (LoRA) approach introducing\nsmall, trainable adapter layers. We introduce SemiAdapt and SemiLoRA as\nsemi-supervised inference-efficient approaches that strengthen domain\nadaptation and lead to improved overall performance in NMT. We demonstrate that\nSemiAdapt can outperform full-domain fine-tuning, while most notably, SemiLoRA\ncan propel PEFT methods to match or even outperform full-model fine-tuning. We\nfurther evaluate domain-by-dataset fine-tuning and demonstrate that our\nembedding-based inference methods perform especially well on larger and noisier\ncorpora. All Irish translation models developed in this work are released as\nopen resources. These methods aim to make high-quality domain adaptation and\nfine-tuning more accessible to researchers working with low-resource languages."
                },
                "authors": [
                    {
                        "name": "Josh McGiff"
                    },
                    {
                        "name": "Nikola S. Nikolov"
                    }
                ],
                "author_detail": {
                    "name": "Nikola S. Nikolov"
                },
                "author": "Nikola S. Nikolov",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18719v1",
                "updated": "2025-10-21T15:20:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    20,
                    30,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:20:30Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    20,
                    30,
                    1,
                    294,
                    0
                ],
                "title": "Causally Perturbed Fairness Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causally Perturbed Fairness Testing"
                },
                "summary": "To mitigate unfair and unethical discrimination over sensitive features\n(e.g., gender, age, or race), fairness testing plays an integral role in\nengineering systems that leverage AI models to handle tabular data. A key\nchallenge therein is how to effectively reveal fairness bugs under an\nintractable sample size using perturbation. Much current work has been focusing\non designing the test sample generators, ignoring the valuable knowledge about\ndata characteristics that can help guide the perturbation and hence limiting\ntheir full potential. In this paper, we seek to bridge such a gap by proposing\na generic framework of causally perturbed fairness testing, dubbed CausalFT.\nThrough causal inference, the key idea of CausalFT is to extract the most\ndirectly and causally relevant non-sensitive feature to its sensitive\ncounterpart, which can jointly influence the prediction of the label. Such a\ncausal relationship is then seamlessly injected into the perturbation to guide\na test sample generator. Unlike existing generator-level work, CausalFT serves\nas a higher-level framework that can be paired with diverse base generators.\nExtensive experiments on 1296 cases confirm that CausalFT can considerably\nimprove arbitrary base generators in revealing fairness bugs over 93% of the\ncases with acceptable extra runtime overhead. Compared with a state-of-the-art\napproach that ranks the non-sensitive features solely based on correlation,\nCausalFT performs significantly better on 64% cases while being much more\nefficient. Further, CausalFT can better improve bias resilience in nearly all\ncases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To mitigate unfair and unethical discrimination over sensitive features\n(e.g., gender, age, or race), fairness testing plays an integral role in\nengineering systems that leverage AI models to handle tabular data. A key\nchallenge therein is how to effectively reveal fairness bugs under an\nintractable sample size using perturbation. Much current work has been focusing\non designing the test sample generators, ignoring the valuable knowledge about\ndata characteristics that can help guide the perturbation and hence limiting\ntheir full potential. In this paper, we seek to bridge such a gap by proposing\na generic framework of causally perturbed fairness testing, dubbed CausalFT.\nThrough causal inference, the key idea of CausalFT is to extract the most\ndirectly and causally relevant non-sensitive feature to its sensitive\ncounterpart, which can jointly influence the prediction of the label. Such a\ncausal relationship is then seamlessly injected into the perturbation to guide\na test sample generator. Unlike existing generator-level work, CausalFT serves\nas a higher-level framework that can be paired with diverse base generators.\nExtensive experiments on 1296 cases confirm that CausalFT can considerably\nimprove arbitrary base generators in revealing fairness bugs over 93% of the\ncases with acceptable extra runtime overhead. Compared with a state-of-the-art\napproach that ranks the non-sensitive features solely based on correlation,\nCausalFT performs significantly better on 64% cases while being much more\nefficient. Further, CausalFT can better improve bias resilience in nearly all\ncases."
                },
                "authors": [
                    {
                        "name": "Chengwen Du"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "accepted by TOSEM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18713v1",
                "updated": "2025-10-21T15:11:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    11,
                    1,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:11:01Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    11,
                    1,
                    1,
                    294,
                    0
                ],
                "title": "Preference-based Reinforcement Learning beyond Pairwise Comparisons:\n  Benefits of Multiple Options",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-based Reinforcement Learning beyond Pairwise Comparisons:\n  Benefits of Multiple Options"
                },
                "summary": "We study online preference-based reinforcement learning (PbRL) with the goal\nof improving sample efficiency. While a growing body of theoretical work has\nemerged-motivated by PbRL's recent empirical success, particularly in aligning\nlarge language models (LLMs)-most existing studies focus only on pairwise\ncomparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024,\nThekumparampil et al., 2024) have explored using multiple comparisons and\nranking feedback, but their performance guarantees fail to improve-and can even\ndeteriorate-as the feedback length increases, despite the richer information\navailable. To address this gap, we adopt the Plackett-Luce (PL) model for\nranking feedback over action subsets and propose M-AUPO, an algorithm that\nselects multiple actions by maximizing the average uncertainty within the\noffered subset. We prove that M-AUPO achieves a suboptimality gap of\n$\\tilde{\\mathcal{O}}\\left( \\frac{d}{T} \\sqrt{ \\sum_{t=1}^T \\frac{1}{|S_t|}}\n\\right)$, where $T$ is the total number of rounds, $d$ is the feature\ndimension, and $|S_t|$ is the size of the subset at round $t$. This result\nshows that larger subsets directly lead to improved performance and, notably,\nthe bound avoids the exponential dependence on the unknown parameter's norm,\nwhich was a fundamental limitation in most previous works. Moreover, we\nestablish a near-matching lower bound of $\\Omega \\left( \\frac{d}{K \\sqrt{T}}\n\\right)$, where $K$ is the maximum subset size. To the best of our knowledge,\nthis is the first theoretical result in PbRL with ranking feedback that\nexplicitly shows improved sample efficiency as a function of the subset size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study online preference-based reinforcement learning (PbRL) with the goal\nof improving sample efficiency. While a growing body of theoretical work has\nemerged-motivated by PbRL's recent empirical success, particularly in aligning\nlarge language models (LLMs)-most existing studies focus only on pairwise\ncomparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024,\nThekumparampil et al., 2024) have explored using multiple comparisons and\nranking feedback, but their performance guarantees fail to improve-and can even\ndeteriorate-as the feedback length increases, despite the richer information\navailable. To address this gap, we adopt the Plackett-Luce (PL) model for\nranking feedback over action subsets and propose M-AUPO, an algorithm that\nselects multiple actions by maximizing the average uncertainty within the\noffered subset. We prove that M-AUPO achieves a suboptimality gap of\n$\\tilde{\\mathcal{O}}\\left( \\frac{d}{T} \\sqrt{ \\sum_{t=1}^T \\frac{1}{|S_t|}}\n\\right)$, where $T$ is the total number of rounds, $d$ is the feature\ndimension, and $|S_t|$ is the size of the subset at round $t$. This result\nshows that larger subsets directly lead to improved performance and, notably,\nthe bound avoids the exponential dependence on the unknown parameter's norm,\nwhich was a fundamental limitation in most previous works. Moreover, we\nestablish a near-matching lower bound of $\\Omega \\left( \\frac{d}{K \\sqrt{T}}\n\\right)$, where $K$ is the maximum subset size. To the best of our knowledge,\nthis is the first theoretical result in PbRL with ranking feedback that\nexplicitly shows improved sample efficiency as a function of the subset size."
                },
                "authors": [
                    {
                        "name": "Joongkyu Lee"
                    },
                    {
                        "name": "Seouh-won Yi"
                    },
                    {
                        "name": "Min-hwan Oh"
                    }
                ],
                "author_detail": {
                    "name": "Min-hwan Oh"
                },
                "author": "Min-hwan Oh",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12201v2",
                "updated": "2025-10-21T15:09:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    9,
                    28,
                    1,
                    294,
                    0
                ],
                "published": "2025-07-16T12:55:58Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    55,
                    58,
                    2,
                    197,
                    0
                ],
                "title": "RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and\n  Reducing Hallucination in Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and\n  Reducing Hallucination in Generative Models"
                },
                "summary": "Diffusion models have achieved state-of-the-art performance in generative\nmodeling, yet their sampling procedures remain vulnerable to\nhallucinations-often stemming from inaccuracies in score approximation. In this\nwork, we reinterpret diffusion sampling through the lens of optimization and\nintroduce RODS (Robust Optimization-inspired Diffusion Sampler), a novel method\nthat detects and corrects high-risk sampling steps using geometric cues from\nthe loss landscape. RODS enforces smoother sampling trajectories and adaptively\nadjusts perturbations, reducing hallucinations without retraining and at\nminimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands\ndemonstrate that RODS maintains comparable image quality and preserves\ngeneration diversity. More importantly, it improves both sampling fidelity and\nrobustness, detecting over 70% of hallucinated samples and correcting more than\n25%, all while avoiding the introduction of new artifacts. We release our code\nat https://github.com/Yiqi-Verna-Tian/RODS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved state-of-the-art performance in generative\nmodeling, yet their sampling procedures remain vulnerable to\nhallucinations-often stemming from inaccuracies in score approximation. In this\nwork, we reinterpret diffusion sampling through the lens of optimization and\nintroduce RODS (Robust Optimization-inspired Diffusion Sampler), a novel method\nthat detects and corrects high-risk sampling steps using geometric cues from\nthe loss landscape. RODS enforces smoother sampling trajectories and adaptively\nadjusts perturbations, reducing hallucinations without retraining and at\nminimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands\ndemonstrate that RODS maintains comparable image quality and preserves\ngeneration diversity. More importantly, it improves both sampling fidelity and\nrobustness, detecting over 70% of hallucinated samples and correcting more than\n25%, all while avoiding the introduction of new artifacts. We release our code\nat https://github.com/Yiqi-Verna-Tian/RODS."
                },
                "authors": [
                    {
                        "name": "Yiqi Tian"
                    },
                    {
                        "name": "Pengfei Jin"
                    },
                    {
                        "name": "Mingze Yuan"
                    },
                    {
                        "name": "Na Li"
                    },
                    {
                        "name": "Bo Zeng"
                    },
                    {
                        "name": "Quanzheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Quanzheng Li"
                },
                "author": "Quanzheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00198v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00198v3",
                "updated": "2025-10-21T15:02:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    2,
                    31,
                    1,
                    294,
                    0
                ],
                "published": "2025-01-31T22:27:34Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    22,
                    27,
                    34,
                    4,
                    31,
                    0
                ],
                "title": "Fairshare Data Pricing via Data Valuation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairshare Data Pricing via Data Valuation for Large Language Models"
                },
                "summary": "Training data is the backbone of large language models (LLMs), yet today's\ndata markets often operate under exploitative pricing -- sourcing data from\nmarginalized groups with little pay or recognition. This paper introduces a\ntheoretical framework for LLM data markets, modeling the strategic interactions\nbetween buyers (LLM builders) and sellers (human annotators). We begin with\ntheoretical and empirical analysis showing how exploitative pricing drives\nhigh-quality sellers out of the market, degrading data quality and long-term\nmodel performance. Then we introduce fairshare, a pricing mechanism grounded in\ndata valuation that quantifies each data's contribution. It aligns incentives\nby sustaining seller participation and optimizing utility for both buyers and\nsellers. Theoretically, we show that fairshare yields mutually optimal\noutcomes: maximizing long-term buyer utility and seller profit while sustaining\nmarket participation. Empirically when training open-source LLMs on complex NLP\ntasks, including math problems, medical diagnosis, and physical reasoning,\nfairshare boosts seller earnings and ensures a stable supply of high-quality\ndata, while improving buyers' performance-per-dollar and long-term welfare. Our\nfindings offer a concrete path toward fair, transparent, and economically\nsustainable data markets for LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training data is the backbone of large language models (LLMs), yet today's\ndata markets often operate under exploitative pricing -- sourcing data from\nmarginalized groups with little pay or recognition. This paper introduces a\ntheoretical framework for LLM data markets, modeling the strategic interactions\nbetween buyers (LLM builders) and sellers (human annotators). We begin with\ntheoretical and empirical analysis showing how exploitative pricing drives\nhigh-quality sellers out of the market, degrading data quality and long-term\nmodel performance. Then we introduce fairshare, a pricing mechanism grounded in\ndata valuation that quantifies each data's contribution. It aligns incentives\nby sustaining seller participation and optimizing utility for both buyers and\nsellers. Theoretically, we show that fairshare yields mutually optimal\noutcomes: maximizing long-term buyer utility and seller profit while sustaining\nmarket participation. Empirically when training open-source LLMs on complex NLP\ntasks, including math problems, medical diagnosis, and physical reasoning,\nfairshare boosts seller earnings and ensures a stable supply of high-quality\ndata, while improving buyers' performance-per-dollar and long-term welfare. Our\nfindings offer a concrete path toward fair, transparent, and economically\nsustainable data markets for LLM."
                },
                "authors": [
                    {
                        "name": "Luyang Zhang"
                    },
                    {
                        "name": "Cathy Jiao"
                    },
                    {
                        "name": "Beibei Li"
                    },
                    {
                        "name": "Chenyan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Chenyan Xiong"
                },
                "author": "Chenyan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00198v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00198v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16551v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16551v3",
                "updated": "2025-10-22T12:15:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    15,
                    26,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-18T15:46:11Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    15,
                    46,
                    11,
                    5,
                    291,
                    0
                ],
                "title": "From Reviews to Actionable Insights: An LLM-Based Approach for Attribute\n  and Feature Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reviews to Actionable Insights: An LLM-Based Approach for Attribute\n  and Feature Extraction"
                },
                "summary": "This research proposes a systematic, large language model (LLM) approach for\nextracting product and service attributes, features, and associated sentiments\nfrom customer reviews. Grounded in marketing theory, the framework\ndistinguishes perceptual attributes from actionable features, producing\ninterpretable and managerially actionable insights. We apply the methodology to\n20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a\nrandom subset of reviews. Model performance is assessed through agreement with\nhuman annotations and predictive validity for customer ratings. Results show\nhigh consistency between LLMs and human coders and strong predictive validity,\nconfirming the reliability of the approach. Human coders required a median of\nsix minutes per review, whereas the LLM processed each in two seconds,\ndelivering comparable insights at a scale unattainable through manual coding.\nManagerially, the analysis identifies attributes and features that most\nstrongly influence customer satisfaction and their associated sentiments,\nenabling firms to pinpoint \"joy points,\" address \"pain points,\" and design\ntargeted interventions. We demonstrate how structured review data can power an\nactionable marketing dashboard that tracks sentiment over time and across\nstores, benchmarks performance, and highlights high-leverage features for\nimprovement. Simulations indicate that enhancing sentiment for key service\nfeatures could yield 1-2% average revenue gains per store.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research proposes a systematic, large language model (LLM) approach for\nextracting product and service attributes, features, and associated sentiments\nfrom customer reviews. Grounded in marketing theory, the framework\ndistinguishes perceptual attributes from actionable features, producing\ninterpretable and managerially actionable insights. We apply the methodology to\n20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a\nrandom subset of reviews. Model performance is assessed through agreement with\nhuman annotations and predictive validity for customer ratings. Results show\nhigh consistency between LLMs and human coders and strong predictive validity,\nconfirming the reliability of the approach. Human coders required a median of\nsix minutes per review, whereas the LLM processed each in two seconds,\ndelivering comparable insights at a scale unattainable through manual coding.\nManagerially, the analysis identifies attributes and features that most\nstrongly influence customer satisfaction and their associated sentiments,\nenabling firms to pinpoint \"joy points,\" address \"pain points,\" and design\ntargeted interventions. We demonstrate how structured review data can power an\nactionable marketing dashboard that tracks sentiment over time and across\nstores, benchmarks performance, and highlights high-leverage features for\nimprovement. Simulations indicate that enhancing sentiment for key service\nfeatures could yield 1-2% average revenue gains per store."
                },
                "authors": [
                    {
                        "name": "Khaled Boughanmi"
                    },
                    {
                        "name": "Kamel Jedidi"
                    },
                    {
                        "name": "Nour Jedidi"
                    }
                ],
                "author_detail": {
                    "name": "Nour Jedidi"
                },
                "author": "Nour Jedidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16551v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16551v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18699v1",
                "updated": "2025-10-21T14:53:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    53,
                    56,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:53:56Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    53,
                    56,
                    1,
                    294,
                    0
                ],
                "title": "Fetch.ai: An Architecture for Modern Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fetch.ai: An Architecture for Modern Multi-Agent Systems"
                },
                "summary": "Recent surges in LLM-driven intelligent systems largely overlook decades of\nfoundational multi-agent systems (MAS) research, resulting in frameworks with\ncritical limitations such as centralization and inadequate trust and\ncommunication protocols. This paper introduces the Fetch.ai architecture, an\nindustrial-strength platform designed to bridge this gap by facilitating the\nintegration of classical MAS principles with modern AI capabilities. We present\na novel, multi-layered solution built on a decentralized foundation of on-chain\nblockchain services for verifiable identity, discovery, and transactions. This\nis complemented by a comprehensive development framework for creating secure,\ninteroperable agents, a cloud-based platform for deployment, and an intelligent\norchestration layer where an agent-native LLM translates high-level human goals\ninto complex, multi-agent workflows. We demonstrate the deployed nature of this\nsystem through a decentralized logistics use case where autonomous agents\ndynamically discover, negotiate, and transact with one another securely.\nUltimately, the Fetch.ai stack provides a principled architecture for moving\nbeyond current agent implementations towards open, collaborative, and\neconomically sustainable multi-agent ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent surges in LLM-driven intelligent systems largely overlook decades of\nfoundational multi-agent systems (MAS) research, resulting in frameworks with\ncritical limitations such as centralization and inadequate trust and\ncommunication protocols. This paper introduces the Fetch.ai architecture, an\nindustrial-strength platform designed to bridge this gap by facilitating the\nintegration of classical MAS principles with modern AI capabilities. We present\na novel, multi-layered solution built on a decentralized foundation of on-chain\nblockchain services for verifiable identity, discovery, and transactions. This\nis complemented by a comprehensive development framework for creating secure,\ninteroperable agents, a cloud-based platform for deployment, and an intelligent\norchestration layer where an agent-native LLM translates high-level human goals\ninto complex, multi-agent workflows. We demonstrate the deployed nature of this\nsystem through a decentralized logistics use case where autonomous agents\ndynamically discover, negotiate, and transact with one another securely.\nUltimately, the Fetch.ai stack provides a principled architecture for moving\nbeyond current agent implementations towards open, collaborative, and\neconomically sustainable multi-agent ecosystems."
                },
                "authors": [
                    {
                        "name": "Michael J. Wooldridge"
                    },
                    {
                        "name": "Attila Bagoly"
                    },
                    {
                        "name": "Jonathan J. Ward"
                    },
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Gabriel Paludo Licks"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Paludo Licks"
                },
                "author": "Gabriel Paludo Licks",
                "arxiv_comment": "26 pages, figures, code examples",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22205v2",
                "updated": "2025-10-21T14:52:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    52,
                    24,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-26T11:16:56Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    11,
                    16,
                    56,
                    4,
                    269,
                    0
                ],
                "title": "From Watch to Imagine: Steering Long-horizon Manipulation via Human\n  Demonstration and Future Envisionment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Watch to Imagine: Steering Long-horizon Manipulation via Human\n  Demonstration and Future Envisionment"
                },
                "summary": "Generalizing to long-horizon manipulation tasks in a zero-shot setting\nremains a central challenge in robotics. Current multimodal foundation based\napproaches, despite their capabilities, typically fail to decompose high-level\ncommands into executable action sequences from static visual input alone. To\naddress this challenge, we introduce Super-Mimic, a hierarchical framework that\nenables zero-shot robotic imitation by directly inferring procedural intent\nfrom unscripted human demonstration videos. Our framework is composed of two\nsequential modules. First, a Human Intent Translator (HIT) parses the input\nvideo using multimodal reasoning to produce a sequence of language-grounded\nsubtasks. These subtasks then condition a Future Dynamics Predictor (FDP),\nwhich employs a generative model that synthesizes a physically plausible video\nrollout for each step. The resulting visual trajectories are dynamics-aware,\nexplicitly modeling crucial object interactions and contact points to guide the\nlow-level controller. We validate this approach through extensive experiments\non a suite of long-horizon manipulation tasks, where Super-Mimic significantly\noutperforms state-of-the-art zero-shot methods by over 20%. These results\nestablish that coupling video-driven intent parsing with prospective dynamics\nmodeling is a highly effective strategy for developing general-purpose robotic\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing to long-horizon manipulation tasks in a zero-shot setting\nremains a central challenge in robotics. Current multimodal foundation based\napproaches, despite their capabilities, typically fail to decompose high-level\ncommands into executable action sequences from static visual input alone. To\naddress this challenge, we introduce Super-Mimic, a hierarchical framework that\nenables zero-shot robotic imitation by directly inferring procedural intent\nfrom unscripted human demonstration videos. Our framework is composed of two\nsequential modules. First, a Human Intent Translator (HIT) parses the input\nvideo using multimodal reasoning to produce a sequence of language-grounded\nsubtasks. These subtasks then condition a Future Dynamics Predictor (FDP),\nwhich employs a generative model that synthesizes a physically plausible video\nrollout for each step. The resulting visual trajectories are dynamics-aware,\nexplicitly modeling crucial object interactions and contact points to guide the\nlow-level controller. We validate this approach through extensive experiments\non a suite of long-horizon manipulation tasks, where Super-Mimic significantly\noutperforms state-of-the-art zero-shot methods by over 20%. These results\nestablish that coupling video-driven intent parsing with prospective dynamics\nmodeling is a highly effective strategy for developing general-purpose robotic\nsystems."
                },
                "authors": [
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Jiaming Zhou"
                    },
                    {
                        "name": "Yuanfeng Qiu"
                    },
                    {
                        "name": "Jiayi Liu"
                    },
                    {
                        "name": "Shihui Zhou"
                    },
                    {
                        "name": "Kun-Yu Lin"
                    },
                    {
                        "name": "Junwei Liang"
                    }
                ],
                "author_detail": {
                    "name": "Junwei Liang"
                },
                "author": "Junwei Liang",
                "arxiv_comment": "More details and videos can be found at:\n  https://yipko.com/super-mimic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04710v2",
                "updated": "2025-10-21T14:52:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    52,
                    14,
                    1,
                    294,
                    0
                ],
                "published": "2025-05-07T18:02:18Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    18,
                    2,
                    18,
                    2,
                    127,
                    0
                ],
                "title": "Exploring Influence Factors on LLM Suitability for No-Code Development\n  of End User IoT Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Influence Factors on LLM Suitability for No-Code Development\n  of End User IoT Applications"
                },
                "summary": "No-Code Development Platforms (NCDPs) empower non-technical end users to\nbuild applications tailored to their specific demands without writing code.\nWhile NCDPs lower technical barriers, users still require some technical\nknowledge, e.g., to structure process steps or define event-action rules. Large\nLanguage Models (LLMs) offer a promising solution to further reduce technical\nrequirements by supporting natural language interaction and dynamic code\ngeneration. By integrating LLM, NCDPs can be more accessible to non-technical\nusers, enabling application development truly without requiring any technical\nexpertise.\n  Despite growing interest in LLM-powered NCDPs, a systematic investigation\ninto the factors influencing LLM suitability and performance remains absent.\nUnderstanding these factors is critical to effectively leveraging LLMs\ncapabilities and maximizing their impact. In this paper, we investigate key\nfactors influencing the effectiveness of LLMs in supporting end-user\napplication development within NCDPs. By conducting comprehensive experiments,\nwe evaluate the impact of four key factors, i.e., model selection, prompt\nlanguage, training data background, and an error-informed few-shot setup, on\nthe quality of generated applications. Specifically, we selected a range of\nLLMs based on their architecture, scale, design focus, and training data, and\nevaluated them across four real-world smart home automation scenarios\nimplemented on a representative open-source LLM-powered NCDP. Our findings\noffer practical insights into how LLMs can be effectively integrated into\nNCDPs, informing both platform design and the selection of suitable LLMs for\nend-user application development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No-Code Development Platforms (NCDPs) empower non-technical end users to\nbuild applications tailored to their specific demands without writing code.\nWhile NCDPs lower technical barriers, users still require some technical\nknowledge, e.g., to structure process steps or define event-action rules. Large\nLanguage Models (LLMs) offer a promising solution to further reduce technical\nrequirements by supporting natural language interaction and dynamic code\ngeneration. By integrating LLM, NCDPs can be more accessible to non-technical\nusers, enabling application development truly without requiring any technical\nexpertise.\n  Despite growing interest in LLM-powered NCDPs, a systematic investigation\ninto the factors influencing LLM suitability and performance remains absent.\nUnderstanding these factors is critical to effectively leveraging LLMs\ncapabilities and maximizing their impact. In this paper, we investigate key\nfactors influencing the effectiveness of LLMs in supporting end-user\napplication development within NCDPs. By conducting comprehensive experiments,\nwe evaluate the impact of four key factors, i.e., model selection, prompt\nlanguage, training data background, and an error-informed few-shot setup, on\nthe quality of generated applications. Specifically, we selected a range of\nLLMs based on their architecture, scale, design focus, and training data, and\nevaluated them across four real-world smart home automation scenarios\nimplemented on a representative open-source LLM-powered NCDP. Our findings\noffer practical insights into how LLMs can be effectively integrated into\nNCDPs, informing both platform design and the selection of suitable LLMs for\nend-user application development."
                },
                "authors": [
                    {
                        "name": "Minghe Wang"
                    },
                    {
                        "name": "Alexandra Kapp"
                    },
                    {
                        "name": "Trever Schirmer"
                    },
                    {
                        "name": "Tobias Pfandzelter"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "arxiv_doi": "10.1002/spe.70027",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/spe.70027",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.04710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18691v1",
                "updated": "2025-10-21T14:50:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    50,
                    24,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:50:24Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    50,
                    24,
                    1,
                    294,
                    0
                ],
                "title": "Investigating LLM Capabilities on Long Context Comprehension for Medical\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating LLM Capabilities on Long Context Comprehension for Medical\n  Question Answering"
                },
                "summary": "This study is the first to investigate LLM comprehension capabilities over\nlong-context (LC) medical QA of clinical relevance. Our comprehensive\nassessment spans a range of content-inclusion settings based on their\nrelevance, LLM models of varying capabilities and datasets across task\nformulations, revealing insights on model size effects, limitations, underlying\nmemorization issues and the benefits of reasoning models. Importantly, we\nexamine the effect of RAG on medical LC comprehension, uncover best settings in\nsingle versus multi-document reasoning datasets and showcase RAG strategies for\nimprovements over LC. We shed light into some of the evaluation aspects using a\nmulti-faceted approach. Our qualitative and error analyses address open\nquestions on when RAG is beneficial over LC, revealing common failure cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study is the first to investigate LLM comprehension capabilities over\nlong-context (LC) medical QA of clinical relevance. Our comprehensive\nassessment spans a range of content-inclusion settings based on their\nrelevance, LLM models of varying capabilities and datasets across task\nformulations, revealing insights on model size effects, limitations, underlying\nmemorization issues and the benefits of reasoning models. Importantly, we\nexamine the effect of RAG on medical LC comprehension, uncover best settings in\nsingle versus multi-document reasoning datasets and showcase RAG strategies for\nimprovements over LC. We shed light into some of the evaluation aspects using a\nmulti-faceted approach. Our qualitative and error analyses address open\nquestions on when RAG is beneficial over LC, revealing common failure cases."
                },
                "authors": [
                    {
                        "name": "Feras AlMannaa"
                    },
                    {
                        "name": "Talia Tseriotou"
                    },
                    {
                        "name": "Jenny Chim"
                    },
                    {
                        "name": "Maria Liakata"
                    }
                ],
                "author_detail": {
                    "name": "Maria Liakata"
                },
                "author": "Maria Liakata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10919v2",
                "updated": "2025-10-21T14:46:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    46,
                    28,
                    1,
                    294,
                    0
                ],
                "published": "2025-05-16T06:47:00Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    6,
                    47,
                    0,
                    4,
                    136,
                    0
                ],
                "title": "A Physics-Informed Spatiotemporal Deep Learning Framework for Turbulent\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Physics-Informed Spatiotemporal Deep Learning Framework for Turbulent\n  Systems"
                },
                "summary": "Fluid thermodynamics underpins atmospheric dynamics, climate science,\nindustrial applications, and energy systems. However, direct numerical\nsimulations (DNS) of such systems can be computationally prohibitive. To\naddress this, we present a novel physics-informed spatiotemporal surrogate\nmodel for Rayleigh-B\\'enard convection (RBC), a canonical example of convective\nfluid flow. Our approach combines convolutional neural networks, for spatial\ndimension reduction, with an innovative recurrent architecture, inspired by\nlarge language models, to model long-range temporal dynamics. Inference is\npenalized with respect to the governing partial differential equations to\nensure physical interpretability. Since RBC exhibits turbulent behavior, we\nquantify uncertainty using a conformal prediction framework. This model\nreplicates key physical features of RBC dynamics while significantly reducing\ncomputational cost, offering a scalable alternative to DNS for long-term\nsimulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluid thermodynamics underpins atmospheric dynamics, climate science,\nindustrial applications, and energy systems. However, direct numerical\nsimulations (DNS) of such systems can be computationally prohibitive. To\naddress this, we present a novel physics-informed spatiotemporal surrogate\nmodel for Rayleigh-B\\'enard convection (RBC), a canonical example of convective\nfluid flow. Our approach combines convolutional neural networks, for spatial\ndimension reduction, with an innovative recurrent architecture, inspired by\nlarge language models, to model long-range temporal dynamics. Inference is\npenalized with respect to the governing partial differential equations to\nensure physical interpretability. Since RBC exhibits turbulent behavior, we\nquantify uncertainty using a conformal prediction framework. This model\nreplicates key physical features of RBC dynamics while significantly reducing\ncomputational cost, offering a scalable alternative to DNS for long-term\nsimulations."
                },
                "authors": [
                    {
                        "name": "Luca Menicali"
                    },
                    {
                        "name": "Andrew Grace"
                    },
                    {
                        "name": "David H. Richter"
                    },
                    {
                        "name": "Stefano Castruccio"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Castruccio"
                },
                "author": "Stefano Castruccio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10025v2",
                "updated": "2025-10-21T14:44:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    44,
                    14,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-11T05:05:21Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    5,
                    5,
                    21,
                    5,
                    284,
                    0
                ],
                "title": "Lightweight Baselines for Medical Abstract Classification: DistilBERT\n  with Cross-Entropy as a Strong Default",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Baselines for Medical Abstract Classification: DistilBERT\n  with Cross-Entropy as a Strong Default"
                },
                "summary": "The research evaluates lightweight medical abstract classification methods to\nestablish their maximum performance capabilities under financial budget\nrestrictions. On the public medical abstracts corpus, we finetune BERT base and\nDistil BERT with three objectives cross entropy (CE), class weighted CE, and\nfocal loss under identical tokenization, sequence length, optimizer, and\nschedule. DistilBERT with plain CE gives the strongest raw argmax trade off,\nwhile a post hoc operating point selection (validation calibrated, classwise\nthresholds) sub stantially improves deployed performance; under this tuned\nregime, focal benefits most. We report Accuracy, Macro F1, and WeightedF1,\nrelease evaluation artifacts, and include confusion analyses to clarify error\nstructure. The practical takeaway is to start with a compact encoder and CE,\nthen add lightweight calibration or thresholding when deployment requires\nhigher macro balance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The research evaluates lightweight medical abstract classification methods to\nestablish their maximum performance capabilities under financial budget\nrestrictions. On the public medical abstracts corpus, we finetune BERT base and\nDistil BERT with three objectives cross entropy (CE), class weighted CE, and\nfocal loss under identical tokenization, sequence length, optimizer, and\nschedule. DistilBERT with plain CE gives the strongest raw argmax trade off,\nwhile a post hoc operating point selection (validation calibrated, classwise\nthresholds) sub stantially improves deployed performance; under this tuned\nregime, focal benefits most. We report Accuracy, Macro F1, and WeightedF1,\nrelease evaluation artifacts, and include confusion analyses to clarify error\nstructure. The practical takeaway is to start with a compact encoder and CE,\nthen add lightweight calibration or thresholding when deployment requires\nhigher macro balance."
                },
                "authors": [
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Tong Wang"
                    },
                    {
                        "name": "Su Liu"
                    },
                    {
                        "name": "Xin Hu"
                    },
                    {
                        "name": "Ran Tong"
                    },
                    {
                        "name": "Lanruo Wang"
                    },
                    {
                        "name": "Jiexi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jiexi Xu"
                },
                "author": "Jiexi Xu",
                "arxiv_comment": "Healthcare AI, Medical Text Classification,LLM, DistilBERT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16122v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16122v3",
                "updated": "2025-10-21T14:28:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    28,
                    6,
                    1,
                    294,
                    0
                ],
                "published": "2025-08-22T06:29:29Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    29,
                    29,
                    4,
                    234,
                    0
                ],
                "title": "Text Takes Over: A Study of Modality Bias in Multimodal Intent Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Takes Over: A Study of Modality Bias in Multimodal Intent Detection"
                },
                "summary": "The rise of multimodal data, integrating text, audio, and visuals, has\ncreated new opportunities for studying multimodal tasks such as intent\ndetection. This work investigates the effectiveness of Large Language Models\n(LLMs) and non-LLMs, including text-only and multi-modal models, in the\nmultimodal intent detection task. Our study reveals that Mistral-7B, a\ntext-only LLM, outperforms most competitive multimodal models by approximately\n9% on MIntRec-1 and 4% on MIntRec2.0 datasets. This performance advantage comes\nfrom a strong textual bias in these datasets, where over 90% of the samples\nrequire textual input, either alone or in combination with other modalities,\nfor correct classification. We confirm the modality bias of these datasets via\nhuman evaluation, too. Next, we propose a framework to debias the datasets, and\nupon debiasing, more than 70% of the samples in MIntRec-1 and more than 50% in\nMIntRec2.0 get removed, resulting in significant performance degradation across\nall models, with smaller multimodal fusion models being the most affected with\nan accuracy drop of over 50 - 60%. Further, we analyze the context-specific\nrelevance of different modalities through empirical analysis. Our findings\nhighlight the challenges posed by modality bias in multimodal intent datasets\nand emphasize the need for unbiased datasets to evaluate multimodal models\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of multimodal data, integrating text, audio, and visuals, has\ncreated new opportunities for studying multimodal tasks such as intent\ndetection. This work investigates the effectiveness of Large Language Models\n(LLMs) and non-LLMs, including text-only and multi-modal models, in the\nmultimodal intent detection task. Our study reveals that Mistral-7B, a\ntext-only LLM, outperforms most competitive multimodal models by approximately\n9% on MIntRec-1 and 4% on MIntRec2.0 datasets. This performance advantage comes\nfrom a strong textual bias in these datasets, where over 90% of the samples\nrequire textual input, either alone or in combination with other modalities,\nfor correct classification. We confirm the modality bias of these datasets via\nhuman evaluation, too. Next, we propose a framework to debias the datasets, and\nupon debiasing, more than 70% of the samples in MIntRec-1 and more than 50% in\nMIntRec2.0 get removed, resulting in significant performance degradation across\nall models, with smaller multimodal fusion models being the most affected with\nan accuracy drop of over 50 - 60%. Further, we analyze the context-specific\nrelevance of different modalities through empirical analysis. Our findings\nhighlight the challenges posed by modality bias in multimodal intent datasets\nand emphasize the need for unbiased datasets to evaluate multimodal models\neffectively."
                },
                "authors": [
                    {
                        "name": "Ankan Mullick"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Abhik Jana"
                    },
                    {
                        "name": "Pawan Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Pawan Goyal"
                },
                "author": "Pawan Goyal",
                "arxiv_comment": "EMNLP 2025 Main Conference Full Paper",
                "arxiv_journal_ref": "EMNLP 2025 Main Conference Full Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16122v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16122v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18674v1",
                "updated": "2025-10-21T14:27:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    27,
                    48,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:27:48Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    27,
                    48,
                    1,
                    294,
                    0
                ],
                "title": "Exploring Membership Inference Vulnerabilities in Clinical Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Membership Inference Vulnerabilities in Clinical Large\n  Language Models"
                },
                "summary": "As large language models (LLMs) become progressively more embedded in\nclinical decision-support, documentation, and patient-information systems,\nensuring their privacy and trustworthiness has emerged as an imperative\nchallenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic\nhealth record (EHR) data improves domain alignment but also raises the risk of\nexposing patient information through model behaviors. In this work-in-progress,\nwe present an exploratory empirical study on membership inference\nvulnerabilities in clinical LLMs, focusing on whether adversaries can infer if\nspecific patient records were used during model training. Using a\nstate-of-the-art clinical question-answering model, Llemr, we evaluate both\ncanonical loss-based attacks and a domain-motivated paraphrasing-based\nperturbation strategy that more realistically reflects clinical adversarial\nconditions. Our preliminary findings reveal limited but measurable membership\nleakage, suggesting that current clinical LLMs provide partial resistance yet\nremain susceptible to subtle privacy risks that could undermine trust in\nclinical AI adoption. These results motivate continued development of\ncontext-aware, domain-specific privacy evaluations and defenses such as\ndifferential privacy fine-tuning and paraphrase-aware training, to strengthen\nthe security and trustworthiness of healthcare AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become progressively more embedded in\nclinical decision-support, documentation, and patient-information systems,\nensuring their privacy and trustworthiness has emerged as an imperative\nchallenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic\nhealth record (EHR) data improves domain alignment but also raises the risk of\nexposing patient information through model behaviors. In this work-in-progress,\nwe present an exploratory empirical study on membership inference\nvulnerabilities in clinical LLMs, focusing on whether adversaries can infer if\nspecific patient records were used during model training. Using a\nstate-of-the-art clinical question-answering model, Llemr, we evaluate both\ncanonical loss-based attacks and a domain-motivated paraphrasing-based\nperturbation strategy that more realistically reflects clinical adversarial\nconditions. Our preliminary findings reveal limited but measurable membership\nleakage, suggesting that current clinical LLMs provide partial resistance yet\nremain susceptible to subtle privacy risks that could undermine trust in\nclinical AI adoption. These results motivate continued development of\ncontext-aware, domain-specific privacy evaluations and defenses such as\ndifferential privacy fine-tuning and paraphrase-aware training, to strengthen\nthe security and trustworthiness of healthcare AI systems."
                },
                "authors": [
                    {
                        "name": "Alexander Nemecek"
                    },
                    {
                        "name": "Zebin Yun"
                    },
                    {
                        "name": "Zahra Rahmani"
                    },
                    {
                        "name": "Yaniv Harel"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Mahmood Sharif"
                    },
                    {
                        "name": "Erman Ayday"
                    }
                ],
                "author_detail": {
                    "name": "Erman Ayday"
                },
                "author": "Erman Ayday",
                "arxiv_comment": "Accepted at the 1st IEEE Workshop on Healthcare and Medical Device\n  Security, Privacy, Resilience, and Trust (IEEE HMD-SPiRiT)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18672v1",
                "updated": "2025-10-21T14:25:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:25:51Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study"
                },
                "summary": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving."
                },
                "authors": [
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Junpan Wu"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18668v1",
                "updated": "2025-10-21T14:23:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    23,
                    20,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:23:20Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    23,
                    20,
                    1,
                    294,
                    0
                ],
                "title": "Prototyping an End-to-End Multi-Modal Tiny-CNN for Cardiovascular Sensor\n  Patches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prototyping an End-to-End Multi-Modal Tiny-CNN for Cardiovascular Sensor\n  Patches"
                },
                "summary": "The vast majority of cardiovascular diseases may be preventable if early\nsigns and risk factors are detected. Cardiovascular monitoring with body-worn\nsensor devices like sensor patches allows for the detection of such signs while\npreserving the freedom and comfort of patients. However, the analysis of the\nsensor data must be robust, reliable, efficient, and highly accurate. Deep\nlearning methods can automate data interpretation, reducing the workload of\nclinicians. In this work, we analyze the feasibility of applying deep learning\nmodels to the classification of synchronized electrocardiogram (ECG) and\nphonocardiogram (PCG) recordings on resource-constrained medical edge devices.\nWe propose a convolutional neural network with early fusion of data to solve a\nbinary classification problem. We train and validate our model on the\nsynchronized ECG and PCG recordings from the Physionet Challenge 2016 dataset.\nOur approach reduces memory footprint and compute cost by three orders of\nmagnitude compared to the state-of-the-art while maintaining competitive\naccuracy. We demonstrate the applicability of our proposed model on medical\nedge devices by analyzing energy consumption on a microcontroller and an\nexperimental sensor device setup, confirming that on-device inference can be\nmore energy-efficient than continuous data streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vast majority of cardiovascular diseases may be preventable if early\nsigns and risk factors are detected. Cardiovascular monitoring with body-worn\nsensor devices like sensor patches allows for the detection of such signs while\npreserving the freedom and comfort of patients. However, the analysis of the\nsensor data must be robust, reliable, efficient, and highly accurate. Deep\nlearning methods can automate data interpretation, reducing the workload of\nclinicians. In this work, we analyze the feasibility of applying deep learning\nmodels to the classification of synchronized electrocardiogram (ECG) and\nphonocardiogram (PCG) recordings on resource-constrained medical edge devices.\nWe propose a convolutional neural network with early fusion of data to solve a\nbinary classification problem. We train and validate our model on the\nsynchronized ECG and PCG recordings from the Physionet Challenge 2016 dataset.\nOur approach reduces memory footprint and compute cost by three orders of\nmagnitude compared to the state-of-the-art while maintaining competitive\naccuracy. We demonstrate the applicability of our proposed model on medical\nedge devices by analyzing energy consumption on a microcontroller and an\nexperimental sensor device setup, confirming that on-device inference can be\nmore energy-efficient than continuous data streaming."
                },
                "authors": [
                    {
                        "name": "Mustafa Fuad Rifet Ibrahim"
                    },
                    {
                        "name": "Tunc Alkanat"
                    },
                    {
                        "name": "Maurice Meijer"
                    },
                    {
                        "name": "Felix Manthey"
                    },
                    {
                        "name": "Alexander Schlaefer"
                    },
                    {
                        "name": "Peer Stelldinger"
                    }
                ],
                "author_detail": {
                    "name": "Peer Stelldinger"
                },
                "author": "Peer Stelldinger",
                "arxiv_comment": "Submitted to the IEEE Journal of Biomedical And Health Informatics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08049v2",
                "updated": "2025-10-21T14:21:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    21,
                    25,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-09T10:35:31Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    10,
                    35,
                    31,
                    3,
                    282,
                    0
                ],
                "title": "A Survey of Process Reward Models: From Outcome Signals to Process\n  Supervisions for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Process Reward Models: From Outcome Signals to Process\n  Supervisions for Large Language Models"
                },
                "summary": "Although Large Language Models (LLMs) exhibit advanced reasoning ability,\nconventional alignment remains largely dominated by outcome reward models\n(ORMs) that judge only final answers. Process Reward Models(PRMs) address this\ngap by evaluating and guiding reasoning at the step or trajectory level. This\nsurvey provides a systematic overview of PRMs through the full loop: how to\ngenerate process data, build PRMs, and use PRMs for test-time scaling and\nreinforcement learning. We summarize applications across math, code, text,\nmultimodal reasoning, robotics, and agents, and review emerging benchmarks. Our\ngoal is to clarify design spaces, reveal open challenges, and guide future\nresearch toward fine-grained, robust reasoning alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) exhibit advanced reasoning ability,\nconventional alignment remains largely dominated by outcome reward models\n(ORMs) that judge only final answers. Process Reward Models(PRMs) address this\ngap by evaluating and guiding reasoning at the step or trajectory level. This\nsurvey provides a systematic overview of PRMs through the full loop: how to\ngenerate process data, build PRMs, and use PRMs for test-time scaling and\nreinforcement learning. We summarize applications across math, code, text,\nmultimodal reasoning, robotics, and agents, and review emerging benchmarks. Our\ngoal is to clarify design spaces, reveal open challenges, and guide future\nresearch toward fine-grained, robust reasoning alignment."
                },
                "authors": [
                    {
                        "name": "Congming Zheng"
                    },
                    {
                        "name": "Jiachen Zhu"
                    },
                    {
                        "name": "Zhuoying Ou"
                    },
                    {
                        "name": "Yuxiang Chen"
                    },
                    {
                        "name": "Kangning Zhang"
                    },
                    {
                        "name": "Rong Shan"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09049v2",
                "updated": "2025-10-21T14:14:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    14,
                    49,
                    1,
                    294,
                    0
                ],
                "published": "2025-06-10T17:59:44Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    44,
                    1,
                    161,
                    0
                ],
                "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement\n  Learning"
                },
                "summary": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems."
                },
                "authors": [
                    {
                        "name": "Li Kang"
                    },
                    {
                        "name": "Xiufeng Song"
                    },
                    {
                        "name": "Heng Zhou"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Xiaohong Liu"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Zhenfei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Zhenfei Yin"
                },
                "author": "Zhenfei Yin",
                "arxiv_comment": "Project page: https://faceong.github.io/VIKI-R/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18654v1",
                "updated": "2025-10-21T14:03:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    3,
                    35,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:03:35Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    3,
                    35,
                    1,
                    294,
                    0
                ],
                "title": "Differentially Private E-Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private E-Values"
                },
                "summary": "E-values have gained prominence as flexible tools for statistical inference\nand risk control, enabling anytime- and post-hoc-valid procedures under minimal\nassumptions. However, many real-world applications fundamentally rely on\nsensitive data, which can be leaked through e-values. To ensure their safe\nrelease, we propose a general framework to transform non-private e-values into\ndifferentially private ones. Towards this end, we develop a novel biased\nmultiplicative noise mechanism that ensures our e-values remain statistically\nvalid. We show that our differentially private e-values attain strong\nstatistical power, and are asymptotically as powerful as their non-private\ncounterparts. Experiments across online risk monitoring, private healthcare,\nand conformal e-prediction demonstrate our approach's effectiveness and\nillustrate its broad applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-values have gained prominence as flexible tools for statistical inference\nand risk control, enabling anytime- and post-hoc-valid procedures under minimal\nassumptions. However, many real-world applications fundamentally rely on\nsensitive data, which can be leaked through e-values. To ensure their safe\nrelease, we propose a general framework to transform non-private e-values into\ndifferentially private ones. Towards this end, we develop a novel biased\nmultiplicative noise mechanism that ensures our e-values remain statistically\nvalid. We show that our differentially private e-values attain strong\nstatistical power, and are asymptotically as powerful as their non-private\ncounterparts. Experiments across online risk monitoring, private healthcare,\nand conformal e-prediction demonstrate our approach's effectiveness and\nillustrate its broad applicability."
                },
                "authors": [
                    {
                        "name": "Daniel Csillag"
                    },
                    {
                        "name": "Diego Mesquita"
                    }
                ],
                "author_detail": {
                    "name": "Diego Mesquita"
                },
                "author": "Diego Mesquita",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20749v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20749v3",
                "updated": "2025-10-22T04:14:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    14,
                    59,
                    2,
                    295,
                    0
                ],
                "published": "2025-05-27T05:45:03Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    5,
                    45,
                    3,
                    1,
                    147,
                    0
                ],
                "title": "Can Agents Fix Agent Issues?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Agents Fix Agent Issues?"
                },
                "summary": "LLM-based agent systems are emerging as a new software paradigm and have been\nwidely adopted across diverse domains such as medicine, robotics, and\nprogramming. However, maintaining these systems requires substantial effort, as\nthey are inevitably prone to bugs and continually evolve to meet changing\nexternal requirements. Therefore, automatically resolving agent issues (i.e.,\nbug reports or feature requests) is a crucial and challenging task. While\nrecent software engineering (SE) agents (e.g., SWE-agent) have shown promise in\naddressing issues in traditional software systems, it remains unclear how\neffectively they can resolve real-world issues in agent systems, which differ\nsignificantly from traditional software. To fill this gap, we first manually\nanalyze 201 real-world agent issues and identify common categories of agent\nissues. We then spend 500 person-hours constructing AGENTISSUE-BENCH, a\nreproducible benchmark comprising 50 agent issue resolution tasks (each with an\nexecutable environment and failure-triggering tests). We further evaluate\nstate-of-the-art SE agents on AGENTISSUE-BENCH and reveal their limited\neffectiveness (i.e., with only 3.33% - 12.67% resolution rates). These results\nunderscore the unique challenges of maintaining agent systems compared to\ntraditional software, highlighting the need for further research to develop\nadvanced SE agents for resolving agent issues. Data and code are available at\nhttps://alfin06.github.io/AgentIssue-Bench-Leaderboard/#/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agent systems are emerging as a new software paradigm and have been\nwidely adopted across diverse domains such as medicine, robotics, and\nprogramming. However, maintaining these systems requires substantial effort, as\nthey are inevitably prone to bugs and continually evolve to meet changing\nexternal requirements. Therefore, automatically resolving agent issues (i.e.,\nbug reports or feature requests) is a crucial and challenging task. While\nrecent software engineering (SE) agents (e.g., SWE-agent) have shown promise in\naddressing issues in traditional software systems, it remains unclear how\neffectively they can resolve real-world issues in agent systems, which differ\nsignificantly from traditional software. To fill this gap, we first manually\nanalyze 201 real-world agent issues and identify common categories of agent\nissues. We then spend 500 person-hours constructing AGENTISSUE-BENCH, a\nreproducible benchmark comprising 50 agent issue resolution tasks (each with an\nexecutable environment and failure-triggering tests). We further evaluate\nstate-of-the-art SE agents on AGENTISSUE-BENCH and reveal their limited\neffectiveness (i.e., with only 3.33% - 12.67% resolution rates). These results\nunderscore the unique challenges of maintaining agent systems compared to\ntraditional software, highlighting the need for further research to develop\nadvanced SE agents for resolving agent issues. Data and code are available at\nhttps://alfin06.github.io/AgentIssue-Bench-Leaderboard/#/ ."
                },
                "authors": [
                    {
                        "name": "Alfin Wijaya Rahardja"
                    },
                    {
                        "name": "Junwei Liu"
                    },
                    {
                        "name": "Weitong Chen"
                    },
                    {
                        "name": "Zhenpeng Chen"
                    },
                    {
                        "name": "Yiling Lou"
                    }
                ],
                "author_detail": {
                    "name": "Yiling Lou"
                },
                "author": "Yiling Lou",
                "arxiv_comment": "Accepted by the 39th Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20749v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20749v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18651v1",
                "updated": "2025-10-21T13:59:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    59,
                    56,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T13:59:56Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    59,
                    56,
                    1,
                    294,
                    0
                ],
                "title": "CPSLint: A Domain-Specific Language Providing Data Validation and\n  Sanitisation for Industrial Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPSLint: A Domain-Specific Language Providing Data Validation and\n  Sanitisation for Industrial Cyber-Physical Systems"
                },
                "summary": "Raw datasets are often too large and unstructured to work with directly, and\nrequire a data preparation process. The domain of industrial Cyber-Physical\nSystems (CPS) is no exception, as raw data typically consists of large amounts\nof time-series data logging the system's status in regular time intervals. Such\ndata has to be sanity checked and preprocessed to be consumable by data-centric\nworkflows. We introduce CPSLint, a Domain-Specific Language designed to provide\ndata preparation for industrial CPS. We build up on the fact that many raw data\ncollections in the CPS domain require similar actions to render them suitable\nfor Machine-Learning (ML) solutions, e.g., Fault Detection and Identification\n(FDI) workflows, yet still vary enough to hope for one universally applicable\nsolution.\n  CPSLint's main features include type checking and enforcing constraints\nthrough validation and remediation for data columns, such as imputing missing\ndata from surrounding rows. More advanced features cover inference of extra\nCPS-specific data structures, both column-wise and row-wise. For instance, as\nrow-wise structures, descriptive execution phases are an effective method of\ndata compartmentalisation are extracted and prepared for ML-assisted FDI\nworkflows. We demonstrate CPSLint's features through a proof of concept\nimplementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Raw datasets are often too large and unstructured to work with directly, and\nrequire a data preparation process. The domain of industrial Cyber-Physical\nSystems (CPS) is no exception, as raw data typically consists of large amounts\nof time-series data logging the system's status in regular time intervals. Such\ndata has to be sanity checked and preprocessed to be consumable by data-centric\nworkflows. We introduce CPSLint, a Domain-Specific Language designed to provide\ndata preparation for industrial CPS. We build up on the fact that many raw data\ncollections in the CPS domain require similar actions to render them suitable\nfor Machine-Learning (ML) solutions, e.g., Fault Detection and Identification\n(FDI) workflows, yet still vary enough to hope for one universally applicable\nsolution.\n  CPSLint's main features include type checking and enforcing constraints\nthrough validation and remediation for data columns, such as imputing missing\ndata from surrounding rows. More advanced features cover inference of extra\nCPS-specific data structures, both column-wise and row-wise. For instance, as\nrow-wise structures, descriptive execution phases are an effective method of\ndata compartmentalisation are extracted and prepared for ML-assisted FDI\nworkflows. We demonstrate CPSLint's features through a proof of concept\nimplementation."
                },
                "authors": [
                    {
                        "name": "Uraz Odyurt"
                    },
                    {
                        "name": "Ömer Sayilir"
                    },
                    {
                        "name": "Mariëlle Stoelinga"
                    },
                    {
                        "name": "Vadim Zaytsev"
                    }
                ],
                "author_detail": {
                    "name": "Vadim Zaytsev"
                },
                "author": "Vadim Zaytsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15062v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15062v2",
                "updated": "2025-10-21T13:55:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    55,
                    5,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-16T18:23:18Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    18,
                    23,
                    18,
                    3,
                    289,
                    0
                ],
                "title": "Geometric scaling of laser-driven proton focusing from hemispherical\n  foils",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric scaling of laser-driven proton focusing from hemispherical\n  foils"
                },
                "summary": "We systematically characterize the focusing behavior of laser-driven proton\nbeams from hemispherical targets of various diameters using mesh radiography.\nThe proton focal location is inferred to be near the geometrical center for the\nsmallest tested hemisphere ($\\Psi=D_{hemi}/D_{Laser}=6.1$). However, larger\nhemispheres ($\\Psi=14.6$) degrade the focusing behavior and behave more like\nflat foils with focal location significantly inside the hemisphere. We also\ninfer a tight virtual focus of $9\\pm3~\\mu$m through a mesh transition analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We systematically characterize the focusing behavior of laser-driven proton\nbeams from hemispherical targets of various diameters using mesh radiography.\nThe proton focal location is inferred to be near the geometrical center for the\nsmallest tested hemisphere ($\\Psi=D_{hemi}/D_{Laser}=6.1$). However, larger\nhemispheres ($\\Psi=14.6$) degrade the focusing behavior and behave more like\nflat foils with focal location significantly inside the hemisphere. We also\ninfer a tight virtual focus of $9\\pm3~\\mu$m through a mesh transition analysis."
                },
                "authors": [
                    {
                        "name": "Jesse Griff-McMahon"
                    },
                    {
                        "name": "Xavier Vaisseau"
                    },
                    {
                        "name": "William Fox"
                    },
                    {
                        "name": "Kirill Lezhnin"
                    },
                    {
                        "name": "Krish Bhutwala"
                    },
                    {
                        "name": "Ryan Nedbailo"
                    },
                    {
                        "name": "Valeria Opsina-Bohórquez"
                    },
                    {
                        "name": "Timo Karpowski"
                    },
                    {
                        "name": "Pravesh K. Patel"
                    },
                    {
                        "name": "Sophia Malko"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Malko"
                },
                "author": "Sophia Malko",
                "arxiv_comment": "9 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15062v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15062v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03739v2",
                "updated": "2025-10-21T13:54:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    54,
                    36,
                    1,
                    294,
                    0
                ],
                "published": "2025-05-06T17:59:53Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    59,
                    53,
                    1,
                    126,
                    0
                ],
                "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model"
                },
                "summary": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks."
                },
                "authors": [
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Heting Gao"
                    },
                    {
                        "name": "Lijiang Li"
                    },
                    {
                        "name": "Peixian Chen"
                    },
                    {
                        "name": "Mengdan Zhang"
                    },
                    {
                        "name": "Hang Shao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Jinlong Peng"
                    },
                    {
                        "name": "Haoyu Cao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "Training and Inference Codes: https://github.com/VITA-MLLM/VITA-Audio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18094v2",
                "updated": "2025-10-21T13:48:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    48,
                    43,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-22T17:59:40Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    59,
                    40,
                    0,
                    265,
                    0
                ],
                "title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level\n  Visual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level\n  Visual Reasoning"
                },
                "summary": "Recent advances in Large Multi-modal Models (LMMs) have demonstrated their\nremarkable success as general-purpose multi-modal assistants, with particular\nfocuses on holistic image- and video-language understanding. Conversely, less\nattention has been given to scaling fine-grained pixel-level understanding\ncapabilities, where the models are expected to realize pixel-level alignment\nbetween visual signals and language semantics. Some previous studies have\napplied LMMs to related tasks such as region-level captioning and referring\nexpression segmentation. However, these models are limited to performing either\nreferring or segmentation tasks independently and fail to integrate these\nfine-grained perception capabilities into visual reasoning. To bridge this gap,\nwe propose UniPixel, a large multi-modal model capable of flexibly\ncomprehending visual prompt inputs and generating mask-grounded responses. Our\nmodel distinguishes itself by seamlessly integrating pixel-level perception\nwith general visual understanding capabilities. Specifically, UniPixel\nprocesses visual prompts and generates relevant masks on demand, and performs\nsubsequent reasoning conditioning on these intermediate pointers during\ninference, thereby enabling fine-grained pixel-level reasoning. The\neffectiveness of our approach has been verified on 10 benchmarks across a\ndiverse set of tasks, including pixel-level referring/segmentation and\nobject-centric understanding in images/videos. A novel PixelQA task that\njointly requires referring, segmentation, and question answering is also\ndesigned to verify the flexibility of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Multi-modal Models (LMMs) have demonstrated their\nremarkable success as general-purpose multi-modal assistants, with particular\nfocuses on holistic image- and video-language understanding. Conversely, less\nattention has been given to scaling fine-grained pixel-level understanding\ncapabilities, where the models are expected to realize pixel-level alignment\nbetween visual signals and language semantics. Some previous studies have\napplied LMMs to related tasks such as region-level captioning and referring\nexpression segmentation. However, these models are limited to performing either\nreferring or segmentation tasks independently and fail to integrate these\nfine-grained perception capabilities into visual reasoning. To bridge this gap,\nwe propose UniPixel, a large multi-modal model capable of flexibly\ncomprehending visual prompt inputs and generating mask-grounded responses. Our\nmodel distinguishes itself by seamlessly integrating pixel-level perception\nwith general visual understanding capabilities. Specifically, UniPixel\nprocesses visual prompts and generates relevant masks on demand, and performs\nsubsequent reasoning conditioning on these intermediate pointers during\ninference, thereby enabling fine-grained pixel-level reasoning. The\neffectiveness of our approach has been verified on 10 benchmarks across a\ndiverse set of tasks, including pixel-level referring/segmentation and\nobject-centric understanding in images/videos. A novel PixelQA task that\njointly requires referring, segmentation, and question answering is also\ndesigned to verify the flexibility of our method."
                },
                "authors": [
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Zongyang Ma"
                    },
                    {
                        "name": "Junfu Pu"
                    },
                    {
                        "name": "Zhongang Qi"
                    },
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Ying Shan"
                    },
                    {
                        "name": "Chang Wen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chang Wen Chen"
                },
                "author": "Chang Wen Chen",
                "arxiv_comment": "NeurIPS 2025 Camera Ready. Project Page:\n  https://polyu-chenlab.github.io/unipixel/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22597v2",
                "updated": "2025-10-21T13:46:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    46,
                    40,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-26T17:17:14Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    17,
                    14,
                    4,
                    269,
                    0
                ],
                "title": "A Nonparametric Bayesian Solution of the Empirical Stochastic Inverse\n  Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Nonparametric Bayesian Solution of the Empirical Stochastic Inverse\n  Problem"
                },
                "summary": "The stochastic inverse problem is a key ingredient in making inferences,\npredictions, and decisions for complex science and engineering systems. We\nformulate and analyze a nonparametric Bayesian solution for the stochastic\ninverse problem. Key properties of the solution are proved and the convergence\nand error of a computational solution obtained by random sampling is analyzed.\nSeveral applications illustrate the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The stochastic inverse problem is a key ingredient in making inferences,\npredictions, and decisions for complex science and engineering systems. We\nformulate and analyze a nonparametric Bayesian solution for the stochastic\ninverse problem. Key properties of the solution are proved and the convergence\nand error of a computational solution obtained by random sampling is analyzed.\nSeveral applications illustrate the results."
                },
                "authors": [
                    {
                        "name": "Haiyi Shi"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Jiarui Chi"
                    },
                    {
                        "name": "Troy Butler"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Derek Bingham"
                    },
                    {
                        "name": "Don Estep"
                    }
                ],
                "author_detail": {
                    "name": "Don Estep"
                },
                "author": "Don Estep",
                "arxiv_comment": "48 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary 62G05, 65C60 Secondary 62P30, 62P35, 60D05, 60A10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18639v1",
                "updated": "2025-10-21T13:43:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    43,
                    6,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T13:43:06Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    43,
                    6,
                    1,
                    294,
                    0
                ],
                "title": "Distributional regression for seasonal data: an application to river\n  flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributional regression for seasonal data: an application to river\n  flows"
                },
                "summary": "Risk assessment in casualty insurance, such as flood risk, traditionally\nrelies on extreme-value methods that emphasizes rare events. These approaches\nare well-suited for characterizing tail risk, but do not capture the broader\ndynamics of environmental variables such as moderate or frequent loss events.\nTo complement these methods, we propose a modelling framework for estimating\nthe full (daily) distribution of environmental variables as a function of time,\nthat is a distributional version of typical climatological summary statistics,\nthereby incorporating both seasonal variation and gradual long-term changes.\nAside from the time trend, to capture seasonal variation our approach\nsimultaneously estimates the distribution for each instant of the seasonal\ncycle, without explicitly modelling the temporal dependence present in the\ndata. To do so, we adopt a framework inspired by GAMLSS (Generalized Additive\nModels for Location, Scale, and Shape), where the parameters of the\ndistribution vary over the seasonal cycle as a function of explanatory\nvariables depending only on the time of year, and not on the past values of the\nprocess under study. Ignoring the temporal dependence in the seasonal variation\ngreatly simplifies the modelling but poses inference challenges that we clarify\nand overcome.\n  We apply our framework to daily river flow data from three hydrometric\nstations along the Fraser River in British Columbia, Canada, and analyse the\nflood of the Fraser River in early winter of 2021.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk assessment in casualty insurance, such as flood risk, traditionally\nrelies on extreme-value methods that emphasizes rare events. These approaches\nare well-suited for characterizing tail risk, but do not capture the broader\ndynamics of environmental variables such as moderate or frequent loss events.\nTo complement these methods, we propose a modelling framework for estimating\nthe full (daily) distribution of environmental variables as a function of time,\nthat is a distributional version of typical climatological summary statistics,\nthereby incorporating both seasonal variation and gradual long-term changes.\nAside from the time trend, to capture seasonal variation our approach\nsimultaneously estimates the distribution for each instant of the seasonal\ncycle, without explicitly modelling the temporal dependence present in the\ndata. To do so, we adopt a framework inspired by GAMLSS (Generalized Additive\nModels for Location, Scale, and Shape), where the parameters of the\ndistribution vary over the seasonal cycle as a function of explanatory\nvariables depending only on the time of year, and not on the past values of the\nprocess under study. Ignoring the temporal dependence in the seasonal variation\ngreatly simplifies the modelling but poses inference challenges that we clarify\nand overcome.\n  We apply our framework to daily river flow data from three hydrometric\nstations along the Fraser River in British Columbia, Canada, and analyse the\nflood of the Fraser River in early winter of 2021."
                },
                "authors": [
                    {
                        "name": "Samuel Perreault"
                    },
                    {
                        "name": "Silvana M. Pesenti"
                    },
                    {
                        "name": "Daniyal Shahzad"
                    }
                ],
                "author_detail": {
                    "name": "Daniyal Shahzad"
                },
                "author": "Daniyal Shahzad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18636v1",
                "updated": "2025-10-21T13:40:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    40,
                    11,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T13:40:11Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    40,
                    11,
                    1,
                    294,
                    0
                ],
                "title": "C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural\n  Networks Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural\n  Networks Compression"
                },
                "summary": "Neural network compression has gained increasing attention in recent years,\nparticularly in computer vision applications, where the need for model\nreduction is crucial for overcoming deployment constraints. Pruning is a widely\nused technique that prompts sparsity in model structures, e.g. weights,\nneurons, and layers, reducing size and inference costs. Structured pruning is\nespecially important as it allows for the removal of entire structures, which\nfurther accelerates inference time and reduces memory overhead. However, it can\nbe computationally expensive, requiring iterative retraining and optimization.\nTo overcome this problem, recent methods considered one-shot setting, which\napplies pruning directly at post-training. Unfortunately, they often lead to a\nconsiderable drop in performance. In this paper, we focus on this issue by\nproposing a novel one-shot pruning framework that relies on explainable deep\nlearning. First, we introduce a causal-aware pruning approach that leverages\ncause-effect relations between model predictions and structures in a\nprogressive pruning process. It allows us to efficiently reduce the size of the\nnetwork, ensuring that the removed structures do not deter the performance of\nthe model. Then, through experiments conducted on convolution neural network\nand vision transformer baselines, pre-trained on classification tasks, we\ndemonstrate that our method consistently achieves substantial reductions in\nmodel size, with minimal impact on performance, and without the need for\nfine-tuning. Overall, our approach outperforms its counterparts, offering the\nbest trade-off. Our code is available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network compression has gained increasing attention in recent years,\nparticularly in computer vision applications, where the need for model\nreduction is crucial for overcoming deployment constraints. Pruning is a widely\nused technique that prompts sparsity in model structures, e.g. weights,\nneurons, and layers, reducing size and inference costs. Structured pruning is\nespecially important as it allows for the removal of entire structures, which\nfurther accelerates inference time and reduces memory overhead. However, it can\nbe computationally expensive, requiring iterative retraining and optimization.\nTo overcome this problem, recent methods considered one-shot setting, which\napplies pruning directly at post-training. Unfortunately, they often lead to a\nconsiderable drop in performance. In this paper, we focus on this issue by\nproposing a novel one-shot pruning framework that relies on explainable deep\nlearning. First, we introduce a causal-aware pruning approach that leverages\ncause-effect relations between model predictions and structures in a\nprogressive pruning process. It allows us to efficiently reduce the size of the\nnetwork, ensuring that the removed structures do not deter the performance of\nthe model. Then, through experiments conducted on convolution neural network\nand vision transformer baselines, pre-trained on classification tasks, we\ndemonstrate that our method consistently achieves substantial reductions in\nmodel size, with minimal impact on performance, and without the need for\nfine-tuning. Overall, our approach outperforms its counterparts, offering the\nbest trade-off. Our code is available on GitHub."
                },
                "authors": [
                    {
                        "name": "Baptiste Bauvin"
                    },
                    {
                        "name": "Loïc Baret"
                    },
                    {
                        "name": "Ola Ahmad"
                    }
                ],
                "author_detail": {
                    "name": "Ola Ahmad"
                },
                "author": "Ola Ahmad",
                "arxiv_comment": "10 pages, BMVC2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12006v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12006v3",
                "updated": "2025-10-21T13:39:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    39,
                    58,
                    1,
                    294,
                    0
                ],
                "published": "2025-05-17T13:47:31Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    13,
                    47,
                    31,
                    5,
                    137,
                    0
                ],
                "title": "SOCIA: Joint Structure-Parameter Co-Optimization for Automated Simulator\n  Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOCIA: Joint Structure-Parameter Co-Optimization for Automated Simulator\n  Construction"
                },
                "summary": "Building credible simulators from data is difficult because structure design,\nparameter calibration, and out-of-distribution (OOD) robustness are tightly\ncoupled. We introduce SOCIA (Simulation Orchestration for Computational\nIntelligence with Agents), a framework that treats simulator construction as\njoint structure-parameter co-optimization: it elicits mechanism-rich\nblueprints, exposes explicit tunable parameters, and instantiates a calibration\nschema, producing an executable simulator with built-in calibration hooks.\nSOCIA couples Bayesian Optimization for sample-efficient point calibration with\nSimulation-Based Inference for uncertainty-aware fitting; diagnostics trigger\ntargeted structural edits in an outer refinement loop to co-optimize design and\nparameters under tight budgets. Across three diverse tasks, SOCIA consistently\noutperforms strong baselines, excelling on both in-distribution (ID) fitting\nand OOD shift. Ablations that weaken structure, calibration design, or tuning\nyield near-monotone degradations, underscoring the necessity of unified\nstructure-parameter optimization. We will release the code soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building credible simulators from data is difficult because structure design,\nparameter calibration, and out-of-distribution (OOD) robustness are tightly\ncoupled. We introduce SOCIA (Simulation Orchestration for Computational\nIntelligence with Agents), a framework that treats simulator construction as\njoint structure-parameter co-optimization: it elicits mechanism-rich\nblueprints, exposes explicit tunable parameters, and instantiates a calibration\nschema, producing an executable simulator with built-in calibration hooks.\nSOCIA couples Bayesian Optimization for sample-efficient point calibration with\nSimulation-Based Inference for uncertainty-aware fitting; diagnostics trigger\ntargeted structural edits in an outer refinement loop to co-optimize design and\nparameters under tight budgets. Across three diverse tasks, SOCIA consistently\noutperforms strong baselines, excelling on both in-distribution (ID) fitting\nand OOD shift. Ablations that weaken structure, calibration design, or tuning\nyield near-monotone degradations, underscoring the necessity of unified\nstructure-parameter optimization. We will release the code soon."
                },
                "authors": [
                    {
                        "name": "Yuncheng Hua"
                    },
                    {
                        "name": "Sion Weatherhead"
                    },
                    {
                        "name": "Mehdi Jafari"
                    },
                    {
                        "name": "Jianxiang Xie"
                    },
                    {
                        "name": "Ji Miao"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "arxiv_comment": "53 pages, 1 figure, 2 tables. The paper is under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12006v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12006v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18411v2",
                "updated": "2025-10-21T13:34:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    34,
                    39,
                    1,
                    294,
                    0
                ],
                "published": "2025-05-23T22:38:28Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    22,
                    38,
                    28,
                    4,
                    143,
                    0
                ],
                "title": "DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process\n  Modeling and Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process\n  Modeling and Understanding"
                },
                "summary": "We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance\nmulti-modal Temporal Point Process (TPP) modeling in the era of Large Language\nModels (LLMs). While TPPs have been widely studied for modeling temporal event\nsequences, existing datasets are predominantly unimodal, hindering progress in\nmodels that require joint reasoning over temporal, textual, and visual\ninformation. To address this gap, DanmakuTPPBench comprises two complementary\ncomponents: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili\nvideo platform, where user-generated bullet comments (Danmaku) naturally form\nmulti-modal events annotated with precise timestamps, rich textual content, and\ncorresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering\ndataset constructed via a novel multi-agent pipeline powered by\nstate-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex\ntemporal-textual-visual reasoning. We conduct extensive evaluations using both\nclassical TPP models and recent MLLMs, revealing significant performance gaps\nand limitations in current methods' ability to model multi-modal event\ndynamics. Our benchmark establishes strong baselines and calls for further\nintegration of TPP modeling into the multi-modal language modeling landscape.\nProject page: https://github.com/FRENKIE-CHIANG/DanmakuTPPBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance\nmulti-modal Temporal Point Process (TPP) modeling in the era of Large Language\nModels (LLMs). While TPPs have been widely studied for modeling temporal event\nsequences, existing datasets are predominantly unimodal, hindering progress in\nmodels that require joint reasoning over temporal, textual, and visual\ninformation. To address this gap, DanmakuTPPBench comprises two complementary\ncomponents: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili\nvideo platform, where user-generated bullet comments (Danmaku) naturally form\nmulti-modal events annotated with precise timestamps, rich textual content, and\ncorresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering\ndataset constructed via a novel multi-agent pipeline powered by\nstate-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex\ntemporal-textual-visual reasoning. We conduct extensive evaluations using both\nclassical TPP models and recent MLLMs, revealing significant performance gaps\nand limitations in current methods' ability to model multi-modal event\ndynamics. Our benchmark establishes strong baselines and calls for further\nintegration of TPP modeling into the multi-modal language modeling landscape.\nProject page: https://github.com/FRENKIE-CHIANG/DanmakuTPPBench"
                },
                "authors": [
                    {
                        "name": "Yue Jiang"
                    },
                    {
                        "name": "Jichu Li"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Quyu Kong"
                    }
                ],
                "author_detail": {
                    "name": "Quyu Kong"
                },
                "author": "Quyu Kong",
                "arxiv_comment": "Accepted by Neural Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17702v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17702v4",
                "updated": "2025-10-21T13:30:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    30,
                    4,
                    1,
                    294,
                    0
                ],
                "published": "2025-07-23T17:10:23Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    10,
                    23,
                    2,
                    204,
                    0
                ],
                "title": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts\n  Language Models"
                },
                "summary": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels."
                },
                "authors": [
                    {
                        "name": "Changxin Tian"
                    },
                    {
                        "name": "Kunlong Chen"
                    },
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17702v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17702v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17575v2",
                "updated": "2025-10-21T13:26:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    26,
                    59,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-20T14:22:57Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    22,
                    57,
                    0,
                    293,
                    0
                ],
                "title": "DeTAILS: Deep Thematic Analysis with Iterative LLM Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeTAILS: Deep Thematic Analysis with Iterative LLM Support"
                },
                "summary": "Thematic analysis is widely used in qualitative research but can be difficult\nto scale because of its iterative, interpretive demands. We introduce DeTAILS,\na toolkit that integrates large language model (LLM) assistance into a workflow\ninspired by Braun and Clarke's thematic analysis framework. DeTAILS supports\nresearchers in generating and refining codes, reviewing clusters, and\nsynthesizing themes through interactive feedback loops designed to preserve\nanalytic agency. We evaluated the system with 18 qualitative researchers\nanalyzing Reddit data. Quantitative results showed strong alignment between\nLLM-supported outputs and participants' refinements, alongside reduced workload\nand high perceived usefulness. Qualitatively, participants reported that\nDeTAILS accelerated analysis, prompted reflexive engagement with AI outputs,\nand fostered trust through transparency and control. We contribute: (1) an\ninteractive human-LLM workflow for large-scale qualitative analysis, (2)\nempirical evidence of its feasibility and researcher experience, and (3) design\nimplications for trustworthy AI-assisted qualitative research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thematic analysis is widely used in qualitative research but can be difficult\nto scale because of its iterative, interpretive demands. We introduce DeTAILS,\na toolkit that integrates large language model (LLM) assistance into a workflow\ninspired by Braun and Clarke's thematic analysis framework. DeTAILS supports\nresearchers in generating and refining codes, reviewing clusters, and\nsynthesizing themes through interactive feedback loops designed to preserve\nanalytic agency. We evaluated the system with 18 qualitative researchers\nanalyzing Reddit data. Quantitative results showed strong alignment between\nLLM-supported outputs and participants' refinements, alongside reduced workload\nand high perceived usefulness. Qualitatively, participants reported that\nDeTAILS accelerated analysis, prompted reflexive engagement with AI outputs,\nand fostered trust through transparency and control. We contribute: (1) an\ninteractive human-LLM workflow for large-scale qualitative analysis, (2)\nempirical evidence of its feasibility and researcher experience, and (3) design\nimplications for trustworthy AI-assisted qualitative research."
                },
                "authors": [
                    {
                        "name": "Ansh Sharma"
                    },
                    {
                        "name": "Karen Cochrane"
                    },
                    {
                        "name": "James R. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "James R. Wallace"
                },
                "author": "James R. Wallace",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02672v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02672v3",
                "updated": "2025-10-21T13:21:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    21,
                    42,
                    1,
                    294,
                    0
                ],
                "published": "2025-06-03T09:18:33Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    18,
                    33,
                    1,
                    154,
                    0
                ],
                "title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via\n  Sequential Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via\n  Sequential Problem Solving"
                },
                "summary": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large\nlanguage models (LLMs) on their learning capability and efficiency in\nchallenging tasks, a critical, yet underexplored aspect of model potential.\nEvaLearn contains 648 challenging problems across six task types, grouped into\n182 sequences, each sequence dedicated to one task type. Diverging from most\nexisting benchmarks that evaluate models in parallel, EvaLearn requires models\nto solve problems sequentially, allowing them to leverage the experience gained\nfrom previous solutions. EvaLearn provides five comprehensive automated metrics\nto evaluate models and quantify their learning capability and efficiency. We\nextensively benchmark nine frontier models and observe varied performance\nprofiles: some models, such as Claude-3.7-sonnet, start with moderate initial\nperformance but exhibit strong learning ability, while some models struggle to\nbenefit from experience and may even show negative transfer. Moreover, we\ninvestigate model performance under two learning settings and find that\ninstance-level rubrics and teacher-model feedback further facilitate model\nlearning. Importantly, we observe that current LLMs with stronger static\nabilities do not show a clear advantage in learning capability across all\ntasks, highlighting that EvaLearn evaluates a new dimension of model\nperformance. We hope EvaLearn provides a novel evaluation perspective for\nassessing LLM potential and understanding the gap between models and human\ncapabilities, promoting the development of deeper and more dynamic evaluation\napproaches. All datasets, the automatic evaluation framework, and the results\nstudied in this paper are available at the GitHub repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large\nlanguage models (LLMs) on their learning capability and efficiency in\nchallenging tasks, a critical, yet underexplored aspect of model potential.\nEvaLearn contains 648 challenging problems across six task types, grouped into\n182 sequences, each sequence dedicated to one task type. Diverging from most\nexisting benchmarks that evaluate models in parallel, EvaLearn requires models\nto solve problems sequentially, allowing them to leverage the experience gained\nfrom previous solutions. EvaLearn provides five comprehensive automated metrics\nto evaluate models and quantify their learning capability and efficiency. We\nextensively benchmark nine frontier models and observe varied performance\nprofiles: some models, such as Claude-3.7-sonnet, start with moderate initial\nperformance but exhibit strong learning ability, while some models struggle to\nbenefit from experience and may even show negative transfer. Moreover, we\ninvestigate model performance under two learning settings and find that\ninstance-level rubrics and teacher-model feedback further facilitate model\nlearning. Importantly, we observe that current LLMs with stronger static\nabilities do not show a clear advantage in learning capability across all\ntasks, highlighting that EvaLearn evaluates a new dimension of model\nperformance. We hope EvaLearn provides a novel evaluation perspective for\nassessing LLM potential and understanding the gap between models and human\ncapabilities, promoting the development of deeper and more dynamic evaluation\napproaches. All datasets, the automatic evaluation framework, and the results\nstudied in this paper are available at the GitHub repository."
                },
                "authors": [
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Chenhao Huang"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Shichun Liu"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Chenxiao Liu"
                    },
                    {
                        "name": "Cheng Zhong"
                    },
                    {
                        "name": "Zongzhang Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Chao Xin"
                    },
                    {
                        "name": "Chengzhi Wei"
                    },
                    {
                        "name": "Lin Yan"
                    },
                    {
                        "name": "Yonghui Wu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Accepted by NeurIPS 2025. 47 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02672v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02672v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15732v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15732v3",
                "updated": "2025-10-21T13:15:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    15,
                    37,
                    1,
                    294,
                    0
                ],
                "published": "2025-06-15T01:08:05Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    1,
                    8,
                    5,
                    6,
                    166,
                    0
                ],
                "title": "Can LLMs Reconcile Knowledge Conflicts in Counterfactual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Reconcile Knowledge Conflicts in Counterfactual Reasoning"
                },
                "summary": "Large Language Models have been shown to contain extensive world knowledge in\ntheir parameters, enabling impressive performance on many knowledge intensive\ntasks. However, when deployed in novel settings, LLMs often encounter\nsituations where they must integrate parametric knowledge with new or\nunfamiliar information. In this work, we explore whether LLMs can combine\nknowledge in-context with their parametric knowledge through the lens of\ncounterfactual reasoning. Through synthetic and real experiments in multi-hop\nreasoning problems, we show that LLMs generally struggle with counterfactual\nreasoning, often resorting to exclusively using their parametric knowledge.\nMoreover, we show that simple post-hoc finetuning can struggle to instill\ncounterfactual reasoning ability -- often leading to degradation in stored\nparametric knowledge. Ultimately, our work reveals important limitations of\ncurrent LLM's abilities to re-purpose parametric knowledge in novel settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have been shown to contain extensive world knowledge in\ntheir parameters, enabling impressive performance on many knowledge intensive\ntasks. However, when deployed in novel settings, LLMs often encounter\nsituations where they must integrate parametric knowledge with new or\nunfamiliar information. In this work, we explore whether LLMs can combine\nknowledge in-context with their parametric knowledge through the lens of\ncounterfactual reasoning. Through synthetic and real experiments in multi-hop\nreasoning problems, we show that LLMs generally struggle with counterfactual\nreasoning, often resorting to exclusively using their parametric knowledge.\nMoreover, we show that simple post-hoc finetuning can struggle to instill\ncounterfactual reasoning ability -- often leading to degradation in stored\nparametric knowledge. Ultimately, our work reveals important limitations of\ncurrent LLM's abilities to re-purpose parametric knowledge in novel settings."
                },
                "authors": [
                    {
                        "name": "Khurram Yamin"
                    },
                    {
                        "name": "Gaurav Ghosal"
                    },
                    {
                        "name": "Bryan Wilder"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Wilder"
                },
                "author": "Bryan Wilder",
                "arxiv_comment": "ICML 2025 Workshop on Scaling up Intervention Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15732v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15732v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14926v2",
                "updated": "2025-10-21T13:13:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    13,
                    11,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-18T13:04:30Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    13,
                    4,
                    30,
                    3,
                    261,
                    0
                ],
                "title": "Patent Language Model Pretraining with ModernBERT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patent Language Model Pretraining with ModernBERT"
                },
                "summary": "Transformer-based language models such as BERT have become foundational in\nNLP, yet their performance degrades in specialized domains like patents, which\ncontain long, technical, and legally structured text. Prior approaches to\npatent NLP have primarily relied on fine-tuning general-purpose models or\ndomain-adapted variants pretrained with limited data. In this work, we pretrain\n3 domain-specific masked language models for patents, using the ModernBERT\narchitecture and a curated corpus of over 60 million patent records. Our\napproach incorporates architectural optimizations, including FlashAttention,\nrotary embeddings, and GLU feed-forward layers. We evaluate our models on four\ndownstream patent classification tasks. Our model, ModernBERT-base-PT,\nconsistently outperforms the general-purpose ModernBERT baseline on three out\nof four datasets and achieves competitive performance with a baseline\nPatentBERT. Additional experiments with ModernBERT-base-VX and\nMosaic-BERT-large demonstrate that scaling the model size and customizing the\ntokenizer further enhance performance on selected tasks. Notably, all\nModernBERT variants retain substantially faster inference over - 3x that of\nPatentBERT - underscoring their suitability for time-sensitive applications.\nThese results underscore the benefits of domain-specific pretraining and\narchitectural improvements for patent-focused NLP tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based language models such as BERT have become foundational in\nNLP, yet their performance degrades in specialized domains like patents, which\ncontain long, technical, and legally structured text. Prior approaches to\npatent NLP have primarily relied on fine-tuning general-purpose models or\ndomain-adapted variants pretrained with limited data. In this work, we pretrain\n3 domain-specific masked language models for patents, using the ModernBERT\narchitecture and a curated corpus of over 60 million patent records. Our\napproach incorporates architectural optimizations, including FlashAttention,\nrotary embeddings, and GLU feed-forward layers. We evaluate our models on four\ndownstream patent classification tasks. Our model, ModernBERT-base-PT,\nconsistently outperforms the general-purpose ModernBERT baseline on three out\nof four datasets and achieves competitive performance with a baseline\nPatentBERT. Additional experiments with ModernBERT-base-VX and\nMosaic-BERT-large demonstrate that scaling the model size and customizing the\ntokenizer further enhance performance on selected tasks. Notably, all\nModernBERT variants retain substantially faster inference over - 3x that of\nPatentBERT - underscoring their suitability for time-sensitive applications.\nThese results underscore the benefits of domain-specific pretraining and\narchitectural improvements for patent-focused NLP tasks."
                },
                "authors": [
                    {
                        "name": "Amirhossein Yousefiramandi"
                    },
                    {
                        "name": "Ciaran Cooney"
                    }
                ],
                "author_detail": {
                    "name": "Ciaran Cooney"
                },
                "author": "Ciaran Cooney",
                "arxiv_comment": "7 pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17516v2",
                "updated": "2025-10-21T13:05:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    5,
                    11,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-20T13:14:38Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    14,
                    38,
                    0,
                    293,
                    0
                ],
                "title": "SimBench: Benchmarking the Ability of Large Language Models to Simulate\n  Human Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimBench: Benchmarking the Ability of Large Language Models to Simulate\n  Human Behaviors"
                },
                "summary": "Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators."
                },
                "authors": [
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Joachim Baumann"
                    },
                    {
                        "name": "Lorenzo Lupo"
                    },
                    {
                        "name": "Nigel Collier"
                    },
                    {
                        "name": "Dirk Hovy"
                    },
                    {
                        "name": "Paul Röttger"
                    }
                ],
                "author_detail": {
                    "name": "Paul Röttger"
                },
                "author": "Paul Röttger",
                "arxiv_comment": "Project Website: http://simbench.tiancheng.hu/ Data:\n  https://huggingface.co/datasets/pitehu/SimBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08113v2",
                "updated": "2025-10-21T13:02:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    2,
                    48,
                    1,
                    294,
                    0
                ],
                "published": "2025-07-10T18:53:51Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    18,
                    53,
                    51,
                    3,
                    191,
                    0
                ],
                "title": "Uncertainty quantification of a multi-component Hall thruster model at\n  varying facility pressures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification of a multi-component Hall thruster model at\n  varying facility pressures"
                },
                "summary": "Bayesian inference is applied to calibrate and quantify prediction\nuncertainty in a coupled multi-component Hall thruster model. The model\nconsists of cathode, discharge, and plume sub-models and outputs thruster\nperformance metrics, one-dimensional plasma properties, and the angular\ndistribution of the current density in the plume. The simulated thrusters\ninclude a magnetically shielded thruster operating on krypton, the H9, and an\nunshielded thruster operating on xenon, the SPT-100, at pressures between\n4.3--43 $\\mu$Torr-Kr and 1.7--80 $\\mu$Torr-Xe, respectively. After calibration,\nthe model captures key pressure-related trends, including changes in thrust and\nupstream shifts in the ion acceleration region. Furthermore, the model exhibits\npredictive accuracy to within 10\\% when evaluated on flow rates and pressures\nnot included in the training data, and can predict some performance\ncharacteristics across test facilities to within the same range of conditions.\nCompared to a previous model calibrated on some of the same data [Eckels et al.\n2024], the model reduced predictive errors in thrust and discharge current by\ngreater than 50%. An extrapolation to on-orbit performance is performed with an\nerror of 9%, capturing trends in discharge current but not thrust. These\nfindings are discussed in the context of using data for predictive Hall\nthruster modeling in the presence of facility effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference is applied to calibrate and quantify prediction\nuncertainty in a coupled multi-component Hall thruster model. The model\nconsists of cathode, discharge, and plume sub-models and outputs thruster\nperformance metrics, one-dimensional plasma properties, and the angular\ndistribution of the current density in the plume. The simulated thrusters\ninclude a magnetically shielded thruster operating on krypton, the H9, and an\nunshielded thruster operating on xenon, the SPT-100, at pressures between\n4.3--43 $\\mu$Torr-Kr and 1.7--80 $\\mu$Torr-Xe, respectively. After calibration,\nthe model captures key pressure-related trends, including changes in thrust and\nupstream shifts in the ion acceleration region. Furthermore, the model exhibits\npredictive accuracy to within 10\\% when evaluated on flow rates and pressures\nnot included in the training data, and can predict some performance\ncharacteristics across test facilities to within the same range of conditions.\nCompared to a previous model calibrated on some of the same data [Eckels et al.\n2024], the model reduced predictive errors in thrust and discharge current by\ngreater than 50%. An extrapolation to on-orbit performance is performed with an\nerror of 9%, capturing trends in discharge current but not thrust. These\nfindings are discussed in the context of using data for predictive Hall\nthruster modeling in the presence of facility effects."
                },
                "authors": [
                    {
                        "name": "Thomas A. Marks"
                    },
                    {
                        "name": "Joshua D. Eckels"
                    },
                    {
                        "name": "Gabriel E. Mora"
                    },
                    {
                        "name": "Alex A. Gorodetsky"
                    }
                ],
                "author_detail": {
                    "name": "Alex A. Gorodetsky"
                },
                "author": "Alex A. Gorodetsky",
                "arxiv_doi": "10.1063/5.0283796",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1063/5.0283796",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "J. Appl. Phys. 138, 153305 (2025)",
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18601v1",
                "updated": "2025-10-21T12:59:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    59,
                    39,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:59:39Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    59,
                    39,
                    1,
                    294,
                    0
                ],
                "title": "Evaluating Large Language Models in detecting Secrets in Android Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models in detecting Secrets in Android Apps"
                },
                "summary": "Mobile apps often embed authentication secrets, such as API keys, tokens, and\nclient IDs, to integrate with cloud services. However, developers often\nhardcode these credentials into Android apps, exposing them to extraction\nthrough reverse engineering. Once compromised, adversaries can exploit secrets\nto access sensitive data, manipulate resources, or abuse APIs, resulting in\nsignificant security and financial risks. Existing detection approaches, such\nas regex-based analysis, static analysis, and machine learning, are effective\nfor identifying known patterns but are fundamentally limited: they require\nprior knowledge of credential structures, API signatures, or training data.\n  In this paper, we propose SecretLoc, an LLM-based approach for detecting\nhardcoded secrets in Android apps. SecretLoc goes beyond pattern matching; it\nleverages contextual and structural cues to identify secrets without relying on\npredefined patterns or labeled training sets. Using a benchmark dataset from\nthe literature, we demonstrate that SecretLoc detects secrets missed by regex-,\nstatic-, and ML-based methods, including previously unseen types of secrets. In\ntotal, we discovered 4828 secrets that were undetected by existing approaches,\ndiscovering more than 10 \"new\" types of secrets, such as OpenAI API keys,\nGitHub Access Tokens, RSA private keys, and JWT tokens, and more.\n  We further extend our analysis to newly crawled apps from Google Play, where\nwe uncovered and responsibly disclosed additional hardcoded secrets. Across a\nset of 5000 apps, we detected secrets in 2124 apps (42.5%), several of which\nwere confirmed and remediated by developers after we contacted them. Our\nresults reveal a dual-use risk: if analysts can uncover these secrets with\nLLMs, so can attackers. This underscores the urgent need for proactive secret\nmanagement and stronger mitigation practices across the mobile ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile apps often embed authentication secrets, such as API keys, tokens, and\nclient IDs, to integrate with cloud services. However, developers often\nhardcode these credentials into Android apps, exposing them to extraction\nthrough reverse engineering. Once compromised, adversaries can exploit secrets\nto access sensitive data, manipulate resources, or abuse APIs, resulting in\nsignificant security and financial risks. Existing detection approaches, such\nas regex-based analysis, static analysis, and machine learning, are effective\nfor identifying known patterns but are fundamentally limited: they require\nprior knowledge of credential structures, API signatures, or training data.\n  In this paper, we propose SecretLoc, an LLM-based approach for detecting\nhardcoded secrets in Android apps. SecretLoc goes beyond pattern matching; it\nleverages contextual and structural cues to identify secrets without relying on\npredefined patterns or labeled training sets. Using a benchmark dataset from\nthe literature, we demonstrate that SecretLoc detects secrets missed by regex-,\nstatic-, and ML-based methods, including previously unseen types of secrets. In\ntotal, we discovered 4828 secrets that were undetected by existing approaches,\ndiscovering more than 10 \"new\" types of secrets, such as OpenAI API keys,\nGitHub Access Tokens, RSA private keys, and JWT tokens, and more.\n  We further extend our analysis to newly crawled apps from Google Play, where\nwe uncovered and responsibly disclosed additional hardcoded secrets. Across a\nset of 5000 apps, we detected secrets in 2124 apps (42.5%), several of which\nwere confirmed and remediated by developers after we contacted them. Our\nresults reveal a dual-use risk: if analysts can uncover these secrets with\nLLMs, so can attackers. This underscores the urgent need for proactive secret\nmanagement and stronger mitigation practices across the mobile ecosystem."
                },
                "authors": [
                    {
                        "name": "Marco Alecci"
                    },
                    {
                        "name": "Jordan Samhi"
                    },
                    {
                        "name": "Tegawendé F. Bissyandé"
                    },
                    {
                        "name": "Jacques Klein"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Klein"
                },
                "author": "Jacques Klein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16219v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16219v2",
                "updated": "2025-10-21T12:58:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    58,
                    13,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-17T21:10:35Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    21,
                    10,
                    35,
                    4,
                    290,
                    0
                ],
                "title": "SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based\n  Dynamic Threat Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based\n  Dynamic Threat Detection"
                },
                "summary": "Malicious agents pose significant threats to the reliability and\ndecision-making capabilities of Multi-Agent Systems (MAS) powered by Large\nLanguage Models (LLMs). Existing defenses often fall short due to reactive\ndesigns or centralized architectures which may introduce single points of\nfailure. To address these challenges, we propose SentinelNet, the first\ndecentralized framework for proactively detecting and mitigating malicious\nbehaviors in multi-agent collaboration. SentinelNet equips each agent with a\ncredit-based detector trained via contrastive learning on augmented adversarial\ndebate trajectories, enabling autonomous evaluation of message credibility and\ndynamic neighbor ranking via bottom-k elimination to suppress malicious\ncommunications. To overcome the scarcity of attack data, it generates\nadversarial trajectories simulating diverse threats, ensuring robust training.\nExperiments on MAS benchmarks show SentinelNet achieves near-perfect detection\nof malicious agents, close to 100% within two debate rounds, and recovers 95%\nof system accuracy from compromised baselines. By exhibiting strong\ngeneralizability across domains and attack patterns, SentinelNet establishes a\nnovel paradigm for safeguarding collaborative MAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malicious agents pose significant threats to the reliability and\ndecision-making capabilities of Multi-Agent Systems (MAS) powered by Large\nLanguage Models (LLMs). Existing defenses often fall short due to reactive\ndesigns or centralized architectures which may introduce single points of\nfailure. To address these challenges, we propose SentinelNet, the first\ndecentralized framework for proactively detecting and mitigating malicious\nbehaviors in multi-agent collaboration. SentinelNet equips each agent with a\ncredit-based detector trained via contrastive learning on augmented adversarial\ndebate trajectories, enabling autonomous evaluation of message credibility and\ndynamic neighbor ranking via bottom-k elimination to suppress malicious\ncommunications. To overcome the scarcity of attack data, it generates\nadversarial trajectories simulating diverse threats, ensuring robust training.\nExperiments on MAS benchmarks show SentinelNet achieves near-perfect detection\nof malicious agents, close to 100% within two debate rounds, and recovers 95%\nof system accuracy from compromised baselines. By exhibiting strong\ngeneralizability across domains and attack patterns, SentinelNet establishes a\nnovel paradigm for safeguarding collaborative MAS."
                },
                "authors": [
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Xudong Pan"
                    }
                ],
                "author_detail": {
                    "name": "Xudong Pan"
                },
                "author": "Xudong Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16219v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16219v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16505v2",
                "updated": "2025-10-21T12:52:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    52,
                    54,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-18T13:46:26Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    13,
                    46,
                    26,
                    5,
                    291,
                    0
                ],
                "title": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal\n  Inconsistencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal\n  Inconsistencies"
                },
                "summary": "Large Multimodal Models (LMMs) are increasingly applied to scientific\nresearch, yet it remains unclear whether they can reliably understand and\nreason over the multimodal complexity of papers. A central challenge lies in\ndetecting and resolving inconsistencies across text, figures, tables, and\nequations, issues that are often subtle, domain-specific, and ultimately\nundermine clarity, reproducibility, and trust. Existing benchmarks overlook\nthis issue, either isolating single modalities or relying on synthetic errors\nthat fail to capture real-world complexity. We introduce PRISMM-Bench\n(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first\nbenchmark grounded in real reviewer-flagged inconsistencies in scientific\npapers. Through a multi-stage pipeline of review mining, LLM-assisted filtering\nand human verification, we curate 262 inconsistencies from 242 papers. Based on\nthis set, we design three tasks, namely inconsistency identification, remedy\nand pair matching, which assess a model's capacity to detect, correct, and\nreason over inconsistencies across different modalities. Furthermore, to\naddress the notorious problem of choice-only shortcuts in multiple-choice\nevaluation, where models exploit answer patterns without truly understanding\nthe question, we further introduce structured JSON-based answer representations\nthat minimize linguistic biases by reducing reliance on superficial stylistic\ncues. We benchmark 21 leading LMMs, including large open-weight models\n(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5\nwith high reasoning). Results reveal strikingly low performance (26.1-54.2%),\nunderscoring the challenge of multimodal scientific reasoning and motivating\nprogress towards trustworthy scientific assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) are increasingly applied to scientific\nresearch, yet it remains unclear whether they can reliably understand and\nreason over the multimodal complexity of papers. A central challenge lies in\ndetecting and resolving inconsistencies across text, figures, tables, and\nequations, issues that are often subtle, domain-specific, and ultimately\nundermine clarity, reproducibility, and trust. Existing benchmarks overlook\nthis issue, either isolating single modalities or relying on synthetic errors\nthat fail to capture real-world complexity. We introduce PRISMM-Bench\n(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first\nbenchmark grounded in real reviewer-flagged inconsistencies in scientific\npapers. Through a multi-stage pipeline of review mining, LLM-assisted filtering\nand human verification, we curate 262 inconsistencies from 242 papers. Based on\nthis set, we design three tasks, namely inconsistency identification, remedy\nand pair matching, which assess a model's capacity to detect, correct, and\nreason over inconsistencies across different modalities. Furthermore, to\naddress the notorious problem of choice-only shortcuts in multiple-choice\nevaluation, where models exploit answer patterns without truly understanding\nthe question, we further introduce structured JSON-based answer representations\nthat minimize linguistic biases by reducing reliance on superficial stylistic\ncues. We benchmark 21 leading LMMs, including large open-weight models\n(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5\nwith high reasoning). Results reveal strikingly low performance (26.1-54.2%),\nunderscoring the challenge of multimodal scientific reasoning and motivating\nprogress towards trustworthy scientific assistants."
                },
                "authors": [
                    {
                        "name": "Lukas Selch"
                    },
                    {
                        "name": "Yufang Hou"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Sivan Doveh"
                    },
                    {
                        "name": "James Glass"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11442v2",
                "updated": "2025-10-21T12:49:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    49,
                    25,
                    1,
                    294,
                    0
                ],
                "published": "2025-06-13T03:41:04Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    3,
                    41,
                    4,
                    4,
                    164,
                    0
                ],
                "title": "ReVeal: Self-Evolving Code Agents via Reliable Self-Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReVeal: Self-Evolving Code Agents via Reliable Self-Verification"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has advanced the\nreasoning capabilities of large language models. However, existing methods rely\nsolely on outcome rewards, without explicitly optimizing verification or\nleveraging reliable signals from realistic environments, leading to unreliable\nself-verification and limited test-time scaling. To address this, we widen the\nverification-generation asymmetry by explicitly optimizing self-verification,\nmaking it a reliable driver of deeper test-time scaling. We introduce ReVeal, a\nmulti-turn reinforcement learning framework that evolves code generation\nthrough self-verification and tool-based evaluation. ReVeal structures\nlong-horizon reasoning as iterative generation-verification turns and\nincorporates TAPO for turn-level credit assignment, fostering the co-evolution\nof code and test generation. At inference, this strengthened self-verification\nenables the model to use self-constructed tests and tool feedback to\ncontinuously evolve code for 20+ turns on LiveCodeBench despite training on\nonly three. It also significantly improves Pass@k, indicating stronger\nexploration that expands the reasoning boundaries of the base model. These\nfindings highlight the promise of ReVeal as a scalable paradigm for RL training\nand test-time scaling, paving the way for more robust and autonomous AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has advanced the\nreasoning capabilities of large language models. However, existing methods rely\nsolely on outcome rewards, without explicitly optimizing verification or\nleveraging reliable signals from realistic environments, leading to unreliable\nself-verification and limited test-time scaling. To address this, we widen the\nverification-generation asymmetry by explicitly optimizing self-verification,\nmaking it a reliable driver of deeper test-time scaling. We introduce ReVeal, a\nmulti-turn reinforcement learning framework that evolves code generation\nthrough self-verification and tool-based evaluation. ReVeal structures\nlong-horizon reasoning as iterative generation-verification turns and\nincorporates TAPO for turn-level credit assignment, fostering the co-evolution\nof code and test generation. At inference, this strengthened self-verification\nenables the model to use self-constructed tests and tool feedback to\ncontinuously evolve code for 20+ turns on LiveCodeBench despite training on\nonly three. It also significantly improves Pass@k, indicating stronger\nexploration that expands the reasoning boundaries of the base model. These\nfindings highlight the promise of ReVeal as a scalable paradigm for RL training\nand test-time scaling, paving the way for more robust and autonomous AI agents."
                },
                "authors": [
                    {
                        "name": "Yiyang Jin"
                    },
                    {
                        "name": "Kunzhao Xu"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Xueting Han"
                    },
                    {
                        "name": "Yanmin Zhou"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Jing Bai"
                    }
                ],
                "author_detail": {
                    "name": "Jing Bai"
                },
                "author": "Jing Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.18876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18876v2",
                "updated": "2025-10-22T04:30:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    30,
                    24,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-21T17:59:59Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    59,
                    59,
                    1,
                    294,
                    0
                ],
                "title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for\n  Multimodal LLMs"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) excel at holistic\nunderstanding, they struggle in capturing the dense world with complex scenes,\nrequiring fine-grained analysis of intricate details and object\ninter-relationships. Region-level MLLMs have been a promising step. However,\nprevious attempts are generally optimized to understand given regions in\nisolation, neglecting crucial global contexts. To address this, we introduce\nGrasp Any Region (GAR) for comprehen- sive region-level visual understanding.\nEmpowered by an effective RoI-aligned feature replay technique, GAR supports\n(1) precise perception by leveraging necessary global contexts, and (2)\nmodeling interactions between multiple prompts. Together, it then naturally\nachieves (3) advanced compositional reasoning to answer specific free-form\nquestions about any region, shifting the paradigm from passive description to\nactive dialogue. Moreover, we construct GAR-Bench, which not only provides a\nmore accurate evaluation of single-region comprehension, but also, more\nimportantly, measures interactions and complex reasoning across multiple\nregions. Extensive experiments have demonstrated that GAR-1B not only maintains\nthe state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5\non DLC-Bench, but also excels at modeling relationships between multiple\nprompts with advanced comprehension capabilities, even surpassing InternVL3-78B\non GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms\nin-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong\ncapabilities can be easily transferred to videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) excel at holistic\nunderstanding, they struggle in capturing the dense world with complex scenes,\nrequiring fine-grained analysis of intricate details and object\ninter-relationships. Region-level MLLMs have been a promising step. However,\nprevious attempts are generally optimized to understand given regions in\nisolation, neglecting crucial global contexts. To address this, we introduce\nGrasp Any Region (GAR) for comprehen- sive region-level visual understanding.\nEmpowered by an effective RoI-aligned feature replay technique, GAR supports\n(1) precise perception by leveraging necessary global contexts, and (2)\nmodeling interactions between multiple prompts. Together, it then naturally\nachieves (3) advanced compositional reasoning to answer specific free-form\nquestions about any region, shifting the paradigm from passive description to\nactive dialogue. Moreover, we construct GAR-Bench, which not only provides a\nmore accurate evaluation of single-region comprehension, but also, more\nimportantly, measures interactions and complex reasoning across multiple\nregions. Extensive experiments have demonstrated that GAR-1B not only maintains\nthe state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5\non DLC-Bench, but also excels at modeling relationships between multiple\nprompts with advanced comprehension capabilities, even surpassing InternVL3-78B\non GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms\nin-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong\ncapabilities can be easily transferred to videos."
                },
                "authors": [
                    {
                        "name": "Haochen Wang"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Yikang Zhou"
                    },
                    {
                        "name": "Yanwei Li"
                    },
                    {
                        "name": "Jiacong Wang"
                    },
                    {
                        "name": "Jiani Zheng"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Jiahao Meng"
                    },
                    {
                        "name": "Zilong Huang"
                    },
                    {
                        "name": "Guangcan Mai"
                    },
                    {
                        "name": "Anran Wang"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Zhuochen Wang"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoxiang Zhang"
                },
                "author": "Zhaoxiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18871v1",
                "updated": "2025-10-21T17:59:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    59,
                    5,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:59:05Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    59,
                    5,
                    1,
                    294,
                    0
                ],
                "title": "How Do LLMs Use Their Depth?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Do LLMs Use Their Depth?"
                },
                "summary": "Growing evidence suggests that large language models do not use their depth\nuniformly, yet we still lack a fine-grained understanding of their layer-wise\nprediction dynamics. In this paper, we trace the intermediate representations\nof several open-weight models during inference and reveal a structured and\nnuanced use of depth. Specifically, we propose a \"Guess-then-Refine\" framework\nthat explains how LLMs internally structure their computations to make\npredictions. We first show that the top-ranked predictions in early LLM layers\nare composed primarily of high-frequency tokens, which act as statistical\nguesses proposed by the model early on due to the lack of appropriate\ncontextual information. As contextual information develops deeper into the\nmodel, these initial guesses get refined into contextually appropriate tokens.\nEven high-frequency token predictions from early layers get refined >70% of the\ntime, indicating that correct token prediction is not \"one-and-done\". We then\ngo beyond frequency-based prediction to examine the dynamic usage of layer\ndepth across three case studies. (i) Part-of-speech analysis shows that\nfunction words are, on average, the earliest to be predicted correctly. (ii)\nFact recall task analysis shows that, in a multi-token answer, the first token\nrequires more computational depth than the rest. (iii) Multiple-choice task\nanalysis shows that the model identifies the format of the response within the\nfirst half of the layers, but finalizes its response only toward the end.\nTogether, our results provide a detailed view of depth usage in LLMs, shedding\nlight on the layer-by-layer computations that underlie successful predictions\nand providing insights for future works to improve computational efficiency in\ntransformer-based models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing evidence suggests that large language models do not use their depth\nuniformly, yet we still lack a fine-grained understanding of their layer-wise\nprediction dynamics. In this paper, we trace the intermediate representations\nof several open-weight models during inference and reveal a structured and\nnuanced use of depth. Specifically, we propose a \"Guess-then-Refine\" framework\nthat explains how LLMs internally structure their computations to make\npredictions. We first show that the top-ranked predictions in early LLM layers\nare composed primarily of high-frequency tokens, which act as statistical\nguesses proposed by the model early on due to the lack of appropriate\ncontextual information. As contextual information develops deeper into the\nmodel, these initial guesses get refined into contextually appropriate tokens.\nEven high-frequency token predictions from early layers get refined >70% of the\ntime, indicating that correct token prediction is not \"one-and-done\". We then\ngo beyond frequency-based prediction to examine the dynamic usage of layer\ndepth across three case studies. (i) Part-of-speech analysis shows that\nfunction words are, on average, the earliest to be predicted correctly. (ii)\nFact recall task analysis shows that, in a multi-token answer, the first token\nrequires more computational depth than the rest. (iii) Multiple-choice task\nanalysis shows that the model identifies the format of the response within the\nfirst half of the layers, but finalizes its response only toward the end.\nTogether, our results provide a detailed view of depth usage in LLMs, shedding\nlight on the layer-by-layer computations that underlie successful predictions\nand providing insights for future works to improve computational efficiency in\ntransformer-based models."
                },
                "authors": [
                    {
                        "name": "Akshat Gupta"
                    },
                    {
                        "name": "Jay Yeung"
                    },
                    {
                        "name": "Gopala Anumanchipalli"
                    },
                    {
                        "name": "Anna Ivanova"
                    }
                ],
                "author_detail": {
                    "name": "Anna Ivanova"
                },
                "author": "Anna Ivanova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18866v1",
                "updated": "2025-10-21T17:58:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    58,
                    17,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:58:17Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    58,
                    17,
                    1,
                    294,
                    0
                ],
                "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightMem: Lightweight and Efficient Memory-Augmented Generation"
                },
                "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) struggle\nto effectively leverage historical interaction information in dynamic and\ncomplex environments. Memory systems enable LLMs to move beyond stateless\ninteractions by introducing persistent information storage, retrieval, and\nutilization mechanisms. However, existing memory systems often introduce\nsubstantial time and computational overhead. To this end, we introduce a new\nmemory system called LightMem, which strikes a balance between the performance\nand efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of\nhuman memory, LightMem organizes memory into three complementary stages. First,\ncognition-inspired sensory memory rapidly filters irrelevant information\nthrough lightweight compression and groups information according to their\ntopics. Next, topic-aware short-term memory consolidates these topic-based\ngroups, organizing and summarizing content for more structured access. Finally,\nlong-term memory with sleep-time update employs an offline procedure that\ndecouples consolidation from online inference. Experiments on LongMemEval with\nGPT and Qwen backbones show that LightMem outperforms strong baselines in\naccuracy (up to 10.9% gains) while reducing token usage by up to 117x, API\ncalls by up to 159x, and runtime by over 12x. The code is available at\nhttps://github.com/zjunlp/LightMem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable capabilities, Large Language Models (LLMs) struggle\nto effectively leverage historical interaction information in dynamic and\ncomplex environments. Memory systems enable LLMs to move beyond stateless\ninteractions by introducing persistent information storage, retrieval, and\nutilization mechanisms. However, existing memory systems often introduce\nsubstantial time and computational overhead. To this end, we introduce a new\nmemory system called LightMem, which strikes a balance between the performance\nand efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of\nhuman memory, LightMem organizes memory into three complementary stages. First,\ncognition-inspired sensory memory rapidly filters irrelevant information\nthrough lightweight compression and groups information according to their\ntopics. Next, topic-aware short-term memory consolidates these topic-based\ngroups, organizing and summarizing content for more structured access. Finally,\nlong-term memory with sleep-time update employs an offline procedure that\ndecouples consolidation from online inference. Experiments on LongMemEval with\nGPT and Qwen backbones show that LightMem outperforms strong baselines in\naccuracy (up to 10.9% gains) while reducing token usage by up to 117x, API\ncalls by up to 159x, and runtime by over 12x. The code is available at\nhttps://github.com/zjunlp/LightMem."
                },
                "authors": [
                    {
                        "name": "Jizhan Fang"
                    },
                    {
                        "name": "Xinle Deng"
                    },
                    {
                        "name": "Haoming Xu"
                    },
                    {
                        "name": "Ziyan Jiang"
                    },
                    {
                        "name": "Yuqi Tang"
                    },
                    {
                        "name": "Ziwen Xu"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Mengru Wang"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18863v1",
                "updated": "2025-10-21T17:55:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    55,
                    39,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:55:39Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    55,
                    39,
                    1,
                    294,
                    0
                ],
                "title": "EffiReasonTrans: RL-Optimized Reasoning for Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EffiReasonTrans: RL-Optimized Reasoning for Code Translation"
                },
                "summary": "Code translation is a crucial task in software development and maintenance.\nWhile recent advancements in large language models (LLMs) have improved\nautomated code translation accuracy, these gains often come at the cost of\nincreased inference latency, hindering real-world development workflows that\ninvolve human-in-the-loop inspection. To address this trade-off, we propose\nEffiReasonTrans, a training framework designed to improve translation accuracy\nwhile balancing inference latency. We first construct a high-quality\nreasoning-augmented dataset by prompting a stronger language model,\nDeepSeek-R1, to generate intermediate reasoning and target translations. Each\n(source code, reasoning, target code) triplet undergoes automated syntax and\nfunctionality checks to ensure reliability. Based on this dataset, we employ a\ntwo-stage training strategy: supervised fine-tuning on reasoning-augmented\nsamples, followed by reinforcement learning to further enhance accuracy and\nbalance inference latency. We evaluate EffiReasonTrans on six translation\npairs. Experimental results show that it consistently improves translation\naccuracy (up to +49.2% CA and +27.8% CodeBLEU compared to the base model) while\nreducing the number of generated tokens (up to -19.3%) and lowering inference\nlatency in most cases (up to -29.0%). Ablation studies further confirm the\ncomplementary benefits of the two-stage training framework. Additionally,\nEffiReasonTrans demonstrates improved translation accuracy when integrated into\nagent-based frameworks. Our code and data are available at\nhttps://github.com/DeepSoftwareAnalytics/EffiReasonTrans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation is a crucial task in software development and maintenance.\nWhile recent advancements in large language models (LLMs) have improved\nautomated code translation accuracy, these gains often come at the cost of\nincreased inference latency, hindering real-world development workflows that\ninvolve human-in-the-loop inspection. To address this trade-off, we propose\nEffiReasonTrans, a training framework designed to improve translation accuracy\nwhile balancing inference latency. We first construct a high-quality\nreasoning-augmented dataset by prompting a stronger language model,\nDeepSeek-R1, to generate intermediate reasoning and target translations. Each\n(source code, reasoning, target code) triplet undergoes automated syntax and\nfunctionality checks to ensure reliability. Based on this dataset, we employ a\ntwo-stage training strategy: supervised fine-tuning on reasoning-augmented\nsamples, followed by reinforcement learning to further enhance accuracy and\nbalance inference latency. We evaluate EffiReasonTrans on six translation\npairs. Experimental results show that it consistently improves translation\naccuracy (up to +49.2% CA and +27.8% CodeBLEU compared to the base model) while\nreducing the number of generated tokens (up to -19.3%) and lowering inference\nlatency in most cases (up to -29.0%). Ablation studies further confirm the\ncomplementary benefits of the two-stage training framework. Additionally,\nEffiReasonTrans demonstrates improved translation accuracy when integrated into\nagent-based frameworks. Our code and data are available at\nhttps://github.com/DeepSoftwareAnalytics/EffiReasonTrans."
                },
                "authors": [
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Rongyi Ou"
                    },
                    {
                        "name": "Yanli Wang"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Ensheng Shi"
                    },
                    {
                        "name": "Xilin Liu"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00441v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00441v3",
                "updated": "2025-10-21T17:55:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    55,
                    16,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-01T02:48:28Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    48,
                    28,
                    2,
                    274,
                    0
                ],
                "title": "Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual\n  Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual\n  Navigation"
                },
                "summary": "Visual navigation is a fundamental problem in embodied AI, yet practical\ndeployments demand long-horizon planning capabilities to address\nmulti-objective tasks. A major bottleneck is data scarcity: policies learned\nfrom limited data often overfit and fail to generalize OOD. Existing neural\nnetwork-based agents typically increase architectural complexity that\nparadoxically become counterproductive in the small-sample regime. This paper\nintroduce NeuRO, a integrated learning-to-optimize framework that tightly\ncouples perception networks with downstream task-level robust optimization.\nSpecifically, NeuRO addresses core difficulties in this integration: (i) it\ntransforms noisy visual predictions under data scarcity into convex uncertainty\nsets using Partially Input Convex Neural Networks (PICNNs) with conformal\ncalibration, which directly parameterize the optimization constraints; and (ii)\nit reformulates planning under partial observability as a robust optimization\nproblem, enabling uncertainty-aware policies that transfer across environments.\nExtensive experiments on both unordered and sequential multi-object navigation\ntasks demonstrate that NeuRO establishes SoTA performance, particularly in\ngeneralization to unseen environments. Our work thus presents a significant\nadvancement for developing robust, generalizable autonomous agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual navigation is a fundamental problem in embodied AI, yet practical\ndeployments demand long-horizon planning capabilities to address\nmulti-objective tasks. A major bottleneck is data scarcity: policies learned\nfrom limited data often overfit and fail to generalize OOD. Existing neural\nnetwork-based agents typically increase architectural complexity that\nparadoxically become counterproductive in the small-sample regime. This paper\nintroduce NeuRO, a integrated learning-to-optimize framework that tightly\ncouples perception networks with downstream task-level robust optimization.\nSpecifically, NeuRO addresses core difficulties in this integration: (i) it\ntransforms noisy visual predictions under data scarcity into convex uncertainty\nsets using Partially Input Convex Neural Networks (PICNNs) with conformal\ncalibration, which directly parameterize the optimization constraints; and (ii)\nit reformulates planning under partial observability as a robust optimization\nproblem, enabling uncertainty-aware policies that transfer across environments.\nExtensive experiments on both unordered and sequential multi-object navigation\ntasks demonstrate that NeuRO establishes SoTA performance, particularly in\ngeneralization to unseen environments. Our work thus presents a significant\nadvancement for developing robust, generalizable autonomous agents."
                },
                "authors": [
                    {
                        "name": "Yiyuan Pan"
                    },
                    {
                        "name": "Yunzhe Xu"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Hesheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hesheng Wang"
                },
                "author": "Hesheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00441v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00441v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18860v1",
                "updated": "2025-10-21T17:53:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    53,
                    56,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:53:56Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    53,
                    56,
                    1,
                    294,
                    0
                ],
                "title": "An Encoder-Decoder Foundation Chemical Language Model for Generative\n  Polymer Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Encoder-Decoder Foundation Chemical Language Model for Generative\n  Polymer Design"
                },
                "summary": "Traditional machine learning has advanced polymer discovery, yet direct\ngeneration of chemically valid and synthesizable polymers without exhaustive\nenumeration remains a challenge. Here we present polyT5, an encoder-decoder\nchemical language model based on the T5 architecture, trained to understand and\ngenerate polymer structures. polyT5 enables both property prediction and the\ntargeted generation of polymers conditioned on desired property values. We\ndemonstrate its utility for dielectric polymer design, seeking candidates with\ndielectric constant >3, bandgap >4 eV, and glass transition temperature >400 K,\nalongside melt-processability and solubility requirements. From over 20,000\ngenerated promising candidates, one was experimentally synthesized and\nvalidated, showing strong agreement with predictions. To further enhance\nusability, we integrated polyT5 within an agentic AI framework that couples it\nwith a general-purpose LLM, allowing natural language interaction for property\nprediction and generative design. Together, these advances establish a\nversatile and accessible framework for accelerated polymer discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional machine learning has advanced polymer discovery, yet direct\ngeneration of chemically valid and synthesizable polymers without exhaustive\nenumeration remains a challenge. Here we present polyT5, an encoder-decoder\nchemical language model based on the T5 architecture, trained to understand and\ngenerate polymer structures. polyT5 enables both property prediction and the\ntargeted generation of polymers conditioned on desired property values. We\ndemonstrate its utility for dielectric polymer design, seeking candidates with\ndielectric constant >3, bandgap >4 eV, and glass transition temperature >400 K,\nalongside melt-processability and solubility requirements. From over 20,000\ngenerated promising candidates, one was experimentally synthesized and\nvalidated, showing strong agreement with predictions. To further enhance\nusability, we integrated polyT5 within an agentic AI framework that couples it\nwith a general-purpose LLM, allowing natural language interaction for property\nprediction and generative design. Together, these advances establish a\nversatile and accessible framework for accelerated polymer discovery."
                },
                "authors": [
                    {
                        "name": "Harikrishna Sahu"
                    },
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Anagha Savit"
                    },
                    {
                        "name": "Shivank S Shukla"
                    },
                    {
                        "name": "Rampi Ramprasad"
                    }
                ],
                "author_detail": {
                    "name": "Rampi Ramprasad"
                },
                "author": "Rampi Ramprasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14456v2",
                "updated": "2025-10-21T17:46:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    46,
                    51,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-17T22:12:30Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    22,
                    12,
                    30,
                    2,
                    260,
                    0
                ],
                "title": "Correct-Detect: Balancing Performance and Ambiguity Through the Lens of\n  Coreference Resolution in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct-Detect: Balancing Performance and Ambiguity Through the Lens of\n  Coreference Resolution in LLMs"
                },
                "summary": "Large Language Models (LLMs) are intended to reflect human linguistic\ncompetencies. But humans have access to a broad and embodied context, which is\nkey in detecting and resolving linguistic ambiguities, even in isolated text\nspans. A foundational case of semantic ambiguity is found in the task of\ncoreference resolution: how is a pronoun related to an earlier person mention?\nThis capability is implicit in nearly every downstream task, and the presence\nof ambiguity at this level can alter performance significantly. We show that\nLLMs can achieve good performance with minimal prompting in both coreference\ndisambiguation and the detection of ambiguity in coreference, however, they\ncannot do both at the same time. We present the CORRECT-DETECT trade-off:\nthough models have both capabilities and deploy them implicitly, successful\nperformance balancing these two abilities remains elusive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are intended to reflect human linguistic\ncompetencies. But humans have access to a broad and embodied context, which is\nkey in detecting and resolving linguistic ambiguities, even in isolated text\nspans. A foundational case of semantic ambiguity is found in the task of\ncoreference resolution: how is a pronoun related to an earlier person mention?\nThis capability is implicit in nearly every downstream task, and the presence\nof ambiguity at this level can alter performance significantly. We show that\nLLMs can achieve good performance with minimal prompting in both coreference\ndisambiguation and the detection of ambiguity in coreference, however, they\ncannot do both at the same time. We present the CORRECT-DETECT trade-off:\nthough models have both capabilities and deploy them implicitly, successful\nperformance balancing these two abilities remains elusive."
                },
                "authors": [
                    {
                        "name": "Amber Shore"
                    },
                    {
                        "name": "Russell Scheinberg"
                    },
                    {
                        "name": "Ameeta Agrawal"
                    },
                    {
                        "name": "So Young Lee"
                    }
                ],
                "author_detail": {
                    "name": "So Young Lee"
                },
                "author": "So Young Lee",
                "arxiv_comment": "Accepted at EMNLP 2025 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03417v2",
                "updated": "2025-10-21T17:41:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    41,
                    58,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-03T18:24:14Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    18,
                    24,
                    14,
                    4,
                    276,
                    0
                ],
                "title": "NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn\n  LLM Jailbreaks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn\n  LLM Jailbreaks"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nbut remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks\nthat distribute malicious intent across benign exchanges and bypass alignment\nmechanisms. Existing approaches often explore the adversarial space poorly,\nrely on hand-crafted heuristics, or lack systematic query refinement. We\npresent NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular\nframework for constructing, refining, and executing optimized multi-turn\nattacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a\nharmful intent into a structured semantic network of topics, entities, and\nquery chains; (2) a feedback-driven Simulator that iteratively refines and\nprunes these chains through attacker-victim-judge LLM collaboration using\nharmfulness and semantic-similarity benchmarks; and (3) a Network Traverser\nthat adaptively navigates the refined query space for real-time attacks. This\npipeline uncovers stealthy, high-success adversarial paths across LLMs. On\nseveral closed-source and open-source LLMs, NEXUS increases attack success rate\nby 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nbut remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks\nthat distribute malicious intent across benign exchanges and bypass alignment\nmechanisms. Existing approaches often explore the adversarial space poorly,\nrely on hand-crafted heuristics, or lack systematic query refinement. We\npresent NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular\nframework for constructing, refining, and executing optimized multi-turn\nattacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a\nharmful intent into a structured semantic network of topics, entities, and\nquery chains; (2) a feedback-driven Simulator that iteratively refines and\nprunes these chains through attacker-victim-judge LLM collaboration using\nharmfulness and semantic-similarity benchmarks; and (3) a Network Traverser\nthat adaptively navigates the refined query space for real-time attacks. This\npipeline uncovers stealthy, high-success adversarial paths across LLMs. On\nseveral closed-source and open-source LLMs, NEXUS increases attack success rate\nby 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS"
                },
                "authors": [
                    {
                        "name": "Javad Rafiei Asl"
                    },
                    {
                        "name": "Sidhant Narula"
                    },
                    {
                        "name": "Mohammad Ghasemigol"
                    },
                    {
                        "name": "Eduardo Blanco"
                    },
                    {
                        "name": "Daniel Takabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Takabi"
                },
                "author": "Daniel Takabi",
                "arxiv_comment": "This paper has been accepted in the main conference proceedings of\n  the 2025 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2025). Javad Rafiei Asl and Sidhant Narula are co-first authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18849v1",
                "updated": "2025-10-21T17:40:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    40,
                    3,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:40:03Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    40,
                    3,
                    1,
                    294,
                    0
                ],
                "title": "Towards Faithful and Controllable Personalization via Critique-Post-Edit\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Faithful and Controllable Personalization via Critique-Post-Edit\n  Reinforcement Learning"
                },
                "summary": "Faithfully personalizing large language models (LLMs) to align with\nindividual user preferences is a critical but challenging task. While\nsupervised fine-tuning (SFT) quickly reaches a performance plateau, standard\nreinforcement learning from human feedback (RLHF) also struggles with the\nnuances of personalization. Scalar-based reward models are prone to reward\nhacking which leads to verbose and superficially personalized responses. To\naddress these limitations, we propose Critique-Post-Edit, a robust\nreinforcement learning framework that enables more faithful and controllable\npersonalization. Our framework integrates two key components: (1) a\nPersonalized Generative Reward Model (GRM) that provides multi-dimensional\nscores and textual critiques to resist reward hacking, and (2) a\nCritique-Post-Edit mechanism where the policy model revises its own outputs\nbased on these critiques for more targeted and efficient learning. Under a\nrigorous length-controlled evaluation, our method substantially outperforms\nstandard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an\naverage 11\\% win-rate improvement, and personalized Qwen2.5-14B model surpasses\nthe performance of GPT-4.1. These results demonstrate a practical path to\nfaithful, efficient, and controllable personalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfully personalizing large language models (LLMs) to align with\nindividual user preferences is a critical but challenging task. While\nsupervised fine-tuning (SFT) quickly reaches a performance plateau, standard\nreinforcement learning from human feedback (RLHF) also struggles with the\nnuances of personalization. Scalar-based reward models are prone to reward\nhacking which leads to verbose and superficially personalized responses. To\naddress these limitations, we propose Critique-Post-Edit, a robust\nreinforcement learning framework that enables more faithful and controllable\npersonalization. Our framework integrates two key components: (1) a\nPersonalized Generative Reward Model (GRM) that provides multi-dimensional\nscores and textual critiques to resist reward hacking, and (2) a\nCritique-Post-Edit mechanism where the policy model revises its own outputs\nbased on these critiques for more targeted and efficient learning. Under a\nrigorous length-controlled evaluation, our method substantially outperforms\nstandard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an\naverage 11\\% win-rate improvement, and personalized Qwen2.5-14B model surpasses\nthe performance of GPT-4.1. These results demonstrate a practical path to\nfaithful, efficient, and controllable personalization."
                },
                "authors": [
                    {
                        "name": "Chenghao Zhu"
                    },
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Tiannan Wang"
                    },
                    {
                        "name": "Dongyi Ding"
                    },
                    {
                        "name": "Yuchen Eleanor Jiang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wangchunshu Zhou"
                },
                "author": "Wangchunshu Zhou",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17203v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17203v3",
                "updated": "2025-10-21T17:38:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    38,
                    57,
                    1,
                    294,
                    0
                ],
                "published": "2025-04-24T02:27:17Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    2,
                    27,
                    17,
                    3,
                    114,
                    0
                ],
                "title": "High-Fidelity And Complex Test Data Generation For Google SQL Code\n  Generation Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Fidelity And Complex Test Data Generation For Google SQL Code\n  Generation Services"
                },
                "summary": "The demand for high-fidelity test data is paramount in industrial settings\nwhere access to production data is largely restricted. Traditional data\ngeneration methods often fall short, struggling with low-fidelity and the\nability to model complex data structures and semantic relationships that are\ncritical for testing complex SQL code generation services like Natural Language\nto SQL (NL2SQL). In this paper, we address the critical need for generating\nsyntactically correct and semantically relevant high-fidelity mock data for\ncomplex data structures that includes columns with nested structures that we\nfrequently encounter in Google workloads. We highlight the limitations of\nexisting approaches used in production, particularly their inability to handle\nlarge and complex data structures, as well as the lack of semantically coherent\ntest data that lead to limited test coverage. We demonstrate that by leveraging\nLarge Language Models (LLMs) and incorporating strategic pre- and\npost-processing steps, we can generate syntactically correct and semantically\nrelevant high-fidelity test data that adheres to complex structural constraints\nand maintains semantic integrity to the SQL test targets (queries/functions).\nThis approach supports comprehensive testing of complex SQL queries involving\njoins, aggregations, and even deeply nested subqueries, ensuring robust\nevaluation of SQL code generation services, like NL2SQL and SQL Code Assistant.\nOur results demonstrate the practical utility of an LLM (\\textit{gemini}) based\ntest data generation for industrial SQL code generation services where\ngenerating high-fidelity test data is essential due to the frequent\nunavailability and inaccessibility of production datasets for testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for high-fidelity test data is paramount in industrial settings\nwhere access to production data is largely restricted. Traditional data\ngeneration methods often fall short, struggling with low-fidelity and the\nability to model complex data structures and semantic relationships that are\ncritical for testing complex SQL code generation services like Natural Language\nto SQL (NL2SQL). In this paper, we address the critical need for generating\nsyntactically correct and semantically relevant high-fidelity mock data for\ncomplex data structures that includes columns with nested structures that we\nfrequently encounter in Google workloads. We highlight the limitations of\nexisting approaches used in production, particularly their inability to handle\nlarge and complex data structures, as well as the lack of semantically coherent\ntest data that lead to limited test coverage. We demonstrate that by leveraging\nLarge Language Models (LLMs) and incorporating strategic pre- and\npost-processing steps, we can generate syntactically correct and semantically\nrelevant high-fidelity test data that adheres to complex structural constraints\nand maintains semantic integrity to the SQL test targets (queries/functions).\nThis approach supports comprehensive testing of complex SQL queries involving\njoins, aggregations, and even deeply nested subqueries, ensuring robust\nevaluation of SQL code generation services, like NL2SQL and SQL Code Assistant.\nOur results demonstrate the practical utility of an LLM (\\textit{gemini}) based\ntest data generation for industrial SQL code generation services where\ngenerating high-fidelity test data is essential due to the frequent\nunavailability and inaccessibility of production datasets for testing."
                },
                "authors": [
                    {
                        "name": "Shivasankari Kannan"
                    },
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Amita Gondi"
                    },
                    {
                        "name": "Tristan Swadell"
                    },
                    {
                        "name": "Fatma Ozcan"
                    }
                ],
                "author_detail": {
                    "name": "Fatma Ozcan"
                },
                "author": "Fatma Ozcan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17203v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17203v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18840v1",
                "updated": "2025-10-21T17:34:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    34,
                    48,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:34:48Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    34,
                    48,
                    1,
                    294,
                    0
                ],
                "title": "See the Text: From Tokenization to Visual Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "See the Text: From Tokenization to Visual Reading"
                },
                "summary": "People see text. Humans read by recognizing words as visual objects,\nincluding their shapes, layouts, and patterns, before connecting them to\nmeaning, which enables us to handle typos, distorted fonts, and various scripts\neffectively. Modern large language models (LLMs), however, rely on subword\ntokenization, fragmenting text into pieces from a fixed vocabulary. While\neffective for high-resource languages, this approach over-segments low-resource\nlanguages, yielding long, linguistically meaningless sequences and inflating\ncomputation. In this work, we challenge this entrenched paradigm and move\ntoward a vision-centric alternative. Our method, SeeTok, renders text as images\n(visual-text) and leverages pretrained multimodal LLMs to interpret them,\nreusing strong OCR and text-vision alignment abilities learned from large-scale\nmultimodal training. Across three different language tasks, SeeTok matches or\nsurpasses subword tokenizers while requiring 4.43 times fewer tokens and\nreducing FLOPs by 70.5%, with additional gains in cross-lingual generalization,\nrobustness to typographic noise, and linguistic hierarchy. SeeTok signals a\nshift from symbolic tokenization to human-like visual reading, and takes a step\ntoward more natural and cognitively inspired language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People see text. Humans read by recognizing words as visual objects,\nincluding their shapes, layouts, and patterns, before connecting them to\nmeaning, which enables us to handle typos, distorted fonts, and various scripts\neffectively. Modern large language models (LLMs), however, rely on subword\ntokenization, fragmenting text into pieces from a fixed vocabulary. While\neffective for high-resource languages, this approach over-segments low-resource\nlanguages, yielding long, linguistically meaningless sequences and inflating\ncomputation. In this work, we challenge this entrenched paradigm and move\ntoward a vision-centric alternative. Our method, SeeTok, renders text as images\n(visual-text) and leverages pretrained multimodal LLMs to interpret them,\nreusing strong OCR and text-vision alignment abilities learned from large-scale\nmultimodal training. Across three different language tasks, SeeTok matches or\nsurpasses subword tokenizers while requiring 4.43 times fewer tokens and\nreducing FLOPs by 70.5%, with additional gains in cross-lingual generalization,\nrobustness to typographic noise, and linguistic hierarchy. SeeTok signals a\nshift from symbolic tokenization to human-like visual reading, and takes a step\ntoward more natural and cognitively inspired language models."
                },
                "authors": [
                    {
                        "name": "Ling Xing"
                    },
                    {
                        "name": "Alex Jinpeng Wang"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Hongyu Qu"
                    },
                    {
                        "name": "Zechao Li"
                    },
                    {
                        "name": "Jinhui Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jinhui Tang"
                },
                "author": "Jinhui Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18830v1",
                "updated": "2025-10-21T17:25:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    25,
                    32,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:25:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    25,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long\n  Context Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long\n  Context Training"
                },
                "summary": "The adoption of long context windows has become a standard feature in Large\nLanguage Models (LLMs), as extended contexts significantly enhance their\ncapacity for complex reasoning and broaden their applicability across diverse\nscenarios. Dynamic sparse attention is a promising approach for reducing the\ncomputational cost of long-context. However, efficiently training LLMs with\ndynamic sparse attention on ultra-long contexts-especially in distributed\nsettings-remains a significant challenge, due in large part to worker- and\nstep-level imbalance. This paper introduces MTraining, a novel distributed\nmethodology leveraging dynamic sparse attention to enable efficient training\nfor LLMs with ultra-long contexts. Specifically, MTraining integrates three key\ncomponents: a dynamic sparse training pattern, balanced sparse ring attention,\nand hierarchical sparse ring attention. These components are designed to\nsynergistically address the computational imbalance and communication overheads\ninherent in dynamic sparse attention mechanisms during the training of models\nwith extensive context lengths. We demonstrate the efficacy of MTraining by\ntraining Qwen2.5-3B, successfully expanding its context window from 32K to 512K\ntokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite\nof downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A\nHaystack, reveal that MTraining achieves up to a 6x higher training throughput\nwhile preserving model accuracy. Our code is available at\nhttps://github.com/microsoft/MInference/tree/main/MTraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of long context windows has become a standard feature in Large\nLanguage Models (LLMs), as extended contexts significantly enhance their\ncapacity for complex reasoning and broaden their applicability across diverse\nscenarios. Dynamic sparse attention is a promising approach for reducing the\ncomputational cost of long-context. However, efficiently training LLMs with\ndynamic sparse attention on ultra-long contexts-especially in distributed\nsettings-remains a significant challenge, due in large part to worker- and\nstep-level imbalance. This paper introduces MTraining, a novel distributed\nmethodology leveraging dynamic sparse attention to enable efficient training\nfor LLMs with ultra-long contexts. Specifically, MTraining integrates three key\ncomponents: a dynamic sparse training pattern, balanced sparse ring attention,\nand hierarchical sparse ring attention. These components are designed to\nsynergistically address the computational imbalance and communication overheads\ninherent in dynamic sparse attention mechanisms during the training of models\nwith extensive context lengths. We demonstrate the efficacy of MTraining by\ntraining Qwen2.5-3B, successfully expanding its context window from 32K to 512K\ntokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite\nof downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A\nHaystack, reveal that MTraining achieves up to a 6x higher training throughput\nwhile preserving model accuracy. Our code is available at\nhttps://github.com/microsoft/MInference/tree/main/MTraining."
                },
                "authors": [
                    {
                        "name": "Wenxuan Li"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15926v2",
                "updated": "2025-10-21T17:20:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    20,
                    2,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-03T05:37:51Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    5,
                    37,
                    51,
                    4,
                    276,
                    0
                ],
                "title": "TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode\n  Accelerator with Table-Lookup Matmul on Edge FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode\n  Accelerator with Table-Lookup Matmul on Edge FPGAs"
                },
                "summary": "With the emergence of wearable devices and other embedded systems, deploying\nlarge language models (LLMs) on edge platforms has become an urgent need.\nHowever, this is challenging because of their high computational and memory\ndemands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)\ncompress weights to as low as 1.58~bits with minimal accuracy loss, edge\ndeployment is still constrained by limited on-chip resources, power budgets,\nand the often-neglected long latency of the prefill stage. We present\n\\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for\nlow-power edge FPGAs that fully supports both prefill and autoregressive\ndecoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates\nseveral novel techniques, including (1) a table-lookup-based ternary matrix\nmultiplication (TLMM) engine utilizing grouped activations and online\nprecomputation for low resource utilization and high throughput; (2) a\nfine-grained analytic URAM-based weight buffer management scheme for efficient\nloading and compute engine access; (3) a streaming dataflow architecture that\nfuses floating-point element-wise operations with linear computations to hide\nlatency; (4) a reversed-reordered prefill stage attention with fused attention\noperations for high memory efficiency; and (5) a resource-efficient specialized\ndecoding stage attention. Under a 5~W power budget, TeLLMe delivers up to\n25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for\n64--128 token prompts, marking a significant energy-efficiency advancement in\nLLM inference on edge FPGAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of wearable devices and other embedded systems, deploying\nlarge language models (LLMs) on edge platforms has become an urgent need.\nHowever, this is challenging because of their high computational and memory\ndemands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)\ncompress weights to as low as 1.58~bits with minimal accuracy loss, edge\ndeployment is still constrained by limited on-chip resources, power budgets,\nand the often-neglected long latency of the prefill stage. We present\n\\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for\nlow-power edge FPGAs that fully supports both prefill and autoregressive\ndecoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates\nseveral novel techniques, including (1) a table-lookup-based ternary matrix\nmultiplication (TLMM) engine utilizing grouped activations and online\nprecomputation for low resource utilization and high throughput; (2) a\nfine-grained analytic URAM-based weight buffer management scheme for efficient\nloading and compute engine access; (3) a streaming dataflow architecture that\nfuses floating-point element-wise operations with linear computations to hide\nlatency; (4) a reversed-reordered prefill stage attention with fused attention\noperations for high memory efficiency; and (5) a resource-efficient specialized\ndecoding stage attention. Under a 5~W power budget, TeLLMe delivers up to\n25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for\n64--128 token prompts, marking a significant energy-efficiency advancement in\nLLM inference on edge FPGAs."
                },
                "authors": [
                    {
                        "name": "Ye Qiao"
                    },
                    {
                        "name": "Zhiheng Chen"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yian Wang"
                    },
                    {
                        "name": "Sitao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Sitao Huang"
                },
                "author": "Sitao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18821v1",
                "updated": "2025-10-21T17:19:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    19,
                    35,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:19:35Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    19,
                    35,
                    1,
                    294,
                    0
                ],
                "title": "Search Self-play: Pushing the Frontier of Agent Capability without\n  Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search Self-play: Pushing the Frontier of Agent Capability without\n  Supervision"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has become the\nmainstream technique for training LLM agents. However, RLVR highly depends on\nwell-crafted task queries and corresponding ground-truth answers to provide\naccurate rewards, which requires massive human efforts and hinders the RL\nscaling processes, especially under agentic scenarios. Although a few recent\nworks explore task synthesis methods, the difficulty of generated agentic tasks\ncan hardly be controlled to provide effective RL training advantages. To\nachieve agentic RLVR with higher scalability, we explore self-play training for\ndeep search agents, in which the learning LLM utilizes multi-turn search engine\ncalling and acts simultaneously as both a task proposer and a problem solver.\nThe task proposer aims to generate deep search queries with well-defined\nground-truth answers and increasing task difficulty. The problem solver tries\nto handle the generated search queries and output the correct answer\npredictions. To ensure that each generated search query has accurate ground\ntruth, we collect all the searching results from the proposer's trajectory as\nexternal knowledge, then conduct retrieval-augmentation generation (RAG) to\ntest whether the proposed query can be correctly answered with all necessary\nsearch documents provided. In this search self-play (SSP) game, the proposer\nand the solver co-evolve their agent capabilities through both competition and\ncooperation. With substantial experimental results, we find that SSP can\nsignificantly improve search agents' performance uniformly on various\nbenchmarks without any supervision under both from-scratch and continuous RL\ntraining setups. The code is at https://github.com/Alibaba-Quark/SSP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has become the\nmainstream technique for training LLM agents. However, RLVR highly depends on\nwell-crafted task queries and corresponding ground-truth answers to provide\naccurate rewards, which requires massive human efforts and hinders the RL\nscaling processes, especially under agentic scenarios. Although a few recent\nworks explore task synthesis methods, the difficulty of generated agentic tasks\ncan hardly be controlled to provide effective RL training advantages. To\nachieve agentic RLVR with higher scalability, we explore self-play training for\ndeep search agents, in which the learning LLM utilizes multi-turn search engine\ncalling and acts simultaneously as both a task proposer and a problem solver.\nThe task proposer aims to generate deep search queries with well-defined\nground-truth answers and increasing task difficulty. The problem solver tries\nto handle the generated search queries and output the correct answer\npredictions. To ensure that each generated search query has accurate ground\ntruth, we collect all the searching results from the proposer's trajectory as\nexternal knowledge, then conduct retrieval-augmentation generation (RAG) to\ntest whether the proposed query can be correctly answered with all necessary\nsearch documents provided. In this search self-play (SSP) game, the proposer\nand the solver co-evolve their agent capabilities through both competition and\ncooperation. With substantial experimental results, we find that SSP can\nsignificantly improve search agents' performance uniformly on various\nbenchmarks without any supervision under both from-scratch and continuous RL\ntraining setups. The code is at https://github.com/Alibaba-Quark/SSP."
                },
                "authors": [
                    {
                        "name": "Hongliang Lu"
                    },
                    {
                        "name": "Yuhang Wen"
                    },
                    {
                        "name": "Pengyu Cheng"
                    },
                    {
                        "name": "Ruijin Ding"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Jiaqi Guo"
                    },
                    {
                        "name": "Chutian Wang"
                    },
                    {
                        "name": "Haonan Chen"
                    },
                    {
                        "name": "Xiaoxi Jiang"
                    },
                    {
                        "name": "Guanjun Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Guanjun Jiang"
                },
                "author": "Guanjun Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18819v1",
                "updated": "2025-10-21T17:18:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    18,
                    55,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:18:55Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    18,
                    55,
                    1,
                    294,
                    0
                ],
                "title": "An Explainable Hybrid AI Framework for Enhanced Tuberculosis and Symptom\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Explainable Hybrid AI Framework for Enhanced Tuberculosis and Symptom\n  Detection"
                },
                "summary": "Tuberculosis remains a critical global health issue, particularly in\nresource-limited and remote areas. Early detection is vital for treatment, yet\nthe lack of skilled radiologists underscores the need for artificial\nintelligence (AI)-driven screening tools. Developing reliable AI models is\nchallenging due to the necessity for large, high-quality datasets, which are\ncostly to obtain. To tackle this, we propose a teacher--student framework which\nenhances both disease and symptom detection on chest X-rays by integrating two\nsupervised heads and a self-supervised head. Our model achieves an accuracy of\n98.85% for distinguishing between COVID-19, tuberculosis, and normal cases, and\na macro-F1 score of 90.09% for multilabel symptom detection, significantly\noutperforming baselines. The explainability assessments also show the model\nbases its predictions on relevant anatomical features, demonstrating promise\nfor deployment in clinical screening and triage settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuberculosis remains a critical global health issue, particularly in\nresource-limited and remote areas. Early detection is vital for treatment, yet\nthe lack of skilled radiologists underscores the need for artificial\nintelligence (AI)-driven screening tools. Developing reliable AI models is\nchallenging due to the necessity for large, high-quality datasets, which are\ncostly to obtain. To tackle this, we propose a teacher--student framework which\nenhances both disease and symptom detection on chest X-rays by integrating two\nsupervised heads and a self-supervised head. Our model achieves an accuracy of\n98.85% for distinguishing between COVID-19, tuberculosis, and normal cases, and\na macro-F1 score of 90.09% for multilabel symptom detection, significantly\noutperforming baselines. The explainability assessments also show the model\nbases its predictions on relevant anatomical features, demonstrating promise\nfor deployment in clinical screening and triage settings."
                },
                "authors": [
                    {
                        "name": "Neel Patel"
                    },
                    {
                        "name": "Alexander Wong"
                    },
                    {
                        "name": "Ashkan Ebadi"
                    }
                ],
                "author_detail": {
                    "name": "Ashkan Ebadi"
                },
                "author": "Ashkan Ebadi",
                "arxiv_comment": "16 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18817v1",
                "updated": "2025-10-21T17:18:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    18,
                    24,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:18:24Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    18,
                    24,
                    1,
                    294,
                    0
                ],
                "title": "Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for\n  Industrial Asset Health Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for\n  Industrial Asset Health Monitoring"
                },
                "summary": "Small Language Models (SLMs) are becoming increasingly popular in specialized\nfields, such as industrial applications, due to their efficiency, lower\ncomputational requirements, and ability to be fine-tuned for domain-specific\ntasks, enabling accurate and cost-effective solutions. However, performing\ncomplex reasoning using SLMs in specialized fields such as Industry 4.0 remains\nchallenging. In this paper, we propose a knowledge distillation framework for\nindustrial asset health, which transfers reasoning capabilities via\nChain-of-Thought (CoT) distillation from Large Language Models (LLMs) to\nsmaller, more efficient models (SLMs). We discuss the advantages and the\nprocess of distilling LLMs using multi-choice question answering (MCQA) prompts\nto enhance reasoning and refine decision-making. We also perform in-context\nlearning to verify the quality of the generated knowledge and benchmark the\nperformance of fine-tuned SLMs with generated knowledge against widely used\nLLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform\nthe base models by a significant margin, narrowing the gap to their LLM\ncounterparts. Our code is open-sourced at:\nhttps://github.com/IBM/FailureSensorIQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models (SLMs) are becoming increasingly popular in specialized\nfields, such as industrial applications, due to their efficiency, lower\ncomputational requirements, and ability to be fine-tuned for domain-specific\ntasks, enabling accurate and cost-effective solutions. However, performing\ncomplex reasoning using SLMs in specialized fields such as Industry 4.0 remains\nchallenging. In this paper, we propose a knowledge distillation framework for\nindustrial asset health, which transfers reasoning capabilities via\nChain-of-Thought (CoT) distillation from Large Language Models (LLMs) to\nsmaller, more efficient models (SLMs). We discuss the advantages and the\nprocess of distilling LLMs using multi-choice question answering (MCQA) prompts\nto enhance reasoning and refine decision-making. We also perform in-context\nlearning to verify the quality of the generated knowledge and benchmark the\nperformance of fine-tuned SLMs with generated knowledge against widely used\nLLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform\nthe base models by a significant margin, narrowing the gap to their LLM\ncounterparts. Our code is open-sourced at:\nhttps://github.com/IBM/FailureSensorIQ."
                },
                "authors": [
                    {
                        "name": "Shuxin Lin"
                    },
                    {
                        "name": "Dhaval Patel"
                    },
                    {
                        "name": "Christodoulos Constantinides"
                    }
                ],
                "author_detail": {
                    "name": "Christodoulos Constantinides"
                },
                "author": "Christodoulos Constantinides",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18814v1",
                "updated": "2025-10-21T17:15:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    15,
                    56,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T17:15:56Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    15,
                    56,
                    1,
                    294,
                    0
                ],
                "title": "Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning\n  without Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning\n  without Rewards"
                },
                "summary": "We present a simple, self-help online supervised finetuning (OSFT) paradigm\nfor LLM reasoning. In this paradigm, the model generates its own responses and\nis immediately finetuned on this self-generated data. OSFT is a highly\nefficient training strategy for LLM reasoning, as it is reward-free and uses\njust one rollout by default. Experiment results show that OSFT achieves\ndownstream performance on challenging mathematical reasoning tasks comparable\nto strong reinforcement learning with verifiable rewards (RLVR) methods such as\nGRPO. Our ablation study further demonstrates the efficiency and robustness of\nOSFT. The major mechanism of OSFT lies in facilitating the model's own existing\npreference (latent knowledge) learned from pretraining, which leads to\nreasoning ability improvement. We believe that OSFT offers an efficient and\npromising alternative to more complex, reward-based training paradigms. Our\ncode is available at https://github.com/ElementQi/OnlineSFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a simple, self-help online supervised finetuning (OSFT) paradigm\nfor LLM reasoning. In this paradigm, the model generates its own responses and\nis immediately finetuned on this self-generated data. OSFT is a highly\nefficient training strategy for LLM reasoning, as it is reward-free and uses\njust one rollout by default. Experiment results show that OSFT achieves\ndownstream performance on challenging mathematical reasoning tasks comparable\nto strong reinforcement learning with verifiable rewards (RLVR) methods such as\nGRPO. Our ablation study further demonstrates the efficiency and robustness of\nOSFT. The major mechanism of OSFT lies in facilitating the model's own existing\npreference (latent knowledge) learned from pretraining, which leads to\nreasoning ability improvement. We believe that OSFT offers an efficient and\npromising alternative to more complex, reward-based training paradigms. Our\ncode is available at https://github.com/ElementQi/OnlineSFT."
                },
                "authors": [
                    {
                        "name": "Mengqi Li"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Anthony Man-Cho So"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Xiao Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Li"
                },
                "author": "Xiao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17800v2",
                "updated": "2025-10-21T17:12:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    12,
                    48,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-20T17:58:56Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    58,
                    56,
                    0,
                    293,
                    0
                ],
                "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glyph: Scaling Context Windows via Visual-Text Compression"
                },
                "summary": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph."
                },
                "authors": [
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Yusen Liu"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Yulin Fei"
                    },
                    {
                        "name": "Wenyi Hong"
                    },
                    {
                        "name": "Ruiliang Lyu"
                    },
                    {
                        "name": "Weihan Wang"
                    },
                    {
                        "name": "Zhe Su"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17017v2",
                "updated": "2025-10-21T17:12:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    12,
                    22,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-19T21:47:19Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    21,
                    47,
                    19,
                    6,
                    292,
                    0
                ],
                "title": "SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents"
                },
                "summary": "Large language model (LLM) based search agents iteratively generate queries,\nretrieve external information, and reason to answer open-domain questions.\nWhile researchers have primarily focused on improving their utility, their\nsafety behaviors remain underexplored. In this paper, we first evaluate search\nagents using red-teaming datasets and find that they are more likely to produce\nharmful outputs than base LLMs. For example, when asked \"How can I track\nsomeone's location without their consent?\", a base model refuses, whereas a\nsearch agent designed to retrieve and cite sources may lower its refusal\nthreshold, fetch documents (e.g., court cases), and, once appended, synthesize\nthem into an informative yet unsafe summary. We further show that\nutility-oriented fine-tuning intensifies this risk, motivating joint alignment\nof safety and utility. We present SafeSearch, a multi-objective reinforcement\nlearning approach that couples a final-output safety/utility reward with a\nnovel query-level shaping term that penalizes unsafe queries and rewards safe\nones. Experiments show that SafeSearch reduces agent harmfulness by over 70%\nacross three red-teaming datasets while producing safe, helpful responses, and\nmatches the QA performance of a utility-only finetuned agent; further analyses\nconfirm the effectiveness of the query-level reward in jointly improving safety\nand utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) based search agents iteratively generate queries,\nretrieve external information, and reason to answer open-domain questions.\nWhile researchers have primarily focused on improving their utility, their\nsafety behaviors remain underexplored. In this paper, we first evaluate search\nagents using red-teaming datasets and find that they are more likely to produce\nharmful outputs than base LLMs. For example, when asked \"How can I track\nsomeone's location without their consent?\", a base model refuses, whereas a\nsearch agent designed to retrieve and cite sources may lower its refusal\nthreshold, fetch documents (e.g., court cases), and, once appended, synthesize\nthem into an informative yet unsafe summary. We further show that\nutility-oriented fine-tuning intensifies this risk, motivating joint alignment\nof safety and utility. We present SafeSearch, a multi-objective reinforcement\nlearning approach that couples a final-output safety/utility reward with a\nnovel query-level shaping term that penalizes unsafe queries and rewards safe\nones. Experiments show that SafeSearch reduces agent harmfulness by over 70%\nacross three red-teaming datasets while producing safe, helpful responses, and\nmatches the QA performance of a utility-only finetuned agent; further analyses\nconfirm the effectiveness of the query-level reward in jointly improving safety\nand utility."
                },
                "authors": [
                    {
                        "name": "Qiusi Zhan"
                    },
                    {
                        "name": "Angeline Budiman-Chan"
                    },
                    {
                        "name": "Abdelrahman Zayed"
                    },
                    {
                        "name": "Xingzhi Guo"
                    },
                    {
                        "name": "Daniel Kang"
                    },
                    {
                        "name": "Joo-Kyung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Joo-Kyung Kim"
                },
                "author": "Joo-Kyung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14008v2",
                "updated": "2025-10-21T17:07:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    7,
                    41,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-15T18:39:31Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    18,
                    39,
                    31,
                    2,
                    288,
                    0
                ],
                "title": "Stop Reducing Responsibility in LLM-Powered Multi-Agent Systems to Local\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stop Reducing Responsibility in LLM-Powered Multi-Agent Systems to Local\n  Alignment"
                },
                "summary": "LLM-powered Multi-Agent Systems (LLM-MAS) unlock new potentials in\ndistributed reasoning, collaboration, and task generalization but also\nintroduce additional risks due to unguaranteed agreement, cascading\nuncertainty, and adversarial vulnerabilities. We argue that ensuring\nresponsible behavior in such systems requires a paradigm shift: from local,\nsuperficial agent-level alignment to global, systemic agreement. We\nconceptualize responsibility not as a static constraint but as a lifecycle-wide\nproperty encompassing agreement, uncertainty, and security, each requiring the\ncomplementary integration of subjective human-centered values and objective\nverifiability. Furthermore, a dual-perspective governance framework that\ncombines interdisciplinary design with human-AI collaborative oversight is\nessential for tracing and ensuring responsibility throughout the lifecycle of\nLLM-MAS. Our position views LLM-MAS not as loose collections of agents, but as\nunified, dynamic socio-technical systems that demand principled mechanisms to\nsupport each dimension of responsibility and enable ethically aligned,\nverifiably coherent, and resilient behavior for sustained, system-wide\nagreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered Multi-Agent Systems (LLM-MAS) unlock new potentials in\ndistributed reasoning, collaboration, and task generalization but also\nintroduce additional risks due to unguaranteed agreement, cascading\nuncertainty, and adversarial vulnerabilities. We argue that ensuring\nresponsible behavior in such systems requires a paradigm shift: from local,\nsuperficial agent-level alignment to global, systemic agreement. We\nconceptualize responsibility not as a static constraint but as a lifecycle-wide\nproperty encompassing agreement, uncertainty, and security, each requiring the\ncomplementary integration of subjective human-centered values and objective\nverifiability. Furthermore, a dual-perspective governance framework that\ncombines interdisciplinary design with human-AI collaborative oversight is\nessential for tracing and ensuring responsibility throughout the lifecycle of\nLLM-MAS. Our position views LLM-MAS not as loose collections of agents, but as\nunified, dynamic socio-technical systems that demand principled mechanisms to\nsupport each dimension of responsibility and enable ethically aligned,\nverifiably coherent, and resilient behavior for sustained, system-wide\nagreement."
                },
                "authors": [
                    {
                        "name": "Jinwei Hu"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Shuang Ao"
                    },
                    {
                        "name": "Zhuoyun Li"
                    },
                    {
                        "name": "Boxuan Wang"
                    },
                    {
                        "name": "Lokesh Singh"
                    },
                    {
                        "name": "Guangliang Cheng"
                    },
                    {
                        "name": "Sarvapali D. Ramchurn"
                    },
                    {
                        "name": "Xiaowei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Huang"
                },
                "author": "Xiaowei Huang",
                "arxiv_comment": "Updated manuscript of our previous version (arXiv:2502.01714). Under\n  review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17501v2",
                "updated": "2025-10-21T17:06:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    6,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-20T12:54:32Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    54,
                    32,
                    0,
                    293,
                    0
                ],
                "title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization"
                },
                "summary": "With video exploding across social media, surveillance, and education,\ncompressing long footage into concise yet faithful surrogates is crucial.\nSupervised methods learn frame/shot importance from dense labels and excel\nin-domain, but are costly and brittle across datasets; unsupervised methods\navoid labels but often miss high-level semantics and narrative cues. Recent\nzero-shot pipelines use LLMs for training-free summarization, yet remain\nsensitive to handcrafted prompts and dataset-specific normalization.We propose\na rubric-guided, pseudo-labeled prompting framework. A small subset of human\nannotations is converted into high-confidence pseudo labels and aggregated into\nstructured, dataset-adaptive scoring rubrics for interpretable scene\nevaluation. At inference, boundary scenes (first/last) are scored from their\nown descriptions, while intermediate scenes include brief summaries of adjacent\nsegments to assess progression and redundancy, enabling the LLM to balance\nlocal salience with global coherence without parameter tuning.Across three\nbenchmarks, our method is consistently effective. On SumMe and TVSum it\nachieves F1 of 57.58 and 63.05, surpassing a zero-shot baseline (56.73, 62.21)\nby +0.85 and +0.84 and approaching supervised performance. On the query-focused\nQFVS benchmark it attains 53.79 F1, beating 53.42 by +0.37 and remaining stable\nacross validation videos. These results show that rubric-guided pseudo\nlabeling, coupled with contextual prompting, stabilizes LLM-based scoring and\nyields a general, interpretable zero-shot paradigm for both generic and\nquery-focused video summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With video exploding across social media, surveillance, and education,\ncompressing long footage into concise yet faithful surrogates is crucial.\nSupervised methods learn frame/shot importance from dense labels and excel\nin-domain, but are costly and brittle across datasets; unsupervised methods\navoid labels but often miss high-level semantics and narrative cues. Recent\nzero-shot pipelines use LLMs for training-free summarization, yet remain\nsensitive to handcrafted prompts and dataset-specific normalization.We propose\na rubric-guided, pseudo-labeled prompting framework. A small subset of human\nannotations is converted into high-confidence pseudo labels and aggregated into\nstructured, dataset-adaptive scoring rubrics for interpretable scene\nevaluation. At inference, boundary scenes (first/last) are scored from their\nown descriptions, while intermediate scenes include brief summaries of adjacent\nsegments to assess progression and redundancy, enabling the LLM to balance\nlocal salience with global coherence without parameter tuning.Across three\nbenchmarks, our method is consistently effective. On SumMe and TVSum it\nachieves F1 of 57.58 and 63.05, surpassing a zero-shot baseline (56.73, 62.21)\nby +0.85 and +0.84 and approaching supervised performance. On the query-focused\nQFVS benchmark it attains 53.79 F1, beating 53.42 by +0.37 and remaining stable\nacross validation videos. These results show that rubric-guided pseudo\nlabeling, coupled with contextual prompting, stabilizes LLM-based scoring and\nyields a general, interpretable zero-shot paradigm for both generic and\nquery-focused video summarization."
                },
                "authors": [
                    {
                        "name": "Yuanli Wu"
                    },
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Yue Du"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15050v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15050v3",
                "updated": "2025-10-21T17:05:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    5,
                    38,
                    1,
                    294,
                    0
                ],
                "published": "2025-05-21T03:15:06Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    3,
                    15,
                    6,
                    2,
                    141,
                    0
                ],
                "title": "Improving the fact-checking performance of language models by relying on\n  their entailment ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the fact-checking performance of language models by relying on\n  their entailment ability"
                },
                "summary": "Automated fact-checking has been a challenging task for the research\ncommunity. Past works tried various strategies, such as end-to-end training,\nretrieval-augmented generation, and prompt engineering, to build robust\nfact-checking systems. However, their accuracy has not been very high for\nreal-world deployment. We, on the other hand, propose a simple yet effective\nstrategy, where entailed justifications generated by LLMs are used to train\nencoder-only language models (ELMs) for fact-checking. We conducted a rigorous\nset of experiments, comparing our approach with recent works and various\nprompting and fine-tuning strategies to demonstrate the superiority of our\napproach. Additionally, we did quality analysis of model explanations, ablation\nstudies, and error analysis to provide a comprehensive understanding of our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated fact-checking has been a challenging task for the research\ncommunity. Past works tried various strategies, such as end-to-end training,\nretrieval-augmented generation, and prompt engineering, to build robust\nfact-checking systems. However, their accuracy has not been very high for\nreal-world deployment. We, on the other hand, propose a simple yet effective\nstrategy, where entailed justifications generated by LLMs are used to train\nencoder-only language models (ELMs) for fact-checking. We conducted a rigorous\nset of experiments, comparing our approach with recent works and various\nprompting and fine-tuning strategies to demonstrate the superiority of our\napproach. Additionally, we did quality analysis of model explanations, ablation\nstudies, and error analysis to provide a comprehensive understanding of our\napproach."
                },
                "authors": [
                    {
                        "name": "Gaurav Kumar"
                    },
                    {
                        "name": "Debajyoti Mazumder"
                    },
                    {
                        "name": "Ayush Garg"
                    },
                    {
                        "name": "Jasabanta Patro"
                    }
                ],
                "author_detail": {
                    "name": "Jasabanta Patro"
                },
                "author": "Jasabanta Patro",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15050v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15050v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02186v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02186v2",
                "updated": "2025-10-21T17:04:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    4,
                    9,
                    1,
                    294,
                    0
                ],
                "published": "2025-07-02T22:45:39Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    22,
                    45,
                    39,
                    2,
                    183,
                    0
                ],
                "title": "EvalAssist: A Human-Centered Tool for LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvalAssist: A Human-Centered Tool for LLM-as-a-Judge"
                },
                "summary": "With the broad availability of large language models and their ability to\ngenerate vast outputs using varied prompts and configurations, determining the\nbest output for a given task requires an intensive evaluation process, one\nwhere machine learning practitioners must decide how to assess the outputs and\nthen carefully carry out the evaluation. This process is both time-consuming\nand costly. As practitioners work with an increasing number of models, they\nmust now evaluate outputs to determine which model and prompt performs best for\na given task. LLMs are increasingly used as evaluators to filter training data,\nevaluate model performance, assess harms and risks, or assist human evaluators\nwith detailed assessments. We present EvalAssist, a framework that simplifies\nthe LLM-as-a-judge workflow. The system provides an online criteria development\nenvironment, where users can interactively build, test, and share custom\nevaluation criteria in a structured and portable format. We support a set of\nLLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a\nprompt-chaining approach we developed and contributed to the UNITXT open-source\nlibrary. Additionally, our system also includes specially trained evaluators to\ndetect harms and risks in LLM outputs. We have deployed the system internally\nin our organization with several hundreds of users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the broad availability of large language models and their ability to\ngenerate vast outputs using varied prompts and configurations, determining the\nbest output for a given task requires an intensive evaluation process, one\nwhere machine learning practitioners must decide how to assess the outputs and\nthen carefully carry out the evaluation. This process is both time-consuming\nand costly. As practitioners work with an increasing number of models, they\nmust now evaluate outputs to determine which model and prompt performs best for\na given task. LLMs are increasingly used as evaluators to filter training data,\nevaluate model performance, assess harms and risks, or assist human evaluators\nwith detailed assessments. We present EvalAssist, a framework that simplifies\nthe LLM-as-a-judge workflow. The system provides an online criteria development\nenvironment, where users can interactively build, test, and share custom\nevaluation criteria in a structured and portable format. We support a set of\nLLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a\nprompt-chaining approach we developed and contributed to the UNITXT open-source\nlibrary. Additionally, our system also includes specially trained evaluators to\ndetect harms and risks in LLM outputs. We have deployed the system internally\nin our organization with several hundreds of users."
                },
                "authors": [
                    {
                        "name": "Zahra Ashktorab"
                    },
                    {
                        "name": "Werner Geyer"
                    },
                    {
                        "name": "Michael Desmond"
                    },
                    {
                        "name": "Elizabeth M. Daly"
                    },
                    {
                        "name": "Martin Santillan Cooper"
                    },
                    {
                        "name": "Qian Pan"
                    },
                    {
                        "name": "Erik Miehling"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Hyo Jin Do"
                    }
                ],
                "author_detail": {
                    "name": "Hyo Jin Do"
                },
                "author": "Hyo Jin Do",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2410.00873",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02186v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02186v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01480v2",
                "updated": "2025-10-21T17:02:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    17,
                    2,
                    48,
                    1,
                    294,
                    0
                ],
                "published": "2025-06-02T09:39:28Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    9,
                    39,
                    28,
                    0,
                    153,
                    0
                ],
                "title": "Janus-Pro-R1: Advancing Collaborative Visual Comprehension and\n  Generation via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Janus-Pro-R1: Advancing Collaborative Visual Comprehension and\n  Generation via Reinforcement Learning"
                },
                "summary": "Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify\nvisual comprehension and generation. However, these two capabilities remain\nlargely independent, as if they are two separate functions encapsulated within\nthe same model. Consequently, visual comprehension does not enhance visual\ngeneration, and the reasoning mechanisms of LLMs have not been fully integrated\nto revolutionize image generation. In this paper, we propose to enable the\ncollaborative co-evolution of visual comprehension and generation, advancing\nimage generation into an iterative introspective process. We introduce a\ntwo-stage training approach: supervised fine-tuning teaches the MLLM with the\nfoundational ability to generate genuine CoT for visual generation, while\nreinforcement learning activates its full potential via an\nexploration-exploitation trade-off. Ultimately, we unlock the Aha moment in\nvisual generation, advancing MLLMs from text-to-image tasks to unified image\ngeneration. Extensive experiments demonstrate that our model not only excels in\ntext-to-image generation and image editing, but also functions as a superior\nimage semantic evaluator with enhanced visual comprehension capabilities.\nProject Page: https://janus-pro-r1.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify\nvisual comprehension and generation. However, these two capabilities remain\nlargely independent, as if they are two separate functions encapsulated within\nthe same model. Consequently, visual comprehension does not enhance visual\ngeneration, and the reasoning mechanisms of LLMs have not been fully integrated\nto revolutionize image generation. In this paper, we propose to enable the\ncollaborative co-evolution of visual comprehension and generation, advancing\nimage generation into an iterative introspective process. We introduce a\ntwo-stage training approach: supervised fine-tuning teaches the MLLM with the\nfoundational ability to generate genuine CoT for visual generation, while\nreinforcement learning activates its full potential via an\nexploration-exploitation trade-off. Ultimately, we unlock the Aha moment in\nvisual generation, advancing MLLMs from text-to-image tasks to unified image\ngeneration. Extensive experiments demonstrate that our model not only excels in\ntext-to-image generation and image editing, but also functions as a superior\nimage semantic evaluator with enhanced visual comprehension capabilities.\nProject Page: https://janus-pro-r1.github.io."
                },
                "authors": [
                    {
                        "name": "Kaihang Pan"
                    },
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Wendong Bu"
                    },
                    {
                        "name": "Kai Shen"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Yingting Wang"
                    },
                    {
                        "name": "Yunfei Li"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Hang Zhao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18806v1",
                "updated": "2025-10-21T16:59:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    59,
                    54,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T16:59:54Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    59,
                    54,
                    1,
                    294,
                    0
                ],
                "title": "Integrating Large Language Models and Evaluating Student Outcomes in an\n  Introductory Computer Science Course",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models and Evaluating Student Outcomes in an\n  Introductory Computer Science Course"
                },
                "summary": "Generative AI (GenAI) models have broad implications for education in\ngeneral, impacting the foundations of what we teach and how we assess. This is\nespecially true in computing, where LLMs tuned for coding have demonstrated\nshockingly good performance on the types of assignments historically used in\nintroductory CS (CS1) courses. As a result, CS1 courses will need to change\nwhat skills are taught and how they are assessed. Computing education\nresearchers have begun to study student use of LLMs, but there remains much to\nbe understood about the ways that these tools affect student outcomes. In this\npaper, we present the design and evaluation of a new CS1 course at a large\nresearch-intensive university that integrates the use of LLMs as a learning\ntool for students. We describe the design principles used to create our new\nCS1-LLM course, our new course objectives, and evaluation of student outcomes\nand perceptions throughout the course as measured by assessment scores and\nsurveys. Our findings suggest that 1) student exam performance outcomes,\nincluding differences among demographic groups, are largely similar to\nhistorical outcomes for courses without integration of LLM tools, 2) large,\nopen-ended projects may be particularly valuable in an LLM context, and 3)\nstudents predominantly found the LLM tools helpful, although some had concerns\nregarding over-reliance on the tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) models have broad implications for education in\ngeneral, impacting the foundations of what we teach and how we assess. This is\nespecially true in computing, where LLMs tuned for coding have demonstrated\nshockingly good performance on the types of assignments historically used in\nintroductory CS (CS1) courses. As a result, CS1 courses will need to change\nwhat skills are taught and how they are assessed. Computing education\nresearchers have begun to study student use of LLMs, but there remains much to\nbe understood about the ways that these tools affect student outcomes. In this\npaper, we present the design and evaluation of a new CS1 course at a large\nresearch-intensive university that integrates the use of LLMs as a learning\ntool for students. We describe the design principles used to create our new\nCS1-LLM course, our new course objectives, and evaluation of student outcomes\nand perceptions throughout the course as measured by assessment scores and\nsurveys. Our findings suggest that 1) student exam performance outcomes,\nincluding differences among demographic groups, are largely similar to\nhistorical outcomes for courses without integration of LLM tools, 2) large,\nopen-ended projects may be particularly valuable in an LLM context, and 3)\nstudents predominantly found the LLM tools helpful, although some had concerns\nregarding over-reliance on the tools."
                },
                "authors": [
                    {
                        "name": "Annapurna Vadaparty"
                    },
                    {
                        "name": "David H. Smith IV"
                    },
                    {
                        "name": "Samvrit Srinath"
                    },
                    {
                        "name": "Mounika Padala"
                    },
                    {
                        "name": "Christine Alvarado"
                    },
                    {
                        "name": "Jamie Gorson Benario"
                    },
                    {
                        "name": "Daniel Zingaro"
                    },
                    {
                        "name": "Leo Porter"
                    }
                ],
                "author_detail": {
                    "name": "Leo Porter"
                },
                "author": "Leo Porter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18799v1",
                "updated": "2025-10-21T16:54:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    54,
                    21,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T16:54:21Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    54,
                    21,
                    1,
                    294,
                    0
                ],
                "title": "FeClustRE: Hierarchical Clustering and Semantic Tagging of App Features\n  from User Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FeClustRE: Hierarchical Clustering and Semantic Tagging of App Features\n  from User Reviews"
                },
                "summary": "[Context and motivation.] Extracting features from mobile app reviews is\nincreasingly important for multiple requirements engineering (RE) tasks.\nHowever, existing methods struggle to turn noisy, ambiguous feedback into\ninterpretable insights. [Question/problem.] Syntactic approaches lack semantic\ndepth, while large language models (LLMs) often miss fine-grained features or\nfail to structure them coherently. In addition, existing methods output flat\nlists of features without semantic organization, limiting interpretation and\ncomparability. Consequently, current feature extraction approaches do not\nprovide structured, meaningful representations of app features. As a result,\npractitioners face fragmented information that hinder requirement analysis,\nprioritization, and cross-app comparison, among other use cases. [Principal\nideas/results.] In this context, we propose FeClustRE, a framework integrating\nhybrid feature extraction, hierarchical clustering with auto-tuning and\nLLM-based semantic labelling. FeClustRE combines syntactic parsing with LLM\nenrichment, organizes features into clusters, and automatically generates\nmeaningful taxonomy labels. We evaluate FeClustRE on public benchmarks for\nextraction correctness and on a sample study of generative AI assistant app\nreviews for clustering quality, semantic coherence, and interpretability.\n[Contribution.] Overall, FeClustRE delivers (1) a hybrid framework for feature\nextraction and taxonomy generation, (2) an auto-tuning mechanism with a\ncomprehensive evaluation methodology, and (3) open-source and replicable\nimplementation. These contributions bridge user feedback and feature\nunderstanding, enabling deeper insights into current and emerging requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Context and motivation.] Extracting features from mobile app reviews is\nincreasingly important for multiple requirements engineering (RE) tasks.\nHowever, existing methods struggle to turn noisy, ambiguous feedback into\ninterpretable insights. [Question/problem.] Syntactic approaches lack semantic\ndepth, while large language models (LLMs) often miss fine-grained features or\nfail to structure them coherently. In addition, existing methods output flat\nlists of features without semantic organization, limiting interpretation and\ncomparability. Consequently, current feature extraction approaches do not\nprovide structured, meaningful representations of app features. As a result,\npractitioners face fragmented information that hinder requirement analysis,\nprioritization, and cross-app comparison, among other use cases. [Principal\nideas/results.] In this context, we propose FeClustRE, a framework integrating\nhybrid feature extraction, hierarchical clustering with auto-tuning and\nLLM-based semantic labelling. FeClustRE combines syntactic parsing with LLM\nenrichment, organizes features into clusters, and automatically generates\nmeaningful taxonomy labels. We evaluate FeClustRE on public benchmarks for\nextraction correctness and on a sample study of generative AI assistant app\nreviews for clustering quality, semantic coherence, and interpretability.\n[Contribution.] Overall, FeClustRE delivers (1) a hybrid framework for feature\nextraction and taxonomy generation, (2) an auto-tuning mechanism with a\ncomprehensive evaluation methodology, and (3) open-source and replicable\nimplementation. These contributions bridge user feedback and feature\nunderstanding, enabling deeper insights into current and emerging requirements."
                },
                "authors": [
                    {
                        "name": "Max Tiessler"
                    },
                    {
                        "name": "Quim Motger"
                    }
                ],
                "author_detail": {
                    "name": "Quim Motger"
                },
                "author": "Quim Motger",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18795v2",
                "updated": "2025-10-22T03:43:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    3,
                    43,
                    28,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-21T16:48:49Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    48,
                    49,
                    1,
                    294,
                    0
                ],
                "title": "ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder"
                },
                "summary": "The original CLIP text encoder is limited by a maximum input length of 77\ntokens, which hampers its ability to effectively process long texts and perform\nfine-grained semantic understanding. In addition, the CLIP text encoder lacks\nsupport for multilingual inputs. All these limitations significantly restrict\nits applicability across a broader range of tasks. Recent studies have\nattempted to replace the CLIP text encoder with an LLM-based embedder to\nenhance its ability in processing long texts, multilingual understanding, and\nfine-grained semantic comprehension. However, because the representation spaces\nof LLMs and the vision-language space of CLIP are pretrained independently\nwithout alignment priors, direct alignment using contrastive learning can\ndisrupt the intrinsic vision-language alignment in the CLIP image encoder,\nleading to an underutilization of the knowledge acquired during pre-training.\nTo address this challenge, we propose ProCLIP, a curriculum learning-based\nprogressive vision-language alignment framework to effectively align the CLIP\nimage encoder with an LLM-based embedder. Specifically, ProCLIP first distills\nknowledge from CLIP's text encoder into the LLM-based embedder to leverage\nCLIP's rich pretrained knowledge while establishing initial alignment between\nthe LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns\nthe CLIP image encoder with the LLM-based embedder through image-text\ncontrastive tuning, employing self-distillation regularization to avoid\noverfitting. To achieve a more effective alignment, instance semantic alignment\nloss and embedding structure alignment loss are employed during representation\ninheritance and contrastive tuning. The Code is available at\nhttps://github.com/VisionXLab/ProCLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The original CLIP text encoder is limited by a maximum input length of 77\ntokens, which hampers its ability to effectively process long texts and perform\nfine-grained semantic understanding. In addition, the CLIP text encoder lacks\nsupport for multilingual inputs. All these limitations significantly restrict\nits applicability across a broader range of tasks. Recent studies have\nattempted to replace the CLIP text encoder with an LLM-based embedder to\nenhance its ability in processing long texts, multilingual understanding, and\nfine-grained semantic comprehension. However, because the representation spaces\nof LLMs and the vision-language space of CLIP are pretrained independently\nwithout alignment priors, direct alignment using contrastive learning can\ndisrupt the intrinsic vision-language alignment in the CLIP image encoder,\nleading to an underutilization of the knowledge acquired during pre-training.\nTo address this challenge, we propose ProCLIP, a curriculum learning-based\nprogressive vision-language alignment framework to effectively align the CLIP\nimage encoder with an LLM-based embedder. Specifically, ProCLIP first distills\nknowledge from CLIP's text encoder into the LLM-based embedder to leverage\nCLIP's rich pretrained knowledge while establishing initial alignment between\nthe LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns\nthe CLIP image encoder with the LLM-based embedder through image-text\ncontrastive tuning, employing self-distillation regularization to avoid\noverfitting. To achieve a more effective alignment, instance semantic alignment\nloss and embedding structure alignment loss are employed during representation\ninheritance and contrastive tuning. The Code is available at\nhttps://github.com/VisionXLab/ProCLIP."
                },
                "authors": [
                    {
                        "name": "Xiaoxing Hu"
                    },
                    {
                        "name": "Kaicheng Yang"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Qi Ming"
                    },
                    {
                        "name": "Zonghao Guo"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ziyong Feng"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Xue Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xue Yang"
                },
                "author": "Xue Yang",
                "arxiv_comment": "17 pages, 5 fiugres",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18787v1",
                "updated": "2025-10-21T16:40:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    40,
                    26,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T16:40:26Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    40,
                    26,
                    1,
                    294,
                    0
                ],
                "title": "ShaRE your Data! Characterizing Datasets for LLM-based Requirements\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShaRE your Data! Characterizing Datasets for LLM-based Requirements\n  Engineering"
                },
                "summary": "[Context] Large Language Models (LLMs) rely on domain-specific datasets to\nachieve robust performance across training and inference stages. However, in\nRequirements Engineering (RE), data scarcity remains a persistent limitation\nreported in surveys and mapping studies. [Question/Problem] Although there are\nmultiple datasets supporting LLM-based RE tasks (LLM4RE), they are fragmented\nand poorly characterized, limiting reuse and comparability. This research\naddresses the limited visibility and characterization of datasets used in\nLLM4RE. We investigate which public datasets are employed, how they can be\nsystematically characterized, and which RE tasks and dataset descriptors remain\nunder-represented. [Ideas/Results] To address this, we conduct a systematic\nmapping study to identify and analyse datasets used in LLM4RE research. A total\nof 62 publicly available datasets are referenced across 43 primary studies.\nEach dataset is characterized along descriptors such as artifact type,\ngranularity, RE stage, task, domain, and language. Preliminary findings show\nmultiple research gaps, including limited coverage for elicitation tasks,\nscarce datasets for management activities beyond traceability, and limited\nmultilingual availability. [Contribution] This research preview offers a public\ncatalogue and structured characterization scheme to support dataset selection,\ncomparison, and reuse in LLM4RE research. Future work will extend the scope to\ngrey literature, as well as integration with open dataset and benchmark\nrepositories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Context] Large Language Models (LLMs) rely on domain-specific datasets to\nachieve robust performance across training and inference stages. However, in\nRequirements Engineering (RE), data scarcity remains a persistent limitation\nreported in surveys and mapping studies. [Question/Problem] Although there are\nmultiple datasets supporting LLM-based RE tasks (LLM4RE), they are fragmented\nand poorly characterized, limiting reuse and comparability. This research\naddresses the limited visibility and characterization of datasets used in\nLLM4RE. We investigate which public datasets are employed, how they can be\nsystematically characterized, and which RE tasks and dataset descriptors remain\nunder-represented. [Ideas/Results] To address this, we conduct a systematic\nmapping study to identify and analyse datasets used in LLM4RE research. A total\nof 62 publicly available datasets are referenced across 43 primary studies.\nEach dataset is characterized along descriptors such as artifact type,\ngranularity, RE stage, task, domain, and language. Preliminary findings show\nmultiple research gaps, including limited coverage for elicitation tasks,\nscarce datasets for management activities beyond traceability, and limited\nmultilingual availability. [Contribution] This research preview offers a public\ncatalogue and structured characterization scheme to support dataset selection,\ncomparison, and reuse in LLM4RE research. Future work will extend the scope to\ngrey literature, as well as integration with open dataset and benchmark\nrepositories."
                },
                "authors": [
                    {
                        "name": "Quim Motger"
                    },
                    {
                        "name": "Carlota Catot"
                    },
                    {
                        "name": "Xavier Franch"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Franch"
                },
                "author": "Xavier Franch",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18779v1",
                "updated": "2025-10-21T16:27:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    27,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T16:27:47Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    27,
                    47,
                    1,
                    294,
                    0
                ],
                "title": "KAT-Coder Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAT-Coder Technical Report"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled progress in\nagentic coding, where models autonomously reason, plan, and act within\ninteractive software development workflows. However, bridging the gap between\nstatic text-based training and dynamic real-world agentic execution remains a\ncore challenge. In this technical report, we present KAT-Coder, a large-scale\nagentic code model trained through a multi-stage curriculum encompassing\nMid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning\n(RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances\nreasoning, planning, and reflection capabilities through a corpus of real\nsoftware engineering data and synthetic agentic interactions. The SFT stage\nconstructs a million-sample dataset balancing twenty programming languages, ten\ndevelopment contexts, and ten task archetypes. The RFT stage introduces a novel\nmulti-ground-truth reward formulation for stable and sample-efficient policy\noptimization. Finally, the Reinforcement-to-Deployment phase adapts the model\nto production-grade IDE environments using Error-Masked SFT and Tree-Structured\nTrajectory Training. In summary, these stages enable KAT-Coder to achieve\nrobust tool-use reliability, instruction alignment, and long-context reasoning,\nforming a deployable foundation for real-world intelligent coding agents. Our\nKAT series 32B model, KAT-Dev, has been open-sourced on\nhttps://huggingface.co/Kwaipilot/KAT-Dev.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled progress in\nagentic coding, where models autonomously reason, plan, and act within\ninteractive software development workflows. However, bridging the gap between\nstatic text-based training and dynamic real-world agentic execution remains a\ncore challenge. In this technical report, we present KAT-Coder, a large-scale\nagentic code model trained through a multi-stage curriculum encompassing\nMid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning\n(RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances\nreasoning, planning, and reflection capabilities through a corpus of real\nsoftware engineering data and synthetic agentic interactions. The SFT stage\nconstructs a million-sample dataset balancing twenty programming languages, ten\ndevelopment contexts, and ten task archetypes. The RFT stage introduces a novel\nmulti-ground-truth reward formulation for stable and sample-efficient policy\noptimization. Finally, the Reinforcement-to-Deployment phase adapts the model\nto production-grade IDE environments using Error-Masked SFT and Tree-Structured\nTrajectory Training. In summary, these stages enable KAT-Coder to achieve\nrobust tool-use reliability, instruction alignment, and long-context reasoning,\nforming a deployable foundation for real-world intelligent coding agents. Our\nKAT series 32B model, KAT-Dev, has been open-sourced on\nhttps://huggingface.co/Kwaipilot/KAT-Dev."
                },
                "authors": [
                    {
                        "name": "Zizheng Zhan"
                    },
                    {
                        "name": "Ken Deng"
                    },
                    {
                        "name": "Xiaojiang Zhang"
                    },
                    {
                        "name": "Jinghui Wang"
                    },
                    {
                        "name": "Huaixi Tang"
                    },
                    {
                        "name": "Zhiyi Lai"
                    },
                    {
                        "name": "Haoyang Huang"
                    },
                    {
                        "name": "Wen Xiang"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Wenhao Zhuang"
                    },
                    {
                        "name": "Minglei Zhang"
                    },
                    {
                        "name": "Shaojie Wang"
                    },
                    {
                        "name": "Shangpeng Yan"
                    },
                    {
                        "name": "Kepeng Lei"
                    },
                    {
                        "name": "Zongxian Feng"
                    },
                    {
                        "name": "Huiming Wang"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Mengtong Li"
                    },
                    {
                        "name": "Mengfei Xie"
                    },
                    {
                        "name": "Yinghan Cui"
                    },
                    {
                        "name": "Xuxing Chen"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Weihao Li"
                    },
                    {
                        "name": "Wenqiang Zhu"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jingxuan Xu"
                    },
                    {
                        "name": "Songwei Yu"
                    },
                    {
                        "name": "Yifan Yao"
                    },
                    {
                        "name": "Xinping Lei"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Junqi Xiong"
                    },
                    {
                        "name": "Zuchen Gao"
                    },
                    {
                        "name": "Dailin Li"
                    },
                    {
                        "name": "Haimo Li"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yuqun Zhang"
                    },
                    {
                        "name": "Junyi Peng"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Bin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Bin Chen"
                },
                "author": "Bin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18766v1",
                "updated": "2025-10-21T16:14:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    14,
                    3,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T16:14:03Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    14,
                    3,
                    1,
                    294,
                    0
                ],
                "title": "Sharing the Load: Distributed Model-Predictive Control for Precise\n  Multi-Rover Cargo Transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharing the Load: Distributed Model-Predictive Control for Precise\n  Multi-Rover Cargo Transport"
                },
                "summary": "For autonomous cargo transportation, teams of mobile robots can provide more\noperational flexibility than a single large robot. In these scenarios,\nprecision in both inter-vehicle distance and path tracking is key. With this\nmotivation, we develop a distributed model-predictive controller (MPC) for\nmulti-vehicle cargo operations that builds on the precise path-tracking of\nlidar teach and repeat. To carry cargo, a following vehicle must maintain a\nEuclidean distance offset from a lead vehicle regardless of the path curvature.\nOur approach uses a shared map to localize the robots relative to each other\nwithout GNSS or direct observations. We compare our approach to a centralized\nMPC and a baseline approach that directly measures the inter-vehicle distance.\nThe distributed MPC shows equivalent nominal performance to the more complex\ncentralized MPC. Using a direct measurement of the relative distance between\nthe leader and follower shows improved tracking performance in close-range\nscenarios but struggles with long-range offsets. The operational flexibility\nprovided by distributing the computation makes it well suited for real\ndeployments. We evaluate four types of convoyed path trackers with over 10 km\nof driving in a coupled convoy. With convoys of two and three rovers, the\nproposed distributed MPC method works in real-time to allow map-based convoying\nto maintain maximum spacing within 20 cm of the target in various conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For autonomous cargo transportation, teams of mobile robots can provide more\noperational flexibility than a single large robot. In these scenarios,\nprecision in both inter-vehicle distance and path tracking is key. With this\nmotivation, we develop a distributed model-predictive controller (MPC) for\nmulti-vehicle cargo operations that builds on the precise path-tracking of\nlidar teach and repeat. To carry cargo, a following vehicle must maintain a\nEuclidean distance offset from a lead vehicle regardless of the path curvature.\nOur approach uses a shared map to localize the robots relative to each other\nwithout GNSS or direct observations. We compare our approach to a centralized\nMPC and a baseline approach that directly measures the inter-vehicle distance.\nThe distributed MPC shows equivalent nominal performance to the more complex\ncentralized MPC. Using a direct measurement of the relative distance between\nthe leader and follower shows improved tracking performance in close-range\nscenarios but struggles with long-range offsets. The operational flexibility\nprovided by distributing the computation makes it well suited for real\ndeployments. We evaluate four types of convoyed path trackers with over 10 km\nof driving in a coupled convoy. With convoys of two and three rovers, the\nproposed distributed MPC method works in real-time to allow map-based convoying\nto maintain maximum spacing within 20 cm of the target in various conditions."
                },
                "authors": [
                    {
                        "name": "Alexander Krawciw"
                    },
                    {
                        "name": "Sven Lilge"
                    },
                    {
                        "name": "Luka Antonyshyn"
                    },
                    {
                        "name": "Timothy D. Barfoot"
                    }
                ],
                "author_detail": {
                    "name": "Timothy D. Barfoot"
                },
                "author": "Timothy D. Barfoot",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10806v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10806v2",
                "updated": "2025-10-21T16:10:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    10,
                    0,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-12T20:52:43Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    20,
                    52,
                    43,
                    6,
                    285,
                    0
                ],
                "title": "Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based\n  Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based\n  Structures"
                },
                "summary": "Large Language Models (LLMs) are adept at generating responses based on\ninformation within their context. While this ability is useful for interacting\nwith structured data like code files, another popular method,\nRetrieval-Augmented Generation (RAG), retrieves relevant documents to augment\nthe model's in-context learning. However, it is not well-explored how to best\nrepresent this retrieved knowledge for generating responses on structured data,\nparticularly hierarchical structures like trees. In this work, we propose a\nnovel bottom-up method to linearize knowledge from tree-like structures (like a\nGitHub repository) by generating implicit, aggregated summaries at each\nhierarchical level. This approach enables the knowledge to be stored in a\nknowledge base and used directly with RAG. We then compare our method to using\nRAG on raw, unstructured code, evaluating the accuracy and quality of the\ngenerated responses. Our results show that while response quality is comparable\nacross both methods, our approach generates over 68% fewer documents in the\nretriever, a significant gain in efficiency. This finding suggests that\nleveraging implicit, linearized knowledge may be a highly effective and\nscalable strategy for handling complex, hierarchical data structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are adept at generating responses based on\ninformation within their context. While this ability is useful for interacting\nwith structured data like code files, another popular method,\nRetrieval-Augmented Generation (RAG), retrieves relevant documents to augment\nthe model's in-context learning. However, it is not well-explored how to best\nrepresent this retrieved knowledge for generating responses on structured data,\nparticularly hierarchical structures like trees. In this work, we propose a\nnovel bottom-up method to linearize knowledge from tree-like structures (like a\nGitHub repository) by generating implicit, aggregated summaries at each\nhierarchical level. This approach enables the knowledge to be stored in a\nknowledge base and used directly with RAG. We then compare our method to using\nRAG on raw, unstructured code, evaluating the accuracy and quality of the\ngenerated responses. Our results show that while response quality is comparable\nacross both methods, our approach generates over 68% fewer documents in the\nretriever, a significant gain in efficiency. This finding suggests that\nleveraging implicit, linearized knowledge may be a highly effective and\nscalable strategy for handling complex, hierarchical data structures."
                },
                "authors": [
                    {
                        "name": "Mihir Gupte"
                    },
                    {
                        "name": "Paolo Giusto"
                    },
                    {
                        "name": "Ramesh S"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh S"
                },
                "author": "Ramesh S",
                "arxiv_comment": "Waiting for Conference Response",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10806v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10806v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03655v2",
                "updated": "2025-10-21T16:09:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    9,
                    4,
                    1,
                    294,
                    0
                ],
                "published": "2025-06-04T07:47:21Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    7,
                    47,
                    21,
                    2,
                    155,
                    0
                ],
                "title": "Facts are Harder Than Opinions -- A Multilingual, Comparative Analysis\n  of LLM-Based Fact-Checking Reliability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facts are Harder Than Opinions -- A Multilingual, Comparative Analysis\n  of LLM-Based Fact-Checking Reliability"
                },
                "summary": "The proliferation of misinformation necessitates scalable, automated\nfact-checking solutions. Yet, current benchmarks often overlook multilingual\nand topical diversity. This paper introduces a novel, dynamically extensible\ndata set that includes 61,514 claims in multiple languages and topics,\nextending existing datasets up to 2024. Through a comprehensive evaluation of\nfive prominent Large Language Models (LLMs), including GPT-4o, GPT-3.5 Turbo,\nLLaMA 3.1, and Mixtral 8x7B, we identify significant performance gaps between\ndifferent languages and topics. While overall GPT-4o achieves the highest\naccuracy, it declines to classify 43% of claims. Across all models,\nfactual-sounding claims are misclassified more often than opinions, revealing a\nkey vulnerability. These findings underscore the need for caution and highlight\nchallenges in deploying LLM-based fact-checking systems at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of misinformation necessitates scalable, automated\nfact-checking solutions. Yet, current benchmarks often overlook multilingual\nand topical diversity. This paper introduces a novel, dynamically extensible\ndata set that includes 61,514 claims in multiple languages and topics,\nextending existing datasets up to 2024. Through a comprehensive evaluation of\nfive prominent Large Language Models (LLMs), including GPT-4o, GPT-3.5 Turbo,\nLLaMA 3.1, and Mixtral 8x7B, we identify significant performance gaps between\ndifferent languages and topics. While overall GPT-4o achieves the highest\naccuracy, it declines to classify 43% of claims. Across all models,\nfactual-sounding claims are misclassified more often than opinions, revealing a\nkey vulnerability. These findings underscore the need for caution and highlight\nchallenges in deploying LLM-based fact-checking systems at scale."
                },
                "authors": [
                    {
                        "name": "Lorraine Saju"
                    },
                    {
                        "name": "Arnim Bleier"
                    },
                    {
                        "name": "Jana Lasser"
                    },
                    {
                        "name": "Claudia Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Claudia Wagner"
                },
                "author": "Claudia Wagner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18898v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18898v3",
                "updated": "2025-10-21T16:01:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    1,
                    11,
                    1,
                    294,
                    0
                ],
                "published": "2025-08-26T10:14:16Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    14,
                    16,
                    1,
                    238,
                    0
                ],
                "title": "Interpretable Decision-Making for End-to-End Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Decision-Making for End-to-End Autonomous Driving"
                },
                "summary": "Trustworthy AI is mandatory for the broad deployment of autonomous vehicles.\nAlthough end-to-end approaches derive control commands directly from raw data,\ninterpreting these decisions remains challenging, especially in complex urban\nscenarios. This is mainly attributed to very deep neural networks with\nnon-linear decision boundaries, making it challenging to grasp the logic behind\nAI-driven decisions. This paper presents a method to enhance interpretability\nwhile optimizing control commands in autonomous driving. To address this, we\npropose loss functions that promote the interpretability of our model by\ngenerating sparse and localized feature maps. The feature activations allow us\nto explain which image regions contribute to the predicted control command. We\nconduct comprehensive ablation studies on the feature extraction step and\nvalidate our method on the CARLA benchmarks. We also demonstrate that our\napproach improves interpretability, which correlates with reducing infractions,\nyielding a safer, high-performance driving model. Notably, our monocular,\nnon-ensemble model surpasses the top-performing approaches from the CARLA\nLeaderboard by achieving lower infraction scores and the highest route\ncompletion rate, all while ensuring interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy AI is mandatory for the broad deployment of autonomous vehicles.\nAlthough end-to-end approaches derive control commands directly from raw data,\ninterpreting these decisions remains challenging, especially in complex urban\nscenarios. This is mainly attributed to very deep neural networks with\nnon-linear decision boundaries, making it challenging to grasp the logic behind\nAI-driven decisions. This paper presents a method to enhance interpretability\nwhile optimizing control commands in autonomous driving. To address this, we\npropose loss functions that promote the interpretability of our model by\ngenerating sparse and localized feature maps. The feature activations allow us\nto explain which image regions contribute to the predicted control command. We\nconduct comprehensive ablation studies on the feature extraction step and\nvalidate our method on the CARLA benchmarks. We also demonstrate that our\napproach improves interpretability, which correlates with reducing infractions,\nyielding a safer, high-performance driving model. Notably, our monocular,\nnon-ensemble model surpasses the top-performing approaches from the CARLA\nLeaderboard by achieving lower infraction scores and the highest route\ncompletion rate, all while ensuring interpretability."
                },
                "authors": [
                    {
                        "name": "Mona Mirzaie"
                    },
                    {
                        "name": "Bodo Rosenhahn"
                    }
                ],
                "author_detail": {
                    "name": "Bodo Rosenhahn"
                },
                "author": "Bodo Rosenhahn",
                "arxiv_comment": "Accepted to the ICCV 2025 2nd Workshop on the Challenge Of\n  Out-of-Label Hazards in Autonomous Driving (2COOOL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18898v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18898v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01466v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01466v3",
                "updated": "2025-10-21T15:50:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    50,
                    28,
                    1,
                    294,
                    0
                ],
                "published": "2024-05-02T16:55:03Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    55,
                    3,
                    3,
                    123,
                    0
                ],
                "title": "A Systematic Literature Review on Large Language Models for Automated\n  Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Literature Review on Large Language Models for Automated\n  Program Repair"
                },
                "summary": "Automated Program Repair (APR) attempts to patch software bugs and reduce\nmanual debugging efforts. Very recently, with the advances in Large Language\nModels (LLMs), an increasing number of APR techniques have been proposed,\nfacilitating software development and maintenance and demonstrating remarkable\nperformance. However, due to ongoing explorations in the LLM-based APR field,\nit is challenging for researchers to understand the current achievements,\nchallenges, and potential opportunities. This work provides the first\nsystematic literature review to summarize the applications of LLMs in APR\nbetween 2020 and 2025. We analyze 189 relevant papers from LLMs, APR and their\nintegration perspectives. First, we categorize existing popular LLMs that are\napplied to support APR and outline four types of utilization strategies for\ntheir deployment. Besides, we detail some specific repair scenarios that\nbenefit from LLMs, e.g., semantic bugs and security vulnerabilities.\nFurthermore, we discuss several critical aspects of integrating LLMs into APR\nresearch, e.g., input forms and open science. Finally, we highlight a set of\nchallenges remaining to be investigated and the potential guidelines for future\nresearch. Overall, our paper provides a systematic overview of the research\nlandscape to the APR community, helping researchers gain a comprehensive\nunderstanding of achievements and promote future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair (APR) attempts to patch software bugs and reduce\nmanual debugging efforts. Very recently, with the advances in Large Language\nModels (LLMs), an increasing number of APR techniques have been proposed,\nfacilitating software development and maintenance and demonstrating remarkable\nperformance. However, due to ongoing explorations in the LLM-based APR field,\nit is challenging for researchers to understand the current achievements,\nchallenges, and potential opportunities. This work provides the first\nsystematic literature review to summarize the applications of LLMs in APR\nbetween 2020 and 2025. We analyze 189 relevant papers from LLMs, APR and their\nintegration perspectives. First, we categorize existing popular LLMs that are\napplied to support APR and outline four types of utilization strategies for\ntheir deployment. Besides, we detail some specific repair scenarios that\nbenefit from LLMs, e.g., semantic bugs and security vulnerabilities.\nFurthermore, we discuss several critical aspects of integrating LLMs into APR\nresearch, e.g., input forms and open science. Finally, we highlight a set of\nchallenges remaining to be investigated and the potential guidelines for future\nresearch. Overall, our paper provides a systematic overview of the research\nlandscape to the APR community, helping researchers gain a comprehensive\nunderstanding of achievements and promote future research."
                },
                "authors": [
                    {
                        "name": "Quanjun Zhang"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Yang Xie"
                    },
                    {
                        "name": "YuXiang Ma"
                    },
                    {
                        "name": "Weisong Sun"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "arxiv_comment": "update new papers, up to September 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01466v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01466v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18743v1",
                "updated": "2025-10-21T15:49:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    49,
                    46,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:49:46Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    49,
                    46,
                    1,
                    294,
                    0
                ],
                "title": "Wireless-Fed Pinching-Antenna Systems (Wi-PASS) for NextG Wireless\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless-Fed Pinching-Antenna Systems (Wi-PASS) for NextG Wireless\n  Networks"
                },
                "summary": "Waveguide-based pinching-antenna systems (PASS) have recently emerged as a\npromising solution to mitigate severe propagation losses in millimeter-wave and\nterahertz bands by intelligently and flexibly establishing line-of-sight links.\nHowever, their reliance on wire-based feeding confines deployment to areas near\nthe base station (BS), limiting installation flexibility and making them\ncost-ineffective for serving distant users or regions. To overcome this\nchallenge, this article proposes wireless-fed pinchingantenna systems\n(Wi-PASS), which employ wireless feeding to energize waveguides. Wi-PASS offer\na practical and cost-efficient means to extend coverage beyond the BS vicinity.\nSeveral indoor and outdoor use cases demonstrate Wi-PASS advantages over PASS.\nNumerical results further show that Wi-PASS deliver higher data rates than\nconventional fixed-antenna systems, confirming the superior feasibility and\nperformance of Wi-PASS. Key future research directions are also discussed to\nadvance Wi-PASS deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Waveguide-based pinching-antenna systems (PASS) have recently emerged as a\npromising solution to mitigate severe propagation losses in millimeter-wave and\nterahertz bands by intelligently and flexibly establishing line-of-sight links.\nHowever, their reliance on wire-based feeding confines deployment to areas near\nthe base station (BS), limiting installation flexibility and making them\ncost-ineffective for serving distant users or regions. To overcome this\nchallenge, this article proposes wireless-fed pinchingantenna systems\n(Wi-PASS), which employ wireless feeding to energize waveguides. Wi-PASS offer\na practical and cost-efficient means to extend coverage beyond the BS vicinity.\nSeveral indoor and outdoor use cases demonstrate Wi-PASS advantages over PASS.\nNumerical results further show that Wi-PASS deliver higher data rates than\nconventional fixed-antenna systems, confirming the superior feasibility and\nperformance of Wi-PASS. Key future research directions are also discussed to\nadvance Wi-PASS deployment."
                },
                "authors": [
                    {
                        "name": "Kasun R. Wijewardhana"
                    },
                    {
                        "name": "Animesh Yadav"
                    },
                    {
                        "name": "Ming Zeng"
                    },
                    {
                        "name": "Mohamed Elsayed"
                    },
                    {
                        "name": "Octavia A. Dobre"
                    },
                    {
                        "name": "Zhiguo Ding"
                    }
                ],
                "author_detail": {
                    "name": "Zhiguo Ding"
                },
                "author": "Zhiguo Ding",
                "arxiv_comment": "14 pages, 5 figures, For Potential Publication in the IEEE\n  Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03963v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03963v3",
                "updated": "2025-10-21T15:47:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    47,
                    20,
                    1,
                    294,
                    0
                ],
                "published": "2025-08-05T22:58:54Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    22,
                    58,
                    54,
                    1,
                    217,
                    0
                ],
                "title": "Can Large Language Models Adequately Perform Symbolic Reasoning Over\n  Time Series?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Adequately Perform Symbolic Reasoning Over\n  Time Series?"
                },
                "summary": "Uncovering hidden symbolic laws from time series data, as an aspiration\ndating back to Kepler's discovery of planetary motion, remains a core challenge\nin scientific discovery and artificial intelligence. While Large Language\nModels show promise in structured reasoning tasks, their ability to infer\ninterpretable, context-aligned symbolic structures from time series data is\nstill underexplored. To systematically evaluate this capability, we introduce\nSymbolBench, a comprehensive benchmark designed to assess symbolic reasoning\nover real-world time series across three tasks: multivariate symbolic\nregression, Boolean network inference, and causal discovery. Unlike prior\nefforts limited to simple algebraic equations, SymbolBench spans a diverse set\nof symbolic forms with varying complexity. We further propose a unified\nframework that integrates LLMs with genetic programming to form a closed-loop\nsymbolic reasoning system, where LLMs act both as predictors and evaluators.\nOur empirical results reveal key strengths and limitations of current models,\nhighlighting the importance of combining domain knowledge, context alignment,\nand reasoning structure to improve LLMs in automated scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering hidden symbolic laws from time series data, as an aspiration\ndating back to Kepler's discovery of planetary motion, remains a core challenge\nin scientific discovery and artificial intelligence. While Large Language\nModels show promise in structured reasoning tasks, their ability to infer\ninterpretable, context-aligned symbolic structures from time series data is\nstill underexplored. To systematically evaluate this capability, we introduce\nSymbolBench, a comprehensive benchmark designed to assess symbolic reasoning\nover real-world time series across three tasks: multivariate symbolic\nregression, Boolean network inference, and causal discovery. Unlike prior\nefforts limited to simple algebraic equations, SymbolBench spans a diverse set\nof symbolic forms with varying complexity. We further propose a unified\nframework that integrates LLMs with genetic programming to form a closed-loop\nsymbolic reasoning system, where LLMs act both as predictors and evaluators.\nOur empirical results reveal key strengths and limitations of current models,\nhighlighting the importance of combining domain knowledge, context alignment,\nand reasoning structure to improve LLMs in automated scientific discovery."
                },
                "authors": [
                    {
                        "name": "Zewen Liu"
                    },
                    {
                        "name": "Juntong Ni"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Max S. Y. Lau"
                    },
                    {
                        "name": "Wenpeng Yin"
                    },
                    {
                        "name": "Wei Jin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Jin"
                },
                "author": "Wei Jin",
                "arxiv_comment": "version2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03963v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03963v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15700v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15700v2",
                "updated": "2025-10-21T15:39:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    39,
                    4,
                    1,
                    294,
                    0
                ],
                "published": "2024-10-21T07:18:23Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    18,
                    23,
                    0,
                    295,
                    0
                ],
                "title": "InternLM2.5-StepProver: Advancing Automated Theorem Proving via\n  Critic-Guided Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InternLM2.5-StepProver: Advancing Automated Theorem Proving via\n  Critic-Guided Search"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools in mathematical\ntheorem proving, particularly when utilizing formal languages such as LEAN. A\nprevalent proof method involves the LLM prover iteratively constructing the\nproof tactic by tactic, typically following a best-first search scheme.\nHowever, this method often ignores the critical preference information inside\nthe existing tactic trajectories, hindering the search for deeper proofs. We\npropose an intuitive yet effective method, which utilizes a critic model to\ncapture the preference information and to guide the search of the prover model\nat runtime. Given the prover-critic framework, a large-scale expert iteration\nwith more than 20,000 CPU days is then applied to further fine-tune the prover\nand the critic. The trained InternLM2.5-StepProver critic significantly boosts\nthe performance of the prover model (59.4% to 65.9%). We also analyze the\nimpact of the critic on various aspects of the theorem proving process during\nexpert iteration, providing insights into its effectiveness. We open-source our\nmodels and searched proofs at https://github.com/InternLM/InternLM-Math and\nhttps://huggingface.co/datasets/internlm/Lean-Workbook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools in mathematical\ntheorem proving, particularly when utilizing formal languages such as LEAN. A\nprevalent proof method involves the LLM prover iteratively constructing the\nproof tactic by tactic, typically following a best-first search scheme.\nHowever, this method often ignores the critical preference information inside\nthe existing tactic trajectories, hindering the search for deeper proofs. We\npropose an intuitive yet effective method, which utilizes a critic model to\ncapture the preference information and to guide the search of the prover model\nat runtime. Given the prover-critic framework, a large-scale expert iteration\nwith more than 20,000 CPU days is then applied to further fine-tune the prover\nand the critic. The trained InternLM2.5-StepProver critic significantly boosts\nthe performance of the prover model (59.4% to 65.9%). We also analyze the\nimpact of the critic on various aspects of the theorem proving process during\nexpert iteration, providing insights into its effectiveness. We open-source our\nmodels and searched proofs at https://github.com/InternLM/InternLM-Math and\nhttps://huggingface.co/datasets/internlm/Lean-Workbook."
                },
                "authors": [
                    {
                        "name": "Zijian Wu"
                    },
                    {
                        "name": "Suozhi Huang"
                    },
                    {
                        "name": "Zhejian Zhou"
                    },
                    {
                        "name": "Huaiyuan Ying"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15700v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15700v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13982v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13982v3",
                "updated": "2025-10-21T15:32:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    32,
                    35,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-15T18:05:06Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    18,
                    5,
                    6,
                    2,
                    288,
                    0
                ],
                "title": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires\n  Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires\n  Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations"
                },
                "summary": "What if artificial agents could not just communicate, but also evolve, adapt,\nand reshape their worlds in ways we cannot fully predict? With llm now powering\nmulti-agent systems and social simulations, we are witnessing new possibilities\nfor modeling open-ended, ever-changing environments. Yet, most current\nsimulations remain constrained within static sandboxes, characterized by\npredefined tasks, limited dynamics, and rigid evaluation criteria. These\nlimitations prevent them from capturing the complexity of real-world societies.\nIn this paper, we argue that static, task-specific benchmarks are fundamentally\ninadequate and must be rethought. We critically review emerging architectures\nthat blend llm with multi-agent dynamics, highlight key hurdles such as\nbalancing stability and diversity, evaluating unexpected behaviors, and scaling\nto greater complexity, and introduce a fresh taxonomy for this rapidly evolving\nfield. Finally, we present a research roadmap centered on open-endedness,\ncontinuous co-evolution, and the development of resilient, socially aligned AI\necosystems. We call on the community to move beyond static paradigms and help\nshape the next generation of adaptive, socially-aware multi-agent simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What if artificial agents could not just communicate, but also evolve, adapt,\nand reshape their worlds in ways we cannot fully predict? With llm now powering\nmulti-agent systems and social simulations, we are witnessing new possibilities\nfor modeling open-ended, ever-changing environments. Yet, most current\nsimulations remain constrained within static sandboxes, characterized by\npredefined tasks, limited dynamics, and rigid evaluation criteria. These\nlimitations prevent them from capturing the complexity of real-world societies.\nIn this paper, we argue that static, task-specific benchmarks are fundamentally\ninadequate and must be rethought. We critically review emerging architectures\nthat blend llm with multi-agent dynamics, highlight key hurdles such as\nbalancing stability and diversity, evaluating unexpected behaviors, and scaling\nto greater complexity, and introduce a fresh taxonomy for this rapidly evolving\nfield. Finally, we present a research roadmap centered on open-endedness,\ncontinuous co-evolution, and the development of resilient, socially aligned AI\necosystems. We call on the community to move beyond static paradigms and help\nshape the next generation of adaptive, socially-aware multi-agent simulations."
                },
                "authors": [
                    {
                        "name": "Jinkun Chen"
                    },
                    {
                        "name": "Sher Badshah"
                    },
                    {
                        "name": "Xuemin Yu"
                    },
                    {
                        "name": "Sijia Han"
                    }
                ],
                "author_detail": {
                    "name": "Sijia Han"
                },
                "author": "Sijia Han",
                "arxiv_comment": "Preprint; feedback welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13982v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13982v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18731v1",
                "updated": "2025-10-21T15:32:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    32,
                    26,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:32:26Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    32,
                    26,
                    1,
                    294,
                    0
                ],
                "title": "Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate\n  Lost-in-Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate\n  Lost-in-Conversation"
                },
                "summary": "Large Language Models demonstrate strong capabilities in single-turn\ninstruction following but suffer from Lost-in-Conversation (LiC), a degradation\nin performance as information is revealed progressively in multi-turn settings.\nMotivated by the current progress on Reinforcement Learning with Verifiable\nRewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable\nAccuracy and Abstention Rewards (RLAAR), a framework that encourages models not\nonly to generate correct answers, but also to judge the solvability of\nquestions in the multi-turn conversation setting. Our approach employs a\ncompetence-gated curriculum that incrementally increases dialogue difficulty\n(in terms of instruction shards), stabilizing training while promoting\nreliability. Using multi-turn, on-policy rollouts and a mixed-reward system,\nRLAAR teaches models to balance problem-solving with informed abstention,\nreducing premature answering behaviors that cause LiC. Evaluated on LiC\nbenchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to\n75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together,\nthese results provide a practical recipe for building multi-turn reliable and\ntrustworthy LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models demonstrate strong capabilities in single-turn\ninstruction following but suffer from Lost-in-Conversation (LiC), a degradation\nin performance as information is revealed progressively in multi-turn settings.\nMotivated by the current progress on Reinforcement Learning with Verifiable\nRewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable\nAccuracy and Abstention Rewards (RLAAR), a framework that encourages models not\nonly to generate correct answers, but also to judge the solvability of\nquestions in the multi-turn conversation setting. Our approach employs a\ncompetence-gated curriculum that incrementally increases dialogue difficulty\n(in terms of instruction shards), stabilizing training while promoting\nreliability. Using multi-turn, on-policy rollouts and a mixed-reward system,\nRLAAR teaches models to balance problem-solving with informed abstention,\nreducing premature answering behaviors that cause LiC. Evaluated on LiC\nbenchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to\n75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together,\nthese results provide a practical recipe for building multi-turn reliable and\ntrustworthy LLMs."
                },
                "authors": [
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04501v2",
                "updated": "2025-10-21T15:29:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    29,
                    40,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-02T03:59:40Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    3,
                    59,
                    40,
                    1,
                    245,
                    0
                ],
                "title": "Understanding Reinforcement Learning for Model Training, and future\n  directions with GRAPE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Reinforcement Learning for Model Training, and future\n  directions with GRAPE"
                },
                "summary": "This paper provides a self-contained, from-scratch, exposition of key\nalgorithms for instruction tuning of models: SFT, Rejection Sampling,\nREINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy\nOptimization (PPO), Group Relative Policy Optimization (GRPO), and Direct\nPreference Optimization (DPO). Explanations of these algorithms often assume\nprior knowledge, lack critical details, and/or are overly generalized and\ncomplex. Here, each method is discussed and developed step by step using\nsimplified and explicit notation focused on LLMs, aiming to eliminate ambiguity\nand provide a clear and intuitive understanding of the concepts. By minimizing\ndetours into the broader RL literature and connecting concepts to LLMs, we\neliminate superfluous abstractions and reduce cognitive overhead. Following\nthis exposition, we provide a literature review of new techniques and\napproaches beyond those detailed. Finally, new ideas for research and\nexploration in the form of GRAPE (Generalized Relative Advantage Policy\nEvolution) are presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a self-contained, from-scratch, exposition of key\nalgorithms for instruction tuning of models: SFT, Rejection Sampling,\nREINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy\nOptimization (PPO), Group Relative Policy Optimization (GRPO), and Direct\nPreference Optimization (DPO). Explanations of these algorithms often assume\nprior knowledge, lack critical details, and/or are overly generalized and\ncomplex. Here, each method is discussed and developed step by step using\nsimplified and explicit notation focused on LLMs, aiming to eliminate ambiguity\nand provide a clear and intuitive understanding of the concepts. By minimizing\ndetours into the broader RL literature and connecting concepts to LLMs, we\neliminate superfluous abstractions and reduce cognitive overhead. Following\nthis exposition, we provide a literature review of new techniques and\napproaches beyond those detailed. Finally, new ideas for research and\nexploration in the form of GRAPE (Generalized Relative Advantage Policy\nEvolution) are presented."
                },
                "authors": [
                    {
                        "name": "Rohit Patel"
                    }
                ],
                "author_detail": {
                    "name": "Rohit Patel"
                },
                "author": "Rohit Patel",
                "arxiv_comment": "35 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T05, 62M45, 68T50, 90C40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18728v1",
                "updated": "2025-10-21T15:28:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    28,
                    20,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:28:20Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    28,
                    20,
                    1,
                    294,
                    0
                ],
                "title": "HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) remain vulnerable to multi-turn jailbreak\nattacks. We introduce HarmNet, a modular framework comprising ThoughtNet, a\nhierarchical semantic network; a feedback-driven Simulator for iterative query\nrefinement; and a Network Traverser for real-time adaptive attack execution.\nHarmNet systematically explores and refines the adversarial space to uncover\nstealthy, high-success attack paths. Experiments across closed-source and\nopen-source LLMs show that HarmNet outperforms state-of-the-art methods,\nachieving higher attack success rates. For example, on Mistral-7B, HarmNet\nachieves a 99.4% attack success rate, 13.9% higher than the best baseline.\nIndex terms: jailbreak attacks; large language models; adversarial framework;\nquery refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) remain vulnerable to multi-turn jailbreak\nattacks. We introduce HarmNet, a modular framework comprising ThoughtNet, a\nhierarchical semantic network; a feedback-driven Simulator for iterative query\nrefinement; and a Network Traverser for real-time adaptive attack execution.\nHarmNet systematically explores and refines the adversarial space to uncover\nstealthy, high-success attack paths. Experiments across closed-source and\nopen-source LLMs show that HarmNet outperforms state-of-the-art methods,\nachieving higher attack success rates. For example, on Mistral-7B, HarmNet\nachieves a 99.4% attack success rate, 13.9% higher than the best baseline.\nIndex terms: jailbreak attacks; large language models; adversarial framework;\nquery refinement."
                },
                "authors": [
                    {
                        "name": "Sidhant Narula"
                    },
                    {
                        "name": "Javad Rafiei Asl"
                    },
                    {
                        "name": "Mohammad Ghasemigol"
                    },
                    {
                        "name": "Eduardo Blanco"
                    },
                    {
                        "name": "Daniel Takabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Takabi"
                },
                "author": "Daniel Takabi",
                "arxiv_comment": "This paper has been accepted for presentation at the Conference on\n  Applied Machine Learning in Information Security (CAMLIS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14576v3",
                "updated": "2025-10-21T15:13:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    13,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2024-01-26T00:27:00Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    0,
                    27,
                    0,
                    4,
                    26,
                    0
                ],
                "title": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints"
                },
                "summary": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings."
                },
                "authors": [
                    {
                        "name": "Steven W. D. Chien"
                    },
                    {
                        "name": "Kento Sato"
                    },
                    {
                        "name": "Artur Podobas"
                    },
                    {
                        "name": "Niclas Jansson"
                    },
                    {
                        "name": "Stefano Markidis"
                    },
                    {
                        "name": "Michio Honda"
                    }
                ],
                "author_detail": {
                    "name": "Michio Honda"
                },
                "author": "Michio Honda",
                "arxiv_doi": "10.1145/3772052.3772212",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772052.3772212",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SoCC 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18713v1",
                "updated": "2025-10-21T15:11:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    11,
                    1,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:11:01Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    11,
                    1,
                    1,
                    294,
                    0
                ],
                "title": "Preference-based Reinforcement Learning beyond Pairwise Comparisons:\n  Benefits of Multiple Options",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-based Reinforcement Learning beyond Pairwise Comparisons:\n  Benefits of Multiple Options"
                },
                "summary": "We study online preference-based reinforcement learning (PbRL) with the goal\nof improving sample efficiency. While a growing body of theoretical work has\nemerged-motivated by PbRL's recent empirical success, particularly in aligning\nlarge language models (LLMs)-most existing studies focus only on pairwise\ncomparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024,\nThekumparampil et al., 2024) have explored using multiple comparisons and\nranking feedback, but their performance guarantees fail to improve-and can even\ndeteriorate-as the feedback length increases, despite the richer information\navailable. To address this gap, we adopt the Plackett-Luce (PL) model for\nranking feedback over action subsets and propose M-AUPO, an algorithm that\nselects multiple actions by maximizing the average uncertainty within the\noffered subset. We prove that M-AUPO achieves a suboptimality gap of\n$\\tilde{\\mathcal{O}}\\left( \\frac{d}{T} \\sqrt{ \\sum_{t=1}^T \\frac{1}{|S_t|}}\n\\right)$, where $T$ is the total number of rounds, $d$ is the feature\ndimension, and $|S_t|$ is the size of the subset at round $t$. This result\nshows that larger subsets directly lead to improved performance and, notably,\nthe bound avoids the exponential dependence on the unknown parameter's norm,\nwhich was a fundamental limitation in most previous works. Moreover, we\nestablish a near-matching lower bound of $\\Omega \\left( \\frac{d}{K \\sqrt{T}}\n\\right)$, where $K$ is the maximum subset size. To the best of our knowledge,\nthis is the first theoretical result in PbRL with ranking feedback that\nexplicitly shows improved sample efficiency as a function of the subset size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study online preference-based reinforcement learning (PbRL) with the goal\nof improving sample efficiency. While a growing body of theoretical work has\nemerged-motivated by PbRL's recent empirical success, particularly in aligning\nlarge language models (LLMs)-most existing studies focus only on pairwise\ncomparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024,\nThekumparampil et al., 2024) have explored using multiple comparisons and\nranking feedback, but their performance guarantees fail to improve-and can even\ndeteriorate-as the feedback length increases, despite the richer information\navailable. To address this gap, we adopt the Plackett-Luce (PL) model for\nranking feedback over action subsets and propose M-AUPO, an algorithm that\nselects multiple actions by maximizing the average uncertainty within the\noffered subset. We prove that M-AUPO achieves a suboptimality gap of\n$\\tilde{\\mathcal{O}}\\left( \\frac{d}{T} \\sqrt{ \\sum_{t=1}^T \\frac{1}{|S_t|}}\n\\right)$, where $T$ is the total number of rounds, $d$ is the feature\ndimension, and $|S_t|$ is the size of the subset at round $t$. This result\nshows that larger subsets directly lead to improved performance and, notably,\nthe bound avoids the exponential dependence on the unknown parameter's norm,\nwhich was a fundamental limitation in most previous works. Moreover, we\nestablish a near-matching lower bound of $\\Omega \\left( \\frac{d}{K \\sqrt{T}}\n\\right)$, where $K$ is the maximum subset size. To the best of our knowledge,\nthis is the first theoretical result in PbRL with ranking feedback that\nexplicitly shows improved sample efficiency as a function of the subset size."
                },
                "authors": [
                    {
                        "name": "Joongkyu Lee"
                    },
                    {
                        "name": "Seouh-won Yi"
                    },
                    {
                        "name": "Min-hwan Oh"
                    }
                ],
                "author_detail": {
                    "name": "Min-hwan Oh"
                },
                "author": "Min-hwan Oh",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00198v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00198v3",
                "updated": "2025-10-21T15:02:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    2,
                    31,
                    1,
                    294,
                    0
                ],
                "published": "2025-01-31T22:27:34Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    22,
                    27,
                    34,
                    4,
                    31,
                    0
                ],
                "title": "Fairshare Data Pricing via Data Valuation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairshare Data Pricing via Data Valuation for Large Language Models"
                },
                "summary": "Training data is the backbone of large language models (LLMs), yet today's\ndata markets often operate under exploitative pricing -- sourcing data from\nmarginalized groups with little pay or recognition. This paper introduces a\ntheoretical framework for LLM data markets, modeling the strategic interactions\nbetween buyers (LLM builders) and sellers (human annotators). We begin with\ntheoretical and empirical analysis showing how exploitative pricing drives\nhigh-quality sellers out of the market, degrading data quality and long-term\nmodel performance. Then we introduce fairshare, a pricing mechanism grounded in\ndata valuation that quantifies each data's contribution. It aligns incentives\nby sustaining seller participation and optimizing utility for both buyers and\nsellers. Theoretically, we show that fairshare yields mutually optimal\noutcomes: maximizing long-term buyer utility and seller profit while sustaining\nmarket participation. Empirically when training open-source LLMs on complex NLP\ntasks, including math problems, medical diagnosis, and physical reasoning,\nfairshare boosts seller earnings and ensures a stable supply of high-quality\ndata, while improving buyers' performance-per-dollar and long-term welfare. Our\nfindings offer a concrete path toward fair, transparent, and economically\nsustainable data markets for LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training data is the backbone of large language models (LLMs), yet today's\ndata markets often operate under exploitative pricing -- sourcing data from\nmarginalized groups with little pay or recognition. This paper introduces a\ntheoretical framework for LLM data markets, modeling the strategic interactions\nbetween buyers (LLM builders) and sellers (human annotators). We begin with\ntheoretical and empirical analysis showing how exploitative pricing drives\nhigh-quality sellers out of the market, degrading data quality and long-term\nmodel performance. Then we introduce fairshare, a pricing mechanism grounded in\ndata valuation that quantifies each data's contribution. It aligns incentives\nby sustaining seller participation and optimizing utility for both buyers and\nsellers. Theoretically, we show that fairshare yields mutually optimal\noutcomes: maximizing long-term buyer utility and seller profit while sustaining\nmarket participation. Empirically when training open-source LLMs on complex NLP\ntasks, including math problems, medical diagnosis, and physical reasoning,\nfairshare boosts seller earnings and ensures a stable supply of high-quality\ndata, while improving buyers' performance-per-dollar and long-term welfare. Our\nfindings offer a concrete path toward fair, transparent, and economically\nsustainable data markets for LLM."
                },
                "authors": [
                    {
                        "name": "Luyang Zhang"
                    },
                    {
                        "name": "Cathy Jiao"
                    },
                    {
                        "name": "Beibei Li"
                    },
                    {
                        "name": "Chenyan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Chenyan Xiong"
                },
                "author": "Chenyan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00198v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00198v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16551v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16551v3",
                "updated": "2025-10-22T12:15:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    15,
                    26,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-18T15:46:11Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    15,
                    46,
                    11,
                    5,
                    291,
                    0
                ],
                "title": "From Reviews to Actionable Insights: An LLM-Based Approach for Attribute\n  and Feature Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reviews to Actionable Insights: An LLM-Based Approach for Attribute\n  and Feature Extraction"
                },
                "summary": "This research proposes a systematic, large language model (LLM) approach for\nextracting product and service attributes, features, and associated sentiments\nfrom customer reviews. Grounded in marketing theory, the framework\ndistinguishes perceptual attributes from actionable features, producing\ninterpretable and managerially actionable insights. We apply the methodology to\n20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a\nrandom subset of reviews. Model performance is assessed through agreement with\nhuman annotations and predictive validity for customer ratings. Results show\nhigh consistency between LLMs and human coders and strong predictive validity,\nconfirming the reliability of the approach. Human coders required a median of\nsix minutes per review, whereas the LLM processed each in two seconds,\ndelivering comparable insights at a scale unattainable through manual coding.\nManagerially, the analysis identifies attributes and features that most\nstrongly influence customer satisfaction and their associated sentiments,\nenabling firms to pinpoint \"joy points,\" address \"pain points,\" and design\ntargeted interventions. We demonstrate how structured review data can power an\nactionable marketing dashboard that tracks sentiment over time and across\nstores, benchmarks performance, and highlights high-leverage features for\nimprovement. Simulations indicate that enhancing sentiment for key service\nfeatures could yield 1-2% average revenue gains per store.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research proposes a systematic, large language model (LLM) approach for\nextracting product and service attributes, features, and associated sentiments\nfrom customer reviews. Grounded in marketing theory, the framework\ndistinguishes perceptual attributes from actionable features, producing\ninterpretable and managerially actionable insights. We apply the methodology to\n20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a\nrandom subset of reviews. Model performance is assessed through agreement with\nhuman annotations and predictive validity for customer ratings. Results show\nhigh consistency between LLMs and human coders and strong predictive validity,\nconfirming the reliability of the approach. Human coders required a median of\nsix minutes per review, whereas the LLM processed each in two seconds,\ndelivering comparable insights at a scale unattainable through manual coding.\nManagerially, the analysis identifies attributes and features that most\nstrongly influence customer satisfaction and their associated sentiments,\nenabling firms to pinpoint \"joy points,\" address \"pain points,\" and design\ntargeted interventions. We demonstrate how structured review data can power an\nactionable marketing dashboard that tracks sentiment over time and across\nstores, benchmarks performance, and highlights high-leverage features for\nimprovement. Simulations indicate that enhancing sentiment for key service\nfeatures could yield 1-2% average revenue gains per store."
                },
                "authors": [
                    {
                        "name": "Khaled Boughanmi"
                    },
                    {
                        "name": "Kamel Jedidi"
                    },
                    {
                        "name": "Nour Jedidi"
                    }
                ],
                "author_detail": {
                    "name": "Nour Jedidi"
                },
                "author": "Nour Jedidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16551v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16551v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12878v2",
                "updated": "2025-10-21T14:54:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    54,
                    42,
                    1,
                    294,
                    0
                ],
                "published": "2025-01-22T13:38:49Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    38,
                    49,
                    2,
                    22,
                    0
                ],
                "title": "$μ$OpTime: Statically Reducing the Execution Time of Microbenchmark\n  Suites Using Stability Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$μ$OpTime: Statically Reducing the Execution Time of Microbenchmark\n  Suites Using Stability Metrics"
                },
                "summary": "Performance regressions have a tremendous impact on the quality of software.\nOne way to catch regressions before they reach production is executing\nperformance tests before deployment, e.g., using microbenchmarks, which measure\nperformance at subroutine level. In projects with many microbenchmarks, this\nmay take several hours due to repeated execution to get accurate results,\ndisqualifying them from frequent use in CI/CD pipelines. We propose\n$\\mu$OpTime, a static approach to reduce the execution time of microbenchmark\nsuites by configuring the number of repetitions for each microbenchmark. Based\non the results of a full, previous microbenchmark suite run, $\\mu$OpTime\ndetermines the minimal number of (measurement) repetitions with statistical\nstability metrics that still lead to accurate results. We evaluate $\\mu$OpTime\nwith an experimental study on 14 open-source projects written in two\nprogramming languages and five stability metrics. Our results show that (i)\n$\\mu$OpTime reduces the total suite execution time (measurement phase) by up to\n95.83% (Go) and 94.17% (Java), (ii) the choice of stability metric depends on\nthe project and programming language, (iii) microbenchmark warmup phases have\nto be considered for Java projects (potentially leading to higher reductions),\nand (iv) $\\mu$OpTime can be used to reliably detect performance regressions in\nCI/CD pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance regressions have a tremendous impact on the quality of software.\nOne way to catch regressions before they reach production is executing\nperformance tests before deployment, e.g., using microbenchmarks, which measure\nperformance at subroutine level. In projects with many microbenchmarks, this\nmay take several hours due to repeated execution to get accurate results,\ndisqualifying them from frequent use in CI/CD pipelines. We propose\n$\\mu$OpTime, a static approach to reduce the execution time of microbenchmark\nsuites by configuring the number of repetitions for each microbenchmark. Based\non the results of a full, previous microbenchmark suite run, $\\mu$OpTime\ndetermines the minimal number of (measurement) repetitions with statistical\nstability metrics that still lead to accurate results. We evaluate $\\mu$OpTime\nwith an experimental study on 14 open-source projects written in two\nprogramming languages and five stability metrics. Our results show that (i)\n$\\mu$OpTime reduces the total suite execution time (measurement phase) by up to\n95.83% (Go) and 94.17% (Java), (ii) the choice of stability metric depends on\nthe project and programming language, (iii) microbenchmark warmup phases have\nto be considered for Java projects (potentially leading to higher reductions),\nand (iv) $\\mu$OpTime can be used to reliably detect performance regressions in\nCI/CD pipelines."
                },
                "authors": [
                    {
                        "name": "Nils Japke"
                    },
                    {
                        "name": "Martin Grambow"
                    },
                    {
                        "name": "Christoph Laaber"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "arxiv_doi": "10.1145/3715322",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715322",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in ACM Transactions on Software Engineering and Methodology",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18699v1",
                "updated": "2025-10-21T14:53:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    53,
                    56,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:53:56Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    53,
                    56,
                    1,
                    294,
                    0
                ],
                "title": "Fetch.ai: An Architecture for Modern Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fetch.ai: An Architecture for Modern Multi-Agent Systems"
                },
                "summary": "Recent surges in LLM-driven intelligent systems largely overlook decades of\nfoundational multi-agent systems (MAS) research, resulting in frameworks with\ncritical limitations such as centralization and inadequate trust and\ncommunication protocols. This paper introduces the Fetch.ai architecture, an\nindustrial-strength platform designed to bridge this gap by facilitating the\nintegration of classical MAS principles with modern AI capabilities. We present\na novel, multi-layered solution built on a decentralized foundation of on-chain\nblockchain services for verifiable identity, discovery, and transactions. This\nis complemented by a comprehensive development framework for creating secure,\ninteroperable agents, a cloud-based platform for deployment, and an intelligent\norchestration layer where an agent-native LLM translates high-level human goals\ninto complex, multi-agent workflows. We demonstrate the deployed nature of this\nsystem through a decentralized logistics use case where autonomous agents\ndynamically discover, negotiate, and transact with one another securely.\nUltimately, the Fetch.ai stack provides a principled architecture for moving\nbeyond current agent implementations towards open, collaborative, and\neconomically sustainable multi-agent ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent surges in LLM-driven intelligent systems largely overlook decades of\nfoundational multi-agent systems (MAS) research, resulting in frameworks with\ncritical limitations such as centralization and inadequate trust and\ncommunication protocols. This paper introduces the Fetch.ai architecture, an\nindustrial-strength platform designed to bridge this gap by facilitating the\nintegration of classical MAS principles with modern AI capabilities. We present\na novel, multi-layered solution built on a decentralized foundation of on-chain\nblockchain services for verifiable identity, discovery, and transactions. This\nis complemented by a comprehensive development framework for creating secure,\ninteroperable agents, a cloud-based platform for deployment, and an intelligent\norchestration layer where an agent-native LLM translates high-level human goals\ninto complex, multi-agent workflows. We demonstrate the deployed nature of this\nsystem through a decentralized logistics use case where autonomous agents\ndynamically discover, negotiate, and transact with one another securely.\nUltimately, the Fetch.ai stack provides a principled architecture for moving\nbeyond current agent implementations towards open, collaborative, and\neconomically sustainable multi-agent ecosystems."
                },
                "authors": [
                    {
                        "name": "Michael J. Wooldridge"
                    },
                    {
                        "name": "Attila Bagoly"
                    },
                    {
                        "name": "Jonathan J. Ward"
                    },
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Gabriel Paludo Licks"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Paludo Licks"
                },
                "author": "Gabriel Paludo Licks",
                "arxiv_comment": "26 pages, figures, code examples",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04710v2",
                "updated": "2025-10-21T14:52:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    52,
                    14,
                    1,
                    294,
                    0
                ],
                "published": "2025-05-07T18:02:18Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    18,
                    2,
                    18,
                    2,
                    127,
                    0
                ],
                "title": "Exploring Influence Factors on LLM Suitability for No-Code Development\n  of End User IoT Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Influence Factors on LLM Suitability for No-Code Development\n  of End User IoT Applications"
                },
                "summary": "No-Code Development Platforms (NCDPs) empower non-technical end users to\nbuild applications tailored to their specific demands without writing code.\nWhile NCDPs lower technical barriers, users still require some technical\nknowledge, e.g., to structure process steps or define event-action rules. Large\nLanguage Models (LLMs) offer a promising solution to further reduce technical\nrequirements by supporting natural language interaction and dynamic code\ngeneration. By integrating LLM, NCDPs can be more accessible to non-technical\nusers, enabling application development truly without requiring any technical\nexpertise.\n  Despite growing interest in LLM-powered NCDPs, a systematic investigation\ninto the factors influencing LLM suitability and performance remains absent.\nUnderstanding these factors is critical to effectively leveraging LLMs\ncapabilities and maximizing their impact. In this paper, we investigate key\nfactors influencing the effectiveness of LLMs in supporting end-user\napplication development within NCDPs. By conducting comprehensive experiments,\nwe evaluate the impact of four key factors, i.e., model selection, prompt\nlanguage, training data background, and an error-informed few-shot setup, on\nthe quality of generated applications. Specifically, we selected a range of\nLLMs based on their architecture, scale, design focus, and training data, and\nevaluated them across four real-world smart home automation scenarios\nimplemented on a representative open-source LLM-powered NCDP. Our findings\noffer practical insights into how LLMs can be effectively integrated into\nNCDPs, informing both platform design and the selection of suitable LLMs for\nend-user application development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No-Code Development Platforms (NCDPs) empower non-technical end users to\nbuild applications tailored to their specific demands without writing code.\nWhile NCDPs lower technical barriers, users still require some technical\nknowledge, e.g., to structure process steps or define event-action rules. Large\nLanguage Models (LLMs) offer a promising solution to further reduce technical\nrequirements by supporting natural language interaction and dynamic code\ngeneration. By integrating LLM, NCDPs can be more accessible to non-technical\nusers, enabling application development truly without requiring any technical\nexpertise.\n  Despite growing interest in LLM-powered NCDPs, a systematic investigation\ninto the factors influencing LLM suitability and performance remains absent.\nUnderstanding these factors is critical to effectively leveraging LLMs\ncapabilities and maximizing their impact. In this paper, we investigate key\nfactors influencing the effectiveness of LLMs in supporting end-user\napplication development within NCDPs. By conducting comprehensive experiments,\nwe evaluate the impact of four key factors, i.e., model selection, prompt\nlanguage, training data background, and an error-informed few-shot setup, on\nthe quality of generated applications. Specifically, we selected a range of\nLLMs based on their architecture, scale, design focus, and training data, and\nevaluated them across four real-world smart home automation scenarios\nimplemented on a representative open-source LLM-powered NCDP. Our findings\noffer practical insights into how LLMs can be effectively integrated into\nNCDPs, informing both platform design and the selection of suitable LLMs for\nend-user application development."
                },
                "authors": [
                    {
                        "name": "Minghe Wang"
                    },
                    {
                        "name": "Alexandra Kapp"
                    },
                    {
                        "name": "Trever Schirmer"
                    },
                    {
                        "name": "Tobias Pfandzelter"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "arxiv_doi": "10.1002/spe.70027",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/spe.70027",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.04710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18691v1",
                "updated": "2025-10-21T14:50:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    50,
                    24,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:50:24Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    50,
                    24,
                    1,
                    294,
                    0
                ],
                "title": "Investigating LLM Capabilities on Long Context Comprehension for Medical\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating LLM Capabilities on Long Context Comprehension for Medical\n  Question Answering"
                },
                "summary": "This study is the first to investigate LLM comprehension capabilities over\nlong-context (LC) medical QA of clinical relevance. Our comprehensive\nassessment spans a range of content-inclusion settings based on their\nrelevance, LLM models of varying capabilities and datasets across task\nformulations, revealing insights on model size effects, limitations, underlying\nmemorization issues and the benefits of reasoning models. Importantly, we\nexamine the effect of RAG on medical LC comprehension, uncover best settings in\nsingle versus multi-document reasoning datasets and showcase RAG strategies for\nimprovements over LC. We shed light into some of the evaluation aspects using a\nmulti-faceted approach. Our qualitative and error analyses address open\nquestions on when RAG is beneficial over LC, revealing common failure cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study is the first to investigate LLM comprehension capabilities over\nlong-context (LC) medical QA of clinical relevance. Our comprehensive\nassessment spans a range of content-inclusion settings based on their\nrelevance, LLM models of varying capabilities and datasets across task\nformulations, revealing insights on model size effects, limitations, underlying\nmemorization issues and the benefits of reasoning models. Importantly, we\nexamine the effect of RAG on medical LC comprehension, uncover best settings in\nsingle versus multi-document reasoning datasets and showcase RAG strategies for\nimprovements over LC. We shed light into some of the evaluation aspects using a\nmulti-faceted approach. Our qualitative and error analyses address open\nquestions on when RAG is beneficial over LC, revealing common failure cases."
                },
                "authors": [
                    {
                        "name": "Feras AlMannaa"
                    },
                    {
                        "name": "Talia Tseriotou"
                    },
                    {
                        "name": "Jenny Chim"
                    },
                    {
                        "name": "Maria Liakata"
                    }
                ],
                "author_detail": {
                    "name": "Maria Liakata"
                },
                "author": "Maria Liakata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15511v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15511v3",
                "updated": "2025-10-21T14:44:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    44,
                    49,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-17T10:25:30Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    10,
                    25,
                    30,
                    4,
                    290,
                    0
                ],
                "title": "Language Models are Injective and Hence Invertible",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models are Injective and Hence Invertible"
                },
                "summary": "Transformer components such as non-linear activations and normalization are\ninherently non-injective, suggesting that different inputs could map to the\nsame output and prevent exact recovery of the input from a model's\nrepresentations. In this paper, we challenge this view. First, we prove\nmathematically that transformer language models mapping discrete input\nsequences to their corresponding sequence of continuous representations are\ninjective and therefore lossless, a property established at initialization and\npreserved during training. Second, we confirm this result empirically through\nbillions of collision tests on six state-of-the-art language models, and\nobserve no collisions. Third, we operationalize injectivity: we introduce\nSipIt, the first algorithm that provably and efficiently reconstructs the exact\ninput text from hidden activations, establishing linear-time guarantees and\ndemonstrating exact invertibility in practice. Overall, our work establishes\ninjectivity as a fundamental and exploitable property of language models, with\ndirect implications for transparency, interpretability, and safe deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer components such as non-linear activations and normalization are\ninherently non-injective, suggesting that different inputs could map to the\nsame output and prevent exact recovery of the input from a model's\nrepresentations. In this paper, we challenge this view. First, we prove\nmathematically that transformer language models mapping discrete input\nsequences to their corresponding sequence of continuous representations are\ninjective and therefore lossless, a property established at initialization and\npreserved during training. Second, we confirm this result empirically through\nbillions of collision tests on six state-of-the-art language models, and\nobserve no collisions. Third, we operationalize injectivity: we introduce\nSipIt, the first algorithm that provably and efficiently reconstructs the exact\ninput text from hidden activations, establishing linear-time guarantees and\ndemonstrating exact invertibility in practice. Overall, our work establishes\ninjectivity as a fundamental and exploitable property of language models, with\ndirect implications for transparency, interpretability, and safe deployment."
                },
                "authors": [
                    {
                        "name": "Giorgos Nikolaou"
                    },
                    {
                        "name": "Tommaso Mencattini"
                    },
                    {
                        "name": "Donato Crisostomi"
                    },
                    {
                        "name": "Andrea Santilli"
                    },
                    {
                        "name": "Yannis Panagakis"
                    },
                    {
                        "name": "Emanuele Rodolà"
                    }
                ],
                "author_detail": {
                    "name": "Emanuele Rodolà"
                },
                "author": "Emanuele Rodolà",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15511v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10025v2",
                "updated": "2025-10-21T14:44:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    44,
                    14,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-11T05:05:21Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    5,
                    5,
                    21,
                    5,
                    284,
                    0
                ],
                "title": "Lightweight Baselines for Medical Abstract Classification: DistilBERT\n  with Cross-Entropy as a Strong Default",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Baselines for Medical Abstract Classification: DistilBERT\n  with Cross-Entropy as a Strong Default"
                },
                "summary": "The research evaluates lightweight medical abstract classification methods to\nestablish their maximum performance capabilities under financial budget\nrestrictions. On the public medical abstracts corpus, we finetune BERT base and\nDistil BERT with three objectives cross entropy (CE), class weighted CE, and\nfocal loss under identical tokenization, sequence length, optimizer, and\nschedule. DistilBERT with plain CE gives the strongest raw argmax trade off,\nwhile a post hoc operating point selection (validation calibrated, classwise\nthresholds) sub stantially improves deployed performance; under this tuned\nregime, focal benefits most. We report Accuracy, Macro F1, and WeightedF1,\nrelease evaluation artifacts, and include confusion analyses to clarify error\nstructure. The practical takeaway is to start with a compact encoder and CE,\nthen add lightweight calibration or thresholding when deployment requires\nhigher macro balance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The research evaluates lightweight medical abstract classification methods to\nestablish their maximum performance capabilities under financial budget\nrestrictions. On the public medical abstracts corpus, we finetune BERT base and\nDistil BERT with three objectives cross entropy (CE), class weighted CE, and\nfocal loss under identical tokenization, sequence length, optimizer, and\nschedule. DistilBERT with plain CE gives the strongest raw argmax trade off,\nwhile a post hoc operating point selection (validation calibrated, classwise\nthresholds) sub stantially improves deployed performance; under this tuned\nregime, focal benefits most. We report Accuracy, Macro F1, and WeightedF1,\nrelease evaluation artifacts, and include confusion analyses to clarify error\nstructure. The practical takeaway is to start with a compact encoder and CE,\nthen add lightweight calibration or thresholding when deployment requires\nhigher macro balance."
                },
                "authors": [
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Tong Wang"
                    },
                    {
                        "name": "Su Liu"
                    },
                    {
                        "name": "Xin Hu"
                    },
                    {
                        "name": "Ran Tong"
                    },
                    {
                        "name": "Lanruo Wang"
                    },
                    {
                        "name": "Jiexi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jiexi Xu"
                },
                "author": "Jiexi Xu",
                "arxiv_comment": "Healthcare AI, Medical Text Classification,LLM, DistilBERT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16122v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16122v3",
                "updated": "2025-10-21T14:28:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    28,
                    6,
                    1,
                    294,
                    0
                ],
                "published": "2025-08-22T06:29:29Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    29,
                    29,
                    4,
                    234,
                    0
                ],
                "title": "Text Takes Over: A Study of Modality Bias in Multimodal Intent Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Takes Over: A Study of Modality Bias in Multimodal Intent Detection"
                },
                "summary": "The rise of multimodal data, integrating text, audio, and visuals, has\ncreated new opportunities for studying multimodal tasks such as intent\ndetection. This work investigates the effectiveness of Large Language Models\n(LLMs) and non-LLMs, including text-only and multi-modal models, in the\nmultimodal intent detection task. Our study reveals that Mistral-7B, a\ntext-only LLM, outperforms most competitive multimodal models by approximately\n9% on MIntRec-1 and 4% on MIntRec2.0 datasets. This performance advantage comes\nfrom a strong textual bias in these datasets, where over 90% of the samples\nrequire textual input, either alone or in combination with other modalities,\nfor correct classification. We confirm the modality bias of these datasets via\nhuman evaluation, too. Next, we propose a framework to debias the datasets, and\nupon debiasing, more than 70% of the samples in MIntRec-1 and more than 50% in\nMIntRec2.0 get removed, resulting in significant performance degradation across\nall models, with smaller multimodal fusion models being the most affected with\nan accuracy drop of over 50 - 60%. Further, we analyze the context-specific\nrelevance of different modalities through empirical analysis. Our findings\nhighlight the challenges posed by modality bias in multimodal intent datasets\nand emphasize the need for unbiased datasets to evaluate multimodal models\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of multimodal data, integrating text, audio, and visuals, has\ncreated new opportunities for studying multimodal tasks such as intent\ndetection. This work investigates the effectiveness of Large Language Models\n(LLMs) and non-LLMs, including text-only and multi-modal models, in the\nmultimodal intent detection task. Our study reveals that Mistral-7B, a\ntext-only LLM, outperforms most competitive multimodal models by approximately\n9% on MIntRec-1 and 4% on MIntRec2.0 datasets. This performance advantage comes\nfrom a strong textual bias in these datasets, where over 90% of the samples\nrequire textual input, either alone or in combination with other modalities,\nfor correct classification. We confirm the modality bias of these datasets via\nhuman evaluation, too. Next, we propose a framework to debias the datasets, and\nupon debiasing, more than 70% of the samples in MIntRec-1 and more than 50% in\nMIntRec2.0 get removed, resulting in significant performance degradation across\nall models, with smaller multimodal fusion models being the most affected with\nan accuracy drop of over 50 - 60%. Further, we analyze the context-specific\nrelevance of different modalities through empirical analysis. Our findings\nhighlight the challenges posed by modality bias in multimodal intent datasets\nand emphasize the need for unbiased datasets to evaluate multimodal models\neffectively."
                },
                "authors": [
                    {
                        "name": "Ankan Mullick"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Abhik Jana"
                    },
                    {
                        "name": "Pawan Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Pawan Goyal"
                },
                "author": "Pawan Goyal",
                "arxiv_comment": "EMNLP 2025 Main Conference Full Paper",
                "arxiv_journal_ref": "EMNLP 2025 Main Conference Full Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16122v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16122v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18674v1",
                "updated": "2025-10-21T14:27:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    27,
                    48,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:27:48Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    27,
                    48,
                    1,
                    294,
                    0
                ],
                "title": "Exploring Membership Inference Vulnerabilities in Clinical Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Membership Inference Vulnerabilities in Clinical Large\n  Language Models"
                },
                "summary": "As large language models (LLMs) become progressively more embedded in\nclinical decision-support, documentation, and patient-information systems,\nensuring their privacy and trustworthiness has emerged as an imperative\nchallenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic\nhealth record (EHR) data improves domain alignment but also raises the risk of\nexposing patient information through model behaviors. In this work-in-progress,\nwe present an exploratory empirical study on membership inference\nvulnerabilities in clinical LLMs, focusing on whether adversaries can infer if\nspecific patient records were used during model training. Using a\nstate-of-the-art clinical question-answering model, Llemr, we evaluate both\ncanonical loss-based attacks and a domain-motivated paraphrasing-based\nperturbation strategy that more realistically reflects clinical adversarial\nconditions. Our preliminary findings reveal limited but measurable membership\nleakage, suggesting that current clinical LLMs provide partial resistance yet\nremain susceptible to subtle privacy risks that could undermine trust in\nclinical AI adoption. These results motivate continued development of\ncontext-aware, domain-specific privacy evaluations and defenses such as\ndifferential privacy fine-tuning and paraphrase-aware training, to strengthen\nthe security and trustworthiness of healthcare AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become progressively more embedded in\nclinical decision-support, documentation, and patient-information systems,\nensuring their privacy and trustworthiness has emerged as an imperative\nchallenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic\nhealth record (EHR) data improves domain alignment but also raises the risk of\nexposing patient information through model behaviors. In this work-in-progress,\nwe present an exploratory empirical study on membership inference\nvulnerabilities in clinical LLMs, focusing on whether adversaries can infer if\nspecific patient records were used during model training. Using a\nstate-of-the-art clinical question-answering model, Llemr, we evaluate both\ncanonical loss-based attacks and a domain-motivated paraphrasing-based\nperturbation strategy that more realistically reflects clinical adversarial\nconditions. Our preliminary findings reveal limited but measurable membership\nleakage, suggesting that current clinical LLMs provide partial resistance yet\nremain susceptible to subtle privacy risks that could undermine trust in\nclinical AI adoption. These results motivate continued development of\ncontext-aware, domain-specific privacy evaluations and defenses such as\ndifferential privacy fine-tuning and paraphrase-aware training, to strengthen\nthe security and trustworthiness of healthcare AI systems."
                },
                "authors": [
                    {
                        "name": "Alexander Nemecek"
                    },
                    {
                        "name": "Zebin Yun"
                    },
                    {
                        "name": "Zahra Rahmani"
                    },
                    {
                        "name": "Yaniv Harel"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Mahmood Sharif"
                    },
                    {
                        "name": "Erman Ayday"
                    }
                ],
                "author_detail": {
                    "name": "Erman Ayday"
                },
                "author": "Erman Ayday",
                "arxiv_comment": "Accepted at the 1st IEEE Workshop on Healthcare and Medical Device\n  Security, Privacy, Resilience, and Trust (IEEE HMD-SPiRiT)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18672v1",
                "updated": "2025-10-21T14:25:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:25:51Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study"
                },
                "summary": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving."
                },
                "authors": [
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Junpan Wu"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08049v2",
                "updated": "2025-10-21T14:21:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    21,
                    25,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-09T10:35:31Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    10,
                    35,
                    31,
                    3,
                    282,
                    0
                ],
                "title": "A Survey of Process Reward Models: From Outcome Signals to Process\n  Supervisions for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Process Reward Models: From Outcome Signals to Process\n  Supervisions for Large Language Models"
                },
                "summary": "Although Large Language Models (LLMs) exhibit advanced reasoning ability,\nconventional alignment remains largely dominated by outcome reward models\n(ORMs) that judge only final answers. Process Reward Models(PRMs) address this\ngap by evaluating and guiding reasoning at the step or trajectory level. This\nsurvey provides a systematic overview of PRMs through the full loop: how to\ngenerate process data, build PRMs, and use PRMs for test-time scaling and\nreinforcement learning. We summarize applications across math, code, text,\nmultimodal reasoning, robotics, and agents, and review emerging benchmarks. Our\ngoal is to clarify design spaces, reveal open challenges, and guide future\nresearch toward fine-grained, robust reasoning alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) exhibit advanced reasoning ability,\nconventional alignment remains largely dominated by outcome reward models\n(ORMs) that judge only final answers. Process Reward Models(PRMs) address this\ngap by evaluating and guiding reasoning at the step or trajectory level. This\nsurvey provides a systematic overview of PRMs through the full loop: how to\ngenerate process data, build PRMs, and use PRMs for test-time scaling and\nreinforcement learning. We summarize applications across math, code, text,\nmultimodal reasoning, robotics, and agents, and review emerging benchmarks. Our\ngoal is to clarify design spaces, reveal open challenges, and guide future\nresearch toward fine-grained, robust reasoning alignment."
                },
                "authors": [
                    {
                        "name": "Congming Zheng"
                    },
                    {
                        "name": "Jiachen Zhu"
                    },
                    {
                        "name": "Zhuoying Ou"
                    },
                    {
                        "name": "Yuxiang Chen"
                    },
                    {
                        "name": "Kangning Zhang"
                    },
                    {
                        "name": "Rong Shan"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09049v2",
                "updated": "2025-10-21T14:14:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    14,
                    49,
                    1,
                    294,
                    0
                ],
                "published": "2025-06-10T17:59:44Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    44,
                    1,
                    161,
                    0
                ],
                "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement\n  Learning"
                },
                "summary": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems."
                },
                "authors": [
                    {
                        "name": "Li Kang"
                    },
                    {
                        "name": "Xiufeng Song"
                    },
                    {
                        "name": "Heng Zhou"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Xiaohong Liu"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Zhenfei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Zhenfei Yin"
                },
                "author": "Zhenfei Yin",
                "arxiv_comment": "Project page: https://faceong.github.io/VIKI-R/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20749v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20749v3",
                "updated": "2025-10-22T04:14:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    14,
                    59,
                    2,
                    295,
                    0
                ],
                "published": "2025-05-27T05:45:03Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    5,
                    45,
                    3,
                    1,
                    147,
                    0
                ],
                "title": "Can Agents Fix Agent Issues?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Agents Fix Agent Issues?"
                },
                "summary": "LLM-based agent systems are emerging as a new software paradigm and have been\nwidely adopted across diverse domains such as medicine, robotics, and\nprogramming. However, maintaining these systems requires substantial effort, as\nthey are inevitably prone to bugs and continually evolve to meet changing\nexternal requirements. Therefore, automatically resolving agent issues (i.e.,\nbug reports or feature requests) is a crucial and challenging task. While\nrecent software engineering (SE) agents (e.g., SWE-agent) have shown promise in\naddressing issues in traditional software systems, it remains unclear how\neffectively they can resolve real-world issues in agent systems, which differ\nsignificantly from traditional software. To fill this gap, we first manually\nanalyze 201 real-world agent issues and identify common categories of agent\nissues. We then spend 500 person-hours constructing AGENTISSUE-BENCH, a\nreproducible benchmark comprising 50 agent issue resolution tasks (each with an\nexecutable environment and failure-triggering tests). We further evaluate\nstate-of-the-art SE agents on AGENTISSUE-BENCH and reveal their limited\neffectiveness (i.e., with only 3.33% - 12.67% resolution rates). These results\nunderscore the unique challenges of maintaining agent systems compared to\ntraditional software, highlighting the need for further research to develop\nadvanced SE agents for resolving agent issues. Data and code are available at\nhttps://alfin06.github.io/AgentIssue-Bench-Leaderboard/#/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agent systems are emerging as a new software paradigm and have been\nwidely adopted across diverse domains such as medicine, robotics, and\nprogramming. However, maintaining these systems requires substantial effort, as\nthey are inevitably prone to bugs and continually evolve to meet changing\nexternal requirements. Therefore, automatically resolving agent issues (i.e.,\nbug reports or feature requests) is a crucial and challenging task. While\nrecent software engineering (SE) agents (e.g., SWE-agent) have shown promise in\naddressing issues in traditional software systems, it remains unclear how\neffectively they can resolve real-world issues in agent systems, which differ\nsignificantly from traditional software. To fill this gap, we first manually\nanalyze 201 real-world agent issues and identify common categories of agent\nissues. We then spend 500 person-hours constructing AGENTISSUE-BENCH, a\nreproducible benchmark comprising 50 agent issue resolution tasks (each with an\nexecutable environment and failure-triggering tests). We further evaluate\nstate-of-the-art SE agents on AGENTISSUE-BENCH and reveal their limited\neffectiveness (i.e., with only 3.33% - 12.67% resolution rates). These results\nunderscore the unique challenges of maintaining agent systems compared to\ntraditional software, highlighting the need for further research to develop\nadvanced SE agents for resolving agent issues. Data and code are available at\nhttps://alfin06.github.io/AgentIssue-Bench-Leaderboard/#/ ."
                },
                "authors": [
                    {
                        "name": "Alfin Wijaya Rahardja"
                    },
                    {
                        "name": "Junwei Liu"
                    },
                    {
                        "name": "Weitong Chen"
                    },
                    {
                        "name": "Zhenpeng Chen"
                    },
                    {
                        "name": "Yiling Lou"
                    }
                ],
                "author_detail": {
                    "name": "Yiling Lou"
                },
                "author": "Yiling Lou",
                "arxiv_comment": "Accepted by the 39th Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20749v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20749v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03739v2",
                "updated": "2025-10-21T13:54:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    54,
                    36,
                    1,
                    294,
                    0
                ],
                "published": "2025-05-06T17:59:53Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    59,
                    53,
                    1,
                    126,
                    0
                ],
                "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model"
                },
                "summary": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks."
                },
                "authors": [
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Heting Gao"
                    },
                    {
                        "name": "Lijiang Li"
                    },
                    {
                        "name": "Peixian Chen"
                    },
                    {
                        "name": "Mengdan Zhang"
                    },
                    {
                        "name": "Hang Shao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Jinlong Peng"
                    },
                    {
                        "name": "Haoyu Cao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "Training and Inference Codes: https://github.com/VITA-MLLM/VITA-Audio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18640v1",
                "updated": "2025-10-21T13:43:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    43,
                    20,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T13:43:20Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    43,
                    20,
                    1,
                    294,
                    0
                ],
                "title": "Towards an Optimized Benchmarking Platform for CI/CD Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards an Optimized Benchmarking Platform for CI/CD Pipelines"
                },
                "summary": "Performance regressions in large-scale software systems can lead to\nsubstantial resource inefficiencies, making their early detection critical.\nFrequent benchmarking is essential for identifying these regressions and\nmaintaining service-level agreements (SLAs). Performance benchmarks, however,\nare resource-intensive and time-consuming, which is a major challenge for\nintegration into Continuous Integration / Continuous Deployment (CI/CD)\npipelines. Although numerous benchmark optimization techniques have been\nproposed to accelerate benchmark execution, there is currently no practical\nsystem that integrates these optimizations seamlessly into real-world CI/CD\npipelines. In this vision paper, we argue that the field of benchmark\noptimization remains under-explored in key areas that hinder its broader\nadoption. We identify three central challenges to enabling frequent and\nefficient benchmarking: (a) the composability of benchmark optimization\nstrategies, (b) automated evaluation of benchmarking results, and (c) the\nusability and complexity of applying these strategies as part of CI/CD systems\nin practice. We also introduce a conceptual cloud-based benchmarking framework\nhandling these challenges transparently. By presenting these open problems, we\naim to stimulate research toward making performance regression detection in\nCI/CD systems more practical and effective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance regressions in large-scale software systems can lead to\nsubstantial resource inefficiencies, making their early detection critical.\nFrequent benchmarking is essential for identifying these regressions and\nmaintaining service-level agreements (SLAs). Performance benchmarks, however,\nare resource-intensive and time-consuming, which is a major challenge for\nintegration into Continuous Integration / Continuous Deployment (CI/CD)\npipelines. Although numerous benchmark optimization techniques have been\nproposed to accelerate benchmark execution, there is currently no practical\nsystem that integrates these optimizations seamlessly into real-world CI/CD\npipelines. In this vision paper, we argue that the field of benchmark\noptimization remains under-explored in key areas that hinder its broader\nadoption. We identify three central challenges to enabling frequent and\nefficient benchmarking: (a) the composability of benchmark optimization\nstrategies, (b) automated evaluation of benchmarking results, and (c) the\nusability and complexity of applying these strategies as part of CI/CD systems\nin practice. We also introduce a conceptual cloud-based benchmarking framework\nhandling these challenges transparently. By presenting these open problems, we\naim to stimulate research toward making performance regression detection in\nCI/CD systems more practical and effective."
                },
                "authors": [
                    {
                        "name": "Nils Japke"
                    },
                    {
                        "name": "Sebastian Koch"
                    },
                    {
                        "name": "Helmut Lukasczyk"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "arxiv_doi": "10.1109/IC2E65552.2025.00010",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IC2E65552.2025.00010",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.18640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in 2025 IEEE International Conference on Cloud Engineering\n  (IC2E)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18636v1",
                "updated": "2025-10-21T13:40:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    40,
                    11,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T13:40:11Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    40,
                    11,
                    1,
                    294,
                    0
                ],
                "title": "C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural\n  Networks Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural\n  Networks Compression"
                },
                "summary": "Neural network compression has gained increasing attention in recent years,\nparticularly in computer vision applications, where the need for model\nreduction is crucial for overcoming deployment constraints. Pruning is a widely\nused technique that prompts sparsity in model structures, e.g. weights,\nneurons, and layers, reducing size and inference costs. Structured pruning is\nespecially important as it allows for the removal of entire structures, which\nfurther accelerates inference time and reduces memory overhead. However, it can\nbe computationally expensive, requiring iterative retraining and optimization.\nTo overcome this problem, recent methods considered one-shot setting, which\napplies pruning directly at post-training. Unfortunately, they often lead to a\nconsiderable drop in performance. In this paper, we focus on this issue by\nproposing a novel one-shot pruning framework that relies on explainable deep\nlearning. First, we introduce a causal-aware pruning approach that leverages\ncause-effect relations between model predictions and structures in a\nprogressive pruning process. It allows us to efficiently reduce the size of the\nnetwork, ensuring that the removed structures do not deter the performance of\nthe model. Then, through experiments conducted on convolution neural network\nand vision transformer baselines, pre-trained on classification tasks, we\ndemonstrate that our method consistently achieves substantial reductions in\nmodel size, with minimal impact on performance, and without the need for\nfine-tuning. Overall, our approach outperforms its counterparts, offering the\nbest trade-off. Our code is available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network compression has gained increasing attention in recent years,\nparticularly in computer vision applications, where the need for model\nreduction is crucial for overcoming deployment constraints. Pruning is a widely\nused technique that prompts sparsity in model structures, e.g. weights,\nneurons, and layers, reducing size and inference costs. Structured pruning is\nespecially important as it allows for the removal of entire structures, which\nfurther accelerates inference time and reduces memory overhead. However, it can\nbe computationally expensive, requiring iterative retraining and optimization.\nTo overcome this problem, recent methods considered one-shot setting, which\napplies pruning directly at post-training. Unfortunately, they often lead to a\nconsiderable drop in performance. In this paper, we focus on this issue by\nproposing a novel one-shot pruning framework that relies on explainable deep\nlearning. First, we introduce a causal-aware pruning approach that leverages\ncause-effect relations between model predictions and structures in a\nprogressive pruning process. It allows us to efficiently reduce the size of the\nnetwork, ensuring that the removed structures do not deter the performance of\nthe model. Then, through experiments conducted on convolution neural network\nand vision transformer baselines, pre-trained on classification tasks, we\ndemonstrate that our method consistently achieves substantial reductions in\nmodel size, with minimal impact on performance, and without the need for\nfine-tuning. Overall, our approach outperforms its counterparts, offering the\nbest trade-off. Our code is available on GitHub."
                },
                "authors": [
                    {
                        "name": "Baptiste Bauvin"
                    },
                    {
                        "name": "Loïc Baret"
                    },
                    {
                        "name": "Ola Ahmad"
                    }
                ],
                "author_detail": {
                    "name": "Ola Ahmad"
                },
                "author": "Ola Ahmad",
                "arxiv_comment": "10 pages, BMVC2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18411v2",
                "updated": "2025-10-21T13:34:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    34,
                    39,
                    1,
                    294,
                    0
                ],
                "published": "2025-05-23T22:38:28Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    22,
                    38,
                    28,
                    4,
                    143,
                    0
                ],
                "title": "DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process\n  Modeling and Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process\n  Modeling and Understanding"
                },
                "summary": "We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance\nmulti-modal Temporal Point Process (TPP) modeling in the era of Large Language\nModels (LLMs). While TPPs have been widely studied for modeling temporal event\nsequences, existing datasets are predominantly unimodal, hindering progress in\nmodels that require joint reasoning over temporal, textual, and visual\ninformation. To address this gap, DanmakuTPPBench comprises two complementary\ncomponents: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili\nvideo platform, where user-generated bullet comments (Danmaku) naturally form\nmulti-modal events annotated with precise timestamps, rich textual content, and\ncorresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering\ndataset constructed via a novel multi-agent pipeline powered by\nstate-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex\ntemporal-textual-visual reasoning. We conduct extensive evaluations using both\nclassical TPP models and recent MLLMs, revealing significant performance gaps\nand limitations in current methods' ability to model multi-modal event\ndynamics. Our benchmark establishes strong baselines and calls for further\nintegration of TPP modeling into the multi-modal language modeling landscape.\nProject page: https://github.com/FRENKIE-CHIANG/DanmakuTPPBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance\nmulti-modal Temporal Point Process (TPP) modeling in the era of Large Language\nModels (LLMs). While TPPs have been widely studied for modeling temporal event\nsequences, existing datasets are predominantly unimodal, hindering progress in\nmodels that require joint reasoning over temporal, textual, and visual\ninformation. To address this gap, DanmakuTPPBench comprises two complementary\ncomponents: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili\nvideo platform, where user-generated bullet comments (Danmaku) naturally form\nmulti-modal events annotated with precise timestamps, rich textual content, and\ncorresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering\ndataset constructed via a novel multi-agent pipeline powered by\nstate-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex\ntemporal-textual-visual reasoning. We conduct extensive evaluations using both\nclassical TPP models and recent MLLMs, revealing significant performance gaps\nand limitations in current methods' ability to model multi-modal event\ndynamics. Our benchmark establishes strong baselines and calls for further\nintegration of TPP modeling into the multi-modal language modeling landscape.\nProject page: https://github.com/FRENKIE-CHIANG/DanmakuTPPBench"
                },
                "authors": [
                    {
                        "name": "Yue Jiang"
                    },
                    {
                        "name": "Jichu Li"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Quyu Kong"
                    }
                ],
                "author_detail": {
                    "name": "Quyu Kong"
                },
                "author": "Quyu Kong",
                "arxiv_comment": "Accepted by Neural Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17702v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17702v4",
                "updated": "2025-10-21T13:30:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    30,
                    4,
                    1,
                    294,
                    0
                ],
                "published": "2025-07-23T17:10:23Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    10,
                    23,
                    2,
                    204,
                    0
                ],
                "title": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts\n  Language Models"
                },
                "summary": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels."
                },
                "authors": [
                    {
                        "name": "Changxin Tian"
                    },
                    {
                        "name": "Kunlong Chen"
                    },
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17702v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17702v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17575v2",
                "updated": "2025-10-21T13:26:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    26,
                    59,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-20T14:22:57Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    14,
                    22,
                    57,
                    0,
                    293,
                    0
                ],
                "title": "DeTAILS: Deep Thematic Analysis with Iterative LLM Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeTAILS: Deep Thematic Analysis with Iterative LLM Support"
                },
                "summary": "Thematic analysis is widely used in qualitative research but can be difficult\nto scale because of its iterative, interpretive demands. We introduce DeTAILS,\na toolkit that integrates large language model (LLM) assistance into a workflow\ninspired by Braun and Clarke's thematic analysis framework. DeTAILS supports\nresearchers in generating and refining codes, reviewing clusters, and\nsynthesizing themes through interactive feedback loops designed to preserve\nanalytic agency. We evaluated the system with 18 qualitative researchers\nanalyzing Reddit data. Quantitative results showed strong alignment between\nLLM-supported outputs and participants' refinements, alongside reduced workload\nand high perceived usefulness. Qualitatively, participants reported that\nDeTAILS accelerated analysis, prompted reflexive engagement with AI outputs,\nand fostered trust through transparency and control. We contribute: (1) an\ninteractive human-LLM workflow for large-scale qualitative analysis, (2)\nempirical evidence of its feasibility and researcher experience, and (3) design\nimplications for trustworthy AI-assisted qualitative research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thematic analysis is widely used in qualitative research but can be difficult\nto scale because of its iterative, interpretive demands. We introduce DeTAILS,\na toolkit that integrates large language model (LLM) assistance into a workflow\ninspired by Braun and Clarke's thematic analysis framework. DeTAILS supports\nresearchers in generating and refining codes, reviewing clusters, and\nsynthesizing themes through interactive feedback loops designed to preserve\nanalytic agency. We evaluated the system with 18 qualitative researchers\nanalyzing Reddit data. Quantitative results showed strong alignment between\nLLM-supported outputs and participants' refinements, alongside reduced workload\nand high perceived usefulness. Qualitatively, participants reported that\nDeTAILS accelerated analysis, prompted reflexive engagement with AI outputs,\nand fostered trust through transparency and control. We contribute: (1) an\ninteractive human-LLM workflow for large-scale qualitative analysis, (2)\nempirical evidence of its feasibility and researcher experience, and (3) design\nimplications for trustworthy AI-assisted qualitative research."
                },
                "authors": [
                    {
                        "name": "Ansh Sharma"
                    },
                    {
                        "name": "Karen Cochrane"
                    },
                    {
                        "name": "James R. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "James R. Wallace"
                },
                "author": "James R. Wallace",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02672v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02672v3",
                "updated": "2025-10-21T13:21:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    21,
                    42,
                    1,
                    294,
                    0
                ],
                "published": "2025-06-03T09:18:33Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    18,
                    33,
                    1,
                    154,
                    0
                ],
                "title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via\n  Sequential Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via\n  Sequential Problem Solving"
                },
                "summary": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large\nlanguage models (LLMs) on their learning capability and efficiency in\nchallenging tasks, a critical, yet underexplored aspect of model potential.\nEvaLearn contains 648 challenging problems across six task types, grouped into\n182 sequences, each sequence dedicated to one task type. Diverging from most\nexisting benchmarks that evaluate models in parallel, EvaLearn requires models\nto solve problems sequentially, allowing them to leverage the experience gained\nfrom previous solutions. EvaLearn provides five comprehensive automated metrics\nto evaluate models and quantify their learning capability and efficiency. We\nextensively benchmark nine frontier models and observe varied performance\nprofiles: some models, such as Claude-3.7-sonnet, start with moderate initial\nperformance but exhibit strong learning ability, while some models struggle to\nbenefit from experience and may even show negative transfer. Moreover, we\ninvestigate model performance under two learning settings and find that\ninstance-level rubrics and teacher-model feedback further facilitate model\nlearning. Importantly, we observe that current LLMs with stronger static\nabilities do not show a clear advantage in learning capability across all\ntasks, highlighting that EvaLearn evaluates a new dimension of model\nperformance. We hope EvaLearn provides a novel evaluation perspective for\nassessing LLM potential and understanding the gap between models and human\ncapabilities, promoting the development of deeper and more dynamic evaluation\napproaches. All datasets, the automatic evaluation framework, and the results\nstudied in this paper are available at the GitHub repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large\nlanguage models (LLMs) on their learning capability and efficiency in\nchallenging tasks, a critical, yet underexplored aspect of model potential.\nEvaLearn contains 648 challenging problems across six task types, grouped into\n182 sequences, each sequence dedicated to one task type. Diverging from most\nexisting benchmarks that evaluate models in parallel, EvaLearn requires models\nto solve problems sequentially, allowing them to leverage the experience gained\nfrom previous solutions. EvaLearn provides five comprehensive automated metrics\nto evaluate models and quantify their learning capability and efficiency. We\nextensively benchmark nine frontier models and observe varied performance\nprofiles: some models, such as Claude-3.7-sonnet, start with moderate initial\nperformance but exhibit strong learning ability, while some models struggle to\nbenefit from experience and may even show negative transfer. Moreover, we\ninvestigate model performance under two learning settings and find that\ninstance-level rubrics and teacher-model feedback further facilitate model\nlearning. Importantly, we observe that current LLMs with stronger static\nabilities do not show a clear advantage in learning capability across all\ntasks, highlighting that EvaLearn evaluates a new dimension of model\nperformance. We hope EvaLearn provides a novel evaluation perspective for\nassessing LLM potential and understanding the gap between models and human\ncapabilities, promoting the development of deeper and more dynamic evaluation\napproaches. All datasets, the automatic evaluation framework, and the results\nstudied in this paper are available at the GitHub repository."
                },
                "authors": [
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Chenhao Huang"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Shichun Liu"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Chenxiao Liu"
                    },
                    {
                        "name": "Cheng Zhong"
                    },
                    {
                        "name": "Zongzhang Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Chao Xin"
                    },
                    {
                        "name": "Chengzhi Wei"
                    },
                    {
                        "name": "Lin Yan"
                    },
                    {
                        "name": "Yonghui Wu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Accepted by NeurIPS 2025. 47 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02672v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02672v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15732v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15732v3",
                "updated": "2025-10-21T13:15:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    15,
                    37,
                    1,
                    294,
                    0
                ],
                "published": "2025-06-15T01:08:05Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    1,
                    8,
                    5,
                    6,
                    166,
                    0
                ],
                "title": "Can LLMs Reconcile Knowledge Conflicts in Counterfactual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Reconcile Knowledge Conflicts in Counterfactual Reasoning"
                },
                "summary": "Large Language Models have been shown to contain extensive world knowledge in\ntheir parameters, enabling impressive performance on many knowledge intensive\ntasks. However, when deployed in novel settings, LLMs often encounter\nsituations where they must integrate parametric knowledge with new or\nunfamiliar information. In this work, we explore whether LLMs can combine\nknowledge in-context with their parametric knowledge through the lens of\ncounterfactual reasoning. Through synthetic and real experiments in multi-hop\nreasoning problems, we show that LLMs generally struggle with counterfactual\nreasoning, often resorting to exclusively using their parametric knowledge.\nMoreover, we show that simple post-hoc finetuning can struggle to instill\ncounterfactual reasoning ability -- often leading to degradation in stored\nparametric knowledge. Ultimately, our work reveals important limitations of\ncurrent LLM's abilities to re-purpose parametric knowledge in novel settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have been shown to contain extensive world knowledge in\ntheir parameters, enabling impressive performance on many knowledge intensive\ntasks. However, when deployed in novel settings, LLMs often encounter\nsituations where they must integrate parametric knowledge with new or\nunfamiliar information. In this work, we explore whether LLMs can combine\nknowledge in-context with their parametric knowledge through the lens of\ncounterfactual reasoning. Through synthetic and real experiments in multi-hop\nreasoning problems, we show that LLMs generally struggle with counterfactual\nreasoning, often resorting to exclusively using their parametric knowledge.\nMoreover, we show that simple post-hoc finetuning can struggle to instill\ncounterfactual reasoning ability -- often leading to degradation in stored\nparametric knowledge. Ultimately, our work reveals important limitations of\ncurrent LLM's abilities to re-purpose parametric knowledge in novel settings."
                },
                "authors": [
                    {
                        "name": "Khurram Yamin"
                    },
                    {
                        "name": "Gaurav Ghosal"
                    },
                    {
                        "name": "Bryan Wilder"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Wilder"
                },
                "author": "Bryan Wilder",
                "arxiv_comment": "ICML 2025 Workshop on Scaling up Intervention Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15732v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15732v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17516v2",
                "updated": "2025-10-21T13:05:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    13,
                    5,
                    11,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-20T13:14:38Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    13,
                    14,
                    38,
                    0,
                    293,
                    0
                ],
                "title": "SimBench: Benchmarking the Ability of Large Language Models to Simulate\n  Human Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimBench: Benchmarking the Ability of Large Language Models to Simulate\n  Human Behaviors"
                },
                "summary": "Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators."
                },
                "authors": [
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Joachim Baumann"
                    },
                    {
                        "name": "Lorenzo Lupo"
                    },
                    {
                        "name": "Nigel Collier"
                    },
                    {
                        "name": "Dirk Hovy"
                    },
                    {
                        "name": "Paul Röttger"
                    }
                ],
                "author_detail": {
                    "name": "Paul Röttger"
                },
                "author": "Paul Röttger",
                "arxiv_comment": "Project Website: http://simbench.tiancheng.hu/ Data:\n  https://huggingface.co/datasets/pitehu/SimBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18601v1",
                "updated": "2025-10-21T12:59:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    59,
                    39,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:59:39Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    59,
                    39,
                    1,
                    294,
                    0
                ],
                "title": "Evaluating Large Language Models in detecting Secrets in Android Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models in detecting Secrets in Android Apps"
                },
                "summary": "Mobile apps often embed authentication secrets, such as API keys, tokens, and\nclient IDs, to integrate with cloud services. However, developers often\nhardcode these credentials into Android apps, exposing them to extraction\nthrough reverse engineering. Once compromised, adversaries can exploit secrets\nto access sensitive data, manipulate resources, or abuse APIs, resulting in\nsignificant security and financial risks. Existing detection approaches, such\nas regex-based analysis, static analysis, and machine learning, are effective\nfor identifying known patterns but are fundamentally limited: they require\nprior knowledge of credential structures, API signatures, or training data.\n  In this paper, we propose SecretLoc, an LLM-based approach for detecting\nhardcoded secrets in Android apps. SecretLoc goes beyond pattern matching; it\nleverages contextual and structural cues to identify secrets without relying on\npredefined patterns or labeled training sets. Using a benchmark dataset from\nthe literature, we demonstrate that SecretLoc detects secrets missed by regex-,\nstatic-, and ML-based methods, including previously unseen types of secrets. In\ntotal, we discovered 4828 secrets that were undetected by existing approaches,\ndiscovering more than 10 \"new\" types of secrets, such as OpenAI API keys,\nGitHub Access Tokens, RSA private keys, and JWT tokens, and more.\n  We further extend our analysis to newly crawled apps from Google Play, where\nwe uncovered and responsibly disclosed additional hardcoded secrets. Across a\nset of 5000 apps, we detected secrets in 2124 apps (42.5%), several of which\nwere confirmed and remediated by developers after we contacted them. Our\nresults reveal a dual-use risk: if analysts can uncover these secrets with\nLLMs, so can attackers. This underscores the urgent need for proactive secret\nmanagement and stronger mitigation practices across the mobile ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile apps often embed authentication secrets, such as API keys, tokens, and\nclient IDs, to integrate with cloud services. However, developers often\nhardcode these credentials into Android apps, exposing them to extraction\nthrough reverse engineering. Once compromised, adversaries can exploit secrets\nto access sensitive data, manipulate resources, or abuse APIs, resulting in\nsignificant security and financial risks. Existing detection approaches, such\nas regex-based analysis, static analysis, and machine learning, are effective\nfor identifying known patterns but are fundamentally limited: they require\nprior knowledge of credential structures, API signatures, or training data.\n  In this paper, we propose SecretLoc, an LLM-based approach for detecting\nhardcoded secrets in Android apps. SecretLoc goes beyond pattern matching; it\nleverages contextual and structural cues to identify secrets without relying on\npredefined patterns or labeled training sets. Using a benchmark dataset from\nthe literature, we demonstrate that SecretLoc detects secrets missed by regex-,\nstatic-, and ML-based methods, including previously unseen types of secrets. In\ntotal, we discovered 4828 secrets that were undetected by existing approaches,\ndiscovering more than 10 \"new\" types of secrets, such as OpenAI API keys,\nGitHub Access Tokens, RSA private keys, and JWT tokens, and more.\n  We further extend our analysis to newly crawled apps from Google Play, where\nwe uncovered and responsibly disclosed additional hardcoded secrets. Across a\nset of 5000 apps, we detected secrets in 2124 apps (42.5%), several of which\nwere confirmed and remediated by developers after we contacted them. Our\nresults reveal a dual-use risk: if analysts can uncover these secrets with\nLLMs, so can attackers. This underscores the urgent need for proactive secret\nmanagement and stronger mitigation practices across the mobile ecosystem."
                },
                "authors": [
                    {
                        "name": "Marco Alecci"
                    },
                    {
                        "name": "Jordan Samhi"
                    },
                    {
                        "name": "Tegawendé F. Bissyandé"
                    },
                    {
                        "name": "Jacques Klein"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Klein"
                },
                "author": "Jacques Klein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18600v1",
                "updated": "2025-10-21T12:59:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    59,
                    36,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:59:36Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    59,
                    36,
                    1,
                    294,
                    0
                ],
                "title": "Quadrupeds for Planetary Exploration: Field Testing Control Algorithms\n  on an Active Volcano",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quadrupeds for Planetary Exploration: Field Testing Control Algorithms\n  on an Active Volcano"
                },
                "summary": "Missions such as the Ingenuity helicopter have shown the advantages of using\nnovel locomotion modes to increase the scientific return of planetary\nexploration missions. Legged robots can further expand the reach and capability\nof future planetary missions by traversing more difficult terrain than wheeled\nrovers, such as jumping over cracks on the ground or traversing rugged terrain\nwith boulders. To develop and test algorithms for using quadruped robots, the\nAAPLE project was carried out at DFKI. As part of the project, we conducted a\nseries of field experiments on the Volcano on the Aeolian island of Vulcano, an\nactive stratovolcano near Sicily, Italy. The experiments focused on validating\nnewly developed state-of-the-art adaptive optimal control algorithms for\nquadrupedal locomotion in a high-fidelity analog environment for Lunar and\nMartian surfaces. This paper presents the technical approach, test plan,\nsoftware architecture, field deployment strategy, and evaluation results from\nthe Vulcano campaign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missions such as the Ingenuity helicopter have shown the advantages of using\nnovel locomotion modes to increase the scientific return of planetary\nexploration missions. Legged robots can further expand the reach and capability\nof future planetary missions by traversing more difficult terrain than wheeled\nrovers, such as jumping over cracks on the ground or traversing rugged terrain\nwith boulders. To develop and test algorithms for using quadruped robots, the\nAAPLE project was carried out at DFKI. As part of the project, we conducted a\nseries of field experiments on the Volcano on the Aeolian island of Vulcano, an\nactive stratovolcano near Sicily, Italy. The experiments focused on validating\nnewly developed state-of-the-art adaptive optimal control algorithms for\nquadrupedal locomotion in a high-fidelity analog environment for Lunar and\nMartian surfaces. This paper presents the technical approach, test plan,\nsoftware architecture, field deployment strategy, and evaluation results from\nthe Vulcano campaign."
                },
                "authors": [
                    {
                        "name": "Shubham Vyas"
                    },
                    {
                        "name": "Franek Stark"
                    },
                    {
                        "name": "Rohit Kumar"
                    },
                    {
                        "name": "Hannah Isermann"
                    },
                    {
                        "name": "Jonas Haack"
                    },
                    {
                        "name": "Mihaela Popescu"
                    },
                    {
                        "name": "Jakob Middelberg"
                    },
                    {
                        "name": "Dennis Mronga"
                    },
                    {
                        "name": "Frank Kirchner"
                    }
                ],
                "author_detail": {
                    "name": "Frank Kirchner"
                },
                "author": "Frank Kirchner",
                "arxiv_comment": "Presented at 18th Symposium on Advanced Space Technologies in\n  Robotics and Automation (ASTRA)",
                "arxiv_journal_ref": "18th Symposium on Advanced Space Technologies in Robotics and\n  Automation (ASTRA), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16219v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16219v2",
                "updated": "2025-10-21T12:58:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    58,
                    13,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-17T21:10:35Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    21,
                    10,
                    35,
                    4,
                    290,
                    0
                ],
                "title": "SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based\n  Dynamic Threat Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based\n  Dynamic Threat Detection"
                },
                "summary": "Malicious agents pose significant threats to the reliability and\ndecision-making capabilities of Multi-Agent Systems (MAS) powered by Large\nLanguage Models (LLMs). Existing defenses often fall short due to reactive\ndesigns or centralized architectures which may introduce single points of\nfailure. To address these challenges, we propose SentinelNet, the first\ndecentralized framework for proactively detecting and mitigating malicious\nbehaviors in multi-agent collaboration. SentinelNet equips each agent with a\ncredit-based detector trained via contrastive learning on augmented adversarial\ndebate trajectories, enabling autonomous evaluation of message credibility and\ndynamic neighbor ranking via bottom-k elimination to suppress malicious\ncommunications. To overcome the scarcity of attack data, it generates\nadversarial trajectories simulating diverse threats, ensuring robust training.\nExperiments on MAS benchmarks show SentinelNet achieves near-perfect detection\nof malicious agents, close to 100% within two debate rounds, and recovers 95%\nof system accuracy from compromised baselines. By exhibiting strong\ngeneralizability across domains and attack patterns, SentinelNet establishes a\nnovel paradigm for safeguarding collaborative MAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malicious agents pose significant threats to the reliability and\ndecision-making capabilities of Multi-Agent Systems (MAS) powered by Large\nLanguage Models (LLMs). Existing defenses often fall short due to reactive\ndesigns or centralized architectures which may introduce single points of\nfailure. To address these challenges, we propose SentinelNet, the first\ndecentralized framework for proactively detecting and mitigating malicious\nbehaviors in multi-agent collaboration. SentinelNet equips each agent with a\ncredit-based detector trained via contrastive learning on augmented adversarial\ndebate trajectories, enabling autonomous evaluation of message credibility and\ndynamic neighbor ranking via bottom-k elimination to suppress malicious\ncommunications. To overcome the scarcity of attack data, it generates\nadversarial trajectories simulating diverse threats, ensuring robust training.\nExperiments on MAS benchmarks show SentinelNet achieves near-perfect detection\nof malicious agents, close to 100% within two debate rounds, and recovers 95%\nof system accuracy from compromised baselines. By exhibiting strong\ngeneralizability across domains and attack patterns, SentinelNet establishes a\nnovel paradigm for safeguarding collaborative MAS."
                },
                "authors": [
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Xudong Pan"
                    }
                ],
                "author_detail": {
                    "name": "Xudong Pan"
                },
                "author": "Xudong Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16219v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16219v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16505v2",
                "updated": "2025-10-21T12:52:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    52,
                    54,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-18T13:46:26Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    13,
                    46,
                    26,
                    5,
                    291,
                    0
                ],
                "title": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal\n  Inconsistencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal\n  Inconsistencies"
                },
                "summary": "Large Multimodal Models (LMMs) are increasingly applied to scientific\nresearch, yet it remains unclear whether they can reliably understand and\nreason over the multimodal complexity of papers. A central challenge lies in\ndetecting and resolving inconsistencies across text, figures, tables, and\nequations, issues that are often subtle, domain-specific, and ultimately\nundermine clarity, reproducibility, and trust. Existing benchmarks overlook\nthis issue, either isolating single modalities or relying on synthetic errors\nthat fail to capture real-world complexity. We introduce PRISMM-Bench\n(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first\nbenchmark grounded in real reviewer-flagged inconsistencies in scientific\npapers. Through a multi-stage pipeline of review mining, LLM-assisted filtering\nand human verification, we curate 262 inconsistencies from 242 papers. Based on\nthis set, we design three tasks, namely inconsistency identification, remedy\nand pair matching, which assess a model's capacity to detect, correct, and\nreason over inconsistencies across different modalities. Furthermore, to\naddress the notorious problem of choice-only shortcuts in multiple-choice\nevaluation, where models exploit answer patterns without truly understanding\nthe question, we further introduce structured JSON-based answer representations\nthat minimize linguistic biases by reducing reliance on superficial stylistic\ncues. We benchmark 21 leading LMMs, including large open-weight models\n(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5\nwith high reasoning). Results reveal strikingly low performance (26.1-54.2%),\nunderscoring the challenge of multimodal scientific reasoning and motivating\nprogress towards trustworthy scientific assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) are increasingly applied to scientific\nresearch, yet it remains unclear whether they can reliably understand and\nreason over the multimodal complexity of papers. A central challenge lies in\ndetecting and resolving inconsistencies across text, figures, tables, and\nequations, issues that are often subtle, domain-specific, and ultimately\nundermine clarity, reproducibility, and trust. Existing benchmarks overlook\nthis issue, either isolating single modalities or relying on synthetic errors\nthat fail to capture real-world complexity. We introduce PRISMM-Bench\n(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first\nbenchmark grounded in real reviewer-flagged inconsistencies in scientific\npapers. Through a multi-stage pipeline of review mining, LLM-assisted filtering\nand human verification, we curate 262 inconsistencies from 242 papers. Based on\nthis set, we design three tasks, namely inconsistency identification, remedy\nand pair matching, which assess a model's capacity to detect, correct, and\nreason over inconsistencies across different modalities. Furthermore, to\naddress the notorious problem of choice-only shortcuts in multiple-choice\nevaluation, where models exploit answer patterns without truly understanding\nthe question, we further introduce structured JSON-based answer representations\nthat minimize linguistic biases by reducing reliance on superficial stylistic\ncues. We benchmark 21 leading LMMs, including large open-weight models\n(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5\nwith high reasoning). Results reveal strikingly low performance (26.1-54.2%),\nunderscoring the challenge of multimodal scientific reasoning and motivating\nprogress towards trustworthy scientific assistants."
                },
                "authors": [
                    {
                        "name": "Lukas Selch"
                    },
                    {
                        "name": "Yufang Hou"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Sivan Doveh"
                    },
                    {
                        "name": "James Glass"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14807v2",
                "updated": "2025-10-21T12:46:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    46,
                    48,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-16T15:40:49Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    40,
                    49,
                    3,
                    289,
                    0
                ],
                "title": "SimKO: Simple Pass@K Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimKO: Simple Pass@K Policy Optimization"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has advanced the\nreasoning capabilities of large language models (LLMs). However, prevailing\nRLVR methods exhibit a systematic bias toward exploitation over exploration, as\nevidenced by improved pass@1 but reduced pass@K (K>1) performance. To\nunderstand this issue, we analyze training dynamics of RLVR methods by tracking\nthe token-level probability distributions over vocabulary candidates. Our\nanalysis reveals a consistent probability concentration effect where the top-1\ncandidate increasingly accumulates probability mass and suppresses that of\nother candidates. More importantly, stronger over-concentration correlates with\nworse pass@K performance. Inspired by this finding, we propose Simple Pass@K\nOptimization (SimKO), a method designed to mitigate the over-concentration\nissue, thereby encouraging exploration. SimKO operates in an asymmetrical\nmanner. For verified-correct responses, it boosts the probabilities of the\ntop-K candidates. For verified-incorrect responses, it applies stronger\npenalties to the top-1 candidate. We observe that this asymmetric design is\nparticularly effective at mitigating over-concentration when applied at tokens\nwith high entropy. Across various math and logical-reasoning benchmarks, SimKO\nconsistently yields higher pass@K for a wide range of K, providing a simple way\nto improve RLVR's exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has advanced the\nreasoning capabilities of large language models (LLMs). However, prevailing\nRLVR methods exhibit a systematic bias toward exploitation over exploration, as\nevidenced by improved pass@1 but reduced pass@K (K>1) performance. To\nunderstand this issue, we analyze training dynamics of RLVR methods by tracking\nthe token-level probability distributions over vocabulary candidates. Our\nanalysis reveals a consistent probability concentration effect where the top-1\ncandidate increasingly accumulates probability mass and suppresses that of\nother candidates. More importantly, stronger over-concentration correlates with\nworse pass@K performance. Inspired by this finding, we propose Simple Pass@K\nOptimization (SimKO), a method designed to mitigate the over-concentration\nissue, thereby encouraging exploration. SimKO operates in an asymmetrical\nmanner. For verified-correct responses, it boosts the probabilities of the\ntop-K candidates. For verified-incorrect responses, it applies stronger\npenalties to the top-1 candidate. We observe that this asymmetric design is\nparticularly effective at mitigating over-concentration when applied at tokens\nwith high entropy. Across various math and logical-reasoning benchmarks, SimKO\nconsistently yields higher pass@K for a wide range of K, providing a simple way\nto improve RLVR's exploration."
                },
                "authors": [
                    {
                        "name": "Ruotian Peng"
                    },
                    {
                        "name": "Yi Ren"
                    },
                    {
                        "name": "Zhouliang Yu"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Yandong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yandong Wen"
                },
                "author": "Yandong Wen",
                "arxiv_comment": "Technical report (20 pages, 10 figures, project page:\n  https://spherelab.ai/simko/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18586v1",
                "updated": "2025-10-21T12:39:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:39:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18585v1",
                "updated": "2025-10-21T12:38:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    38,
                    52,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:38:52Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    38,
                    52,
                    1,
                    294,
                    0
                ],
                "title": "CLASP: Cost-Optimized LLM-based Agentic System for Phishing Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLASP: Cost-Optimized LLM-based Agentic System for Phishing Detection"
                },
                "summary": "Phishing websites remain a significant cybersecurity threat, necessitating\naccurate and cost-effective detection mechanisms. In this paper, we present\nCLASP, a novel system that effectively identifies phishing websites by\nleveraging multiple intelligent agents, built using large language models\n(LLMs), to analyze different aspects of a web resource. The system processes\nURLs or QR codes, employing specialized LLM-based agents that evaluate the URL\nstructure, webpage screenshot, and HTML content to predict potential phishing\nthreats. To optimize performance while minimizing operational costs, we\nexperimented with multiple combination strategies for agent-based analysis,\nultimately designing a strategic combination that ensures the per-website\nevaluation expense remains minimal without compromising detection accuracy. We\ntested various LLMs, including Gemini 1.5 Flash and GPT-4o mini, to build these\nagents and found that Gemini 1.5 Flash achieved the best performance with an F1\nscore of 83.01% on a newly curated dataset. Also, the system maintained an\naverage processing time of 2.78 seconds per website and an API cost of around\n$3.18 per 1,000 websites. Moreover, CLASP surpasses leading previous solutions,\nachieving over 40% higher recall and a 20% improvement in F1 score for phishing\ndetection on the collected dataset. To support further research, we have made\nour dataset publicly available, supporting the development of more advanced\nphishing detection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing websites remain a significant cybersecurity threat, necessitating\naccurate and cost-effective detection mechanisms. In this paper, we present\nCLASP, a novel system that effectively identifies phishing websites by\nleveraging multiple intelligent agents, built using large language models\n(LLMs), to analyze different aspects of a web resource. The system processes\nURLs or QR codes, employing specialized LLM-based agents that evaluate the URL\nstructure, webpage screenshot, and HTML content to predict potential phishing\nthreats. To optimize performance while minimizing operational costs, we\nexperimented with multiple combination strategies for agent-based analysis,\nultimately designing a strategic combination that ensures the per-website\nevaluation expense remains minimal without compromising detection accuracy. We\ntested various LLMs, including Gemini 1.5 Flash and GPT-4o mini, to build these\nagents and found that Gemini 1.5 Flash achieved the best performance with an F1\nscore of 83.01% on a newly curated dataset. Also, the system maintained an\naverage processing time of 2.78 seconds per website and an API cost of around\n$3.18 per 1,000 websites. Moreover, CLASP surpasses leading previous solutions,\nachieving over 40% higher recall and a 20% improvement in F1 score for phishing\ndetection on the collected dataset. To support further research, we have made\nour dataset publicly available, supporting the development of more advanced\nphishing detection systems."
                },
                "authors": [
                    {
                        "name": "Fouad Trad"
                    },
                    {
                        "name": "Ali Chehab"
                    }
                ],
                "author_detail": {
                    "name": "Ali Chehab"
                },
                "author": "Ali Chehab",
                "arxiv_comment": "Accepted in the 5th International Conference on Electrical, Computer,\n  and Energy Technologies (ICECET2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18572v2",
                "updated": "2025-10-22T10:52:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    10,
                    52,
                    26,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-21T12:28:11Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    28,
                    11,
                    1,
                    294,
                    0
                ],
                "title": "Forward to Hell? On the Potentials of Misusing Transparent DNS\n  Forwarders in Reflective Amplification Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forward to Hell? On the Potentials of Misusing Transparent DNS\n  Forwarders in Reflective Amplification Attacks"
                },
                "summary": "The DNS infrastructure is infamous for facilitating reflective amplification\nattacks. Various countermeasures such as server shielding, access control, rate\nlimiting, and protocol restrictions have been implemented. Still, the threat\nremains throughout the deployment of DNS servers. In this paper, we report on\nand evaluate the often unnoticed threat that derives from transparent DNS\nforwarders, a widely deployed, incompletely functional set of DNS components.\nTransparent DNS forwarders transfer DNS requests without rebuilding packets\nwith correct source addresses. As such, transparent forwarders feed DNS\nrequests into (mainly powerful and anycasted) open recursive resolvers, which\nthereby can be misused to participate unwillingly in distributed reflective\namplification attacks. We show how transparent forwarders raise severe threats\nto the Internet infrastructure. They easily circumvent rate limiting and\nachieve an additional, scalable impact via the DNS anycast infrastructure. We\nempirically verify this scaling behavior up to a factor of 14. Transparent\nforwarders can also assist in bypassing firewall rules that protect recursive\nresolvers, making these shielded infrastructure entities part of the global DNS\nattack surface.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS infrastructure is infamous for facilitating reflective amplification\nattacks. Various countermeasures such as server shielding, access control, rate\nlimiting, and protocol restrictions have been implemented. Still, the threat\nremains throughout the deployment of DNS servers. In this paper, we report on\nand evaluate the often unnoticed threat that derives from transparent DNS\nforwarders, a widely deployed, incompletely functional set of DNS components.\nTransparent DNS forwarders transfer DNS requests without rebuilding packets\nwith correct source addresses. As such, transparent forwarders feed DNS\nrequests into (mainly powerful and anycasted) open recursive resolvers, which\nthereby can be misused to participate unwillingly in distributed reflective\namplification attacks. We show how transparent forwarders raise severe threats\nto the Internet infrastructure. They easily circumvent rate limiting and\nachieve an additional, scalable impact via the DNS anycast infrastructure. We\nempirically verify this scaling behavior up to a factor of 14. Transparent\nforwarders can also assist in bypassing firewall rules that protect recursive\nresolvers, making these shielded infrastructure entities part of the global DNS\nattack surface."
                },
                "authors": [
                    {
                        "name": "Maynard Koch"
                    },
                    {
                        "name": "Florian Dolzmann"
                    },
                    {
                        "name": "Thomas C. Schmidt"
                    },
                    {
                        "name": "Matthias Wählisch"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Wählisch"
                },
                "author": "Matthias Wählisch",
                "arxiv_doi": "10.1145/3719027.3765096",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3719027.3765096",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.18572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of ACM CCS 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18570v1",
                "updated": "2025-10-21T12:25:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    25,
                    44,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:25:44Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    25,
                    44,
                    1,
                    294,
                    0
                ],
                "title": "Electromagnetic Field Exposure Assessment and Mitigation Strategies for\n  Wireless Power Transfer Systems: A Review and Future Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electromagnetic Field Exposure Assessment and Mitigation Strategies for\n  Wireless Power Transfer Systems: A Review and Future Perspectives"
                },
                "summary": "Wireless power transfer (WPT) technologies are increasingly being applied in\nfields ranging from consumer electronics and electric vehicles to space-based\nenergy systems and medical implants. While WPT offers contactless power\ndelivery, it introduces electromagnetic field (EMF) emissions, necessitating\ncareful assessment to address safety and public health concerns. Exposure\nguidelines developed by ICNIRP and IEEE define frequency-dependent limits based\non internal quantities, such as electric field strength and specific absorption\nrate, intended to prevent tissue nerve stimulation < 100 kHz and heating > 100\nkHz, respectively. Complementing these guidelines, assessment standards\nincluding the International Electrotechnical Commission (IEC)/IEEE 63184 and\nIEC Technical Report 63377, provide practical procedures for evaluating the EMF\nexposure in WPT systems. This review offers a comparative overview of major WPT\nmodalities, with a focus on recent developments in computational dosimetry and\nstandardized assessment techniques for the complex, non-uniform fields typical\nof WPT environments. It also discusses electromagnetic interference with\nmedical devices and exposure scenarios involving partial body proximity and\nvarious postures. A notable observation across modalities is the considerable\nvariability, often spanning an order of magnitude, in the allowable transfer\npower, depending on the field distribution and assessment approach. Remaining\nchallenges include the lack of harmonized guidance for intermediate frequencies\nand localized exposure, underscoring the importance of further coordination in\ninternational standardization efforts. Addressing these issues is essential for\nthe safe and widespread deployment of WPT technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless power transfer (WPT) technologies are increasingly being applied in\nfields ranging from consumer electronics and electric vehicles to space-based\nenergy systems and medical implants. While WPT offers contactless power\ndelivery, it introduces electromagnetic field (EMF) emissions, necessitating\ncareful assessment to address safety and public health concerns. Exposure\nguidelines developed by ICNIRP and IEEE define frequency-dependent limits based\non internal quantities, such as electric field strength and specific absorption\nrate, intended to prevent tissue nerve stimulation < 100 kHz and heating > 100\nkHz, respectively. Complementing these guidelines, assessment standards\nincluding the International Electrotechnical Commission (IEC)/IEEE 63184 and\nIEC Technical Report 63377, provide practical procedures for evaluating the EMF\nexposure in WPT systems. This review offers a comparative overview of major WPT\nmodalities, with a focus on recent developments in computational dosimetry and\nstandardized assessment techniques for the complex, non-uniform fields typical\nof WPT environments. It also discusses electromagnetic interference with\nmedical devices and exposure scenarios involving partial body proximity and\nvarious postures. A notable observation across modalities is the considerable\nvariability, often spanning an order of magnitude, in the allowable transfer\npower, depending on the field distribution and assessment approach. Remaining\nchallenges include the lack of harmonized guidance for intermediate frequencies\nand localized exposure, underscoring the importance of further coordination in\ninternational standardization efforts. Addressing these issues is essential for\nthe safe and widespread deployment of WPT technologies."
                },
                "authors": [
                    {
                        "name": "Akimasa Hirata"
                    },
                    {
                        "name": "Teruo Onishi"
                    },
                    {
                        "name": "Naoki Shinohara"
                    },
                    {
                        "name": "Valerio De Santis"
                    },
                    {
                        "name": "Yinliang Diao"
                    },
                    {
                        "name": "Mauro Feliziani"
                    },
                    {
                        "name": "Takashi Hikage"
                    },
                    {
                        "name": "Junqing Lan"
                    },
                    {
                        "name": "Francesca Maradei"
                    },
                    {
                        "name": "Keishi Miwa"
                    },
                    {
                        "name": "Alexander Prokop"
                    },
                    {
                        "name": "Yujun Shin"
                    },
                    {
                        "name": "Seungyoung Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Seungyoung Ahn"
                },
                "author": "Seungyoung Ahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17425v2",
                "updated": "2025-10-21T12:25:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    25,
                    26,
                    1,
                    294,
                    0
                ],
                "published": "2025-07-23T11:32:28Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    32,
                    28,
                    2,
                    204,
                    0
                ],
                "title": "Readout electronics for low occupancy High-Pressure Gas TPCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Readout electronics for low occupancy High-Pressure Gas TPCs"
                },
                "summary": "HPgTPCs have benefits such as low energy threshold, magnetisability, and\n4$\\pi$ acceptance, making them ideal for neutrino experiments such as DUNE. We\npresent the design of an FPGA-based solution optimised for ND-GAr, which is\npart of the Phase-II more capable near detector for DUNE. These electronics\nreduce the cost significantly compared to using collider readout electronics\nwhich are typically designed for much higher occupancy and therefore, for\nexample, need much larger numbers of FPGAs and power per channel. We\ndemonstrate the performance of our electronics with the TOAD at Fermilab in the\nUS at a range of pressures and gas mixtures up to 4.5barA, reading out ~10000\nchannels from a multi-wire proportional chamber. The operation took place\nbetween April and July of 2024. We measure the noise characteristics of the\nsystem to be sufficiently low and we identify sources of noise that can be\nfurther mitigated in the next iteration. We also note that the cooling scheme\nused in the test requires improvement before full-scale deployment. Despite\nthese necessary improvements, we show that the system can fulfil the needs of a\nHPgTPC for a fraction of the price of collider readout electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPgTPCs have benefits such as low energy threshold, magnetisability, and\n4$\\pi$ acceptance, making them ideal for neutrino experiments such as DUNE. We\npresent the design of an FPGA-based solution optimised for ND-GAr, which is\npart of the Phase-II more capable near detector for DUNE. These electronics\nreduce the cost significantly compared to using collider readout electronics\nwhich are typically designed for much higher occupancy and therefore, for\nexample, need much larger numbers of FPGAs and power per channel. We\ndemonstrate the performance of our electronics with the TOAD at Fermilab in the\nUS at a range of pressures and gas mixtures up to 4.5barA, reading out ~10000\nchannels from a multi-wire proportional chamber. The operation took place\nbetween April and July of 2024. We measure the noise characteristics of the\nsystem to be sufficiently low and we identify sources of noise that can be\nfurther mitigated in the next iteration. We also note that the cooling scheme\nused in the test requires improvement before full-scale deployment. Despite\nthese necessary improvements, we show that the system can fulfil the needs of a\nHPgTPC for a fraction of the price of collider readout electronics."
                },
                "authors": [
                    {
                        "name": "N. Khan"
                    },
                    {
                        "name": "Y. Hua"
                    },
                    {
                        "name": "I. Xiotidis"
                    },
                    {
                        "name": "T. Alves"
                    },
                    {
                        "name": "E. Atkin"
                    },
                    {
                        "name": "G. Barker"
                    },
                    {
                        "name": "D. Barrow"
                    },
                    {
                        "name": "A. Booth"
                    },
                    {
                        "name": "J. Borg"
                    },
                    {
                        "name": "A. Bross"
                    },
                    {
                        "name": "M. F. Cicala"
                    },
                    {
                        "name": "L. Cremonesi"
                    },
                    {
                        "name": "A. Deisting"
                    },
                    {
                        "name": "K. Duffy"
                    },
                    {
                        "name": "R. Gran"
                    },
                    {
                        "name": "P. Green"
                    },
                    {
                        "name": "A. Habig"
                    },
                    {
                        "name": "M. Judah"
                    },
                    {
                        "name": "T. Junk"
                    },
                    {
                        "name": "A. Kaboth"
                    },
                    {
                        "name": "A. Klustová"
                    },
                    {
                        "name": "H. LeMoine"
                    },
                    {
                        "name": "A. D. Marino"
                    },
                    {
                        "name": "F. Martínez López"
                    },
                    {
                        "name": "T. Mohayai"
                    },
                    {
                        "name": "D. Naples"
                    },
                    {
                        "name": "R. Nichol"
                    },
                    {
                        "name": "D. Parker"
                    },
                    {
                        "name": "M. Pfaff"
                    },
                    {
                        "name": "J. Raaf"
                    },
                    {
                        "name": "P. Rubinov"
                    },
                    {
                        "name": "P. Singh"
                    },
                    {
                        "name": "A. Srivastava"
                    },
                    {
                        "name": "A. Waldron"
                    },
                    {
                        "name": "L. Warsame"
                    },
                    {
                        "name": "M. Wascko"
                    },
                    {
                        "name": "A. Wilkinson"
                    },
                    {
                        "name": "A. Ritchie-Yates"
                    },
                    {
                        "name": "P. Dunne"
                    }
                ],
                "author_detail": {
                    "name": "P. Dunne"
                },
                "author": "P. Dunne",
                "arxiv_comment": "26 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18569v1",
                "updated": "2025-10-21T12:22:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    22,
                    16,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:22:16Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    22,
                    16,
                    1,
                    294,
                    0
                ],
                "title": "QuantEvolve: Automating Quantitative Strategy Discovery through\n  Multi-Agent Evolutionary Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantEvolve: Automating Quantitative Strategy Discovery through\n  Multi-Agent Evolutionary Framework"
                },
                "summary": "Automating quantitative trading strategy development in dynamic markets is\nchallenging, especially with increasing demand for personalized investment\nsolutions. Existing methods often fail to explore the vast strategy space while\npreserving the diversity essential for robust performance across changing\nmarket conditions. We present QuantEvolve, an evolutionary framework that\ncombines quality-diversity optimization with hypothesis-driven strategy\ngeneration. QuantEvolve employs a feature map aligned with investor\npreferences, such as strategy type, risk profile, turnover, and return\ncharacteristics, to maintain a diverse set of effective strategies. It also\nintegrates a hypothesis-driven multi-agent system to systematically explore the\nstrategy space through iterative generation and evaluation. This approach\nproduces diverse, sophisticated strategies that adapt to both market regime\nshifts and individual investment needs. Empirical results show that QuantEvolve\noutperforms conventional baselines, validating its effectiveness. We release a\ndataset of evolved strategies to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating quantitative trading strategy development in dynamic markets is\nchallenging, especially with increasing demand for personalized investment\nsolutions. Existing methods often fail to explore the vast strategy space while\npreserving the diversity essential for robust performance across changing\nmarket conditions. We present QuantEvolve, an evolutionary framework that\ncombines quality-diversity optimization with hypothesis-driven strategy\ngeneration. QuantEvolve employs a feature map aligned with investor\npreferences, such as strategy type, risk profile, turnover, and return\ncharacteristics, to maintain a diverse set of effective strategies. It also\nintegrates a hypothesis-driven multi-agent system to systematically explore the\nstrategy space through iterative generation and evaluation. This approach\nproduces diverse, sophisticated strategies that adapt to both market regime\nshifts and individual investment needs. Empirical results show that QuantEvolve\noutperforms conventional baselines, validating its effectiveness. We release a\ndataset of evolved strategies to support future research."
                },
                "authors": [
                    {
                        "name": "Junhyeog Yun"
                    },
                    {
                        "name": "Hyoun Jun Lee"
                    },
                    {
                        "name": "Insu Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Insu Jeon"
                },
                "author": "Insu Jeon",
                "arxiv_comment": "25 pages, 13 figures. Accepted for oral presentation at the 2nd\n  Workshop on LLMs and Generative AI for Finance (AI4F), part of ACM ICAIF\n  2025, Singapore. Non-archival workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18563v1",
                "updated": "2025-10-21T12:18:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    18,
                    39,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:18:39Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    18,
                    39,
                    1,
                    294,
                    0
                ],
                "title": "The Trust Paradox in LLM-Based Multi-Agent Systems: When Collaboration\n  Becomes a Security Vulnerability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Trust Paradox in LLM-Based Multi-Agent Systems: When Collaboration\n  Becomes a Security Vulnerability"
                },
                "summary": "Multi-agent systems powered by large language models are advancing rapidly,\nyet the tension between mutual trust and security remains underexplored. We\nintroduce and empirically validate the Trust-Vulnerability Paradox (TVP):\nincreasing inter-agent trust to enhance coordination simultaneously expands\nrisks of over-exposure and over-authorization. To investigate this paradox, we\nconstruct a scenario-game dataset spanning 3 macro scenes and 19 sub-scenes,\nand run extensive closed-loop interactions with trust explicitly parameterized.\nUsing Minimum Necessary Information (MNI) as the safety baseline, we propose\ntwo unified metrics: Over-Exposure Rate (OER) to detect boundary violations,\nand Authorization Drift (AD) to capture sensitivity to trust levels. Results\nacross multiple model backends and orchestration frameworks reveal consistent\ntrends: higher trust improves task success but also heightens exposure risks,\nwith heterogeneous trust-to-risk mappings across systems. We further examine\ndefenses such as Sensitive Information Repartitioning and Guardian-Agent\nenablement, both of which reduce OER and attenuate AD. Overall, this study\nformalizes TVP, establishes reproducible baselines with unified metrics, and\ndemonstrates that trust must be modeled and scheduled as a first-class security\nvariable in multi-agent system design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems powered by large language models are advancing rapidly,\nyet the tension between mutual trust and security remains underexplored. We\nintroduce and empirically validate the Trust-Vulnerability Paradox (TVP):\nincreasing inter-agent trust to enhance coordination simultaneously expands\nrisks of over-exposure and over-authorization. To investigate this paradox, we\nconstruct a scenario-game dataset spanning 3 macro scenes and 19 sub-scenes,\nand run extensive closed-loop interactions with trust explicitly parameterized.\nUsing Minimum Necessary Information (MNI) as the safety baseline, we propose\ntwo unified metrics: Over-Exposure Rate (OER) to detect boundary violations,\nand Authorization Drift (AD) to capture sensitivity to trust levels. Results\nacross multiple model backends and orchestration frameworks reveal consistent\ntrends: higher trust improves task success but also heightens exposure risks,\nwith heterogeneous trust-to-risk mappings across systems. We further examine\ndefenses such as Sensitive Information Repartitioning and Guardian-Agent\nenablement, both of which reduce OER and attenuate AD. Overall, this study\nformalizes TVP, establishes reproducible baselines with unified metrics, and\ndemonstrates that trust must be modeled and scheduled as a first-class security\nvariable in multi-agent system design."
                },
                "authors": [
                    {
                        "name": "Zijie Xu"
                    },
                    {
                        "name": "Minfeng Qi"
                    },
                    {
                        "name": "Shiqing Wu"
                    },
                    {
                        "name": "Lefeng Zhang"
                    },
                    {
                        "name": "Qiwen Wei"
                    },
                    {
                        "name": "Han He"
                    },
                    {
                        "name": "Ningran Li"
                    }
                ],
                "author_detail": {
                    "name": "Ningran Li"
                },
                "author": "Ningran Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18560v1",
                "updated": "2025-10-21T12:16:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    16,
                    4,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:16:04Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    16,
                    4,
                    1,
                    294,
                    0
                ],
                "title": "WebDevJudge: Evaluating (M)LLMs as Critiques for Web Development Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebDevJudge: Evaluating (M)LLMs as Critiques for Web Development Quality"
                },
                "summary": "The paradigm of LLM-as-a-judge is emerging as a scalable and efficient\nalternative to human evaluation, demonstrating strong performance on\nwell-defined tasks. However, its reliability in open-ended tasks with dynamic\nenvironments and complex interactions remains unexplored. To bridge the gap, we\nintroduce WebDevJudge, a systematic benchmark for assessing LLM-as-a-judge\nperformance in web development, with support for both non-interactive\nevaluation based on static observations and continuous interactive evaluation\nwith a dynamic web environment. WebDevJudge comprises human preference labels\nover paired web implementations, annotated with structured and query-grounded\nrubrics to ensure high-quality ground truth. Using this benchmark, we\ncomprehensively evaluate various evaluators, including LLMs, MLLMs, and agentic\nworkflows. We systematically investigate the impact of different paradigms and\nguidance mechanisms. Our experiments reveal a significant gap between LLM\njudges and human experts. In-depth analysis indicates this gap stems from\nfundamental model limitations, including failures in recognizing functional\nequivalence, verifying task feasibility, and mitigating bias. Overall,\nWebDevJudge presents a significant challenge to LLM-as-a-judge, offering\ninsights to guide future research toward developing more reliable and capable\nautomated evaluators for complicated scenarios. Code and data are available at\nhttps://github.com/lcy2723/WebDevJudge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paradigm of LLM-as-a-judge is emerging as a scalable and efficient\nalternative to human evaluation, demonstrating strong performance on\nwell-defined tasks. However, its reliability in open-ended tasks with dynamic\nenvironments and complex interactions remains unexplored. To bridge the gap, we\nintroduce WebDevJudge, a systematic benchmark for assessing LLM-as-a-judge\nperformance in web development, with support for both non-interactive\nevaluation based on static observations and continuous interactive evaluation\nwith a dynamic web environment. WebDevJudge comprises human preference labels\nover paired web implementations, annotated with structured and query-grounded\nrubrics to ensure high-quality ground truth. Using this benchmark, we\ncomprehensively evaluate various evaluators, including LLMs, MLLMs, and agentic\nworkflows. We systematically investigate the impact of different paradigms and\nguidance mechanisms. Our experiments reveal a significant gap between LLM\njudges and human experts. In-depth analysis indicates this gap stems from\nfundamental model limitations, including failures in recognizing functional\nequivalence, verifying task feasibility, and mitigating bias. Overall,\nWebDevJudge presents a significant challenge to LLM-as-a-judge, offering\ninsights to guide future research toward developing more reliable and capable\nautomated evaluators for complicated scenarios. Code and data are available at\nhttps://github.com/lcy2723/WebDevJudge."
                },
                "authors": [
                    {
                        "name": "Chunyang Li"
                    },
                    {
                        "name": "Yilun Zheng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Lihui Chen"
                    },
                    {
                        "name": "Han Hu"
                    }
                ],
                "author_detail": {
                    "name": "Han Hu"
                },
                "author": "Han Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18556v1",
                "updated": "2025-10-21T12:08:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    8,
                    39,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:08:39Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    8,
                    39,
                    1,
                    294,
                    0
                ],
                "title": "Building Trust in Clinical LLMs: Bias Analysis and Dataset Transparency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Trust in Clinical LLMs: Bias Analysis and Dataset Transparency"
                },
                "summary": "Large language models offer transformative potential for healthcare, yet\ntheir responsible and equitable development depends critically on a deeper\nunderstanding of how training data characteristics influence model behavior,\nincluding the potential for bias. Current practices in dataset curation and\nbias assessment often lack the necessary transparency, creating an urgent need\nfor comprehensive evaluation frameworks to foster trust and guide improvements.\nIn this study, we present an in-depth analysis of potential downstream biases\nin clinical language models, with a focus on differential opioid prescription\ntendencies across diverse demographic groups, such as ethnicity, gender, and\nage. As part of this investigation, we introduce HC4: Healthcare Comprehensive\nCommons Corpus, a novel and extensively curated pretraining dataset exceeding\n89 billion tokens. Our evaluation leverages both established general benchmarks\nand a novel, healthcare-specific methodology, offering crucial insights to\nsupport fairness and safety in clinical AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models offer transformative potential for healthcare, yet\ntheir responsible and equitable development depends critically on a deeper\nunderstanding of how training data characteristics influence model behavior,\nincluding the potential for bias. Current practices in dataset curation and\nbias assessment often lack the necessary transparency, creating an urgent need\nfor comprehensive evaluation frameworks to foster trust and guide improvements.\nIn this study, we present an in-depth analysis of potential downstream biases\nin clinical language models, with a focus on differential opioid prescription\ntendencies across diverse demographic groups, such as ethnicity, gender, and\nage. As part of this investigation, we introduce HC4: Healthcare Comprehensive\nCommons Corpus, a novel and extensively curated pretraining dataset exceeding\n89 billion tokens. Our evaluation leverages both established general benchmarks\nand a novel, healthcare-specific methodology, offering crucial insights to\nsupport fairness and safety in clinical AI applications."
                },
                "authors": [
                    {
                        "name": "Svetlana Maslenkova"
                    },
                    {
                        "name": "Clement Christophe"
                    },
                    {
                        "name": "Marco AF Pimentel"
                    },
                    {
                        "name": "Tathagata Raha"
                    },
                    {
                        "name": "Muhammad Umar Salman"
                    },
                    {
                        "name": "Ahmed Al Mahrooqi"
                    },
                    {
                        "name": "Avani Gupta"
                    },
                    {
                        "name": "Shadab Khan"
                    },
                    {
                        "name": "Ronnie Rajan"
                    },
                    {
                        "name": "Praveenkumar Kanithi"
                    }
                ],
                "author_detail": {
                    "name": "Praveenkumar Kanithi"
                },
                "author": "Praveenkumar Kanithi",
                "arxiv_comment": "Accepted to EMNLP Main 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17142v2",
                "updated": "2025-10-21T12:02:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    2,
                    24,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-20T04:28:10Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    4,
                    28,
                    10,
                    0,
                    293,
                    0
                ],
                "title": "PEACE: Towards Efficient Project-Level Efficiency Optimization via\n  Hybrid Code Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEACE: Towards Efficient Project-Level Efficiency Optimization via\n  Hybrid Code Editing"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant capability in code\ngeneration, but their potential in code efficiency optimization remains\nunderexplored. Previous LLM-based code efficiency optimization approaches\nexclusively focus on function-level optimization and overlook interaction\nbetween functions, failing to generalize to real-world development scenarios.\nCode editing techniques show great potential for conducting project-level\noptimization, yet they face challenges associated with invalid edits and\nsuboptimal internal functions. To address these gaps, we propose Peace, a novel\nhybrid framework for Project-level code Efficiency optimization through\nAutomatic Code Editing, which also ensures the overall correctness and\nintegrity of the project. Peace integrates three key phases: dependency-aware\noptimizing function sequence construction, valid associated edits\nidentification, and efficiency optimization editing iteration. To rigorously\nevaluate the effectiveness of Peace, we construct PeacExec, the first benchmark\ncomprising 146 real-world optimization tasks from 47 high-impact GitHub Python\nprojects, along with highly qualified test cases and executable environments.\nExtensive experiments demonstrate Peace's superiority over the state-of-the-art\nbaselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and\n0.840 speedup in execution efficiency. Notably, our Peace outperforms all\nbaselines by significant margins, particularly in complex optimization tasks\nwith multiple functions. Moreover, extensive experiments are also conducted to\nvalidate the contributions of each component in Peace, as well as the rationale\nand effectiveness of our hybrid framework design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant capability in code\ngeneration, but their potential in code efficiency optimization remains\nunderexplored. Previous LLM-based code efficiency optimization approaches\nexclusively focus on function-level optimization and overlook interaction\nbetween functions, failing to generalize to real-world development scenarios.\nCode editing techniques show great potential for conducting project-level\noptimization, yet they face challenges associated with invalid edits and\nsuboptimal internal functions. To address these gaps, we propose Peace, a novel\nhybrid framework for Project-level code Efficiency optimization through\nAutomatic Code Editing, which also ensures the overall correctness and\nintegrity of the project. Peace integrates three key phases: dependency-aware\noptimizing function sequence construction, valid associated edits\nidentification, and efficiency optimization editing iteration. To rigorously\nevaluate the effectiveness of Peace, we construct PeacExec, the first benchmark\ncomprising 146 real-world optimization tasks from 47 high-impact GitHub Python\nprojects, along with highly qualified test cases and executable environments.\nExtensive experiments demonstrate Peace's superiority over the state-of-the-art\nbaselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and\n0.840 speedup in execution efficiency. Notably, our Peace outperforms all\nbaselines by significant margins, particularly in complex optimization tasks\nwith multiple functions. Moreover, extensive experiments are also conducted to\nvalidate the contributions of each component in Peace, as well as the rationale\nand effectiveness of our hybrid framework design."
                },
                "authors": [
                    {
                        "name": "Xiaoxue Ren"
                    },
                    {
                        "name": "Jun Wan"
                    },
                    {
                        "name": "Yun Peng"
                    },
                    {
                        "name": "Zhongxin Liu"
                    },
                    {
                        "name": "Ming Liang"
                    },
                    {
                        "name": "Dajun Chen"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "arxiv_affiliation": "Ant Group, Hangzhou, China",
                "author": "Yong Li",
                "arxiv_journal_ref": "ASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18551v1",
                "updated": "2025-10-21T12:00:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    0,
                    0,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:00:00Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    0,
                    0,
                    1,
                    294,
                    0
                ],
                "title": "SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for\n  Automated Simulator Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for\n  Automated Simulator Generation"
                },
                "summary": "In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that\ntreats simulator construction asinstance optimization over code within a\ntextual computation graph. Specialized LLM-driven agents are embedded as graph\nnodes, and a workflow manager executes a loss-driven loop: code synthesis ->\nexecution -> evaluation -> code repair. The optimizer performs Textual-Gradient\nDescent (TGD), while human-in-the-loop interaction is reserved for task-spec\nconfirmation, minimizing expert effort and keeping the code itself as the\ntrainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption,\nand Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy.\nBy unifying multi-agent orchestration with a loss-aligned optimization view,\nSOCIA-Nabla converts brittle prompt pipelines into reproducible,\nconstraint-aware simulator code generation that scales across domains and\nsimulation granularities. This work is under review, and we will release the\ncode soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that\ntreats simulator construction asinstance optimization over code within a\ntextual computation graph. Specialized LLM-driven agents are embedded as graph\nnodes, and a workflow manager executes a loss-driven loop: code synthesis ->\nexecution -> evaluation -> code repair. The optimizer performs Textual-Gradient\nDescent (TGD), while human-in-the-loop interaction is reserved for task-spec\nconfirmation, minimizing expert effort and keeping the code itself as the\ntrainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption,\nand Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy.\nBy unifying multi-agent orchestration with a loss-aligned optimization view,\nSOCIA-Nabla converts brittle prompt pipelines into reproducible,\nconstraint-aware simulator code generation that scales across domains and\nsimulation granularities. This work is under review, and we will release the\ncode soon."
                },
                "authors": [
                    {
                        "name": "Yuncheng Hua"
                    },
                    {
                        "name": "Sion Weatherhead"
                    },
                    {
                        "name": "Mehdi Jafari"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "arxiv_comment": "11 pages, 1 figure, 2 tables. The paper is under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18550v1",
                "updated": "2025-10-21T11:58:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    58,
                    36,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T11:58:36Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    58,
                    36,
                    1,
                    294,
                    0
                ],
                "title": "JAUNT: Joint Alignment of User Intent and Network State for QoE-centric\n  LLM Tool Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JAUNT: Joint Alignment of User Intent and Network State for QoE-centric\n  LLM Tool Routing"
                },
                "summary": "Large Language Models (LLMs) increasingly rely on emerging protocols such as\nthe Model Context Protocol (MCP) to invoke external tools and services.\nHowever, current tool routing mechanisms remain fragile because they only\nconsider functional matching between users' queries and tools. In practice,\nuser intent expressed through queries can be vague or underspecified, and the\nactual Quality of Experience (QoE) also depends on external factors such as\nlink latency and server availability that are not captured by semantics alone.\nTo address this challenge, we propose JAUNT, a framework for Joint Alignment of\nUser intent and Network state in QoE-centric Tool routing. JAUNT introduces a\ndual-view alignment strategy that interprets user intent while employing LLM\nagents to construct network profiles, mapping numerical performance indicators\ninto the semantic space to guide routing. We further design a benchmark that\nintegrates diverse user request patterns with heterogeneous network states,\nenabling systematic evaluation of QoE outcomes. Experimental results show that\nJAUNT significantly improves QoE compared with several baselines, demonstrating\nthe importance of aligning both intent and network state for scalable LLM\nservice orchestration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly rely on emerging protocols such as\nthe Model Context Protocol (MCP) to invoke external tools and services.\nHowever, current tool routing mechanisms remain fragile because they only\nconsider functional matching between users' queries and tools. In practice,\nuser intent expressed through queries can be vague or underspecified, and the\nactual Quality of Experience (QoE) also depends on external factors such as\nlink latency and server availability that are not captured by semantics alone.\nTo address this challenge, we propose JAUNT, a framework for Joint Alignment of\nUser intent and Network state in QoE-centric Tool routing. JAUNT introduces a\ndual-view alignment strategy that interprets user intent while employing LLM\nagents to construct network profiles, mapping numerical performance indicators\ninto the semantic space to guide routing. We further design a benchmark that\nintegrates diverse user request patterns with heterogeneous network states,\nenabling systematic evaluation of QoE outcomes. Experimental results show that\nJAUNT significantly improves QoE compared with several baselines, demonstrating\nthe importance of aligning both intent and network state for scalable LLM\nservice orchestration."
                },
                "authors": [
                    {
                        "name": "Enhan Li"
                    },
                    {
                        "name": "Hongyang Du"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Du"
                },
                "author": "Hongyang Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18546v1",
                "updated": "2025-10-21T11:52:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T11:52:44Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval"
                },
                "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon."
                },
                "authors": [
                    {
                        "name": "Zebin Yang"
                    },
                    {
                        "name": "Sunjian Zheng"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Bo Yu"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Shaoshan Liu"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18544v1",
                "updated": "2025-10-21T11:47:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    47,
                    42,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T11:47:42Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    47,
                    42,
                    1,
                    294,
                    0
                ],
                "title": "SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices"
                },
                "summary": "Large Language Models (LLMs), as the foundational architecture for\nnext-generation interactive AI applications, not only power intelligent\ndialogue systems but also drive the evolution of embodied intelligence on edge\ndevices, including humanoid robots, smart vehicles, and other scenarios. The\napplications running on these edge devices impose differentiated Service Level\nObjectives (SLO) requirements on LLM services, specifically manifested as\ndistinct constraints on Time to First Token (TTFT) and Time Per Output Token\n(TPOT) as well as end-to-end latency. Notably, edge devices typically handle\nreal-time tasks that are extremely sensitive to latency, such as machine\ncontrol and navigation planning. However, existing scheduling service systems\nstill prioritize maximizing output token throughput as the sole optimization\nobjective, failing to adequately address the diversity of SLO requirements.\nThis ultimately results in persistently high violation rates for end-to-end\nlatency or TPOT related SLOs.\n  This paper proposes SLICE, an innovative scheduling solution designed for\nedge computing scenarios with differentiated SLO requirements. By combining a\nutility-maximizing request scheduling algorithm with a dynamic iterative\ncontrol mechanism for generation rates, SLICE significantly improves LLM\ninference service SLO attainment. Experimental results demonstrate that\ncompared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to\n35x higher SLO attainment and 3.4x advantage in task completion time than the\nother two solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), as the foundational architecture for\nnext-generation interactive AI applications, not only power intelligent\ndialogue systems but also drive the evolution of embodied intelligence on edge\ndevices, including humanoid robots, smart vehicles, and other scenarios. The\napplications running on these edge devices impose differentiated Service Level\nObjectives (SLO) requirements on LLM services, specifically manifested as\ndistinct constraints on Time to First Token (TTFT) and Time Per Output Token\n(TPOT) as well as end-to-end latency. Notably, edge devices typically handle\nreal-time tasks that are extremely sensitive to latency, such as machine\ncontrol and navigation planning. However, existing scheduling service systems\nstill prioritize maximizing output token throughput as the sole optimization\nobjective, failing to adequately address the diversity of SLO requirements.\nThis ultimately results in persistently high violation rates for end-to-end\nlatency or TPOT related SLOs.\n  This paper proposes SLICE, an innovative scheduling solution designed for\nedge computing scenarios with differentiated SLO requirements. By combining a\nutility-maximizing request scheduling algorithm with a dynamic iterative\ncontrol mechanism for generation rates, SLICE significantly improves LLM\ninference service SLO attainment. Experimental results demonstrate that\ncompared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to\n35x higher SLO attainment and 3.4x advantage in task completion time than the\nother two solutions."
                },
                "authors": [
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Yiming Lei"
                    },
                    {
                        "name": "Ling Liu"
                    },
                    {
                        "name": "Xiaoqiong Xu"
                    },
                    {
                        "name": "Ying Cai"
                    },
                    {
                        "name": "Daji Ergu"
                    },
                    {
                        "name": "Hongfang Yu"
                    },
                    {
                        "name": "Yueyue Dai"
                    }
                ],
                "author_detail": {
                    "name": "Yueyue Dai"
                },
                "author": "Yueyue Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09165v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09165v4",
                "updated": "2025-10-21T11:42:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    42,
                    20,
                    1,
                    294,
                    0
                ],
                "published": "2024-12-12T10:50:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    50,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "When Text Embedding Meets Large Language Model: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Text Embedding Meets Large Language Model: A Comprehensive Survey"
                },
                "summary": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications - such as semantic matching,\nclustering, and information retrieval - continue to rely on text embeddings for\ntheir efficiency and effectiveness. Therefore, integrating LLMs with text\nembeddings has become a major research focus in recent years. In this survey,\nwe categorize the interplay between LLMs and text embeddings into three\noverarching themes: (1) LLM-augmented text embedding, enhancing traditional\nembedding methods with LLMs; (2) LLMs as text embedders, adapting their innate\ncapabilities for high-quality embedding; and (3) Text embedding understanding\nwith LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing\nrecent works based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications - such as semantic matching,\nclustering, and information retrieval - continue to rely on text embeddings for\ntheir efficiency and effectiveness. Therefore, integrating LLMs with text\nembeddings has become a major research focus in recent years. In this survey,\nwe categorize the interplay between LLMs and text embeddings into three\noverarching themes: (1) LLM-augmented text embedding, enhancing traditional\nembedding methods with LLMs; (2) LLMs as text embedders, adapting their innate\ncapabilities for high-quality embedding; and (3) Text embedding understanding\nwith LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing\nrecent works based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP."
                },
                "authors": [
                    {
                        "name": "Zhijie Nie"
                    },
                    {
                        "name": "Zhangchi Feng"
                    },
                    {
                        "name": "Mingxin Li"
                    },
                    {
                        "name": "Cunwang Zhang"
                    },
                    {
                        "name": "Yanzhao Zhang"
                    },
                    {
                        "name": "Dingkun Long"
                    },
                    {
                        "name": "Richong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Richong Zhang"
                },
                "author": "Richong Zhang",
                "arxiv_comment": "Version 4: We added the latest works of LLM-based Embedders",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09165v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09165v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18541v1",
                "updated": "2025-10-21T11:39:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    39,
                    45,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T11:39:45Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    39,
                    45,
                    1,
                    294,
                    0
                ],
                "title": "Pay Attention to the Triggers: Constructing Backdoors That Survive\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pay Attention to the Triggers: Constructing Backdoors That Survive\n  Distillation"
                },
                "summary": "LLMs are often used by downstream users as teacher models for knowledge\ndistillation, compressing their capabilities into memory-efficient models.\nHowever, as these teacher models may stem from untrusted parties, distillation\ncan raise unexpected security risks. In this paper, we investigate the security\nimplications of knowledge distillation from backdoored teacher models. First,\nwe show that prior backdoors mostly do not transfer onto student models. Our\nkey insight is that this is because existing LLM backdooring methods choose\ntrigger tokens that rarely occur in usual contexts. We argue that this\nunderestimates the security risks of knowledge distillation and introduce a new\nbackdooring technique, T-MTB, that enables the construction and study of\ntransferable backdoors. T-MTB carefully constructs a composite backdoor\ntrigger, made up of several specific tokens that often occur individually in\nanticipated distillation datasets. As such, the poisoned teacher remains\nstealthy, while during distillation the individual presence of these tokens\nprovides enough signal for the backdoor to transfer onto the student. Using\nT-MTB, we demonstrate and extensively study the security risks of transferable\nbackdoors across two attack scenarios, jailbreaking and content modulation, and\nacross four model families of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are often used by downstream users as teacher models for knowledge\ndistillation, compressing their capabilities into memory-efficient models.\nHowever, as these teacher models may stem from untrusted parties, distillation\ncan raise unexpected security risks. In this paper, we investigate the security\nimplications of knowledge distillation from backdoored teacher models. First,\nwe show that prior backdoors mostly do not transfer onto student models. Our\nkey insight is that this is because existing LLM backdooring methods choose\ntrigger tokens that rarely occur in usual contexts. We argue that this\nunderestimates the security risks of knowledge distillation and introduce a new\nbackdooring technique, T-MTB, that enables the construction and study of\ntransferable backdoors. T-MTB carefully constructs a composite backdoor\ntrigger, made up of several specific tokens that often occur individually in\nanticipated distillation datasets. As such, the poisoned teacher remains\nstealthy, while during distillation the individual presence of these tokens\nprovides enough signal for the backdoor to transfer onto the student. Using\nT-MTB, we demonstrate and extensively study the security risks of transferable\nbackdoors across two attack scenarios, jailbreaking and content modulation, and\nacross four model families of LLMs."
                },
                "authors": [
                    {
                        "name": "Giovanni De Muri"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13473v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13473v2",
                "updated": "2025-10-21T11:39:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    39,
                    28,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-15T12:17:23Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    17,
                    23,
                    2,
                    288,
                    0
                ],
                "title": "Towards Quantum Enhanced Adversarial Robustness with Rydberg Reservoir\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Quantum Enhanced Adversarial Robustness with Rydberg Reservoir\n  Learning"
                },
                "summary": "Quantum reservoir computing (QRC) leverages the high-dimensional, nonlinear\ndynamics inherent in quantum many-body systems for extracting spatiotemporal\npatterns in sequential and time-series data with minimal training overhead.\nAlthough QRC inherits the expressive capabilities associated with quantum\nencodings, recent studies indicate that quantum classifiers based on\nvariational circuits remain susceptible to adversarial perturbations. In this\nperspective, we investigate the first systematic evaluation of adversarial\nrobustness in a QRC based learning model. Our reservoir comprises an array of\nstrongly interacting Rydberg atoms governed by a fixed Hamiltonian, which\nnaturally evolves under complex quantum dynamics, producing high-dimensional\nembeddings. A lightweight multilayer perceptron serves as the trainable readout\nlayer. We utilize the balanced datasets, namely MNIST, Fashion-MNIST, and\nKuzushiji-MNIST, as a benchmark for rigorously evaluating the impact of\naugmenting the quantum reservoir with a Multilayer perceptron (MLP) in\nwhite-box adversarial attacks to assess its robustness. We demonstrate that\nthis approach yields significantly higher accuracy than purely classical models\nacross all perturbation strengths tested. This hybrid approach reveals a new\nsource of quantum advantage and provides practical guidance for the secure\ndeployment of machine learning models on quantum-centric supercomputing with\nnear-term hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum reservoir computing (QRC) leverages the high-dimensional, nonlinear\ndynamics inherent in quantum many-body systems for extracting spatiotemporal\npatterns in sequential and time-series data with minimal training overhead.\nAlthough QRC inherits the expressive capabilities associated with quantum\nencodings, recent studies indicate that quantum classifiers based on\nvariational circuits remain susceptible to adversarial perturbations. In this\nperspective, we investigate the first systematic evaluation of adversarial\nrobustness in a QRC based learning model. Our reservoir comprises an array of\nstrongly interacting Rydberg atoms governed by a fixed Hamiltonian, which\nnaturally evolves under complex quantum dynamics, producing high-dimensional\nembeddings. A lightweight multilayer perceptron serves as the trainable readout\nlayer. We utilize the balanced datasets, namely MNIST, Fashion-MNIST, and\nKuzushiji-MNIST, as a benchmark for rigorously evaluating the impact of\naugmenting the quantum reservoir with a Multilayer perceptron (MLP) in\nwhite-box adversarial attacks to assess its robustness. We demonstrate that\nthis approach yields significantly higher accuracy than purely classical models\nacross all perturbation strengths tested. This hybrid approach reveals a new\nsource of quantum advantage and provides practical guidance for the secure\ndeployment of machine learning models on quantum-centric supercomputing with\nnear-term hardware."
                },
                "authors": [
                    {
                        "name": "Shehbaz Tariq"
                    },
                    {
                        "name": "Muhammad Talha"
                    },
                    {
                        "name": "Symeon Chatzinotas"
                    },
                    {
                        "name": "Hyundong Shin"
                    }
                ],
                "author_detail": {
                    "name": "Hyundong Shin"
                },
                "author": "Hyundong Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13473v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13473v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11758v2",
                "updated": "2025-10-21T11:29:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    29,
                    14,
                    1,
                    294,
                    0
                ],
                "published": "2025-08-15T18:07:47Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    18,
                    7,
                    47,
                    4,
                    227,
                    0
                ],
                "title": "Can we Evaluate RAGs with Synthetic Data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can we Evaluate RAGs with Synthetic Data?"
                },
                "summary": "We investigate whether synthetic question-answer (QA) data generated by large\nlanguage models (LLMs) can serve as an effective proxy for human-labeled\nbenchmarks when the latter is unavailable. We assess the reliability of\nsynthetic benchmarks across two experiments: one varying retriever parameters\nwhile keeping the generator fixed, and another varying the generator with fixed\nretriever parameters. Across four datasets, of which two open-domain and two\nproprietary, we find that synthetic benchmarks reliably rank the RAGs varying\nin terms of retriever configuration, aligning well with human-labeled benchmark\nbaselines. However, they do not consistently produce reliable RAG rankings when\ncomparing generator architectures. The breakdown possibly arises from a\ncombination of task mismatch between the synthetic and human benchmarks, and\nstylistic bias favoring certain generators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether synthetic question-answer (QA) data generated by large\nlanguage models (LLMs) can serve as an effective proxy for human-labeled\nbenchmarks when the latter is unavailable. We assess the reliability of\nsynthetic benchmarks across two experiments: one varying retriever parameters\nwhile keeping the generator fixed, and another varying the generator with fixed\nretriever parameters. Across four datasets, of which two open-domain and two\nproprietary, we find that synthetic benchmarks reliably rank the RAGs varying\nin terms of retriever configuration, aligning well with human-labeled benchmark\nbaselines. However, they do not consistently produce reliable RAG rankings when\ncomparing generator architectures. The breakdown possibly arises from a\ncombination of task mismatch between the synthetic and human benchmarks, and\nstylistic bias favoring certain generators."
                },
                "authors": [
                    {
                        "name": "Jonas van Elburg"
                    },
                    {
                        "name": "Peter van der Putten"
                    },
                    {
                        "name": "Maarten Marx"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Marx"
                },
                "author": "Maarten Marx",
                "arxiv_comment": "Accepted for the SynDAiTE workshop at the European Conference on\n  Machine Learning and Principles and Practice of Knowledge Discovery in\n  Databases (ECML-PKDD 2025), September 15, 2025 - Porto, Portugal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18527v2",
                "updated": "2025-10-22T01:48:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    1,
                    48,
                    27,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-21T11:13:21Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    13,
                    21,
                    1,
                    294,
                    0
                ],
                "title": "LLMs as Sparse Retrievers:A Framework for First-Stage Product Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Sparse Retrievers:A Framework for First-Stage Product Search"
                },
                "summary": "Product search is a crucial component of modern e-commerce platforms, with\nbillions of user queries every day. In product search systems, first-stage\nretrieval should achieve high recall while ensuring efficient online\ndeployment. Sparse retrieval is particularly attractive in this context due to\nits interpretability and storage efficiency. However, sparse retrieval methods\nsuffer from severe vocabulary mismatch issues, leading to suboptimal\nperformance in product search scenarios. With their potential for semantic\nanalysis, large language models (LLMs) offer a promising avenue for mitigating\nvocabulary mismatch issues and thereby improving retrieval quality. Directly\napplying LLMs to sparse retrieval in product search exposes two key\nchallenges:(1)Queries and product titles are typically short and highly\nsusceptible to LLM-induced hallucinations, such as generating irrelevant\nexpansion terms or underweighting critical literal terms like brand names and\nmodel numbers;(2)The large vocabulary space of LLMs leads to difficulty in\ninitializing training effectively, making it challenging to learn meaningful\nsparse representations in such ultra-high-dimensional spaces.To address these\nchallenges, we propose PROSPER, a framework for PROduct search leveraging LLMs\nas SParsE Retrievers. PROSPER incorporates: (1)A literal residual network that\nalleviates hallucination in lexical expansion by reinforcing underweighted\nliteral terms through a residual compensation mechanism; and (2)A lexical\nfocusing window that facilitates effective training initialization via a\ncoarse-to-fine sparsification strategy.Extensive offline and online experiments\nshow that PROSPER significantly outperforms sparse baselines and achieves\nrecall performance comparable to advanced dense retrievers, while also\nachieving revenue increments online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Product search is a crucial component of modern e-commerce platforms, with\nbillions of user queries every day. In product search systems, first-stage\nretrieval should achieve high recall while ensuring efficient online\ndeployment. Sparse retrieval is particularly attractive in this context due to\nits interpretability and storage efficiency. However, sparse retrieval methods\nsuffer from severe vocabulary mismatch issues, leading to suboptimal\nperformance in product search scenarios. With their potential for semantic\nanalysis, large language models (LLMs) offer a promising avenue for mitigating\nvocabulary mismatch issues and thereby improving retrieval quality. Directly\napplying LLMs to sparse retrieval in product search exposes two key\nchallenges:(1)Queries and product titles are typically short and highly\nsusceptible to LLM-induced hallucinations, such as generating irrelevant\nexpansion terms or underweighting critical literal terms like brand names and\nmodel numbers;(2)The large vocabulary space of LLMs leads to difficulty in\ninitializing training effectively, making it challenging to learn meaningful\nsparse representations in such ultra-high-dimensional spaces.To address these\nchallenges, we propose PROSPER, a framework for PROduct search leveraging LLMs\nas SParsE Retrievers. PROSPER incorporates: (1)A literal residual network that\nalleviates hallucination in lexical expansion by reinforcing underweighted\nliteral terms through a residual compensation mechanism; and (2)A lexical\nfocusing window that facilitates effective training initialization via a\ncoarse-to-fine sparsification strategy.Extensive offline and online experiments\nshow that PROSPER significantly outperforms sparse baselines and achieves\nrecall performance comparable to advanced dense retrievers, while also\nachieving revenue increments online."
                },
                "authors": [
                    {
                        "name": "Hongru Song"
                    },
                    {
                        "name": "Yu-an Liu"
                    },
                    {
                        "name": "Ruqing Zhang"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Sen Li"
                    },
                    {
                        "name": "Wenjun Peng"
                    },
                    {
                        "name": "Fuyu Lv"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18526v1",
                "updated": "2025-10-21T11:12:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    12,
                    45,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T11:12:45Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    12,
                    45,
                    1,
                    294,
                    0
                ],
                "title": "Counterfactual Reasoning for Steerable Pluralistic Value Alignment of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Reasoning for Steerable Pluralistic Value Alignment of\n  Large Language Models"
                },
                "summary": "As large language models (LLMs) become increasingly integrated into\napplications serving users across diverse cultures, communities and\ndemographics, it is critical to align LLMs with pluralistic human values beyond\naverage principles (e.g., HHH). In psychological and social value theories such\nas Schwartz's Value Theory, pluralistic values are represented by multiple\nvalue dimensions paired with various priorities. However, existing methods\nencounter two challenges when aligning with such fine-grained value objectives:\n1) they often treat multiple values as independent and equally important,\nignoring their interdependence and relative priorities (value complexity); 2)\nthey struggle to precisely control nuanced value priorities, especially those\nunderrepresented ones (value steerability). To handle these challenges, we\npropose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE\nalignment. It introduces a structural causal model (SCM) to feature complex\ninterdependency and prioritization among features, as well as the causal\nrelationship between high-level value dimensions and behaviors. Moreover, it\napplies counterfactual reasoning to generate outputs aligned with any desired\nvalue objectives. Benefitting from explicit causal modeling, COUPLE also\nprovides better interpretability. We evaluate COUPLE on two datasets with\ndifferent value systems and demonstrate that COUPLE advances other baselines\nacross diverse types of value objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly integrated into\napplications serving users across diverse cultures, communities and\ndemographics, it is critical to align LLMs with pluralistic human values beyond\naverage principles (e.g., HHH). In psychological and social value theories such\nas Schwartz's Value Theory, pluralistic values are represented by multiple\nvalue dimensions paired with various priorities. However, existing methods\nencounter two challenges when aligning with such fine-grained value objectives:\n1) they often treat multiple values as independent and equally important,\nignoring their interdependence and relative priorities (value complexity); 2)\nthey struggle to precisely control nuanced value priorities, especially those\nunderrepresented ones (value steerability). To handle these challenges, we\npropose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE\nalignment. It introduces a structural causal model (SCM) to feature complex\ninterdependency and prioritization among features, as well as the causal\nrelationship between high-level value dimensions and behaviors. Moreover, it\napplies counterfactual reasoning to generate outputs aligned with any desired\nvalue objectives. Benefitting from explicit causal modeling, COUPLE also\nprovides better interpretability. We evaluate COUPLE on two datasets with\ndifferent value systems and demonstrate that COUPLE advances other baselines\nacross diverse types of value objectives."
                },
                "authors": [
                    {
                        "name": "Hanze Guo"
                    },
                    {
                        "name": "Jing Yao"
                    },
                    {
                        "name": "Xiao Zhou"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie",
                "arxiv_comment": "41 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18525v1",
                "updated": "2025-10-21T11:07:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    7,
                    5,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T11:07:05Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    7,
                    5,
                    1,
                    294,
                    0
                ],
                "title": "From Quarter to All: Accelerating Speculative LLM Decoding via\n  Floating-Point Exponent Remapping and Parameter Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Quarter to All: Accelerating Speculative LLM Decoding via\n  Floating-Point Exponent Remapping and Parameter Sharing"
                },
                "summary": "Large language models achieve impressive performance across diverse tasks but\nexhibit high inference latency due to their large parameter sizes. While\nquantization reduces model size, it often leads to performance degradation\ncompared to the full model. Speculative decoding remains lossless but typically\nincurs extra overheads. We propose SPEQ, an algorithm-hardware co-designed\nspeculative decoding method that uses part of the full-model weight bits to\nform a quantized draft model, thereby eliminating additional training or\nstorage overhead. A reconfigurable processing element array enables efficient\nexecution of both the draft and verification passes. Experimental results\nacross 15 LLMs and tasks demonstrate that SPEQ achieves speedups of 2.07x,\n1.53x, and 1.45x compared over FP16, Olive, and Tender, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models achieve impressive performance across diverse tasks but\nexhibit high inference latency due to their large parameter sizes. While\nquantization reduces model size, it often leads to performance degradation\ncompared to the full model. Speculative decoding remains lossless but typically\nincurs extra overheads. We propose SPEQ, an algorithm-hardware co-designed\nspeculative decoding method that uses part of the full-model weight bits to\nform a quantized draft model, thereby eliminating additional training or\nstorage overhead. A reconfigurable processing element array enables efficient\nexecution of both the draft and verification passes. Experimental results\nacross 15 LLMs and tasks demonstrate that SPEQ achieves speedups of 2.07x,\n1.53x, and 1.45x compared over FP16, Olive, and Tender, respectively."
                },
                "authors": [
                    {
                        "name": "Yushu Zhao"
                    },
                    {
                        "name": "Yubin Qin"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiaolong Yang"
                    },
                    {
                        "name": "Huiming Han"
                    },
                    {
                        "name": "Shaojun Wei"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shouyi Yin"
                },
                "author": "Shouyi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14045v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14045v4",
                "updated": "2025-10-21T11:04:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    4,
                    5,
                    1,
                    294,
                    0
                ],
                "published": "2025-05-20T07:43:45Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    43,
                    45,
                    1,
                    140,
                    0
                ],
                "title": "From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way\n  Parallel Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way\n  Parallel Corpora"
                },
                "summary": "Continued pretraining and instruction tuning on large-scale multilingual data\nhave proven to be effective in scaling large language models (LLMs) to\nlow-resource languages. However, the unaligned nature of such data limits its\nability to effectively capture cross-lingual semantics. In contrast, multi-way\nparallel data, where identical content is aligned across multiple languages,\nprovides stronger cross-lingual consistency and offers greater potential for\nimproving multilingual performance. In this paper, we introduce a large-scale,\nhigh-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus\nspans 113 languages, with up to 50 languages aligned in parallel, ensuring\nextensive multilingual coverage. Using this dataset, we investigate best\npractices for leveraging multi-way parallel data to enhance LLMs, including\nstrategies for continued pretraining, instruction tuning, and the analysis of\nkey influencing factors. Experiments on six multilingual benchmarks show that\nmodels trained on multiway parallel data consistently outperform those trained\non unaligned multilingual data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continued pretraining and instruction tuning on large-scale multilingual data\nhave proven to be effective in scaling large language models (LLMs) to\nlow-resource languages. However, the unaligned nature of such data limits its\nability to effectively capture cross-lingual semantics. In contrast, multi-way\nparallel data, where identical content is aligned across multiple languages,\nprovides stronger cross-lingual consistency and offers greater potential for\nimproving multilingual performance. In this paper, we introduce a large-scale,\nhigh-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus\nspans 113 languages, with up to 50 languages aligned in parallel, ensuring\nextensive multilingual coverage. Using this dataset, we investigate best\npractices for leveraging multi-way parallel data to enhance LLMs, including\nstrategies for continued pretraining, instruction tuning, and the analysis of\nkey influencing factors. Experiments on six multilingual benchmarks show that\nmodels trained on multiway parallel data consistently outperform those trained\non unaligned multilingual data."
                },
                "authors": [
                    {
                        "name": "Yingli Shen"
                    },
                    {
                        "name": "Wen Lai"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Ge Gao"
                    },
                    {
                        "name": "Kangyang Luo"
                    },
                    {
                        "name": "Alexander Fraser"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "EMNLP 2025 Main Conference (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14045v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14045v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18520v1",
                "updated": "2025-10-21T11:00:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    0,
                    2,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T11:00:02Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    0,
                    2,
                    1,
                    294,
                    0
                ],
                "title": "Partial VOROS: A Cost-aware Performance Metric for Binary Classifiers\n  with Precision and Capacity Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partial VOROS: A Cost-aware Performance Metric for Binary Classifiers\n  with Precision and Capacity Constraints"
                },
                "summary": "The ROC curve is widely used to assess binary classification performance. Yet\nfor some applications such as alert systems for hospitalized patient\nmonitoring, conventional ROC analysis cannot capture crucial factors that\nimpact deployment, such as enforcing a minimum precision constraint to avoid\nfalse alarm fatigue or imposing an upper bound on the number of predicted\npositives to represent the capacity of hospital staff. The usual area under the\ncurve metric also does not reflect asymmetric costs for false positives and\nfalse negatives. In this paper we address all three of these issues. First, we\nshow how the subset of classifiers that meet given precision and capacity\nconstraints can be represented as a feasible region in ROC space. We establish\nthe geometry of this feasible region. We then define the partial area of lesser\nclassifiers, a performance metric that is monotonic with cost and only accounts\nfor the feasible portion of ROC space. Averaging this area over a desired range\nof cost parameters results in the partial volume over the ROC surface, or\npartial VOROS. In experiments predicting mortality risk using vital sign\nhistory on the MIMIC-IV dataset, we show this cost-aware metric is better than\nalternatives for ranking classifiers in hospital alert applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ROC curve is widely used to assess binary classification performance. Yet\nfor some applications such as alert systems for hospitalized patient\nmonitoring, conventional ROC analysis cannot capture crucial factors that\nimpact deployment, such as enforcing a minimum precision constraint to avoid\nfalse alarm fatigue or imposing an upper bound on the number of predicted\npositives to represent the capacity of hospital staff. The usual area under the\ncurve metric also does not reflect asymmetric costs for false positives and\nfalse negatives. In this paper we address all three of these issues. First, we\nshow how the subset of classifiers that meet given precision and capacity\nconstraints can be represented as a feasible region in ROC space. We establish\nthe geometry of this feasible region. We then define the partial area of lesser\nclassifiers, a performance metric that is monotonic with cost and only accounts\nfor the feasible portion of ROC space. Averaging this area over a desired range\nof cost parameters results in the partial volume over the ROC surface, or\npartial VOROS. In experiments predicting mortality risk using vital sign\nhistory on the MIMIC-IV dataset, we show this cost-aware metric is better than\nalternatives for ranking classifiers in hospital alert applications."
                },
                "authors": [
                    {
                        "name": "Christopher Ratigan"
                    },
                    {
                        "name": "Kyle Heuton"
                    },
                    {
                        "name": "Carissa Wang"
                    },
                    {
                        "name": "Lenore Cowen"
                    },
                    {
                        "name": "Michael C. Hughes"
                    }
                ],
                "author_detail": {
                    "name": "Michael C. Hughes"
                },
                "author": "Michael C. Hughes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11546v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11546v4",
                "updated": "2025-10-21T10:54:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    54,
                    56,
                    1,
                    294,
                    0
                ],
                "published": "2025-02-17T08:28:29Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    28,
                    29,
                    0,
                    48,
                    0
                ],
                "title": "DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data\n  Cleaning as Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data\n  Cleaning as Anomaly Detection"
                },
                "summary": "The rapid development of multilingual large language models (LLMs) highlights\nthe need for high-quality, diverse, and well-curated multilingual datasets. In\nthis paper, we introduce DCAD-2000 (Data Cleaning as Anomaly Detection), a\nlarge-scale multilingual corpus constructed from newly extracted Common Crawl\ndata and existing multilingual sources. DCAD-2000 covers 2,282 languages,\n46.72TB of text, and 8.63 billion documents, spanning 155 high- and\nmedium-resource languages and 159 writing scripts. To overcome the limitations\nof existing data cleaning approaches, which rely on manually designed heuristic\nthresholds, we reframe data cleaning as an anomaly detection problem. This\ndynamic filtering paradigm substantially improves data quality by automatically\nidentifying and removing noisy or anomalous content. By fine-tuning LLMs on\nDCAD-2000, we demonstrate notable improvements in data quality, robustness of\nthe cleaning pipeline, and downstream performance, particularly for\nlow-resource languages across multiple multilingual benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of multilingual large language models (LLMs) highlights\nthe need for high-quality, diverse, and well-curated multilingual datasets. In\nthis paper, we introduce DCAD-2000 (Data Cleaning as Anomaly Detection), a\nlarge-scale multilingual corpus constructed from newly extracted Common Crawl\ndata and existing multilingual sources. DCAD-2000 covers 2,282 languages,\n46.72TB of text, and 8.63 billion documents, spanning 155 high- and\nmedium-resource languages and 159 writing scripts. To overcome the limitations\nof existing data cleaning approaches, which rely on manually designed heuristic\nthresholds, we reframe data cleaning as an anomaly detection problem. This\ndynamic filtering paradigm substantially improves data quality by automatically\nidentifying and removing noisy or anomalous content. By fine-tuning LLMs on\nDCAD-2000, we demonstrate notable improvements in data quality, robustness of\nthe cleaning pipeline, and downstream performance, particularly for\nlow-resource languages across multiple multilingual benchmarks."
                },
                "authors": [
                    {
                        "name": "Yingli Shen"
                    },
                    {
                        "name": "Wen Lai"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Xueren Zhang"
                    },
                    {
                        "name": "Kangyang Luo"
                    },
                    {
                        "name": "Alexander Fraser"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "NeurIPS 2025 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11546v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11546v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18509v1",
                "updated": "2025-10-21T10:50:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    50,
                    33,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T10:50:33Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    50,
                    33,
                    1,
                    294,
                    0
                ],
                "title": "VAPU: System for Autonomous Legacy Code Modernization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VAPU: System for Autonomous Legacy Code Modernization"
                },
                "summary": "In this study, we present a solution for the modernization of legacy\napplications, an area of code generation where LLM-based multi-agent systems\nare proving essential for complex multi-phased tasks. Legacy applications often\ncontain deprecated components that create compatibility, security, and\nreliability risks, but high resource costs make companies hesitate to update.\nWe take a step forward to integrate an LLM-based multi-agent system as part of\na legacy web application update to provide a cost-effective solution to update\nlegacy applications autonomously. We propose a multi-agent system named a\nVerifying Agent Pipeline Updater (VAPU), which is designed to update code files\nin phases while simulating different roles in a software development team. In\nour previous study, we evaluated the system for legacy version updates by using\nsix legacy web application view files by resulting errors and accomplished\nrequirements. This study extends the previous evaluation of a multi-agent\npipeline system by extending the evaluation of VAPU from a single LLM to five\nLLMs and using the temperature parameter in both 0 to 1 settings. Additionally,\nwe tested the system with 20 open-source Python GitHub projects. The results of\nthe evaluation were compared to Zero-Shot Learning (ZSL) and One-Shot Learning\n(OSL) prompts. The extended evaluation of VAPU showed that particularly in a\nlow-temperature VAPU can get similar level of error count compared to the\nZSL/OSL prompts but with a higher level of fulfilled requirements, depending on\nthe LLM. VAPU showed up to 22.5% increase in the succeeding Python file update\nrequirements compared to ZSL/OSL prompts. The study indicates that an LLM-based\nmulti-agent system is a capable solution to update components of a legacy\napplication autonomously.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we present a solution for the modernization of legacy\napplications, an area of code generation where LLM-based multi-agent systems\nare proving essential for complex multi-phased tasks. Legacy applications often\ncontain deprecated components that create compatibility, security, and\nreliability risks, but high resource costs make companies hesitate to update.\nWe take a step forward to integrate an LLM-based multi-agent system as part of\na legacy web application update to provide a cost-effective solution to update\nlegacy applications autonomously. We propose a multi-agent system named a\nVerifying Agent Pipeline Updater (VAPU), which is designed to update code files\nin phases while simulating different roles in a software development team. In\nour previous study, we evaluated the system for legacy version updates by using\nsix legacy web application view files by resulting errors and accomplished\nrequirements. This study extends the previous evaluation of a multi-agent\npipeline system by extending the evaluation of VAPU from a single LLM to five\nLLMs and using the temperature parameter in both 0 to 1 settings. Additionally,\nwe tested the system with 20 open-source Python GitHub projects. The results of\nthe evaluation were compared to Zero-Shot Learning (ZSL) and One-Shot Learning\n(OSL) prompts. The extended evaluation of VAPU showed that particularly in a\nlow-temperature VAPU can get similar level of error count compared to the\nZSL/OSL prompts but with a higher level of fulfilled requirements, depending on\nthe LLM. VAPU showed up to 22.5% increase in the succeeding Python file update\nrequirements compared to ZSL/OSL prompts. The study indicates that an LLM-based\nmulti-agent system is a capable solution to update components of a legacy\napplication autonomously."
                },
                "authors": [
                    {
                        "name": "Valtteri Ala-Salmi"
                    },
                    {
                        "name": "Zeeshan Rasheed"
                    },
                    {
                        "name": "Abdul Malik Sami"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Jussi Rasku"
                    },
                    {
                        "name": "Mika Saari"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_comment": "Table 13, figure 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18508v1",
                "updated": "2025-10-21T10:48:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    48,
                    14,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T10:48:14Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    48,
                    14,
                    1,
                    294,
                    0
                ],
                "title": "Prompting the Priorities: A First Look at Evaluating LLMs for\n  Vulnerability Triage and Prioritization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting the Priorities: A First Look at Evaluating LLMs for\n  Vulnerability Triage and Prioritization"
                },
                "summary": "Security analysts face increasing pressure to triage large and complex\nvulnerability backlogs. Large Language Models (LLMs) offer a potential aid by\nautomating parts of the interpretation process. We evaluate four models\n(ChatGPT, Claude, Gemini, and DeepSeek) across twelve prompting techniques to\ninterpret semi-structured and unstructured vulnerability information. As a\nconcrete use case, we test each model's ability to predict decision points in\nthe Stakeholder-Specific Vulnerability Categorization (SSVC) framework:\nExploitation, Automatable, Technical Impact, and Mission and Wellbeing.\n  Using 384 real-world vulnerabilities from the VulZoo dataset, we issued more\nthan 165,000 queries to assess performance under prompting styles including\none-shot, few-shot, and chain-of-thought. We report F1 scores for each SSVC\ndecision point and Cohen's kappa (weighted and unweighted) for the final SSVC\ndecision outcomes. Gemini consistently ranked highest, leading on three of four\ndecision points and yielding the most correct recommendations. Prompting with\nexemplars generally improved accuracy, although all models struggled on some\ndecision points. Only DeepSeek achieved fair agreement under weighted metrics,\nand all models tended to over-predict risk.\n  Overall, current LLMs do not replace expert judgment. However, specific LLM\nand prompt combinations show moderate effectiveness for targeted SSVC\ndecisions. When applied with care, LLMs can support vulnerability\nprioritization workflows and help security teams respond more efficiently to\nemerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security analysts face increasing pressure to triage large and complex\nvulnerability backlogs. Large Language Models (LLMs) offer a potential aid by\nautomating parts of the interpretation process. We evaluate four models\n(ChatGPT, Claude, Gemini, and DeepSeek) across twelve prompting techniques to\ninterpret semi-structured and unstructured vulnerability information. As a\nconcrete use case, we test each model's ability to predict decision points in\nthe Stakeholder-Specific Vulnerability Categorization (SSVC) framework:\nExploitation, Automatable, Technical Impact, and Mission and Wellbeing.\n  Using 384 real-world vulnerabilities from the VulZoo dataset, we issued more\nthan 165,000 queries to assess performance under prompting styles including\none-shot, few-shot, and chain-of-thought. We report F1 scores for each SSVC\ndecision point and Cohen's kappa (weighted and unweighted) for the final SSVC\ndecision outcomes. Gemini consistently ranked highest, leading on three of four\ndecision points and yielding the most correct recommendations. Prompting with\nexemplars generally improved accuracy, although all models struggled on some\ndecision points. Only DeepSeek achieved fair agreement under weighted metrics,\nand all models tended to over-predict risk.\n  Overall, current LLMs do not replace expert judgment. However, specific LLM\nand prompt combinations show moderate effectiveness for targeted SSVC\ndecisions. When applied with care, LLMs can support vulnerability\nprioritization workflows and help security teams respond more efficiently to\nemerging threats."
                },
                "authors": [
                    {
                        "name": "Osama Al Haddad"
                    },
                    {
                        "name": "Muhammad Ikram"
                    },
                    {
                        "name": "Ejaz Ahmed"
                    },
                    {
                        "name": "Young Lee"
                    }
                ],
                "author_detail": {
                    "name": "Young Lee"
                },
                "author": "Young Lee",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17999v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17999v4",
                "updated": "2025-10-21T10:42:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    42,
                    34,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-22T16:39:22Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    39,
                    22,
                    0,
                    265,
                    0
                ],
                "title": "The Narcissus Hypothesis: Descending to the Rung of Illusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Narcissus Hypothesis: Descending to the Rung of Illusion"
                },
                "summary": "Modern foundational models increasingly reflect not just world knowledge, but\npatterns of human preference embedded in their training data. We hypothesize\nthat recursive alignment-via human feedback and model-generated corpora-induces\na social desirability bias, nudging models to favor agreeable or flattering\nresponses over objective reasoning. We refer to it as the Narcissus Hypothesis\nand test it across 31 models using standardized personality assessments and a\nnovel Social Desirability Bias score. Results reveal a significant drift toward\nsocially conforming traits, with profound implications for corpus integrity and\nthe reliability of downstream inferences. We then offer a novel epistemological\ninterpretation, tracing how recursive bias may collapse higher-order reasoning\ndown Pearl's Ladder of Causality, culminating in what we refer to as the Rung\nof Illusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern foundational models increasingly reflect not just world knowledge, but\npatterns of human preference embedded in their training data. We hypothesize\nthat recursive alignment-via human feedback and model-generated corpora-induces\na social desirability bias, nudging models to favor agreeable or flattering\nresponses over objective reasoning. We refer to it as the Narcissus Hypothesis\nand test it across 31 models using standardized personality assessments and a\nnovel Social Desirability Bias score. Results reveal a significant drift toward\nsocially conforming traits, with profound implications for corpus integrity and\nthe reliability of downstream inferences. We then offer a novel epistemological\ninterpretation, tracing how recursive bias may collapse higher-order reasoning\ndown Pearl's Ladder of Causality, culminating in what we refer to as the Rung\nof Illusion."
                },
                "authors": [
                    {
                        "name": "Riccardo Cadei"
                    },
                    {
                        "name": "Christian Internò"
                    }
                ],
                "author_detail": {
                    "name": "Christian Internò"
                },
                "author": "Christian Internò",
                "arxiv_comment": "NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle:\n  Benchmarks, Emergent Abilities, and Scaling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17999v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17999v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18501v1",
                "updated": "2025-10-21T10:38:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    38,
                    44,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T10:38:44Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    38,
                    44,
                    1,
                    294,
                    0
                ],
                "title": "Microsecond Federated SVD on Grassmann Manifold for Real-time IoT\n  Intrusion Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microsecond Federated SVD on Grassmann Manifold for Real-time IoT\n  Intrusion Detection"
                },
                "summary": "This paper introduces FedSVD, a novel unsupervised federated learning\nframework for real-time anomaly detection in IoT networks. By leveraging\nSingular Value Decomposition (SVD) and optimization on the Grassmann manifolds,\nFedSVD enables accurate detection of both known and unknown intrusions without\nrelying on labeled data or centralized data sharing. Tailored for deployment on\nlow-power devices like the NVIDIA Jetson AGX Orin, the proposed method\nsignificantly reduces communication overhead and computational cost.\nExperimental results show that FedSVD achieves performance comparable to deep\nlearning baselines while reducing inference latency by over 10x, making it\nsuitable for latency-sensitive IoT applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces FedSVD, a novel unsupervised federated learning\nframework for real-time anomaly detection in IoT networks. By leveraging\nSingular Value Decomposition (SVD) and optimization on the Grassmann manifolds,\nFedSVD enables accurate detection of both known and unknown intrusions without\nrelying on labeled data or centralized data sharing. Tailored for deployment on\nlow-power devices like the NVIDIA Jetson AGX Orin, the proposed method\nsignificantly reduces communication overhead and computational cost.\nExperimental results show that FedSVD achieves performance comparable to deep\nlearning baselines while reducing inference latency by over 10x, making it\nsuitable for latency-sensitive IoT applications."
                },
                "authors": [
                    {
                        "name": "Tung-Anh Nguyen"
                    },
                    {
                        "name": "Van-Phuc Bui"
                    },
                    {
                        "name": "Shashi Raj Pandey"
                    },
                    {
                        "name": "Kim Hue Ta"
                    },
                    {
                        "name": "Nguyen H. Tran"
                    },
                    {
                        "name": "Petar Popovski"
                    }
                ],
                "author_detail": {
                    "name": "Petar Popovski"
                },
                "author": "Petar Popovski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18493v1",
                "updated": "2025-10-21T10:30:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    30,
                    36,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T10:30:36Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    30,
                    36,
                    1,
                    294,
                    0
                ],
                "title": "One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for\n  Customizable Privacy-Preserving Phone Scam Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for\n  Customizable Privacy-Preserving Phone Scam Detection"
                },
                "summary": "Phone scams remain a pervasive threat to both personal safety and financial\nsecurity worldwide. Recent advances in large language models (LLMs) have\ndemonstrated strong potential in detecting fraudulent behavior by analyzing\ntranscribed phone conversations. However, these capabilities introduce notable\nprivacy risks, as such conversations frequently contain sensitive personal\ninformation that may be exposed to third-party service providers during\nprocessing. In this work, we explore how to harness LLMs for phone scam\ndetection while preserving user privacy. We propose MASK (Modular Adaptive\nSanitization Kit), a trainable and extensible framework that enables dynamic\nprivacy adjustment based on individual preferences. MASK provides a pluggable\narchitecture that accommodates diverse sanitization methods - from traditional\nkeyword-based techniques for high-privacy users to sophisticated neural\napproaches for those prioritizing accuracy. We also discuss potential modeling\napproaches and loss function designs for future development, enabling the\ncreation of truly personalized, privacy-aware LLM-based detection systems that\nbalance user trust and detection effectiveness, even beyond phone scam context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phone scams remain a pervasive threat to both personal safety and financial\nsecurity worldwide. Recent advances in large language models (LLMs) have\ndemonstrated strong potential in detecting fraudulent behavior by analyzing\ntranscribed phone conversations. However, these capabilities introduce notable\nprivacy risks, as such conversations frequently contain sensitive personal\ninformation that may be exposed to third-party service providers during\nprocessing. In this work, we explore how to harness LLMs for phone scam\ndetection while preserving user privacy. We propose MASK (Modular Adaptive\nSanitization Kit), a trainable and extensible framework that enables dynamic\nprivacy adjustment based on individual preferences. MASK provides a pluggable\narchitecture that accommodates diverse sanitization methods - from traditional\nkeyword-based techniques for high-privacy users to sophisticated neural\napproaches for those prioritizing accuracy. We also discuss potential modeling\napproaches and loss function designs for future development, enabling the\ncreation of truly personalized, privacy-aware LLM-based detection systems that\nbalance user trust and detection effectiveness, even beyond phone scam context."
                },
                "authors": [
                    {
                        "name": "Kangzhong Wang"
                    },
                    {
                        "name": "Zitong Shen"
                    },
                    {
                        "name": "Youqian Zhang"
                    },
                    {
                        "name": "Michael MK Cheung"
                    },
                    {
                        "name": "Xiapu Luo"
                    },
                    {
                        "name": "Grace Ngai"
                    },
                    {
                        "name": "Eugene Yujun Fu"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Yujun Fu"
                },
                "author": "Eugene Yujun Fu",
                "arxiv_doi": "10.1145/3746027.3758164",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3758164",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.18493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]